[{"body":"If you are upgrading the Dell CSI Operator from v1.1.0 or v1.2.0 to v1.3.0, then follow the instructions below. If you are trying to upgrade the Operator from an older version, please refer the instructions here\nUsing Installation Script Run the following command to upgrade the operator from v1.2.0 release\n$ bash scripts/install.sh --upgrade Using OLM The upgrade of the Dell CSI Operator is done via Operator Lifecycle Manager. If the InstallPlan for the Operator subscription is set to Automatic, the operator will be automatically upgraded to the new version. If the InstallPlan is set to Manual, then a Cluster Administrator would need to approve the upgrade.\nUpgrade Operator from version older than v1.1.0 to v1.3.0  Uninstall the old version of the Operator If required, upgrade your cluster to a supported version Follow the installation instructions to install the v1.3.0 of the Operator here  ","excerpt":"If you are upgrading the Dell CSI Operator from v1.1.0 or v1.2.0 to v1.3.0, then follow the …","ref":"/ansible-docs/v1/upgradation/drivers/operator/","title":"Dell CSI Operator"},{"body":"What is a container : https://www.docker.com/resources/what-container\nWhat is container orchestration : https://www.infoworld.com/article/3268073/what-is-kubernetes-container-orchestration-explained.html\nKubernetes (k8s) : https://kubernetes.io/\nDocker : https://www.docker.com/\nUnderstanding CSI : https://medium.com/google-cloud/understanding-the-container-storage-interface-csi-ddbeb966a3b\nHow to write a CSI plugin: https://arslan.io/2018/06/21/how-to-write-a-container-storage-interface-csi-plugin/\n","excerpt":"What is a container : https://www.docker.com/resources/what-container\nWhat is container …","ref":"/ansible-docs/v1/grasp/start/","title":"Getting Started"},{"body":"What is a container : https://www.docker.com/resources/what-container\nWhat is container orchestration : https://www.infoworld.com/article/3268073/what-is-kubernetes-container-orchestration-explained.html\nKubernetes (k8s) : https://kubernetes.io/\nDocker : https://www.docker.com/\nUnderstanding CSI : https://medium.com/google-cloud/understanding-the-container-storage-interface-csi-ddbeb966a3b\nHow to write a CSI plugin: https://arslan.io/2018/06/21/how-to-write-a-container-storage-interface-csi-plugin/\n","excerpt":"What is a container : https://www.docker.com/resources/what-container\nWhat is container …","ref":"/ansible-docs/v2/grasp/start/","title":"Getting Started"},{"body":"","excerpt":"","ref":"/ansible-docs/docs/network/introduction/","title":"Introduction"},{"body":"","excerpt":"","ref":"/ansible-docs/docs/server/introduction/","title":"Introduction"},{"body":"Automating Dell EMC Storage with the Ansible Automation Platform The Dell EMC storage portfolio provides deep integration with Ansible through a number of modules to enable configuration management and workflow automation for a number of use cases including SAN zone setup, provisioning, snapshots, remote replication as well as data and workload mobility across the hybrid cloud.\nSolution Benefits Ansible modules for Dell EMC storage platforms: PowerMax, PowerStore, PowerScale, PowerFlex, Unity XT and VPLEX help customers improve efficiency, flexibility, and agility to run their datacenter operations.Gone are the days when automation meant specialized programming skills like Perl, Python or hard to use shell scripting.Instead, Ansible modules for Dell EMC storage do the heavy lifting of coding REST API interactions with storage platforms and expose the storage functionality to the Ansible engine.IT users simply need to specify the storage management actions and the desired configuration in a simple key-value pair format in a YAML file.Dell EMC storage Ansible plugins verify if the desired state has been reached and execute the actions only if the desired state is not reached.This underlying execution intelligence (idempotency) avoids inadvertent execution of actions and any resulting drift in the infrastructure configuration.This approach helps makes it easier to attain consistent and highly scalable operations over the entire IT infrastructure stack for a variety of use cases like DevOps process automation and managing large scale application deployments.\nGetting Started: Looking to learn more about the Dell Certified Content Collection? Here are some great resources to get you started:\n Dell EMC storage automation YouTube Dell EMC storage PowerMax workflows automated by Ansible Dell EMC storage solution brief Dell EMC storage best practices guide  Get Started Explore the Community Ansible Collection for Dell EMC\nDownload the Supported Ansible Collection for Dell EMC from Automation Hub (Ansible subscription required)\nContact us for an infrastructure automation demo.\nDell EMC DevOps and Automation resources\nDell EMC Storage Automation and Developer\n","excerpt":"Automating Dell EMC Storage with the Ansible Automation Platform The Dell EMC storage portfolio …","ref":"/ansible-docs/docs/storage/introduction/","title":"Introduction"},{"body":"The CSI Drivers by Dell EMC implement an interface between CSI (CSI spec v1.2) enabled Container Orchestrator (CO) and Dell EMC Storage Arrays. It is a plug-in that is installed into Kubernetes to provide persistent storage using Dell storage system.\nThe following are the drivers provided for the Dell storage family:\n   Release/Drivers PowerScale/Isilon Unity PowerStore PowerFlex/VxFlex OS PowerMax     Current v1.5 v1.5 v1.3 v1.4 v1.6   Previous v1.4 v1.4 v1.2 v1.3 v1.5   Older v1.3 v1.3 v1.1 v1.2 v1.4    NOTE: This doc version is no longer supported by us. You can check our latest version\nArchitecture Features and capabilities Supported Platforms    PowerMax PowerFlex/VxFlex OS Unity PowerScale/Isilon PowerStore     Storage Array 5978.479.479, 5978.669.669, Unisphere 9.1, 9.2 3.5.x OE 5.0.2, 5.0.3, 5.0.4, 5.0.5, 5.0.6 OneFS 8.1, 8.2, 9.0, 9.1 1.0.x   Kubernetes 1.18, 1.19, 1.20 1.18, 1.19, 1.20 1.18, 1.19, 1.20 1.18, 1.19, 1.20 1.18, 1.19, 1.20   RHEL 7.8, 7.9, 8.3 7.8, 7.9, 8.3 7.8, 7.9, 8.3 7.8, 7.9, 8.3 7.8, 7.9, 8.3   Ubuntu 20.04 20.04 20.04 20.04 20.04   CentOS 7.8, 7.9 7.8, 7.9 7.8, 7.9 7.8, 7.9 7.8, 7.9   SLES no 15SP2 15SP2 no no   Fedora Core OS no 5.10 no no no   OpenShift 4.6, 4.7 4.6, 4.7 4.6, 4.7 4.6, 4.7 4.6, 4.7   Docker EE 3.1 3.1 3.1 3.1 3.1   Google Anthos 1.5 no no no 1.5    CSI Driver Capabilities   Features PowerMax PowerFlex/VxFlexOS Unity PowerScale/Isilon PowerStore     Static Provisioning yes yes yes yes yes   Dynamic Provisioning yes yes yes yes yes   Expand Persistent Volume yes yes yes yes yes   Create VolumeSnapshot yes yes yes yes yes   Create Volume from Snapshot yes yes yes yes yes   Delete Snapshot yes yes yes yes yes   Access Mode RWO(FC/iSCSI)\nRWO/RWX/ROX(Raw block) RWO\nRWO/RWX/ROX(Raw block) RWO(FC/iSCSI)\nRWO/RWX(RawBlock)\nRWO/RWX/ROX(NFS) RWO/RWX/ROX RWO(FC/iSCSI)\nRWO/RWX/ROX(RawBlock, NFS)   CSI Volume Cloning yes yes yes yes yes   CSI Raw Block Volume yes yes yes no yes   CSI Ephemeral Volume no yes yes yes yes   Topology yes yes yes yes yes   Multi-array yes (via Unisphere) yes yes yes yes    Backend Storage Details   Features PowerMax VxFlexOS/PowerFlex Unity Isilon/PowerScale PowerStore     Fibre Channel yes N/A yes N/A yes   iSCSI yes N/A yes N/A yes   NFS N/A N/A yes yes yes   Other N/A ScaleIO protocol N/A N/A N/A   Supported FS ext4 / xfs ext4 / xfs ext3 / ext4 / xfs / NFS NFS ext3 / ext4 / xfs / NFS   Thin / Thick provisioning yes yes yes N/A yes   Platform-specific configurable settings Service Level selection\niSCSI CHAP - Host IO Limit\nTiering Policy\nNFS Host IO size\nSnapshot Retention duration Access Zone\nNFS version (3 or 4);Configurable Export IPs iSCSI CHAP    ","excerpt":"The CSI Drivers by Dell EMC implement an interface between CSI (CSI spec v1.2) enabled Container …","ref":"/ansible-docs/v1/dell-csi-driver/","title":"Introduction"},{"body":"The CSI Drivers by Dell EMC implement an interface between CSI enabled Container Orchestrator (CO) and Dell EMC Storage Arrays. It is a plug-in that is installed into Kubernetes to provide persistent storage using Dell storage system.\nThe following are the drivers provided for the Dell storage family:\n   Release PowerScale/Isilon Unity PowerStore PowerFlex/VxFlex OS PowerMax     Current version v1.4 v1.4 v1.2 v1.3 v1.5   Older Versions v1.3 v1.3 v1.1 v1.2 v1.4    NOTE: This doc version is no longer supported by us. You can check our latest version\nArchitecture Features and capabilities Supported Platforms   Features PowerMax PowerFlex/VxFlex OS Unity PowerScale/Isilon PowerStore     Storage Array 5978.479.479, 5978.669.669 3.0.x, 3.5.x 5.0.0, 5.0.1, 5.0.2, 5.0.3 OneFS 8.1, 8.2, 9.0, 9.1 1.0.x   Kubernetes 1.17, 1.18, 1.19 1.17, 1.18, 1.19 1.17, 1.18, 1.19 1.17, 1.18, 1.19 1.17, 1.18, 1.19   RHEL 7.7, 7.8, 7.9 7.7, 7.8, 7.9 7.7, 7.8,7.9 7.7, 7.8,7.9 7.7, 7.8, 7.9   Ubuntu 20.04 20.04 20.04 20.04 20.04   CentOS 7.6, 7.7, 7.8 7.6, 7.7, 7.8 7.6, 7.7, 7.8 7.6, 7.7, 7.8 7.6, 7.7, 7.8   SLES no 15SP2 no no no   OpenShift 4.5, 4.6 4.5, 4.6 4.5, 4.6 4.5, 4.6 4.5, 4.6   Docker EE 3.1 3.1 3.1 3.1 3.1   Google Anthos 1.5 no no no 1.5    CSI Driver Capabilities   Features PowerMax PowerFlex/VxFlexOS Unity PowerScale/Isilon PowerStore     Static Provisioning yes yes yes yes yes   Dynamic Provisioning yes yes yes yes yes   Expand Persistent Volume yes yes yes yes yes   Create VolumeSnapshot yes yes yes yes yes   Create Volume from Snapshot yes yes yes yes yes   Delete Snapshot yes yes yes yes yes   Access Mode RWO/RWX/ROX RWO RWO(FC/iSCSI)\nRWO/RWX/ROX(NFS)\nRWO/RWX/ROX(Raw block FC and iSCSI) RWO/RWX/ROX RWO(FC/iSCSI)\nRWO/RWX/ROX(RawBlock, NFS)   CSI Volume Cloning yes yes yes yes yes   CSI Raw Block Volume yes yes yes no yes   CSI Ephemeral Volume no no yes yes yes   Topology yes yes yes yes yes   Multi-array yes (via Unisphere) no yes (with single driver) no no    Backend Storage Details   Features PowerMax VxFlexOS/PowerFlex Unity Isilon/PowerScale PowerStore     Fibre Channel yes N/A yes N/A yes   iSCSI yes N/A yes N/A yes   NFS N/A N/A yes yes yes   Other N/A ScaleIO protocol N/A N/A N/A   Supported FS ext4 / xfs ext4 / xfs ext3 / ext4 / xfs / NFS NFS ext3 / ext4 / xfs / NFS   Thin / Thick provisioning yes yes yes N/A yes   Platform-specific configurable settings Service Level selection\niSCSI CHAP - Host IO Limit\nTiering\nNFS host IO size\nSnapshot retention duration Access Zone\nNFS version (3 or 4) iSCSI CHAP    ","excerpt":"The CSI Drivers by Dell EMC implement an interface between CSI enabled Container Orchestrator (CO) …","ref":"/ansible-docs/v2/dell-csi-driver/","title":"Introduction"},{"body":"","excerpt":"","ref":"/ansible-docs/docs/network/licensing/","title":"Licensing"},{"body":"","excerpt":"","ref":"/ansible-docs/docs/server/licensing/","title":"Licensing"},{"body":"Ansible collection is released and licensed under the GPL-3.0 license. Ansible modules and module utilities that are part of the Ansible collection are released and licensed under the Apache 2.0 license.\n","excerpt":"Ansible collection is released and licensed under the GPL-3.0 license. Ansible modules and module …","ref":"/ansible-docs/docs/storage/licensing/","title":"Licensing"},{"body":"","excerpt":"","ref":"/ansible-docs/docs/server/platforms/","title":"Platforms"},{"body":"","excerpt":"","ref":"/ansible-docs/docs/storage/platforms/","title":"Platforms"},{"body":"","excerpt":"","ref":"/ansible-docs/docs/server/platforms/powerflex/","title":"PowerFlex"},{"body":"","excerpt":"","ref":"/ansible-docs/docs/storage/platforms/powerflex/","title":"PowerFlex"},{"body":"You can upgrade the CSI Driver for Dell EMC PowerFlex using Helm or Dell CSI Operator.\nUpdate Driver from v1.2/v1.3 to v1.4 using Helm Steps\n Run git clone https://github.com/dell/csi-powerflex.git to clone the git repository and get the v1.4 driver. You need to create config.json with configuration of your system. Check this section in installation documentation: Install the Driver You must set the only system managed in v1.3/v1.2 driver as default in config.json in v1.4 so that the driver know the existing volumes belong to that system. Update values file as needed. Run the csi-install script with the option --upgrade by running: cd ../dell-csi-helm-installer \u0026amp;\u0026amp; ./csi-install.sh --namespace vxflexos --values ./myvalues.yaml --upgrade.  Update Driver from pre-v1.2 to v1.4 using Helm A direct upgrade of the driver from an older version pre-v1.2 to version 1.4 is not supported because of breaking changes in Kubernetes APIs in the migration from alpha snapshots to beta snapshots. In order to update the driver in this situation you need to remove alpha snapshot related artifacts.\nSteps\n Before deleting the alpha snapshot CRDs, ensure that their version is v1alpha1 by examining the output of the kubectl get crd command. Delete any VolumeSnapshotClass present in the cluster. Delete all the alpha snapshot CRDs from the cluster by running the following commands: kubectl delete crd volumesnapshotclasses.snapshot.storage.k8s.io kubectl delete crd volumesnapshotcontents.snapshot.storage.k8s.io kubectl delete crd volumesnapshots.snapshot.storage.k8s.io  Uninstall the driver using the csi-uninstall.sh script by running the command: ./csi-uninstall.sh --namespace vxflexos. Install the driver using the steps described in the Installation Using Helm section for the CSI PowerFlex driver.  NOTE:\n If you are upgrading from a driver version which was installed using Helm v2, ensure that you install Helm3 before installing the driver. Installation of the CSI Driver for Dell EMC PowerFlex version 1.4 driver is not supported on Kubernetes upstream clusters running version 1.17. You must upgrade your cluster to 1.18, 1.19, or 1.20 before attempting to install the new version of the driver. To update any installation parameter after the driver has been installed, change the myvalues.yaml file and run the install script with the option --upgrade, for example: ./csi-install.sh --namespace vxflexos --values ./myvalues.yaml --upgrade.  Upgrade using Dell CSI Operator: Follow the instructions for upgrade on the Dell CSI Operator GitHub page.\n","excerpt":"You can upgrade the CSI Driver for Dell EMC PowerFlex using Helm or Dell CSI Operator.\nUpdate Driver …","ref":"/ansible-docs/v1/upgradation/drivers/powerflex/","title":"PowerFlex"},{"body":"You can upgrade the CSI Driver for Dell EMC PowerFlex using Helm or Dell CSI Operator.\nUpdate Driver from v1.2 to v1.3 using Helm Steps\n Run git clone https://github.com/dell/csi-powerflex.git to clone the git repository and get the v1.3 driver. Update values file as needed. Run the csi-install script with the option --upgrade by running: cd ../dell-csi-helm-installer \u0026amp;\u0026amp; ./csi-install.sh --namespace vxflexos --values ./myvalues.yaml --upgrade.  Update Driver from pre-v1.2 to v1.3 using Helm A direct upgrade of the driver from an older version pre-v1.2 to version 1.3 is not supported because of breaking changes in Kubernetes APIs in the migration from alpha snapshots to beta snapshots. In order to update the driver in this situation you need to remove alpha snapshot related artifacts.\nSteps\n Before deleting the alpha snapshot CRDs, ensure that their version is v1alpha1 by examining the output of the kubectl get crd command. Delete any VolumeSnapshotClass present in the cluster. Delete all the alpha snapshot CRDs from the cluster by running the following commands: kubectl delete crd volumesnapshotclasses.snapshot.storage.k8s.io kubectl delete crd volumesnapshotcontents.snapshot.storage.k8s.io kubectl delete crd volumesnapshots.snapshot.storage.k8s.io  Uninstall the driver using the csi-uninstall.sh script by running the command: ./csi-uninstall.sh --namespace vxflexos. Install the driver using the steps described in the Installation Using Helm section for the CSI PowerFlex driver.  NOTE:\n If you are upgrading from a driver version which was installed using Helm v2, ensure that you install Helm3 before installing the driver. Installation of the CSI Driver for Dell EMC PowerFlex version 1.3 driver is not supported on Kubernetes upstream clusters running version 1.16. You must upgrade your cluster to 1.17, 1.18, or 1.19 before attempting to install the new version of the driver. To update any installation parameter after the driver has been installed, change the myvalues.yaml file and run the install script with the option --upgrade, for example: ./csi-install.sh --namespace vxflexos --values ./myvalues.yaml --upgrade.  Upgrade using Dell CSI Operator: Follow the instructions for upgrade on the Dell CSI Operator GitHub page.\n","excerpt":"You can upgrade the CSI Driver for Dell EMC PowerFlex using Helm or Dell CSI Operator.\nUpdate Driver …","ref":"/ansible-docs/v2/upgradation/drivers/powerflex/","title":"PowerFlex"},{"body":"Ansible Modules for Dell EMC PowerFlex Product Guide 1.0  © 2021 Dell Inc. or its subsidiaries. All rights reserved. Dell EMC, and other trademarks are trademarks of Dell Inc. or its subsidiaries. Other trademarks may be trademarks of their respective owners.\n Contents   Common access parameters\n  Gatherfacts module\n Synopsis Parameters Examples Return Values Authors    SDC module\n Synopsis Parameters Examples Return Values Authors    Volume module\n Synopsis Parameters Examples Return Values Authors    Snapshot module\n Synopsis Parameters Examples Return Values Authors    Storage pool module\n Synopsis Parameters Examples Return Values Authors    Common access parameters These parameters are applicable to all modules, along with module-specific parameters.\nNOTE: If the parameter is mandatory, then required=True else it is an optional parameter. This is applicable to all the module specific parameters also.\n Parameter Choices/Defaults Comments    gateway_host  type=string, required=True      IP or FQDN of the PowerFlex gateway host.     port  type=integer    Default:443   Port number through which communication happens with PowerFlex gateway host.     username  type=string , required=True      The username of the PowerFlex gateway host.     password  type=string , required=True      The password of the PowerFlex gateway host.     verifycert  type=boolean    Choices: no yes\u0026nbsp;\u0026larr;    Boolean variable to specify whether or not to validate SSL certificate. True - Indicates that the SSL certificate should be verified. False - Indicates that the SSL certificate should not be verified.     Gather Facts Module Synopsis  Gathering information about Dell EMC PowerFlex storage system includes Get the API details of a PowerFlex array, Get list of volumes in PowerFlex array, Get list of SDSs in a PowerFlex array, Get list of SDCs in a PowerFlex array, Get list of storage pools in PowerFlex array, Get list of protection domains in a PowerFlex array, Get list of snapshot policies in a PowerFlex array.  Parameters  Parameter Choices/Defaults Comments    filters  type=list , elements=dictionary      List of filters to support filtered output for storage entities. Each filter is a list of filter_key, filter_operator, filter_value. Supports passing of multiple filters.      filter_key  type=string , required=True      Name identifier of the filter.      filter_operator  type=string , required=True    Choices: equal    Operation to be performed on filter key.      filter_value  type=string , required=True      Value of the filter key.     gather_subset  type=list , elements=string    Choices: vol storage_pool protection_domain sdc sds snapshot_policy    List of string variables to specify the Powerflex storage system entities for which information is required. vol storage_pool protection_domain sdc sds snapshot_policy     Examples - name: Get detailed list of PowerFlex entities. dellemc_powerflex_gatherfacts: gateway_host: \u0026quot;{{gateway_host}}\u0026quot; username: \u0026quot;{{username}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; gather_subset: - vol - storage_pool - protection_domain - sdc - sds - snapshot_policy - name: Get a subset list of PowerFlex volumes. dellemc_powerflex_gatherfacts: gateway_host: \u0026quot;{{gateway_host}}\u0026quot; username: \u0026quot;{{username}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; gather_subset: - vol filters: - filter_key: \u0026quot;name\u0026quot; filter_operator: \u0026quot;equal\u0026quot; filter_value: \u0026quot;ansible_test\u0026quot; Return Values  Key Returned Description    API_Version  type=string   always  API version of PowerFlex API Gateway.      Array_Details  type=list , elements=string   always  System entities of PowerFlex storage array.     \u0026nbsp;  id  type=string   success  The ID of the system     \u0026nbsp;  installId  type=string   success  installation Id     \u0026nbsp;  mdmSecurityPolicy  type=string   success  mdm security policy     \u0026nbsp;  systemVersionName  type=string   success  system version and name      changed  type=boolean   always  Whether or not the resource has changed      Protection_Domains  complex   always  Details of all protection domains     \u0026nbsp;  id  type=string   success  protection domain id     \u0026nbsp;  name  type=string   success  protection domain name      SDCs  complex   always  Details of storage data clients     \u0026nbsp;  id  type=string   success  storage data client id     \u0026nbsp;  name  type=string   success  storage data client name      SDSs  complex   always  Details of storage data servers     \u0026nbsp;  id  type=string   success  storage data server id     \u0026nbsp;  name  type=string   success  storage data server name      Snapshot_Policies  complex   always  Details of snapshot policies     \u0026nbsp;  id  type=string   success  snapshot policy id     \u0026nbsp;  name  type=string   success  snapshot policy name      Storage_Pools  complex   always  Details of storage pools     \u0026nbsp;  id  type=string   success  storage pool id     \u0026nbsp;  name  type=string   success  storage pool name      Volumes  complex   always  Details of volumes     \u0026nbsp;  id  type=string   success  volume id     \u0026nbsp;  name  type=string   success  volume name      Authors  Arindam Datta (@dattaarindam) \u0026lt;ansible.team@dell.com\u0026gt;  SDC Module Synopsis  Managing SDC\u0026rsquo;s on PowerFlex storage system includes getting details of SDC and renaming SDC.  Parameters  Parameter Choices/Defaults Comments    sdc_id  type=string      ID of the SDC. Specify either sdc_name, sdc_id or sdc_ip for get/rename operation. Mutually exclusive with sdc_name and sdc_ip.     sdc_ip  type=string      IP of the SDC. Specify either sdc_name, sdc_id or sdc_ip for get/rename operation. Mutually exclusive with sdc_id and sdc_name.     sdc_name  type=string      Name of the SDC. Specify either sdc_name, sdc_id or sdc_ip for get/rename operation. Mutually exclusive with sdc_id and sdc_ip.     sdc_new_name  type=string      New name of the SDC. Used to rename the SDC.     state  type=string , required=True    Choices: present absent    State of the storage pool.     Examples - name: Get SDC details using SDC ip dellemc_powerflex_sdc: gateway_host: \u0026quot;{{gateway_host}}\u0026quot; username: \u0026quot;{{username}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; sdc_ip: \u0026quot;{{sdc_ip}}\u0026quot; state: \u0026quot;present\u0026quot; - name: Rename SDC using SDC name dellemc_powerflex_sdc: gateway_host: \u0026quot;{{gateway_host}}\u0026quot; username: \u0026quot;{{username}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; sdc_name: \u0026quot;centos_sdc\u0026quot; sdc_new_name: \u0026quot;centos_sdc_renamed\u0026quot; state: \u0026quot;present\u0026quot; Return Values  Key Returned Description    changed  type=boolean   always  Whether or not the resource has changed      sdc_details  complex   When SDC exists  Details of the SDC     \u0026nbsp;  id  type=string   success  The ID of the SDC     \u0026nbsp;  mapped_volumes  type=list , elements=string   success  The details of the mapped volumes     \u0026nbsp; \u0026nbsp;  id  type=string   success  The ID of the volume     \u0026nbsp; \u0026nbsp;  name  type=string   success  The name of the volume     \u0026nbsp; \u0026nbsp;  volumeType  type=string   success  Type of the volume     \u0026nbsp;  name  type=string   success  Name of the SDC     \u0026nbsp;  osType  type=string   success  OS type of the SDC     \u0026nbsp;  sdcApproved  type=boolean   success  Indicates whether an SDC has approved access to the system     \u0026nbsp;  sdcIp  type=string   success  IP of the SDC      Authors  Akash Shendge (@shenda1) \u0026lt;ansible.team@dell.com\u0026gt;  Volume Module Synopsis  Managing volumes on PowerFlex storage system includes creating new volume, getting details of volume, adding/removing snapshot policy to/from volume, mapping/unmapping volume to/from SDC, listing snapshots associated with a volume, modifying attributes of volume and deleting volume.  Parameters  Parameter Choices/Defaults Comments    allow_multiple_mappings  type=boolean    Choices: no yes    Specifies whether to allow multiple mappings or not. If the volume is mapped to one SDC then for every new mapping allow_multiple_mappings has to be passed as True.     auto_snap_remove_type  type=string    Choices: remove detach    Whether to remove or detach the snapshot policy. To remove/detach snapshot policy, empty snapshot_policy_id/snapshot_policy_name is to be passed along with auto_snap_remove_type. If the snapshot policy name/id is passed empty then auto_snap_remove_type is defaulted to \u0026#x27;detach\u0026#x27;.     cap_unit  type=string    Choices: GB TB    The unit of the volume size. It defaults to \u0026#x27;GB\u0026#x27;.     compression_type  type=string    Choices: NORMAL NONE    Type of the compression method.     delete_snapshots  type=boolean    Choices: no yes    If True, the volume and all its dependent snapshots will be deleted. If False, only the volume will be deleted. delete_snapshots parameter can be specified only when the state is absent. delete_snapshots defaults to False.     protection_domain_id  type=string      The id of the protection domain. While creation of a volume, if more than one storage pool exists with the same name then either protection domain name or id must be mentioned along with it. protection_domain_name and protection_domain_id are mutually exclusive parameters.     protection_domain_name  type=string      The name of the protection domain. While creation of a volume, if more than one storage pool exists with the same name then either protection domain name or id must be mentioned along with it. protection_domain_name and protection_domain_name are mutually exclusive parameters.     sdc  type=list , elements=dictionary      Specifies SDC parameters      access_mode  type=string    Choices: READ_WRITE READ_ONLY NO_ACCESS    Define the access mode for all mappings of the volume.      bandwidth_limit  type=integer      Limit of volume network bandwidth. Need to mention in multiple of 1024 Kbps. To set no limit, 0 is to be passed.      iops_limit  type=integer      Limit of volume IOPS. Minimum IOPS limit is 11 and specify 0 for unlimited iops.      sdc_id  type=string      ID of the SDC. Specify either sdc_name, sdc_id or sdc_ip. Mutually exclusive with sdc_name and sdc_ip.      sdc_ip  type=string      IP of the SDC. Specify either sdc_name, sdc_id or sdc_ip. Mutually exclusive with sdc_id and sdc_ip.      sdc_name  type=string      Name of the SDC. Specify either sdc_name, sdc_id or sdc_ip. Mutually exclusive with sdc_id and sdc_ip.     sdc_state  type=string    Choices: mapped unmapped    Mapping state of the SDC.     size  type=integer      The size of the volume. Size of the volume will be assigned as higher multiple of 8 GB.     snapshot_policy_id  type=string      ID of the snapshot policy. To remove/detach snapshot policy, empty snapshot_policy_id/snapshot_policy_name is to be passed along with auto_snap_remove_type.     snapshot_policy_name  type=string      Name of the snapshot policy. To remove/detach snapshot policy, empty snapshot_policy_id/snapshot_policy_name is to be passed along with auto_snap_remove_type.     state  type=string , required=True    Choices: present absent    State of the volume.     storage_pool_id  type=string      The id of the storage pool. Either name or the id of the storage pool is required for creating a volume. storage_pool_name and storage_pool_id are mutually exclusive parameters.     storage_pool_name  type=string      The name of the storage pool. Either name or the id of the storage pool is required for creating a volume. During creation, If storage pool name is provided then either protection domain name or id must be mentioned along with it. storage_pool_name and storage_pool_id are mutually exclusive parameters.     use_rmcache  type=boolean    Choices: no yes    Whether to use RM Cache or not.     vol_id  type=string      The ID of the volume. Except create operation, all other operations can be performed using vol_id. vol_name and vol_id are mutually exclusive parameters.     vol_name  type=string      The name of the volume. Mandatory for create operation. vol_name is unique across the PowerFlex array. vol_name and vol_id are mutually exclusive parameters.     vol_new_name  type=string      New name of the volume. Used to rename the volume.     vol_type  type=string    Choices: THICK_PROVISIONED THIN_PROVISIONED    The type of volume provisioning.     Examples - name: Create a volume dellemc_powerflex_volume: gateway_host: \u0026quot;{{gateway_host}}\u0026quot; username: \u0026quot;{{username}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; port: \u0026quot;{{port}}\u0026quot; vol_name: \u0026quot;sample_volume\u0026quot; storage_pool_name: \u0026quot;pool_1\u0026quot; protection_domain_name: \u0026quot;pd_1\u0026quot; vol_type: \u0026quot;THICK_PROVISIONED\u0026quot; compression_type: \u0026quot;NORMAL\u0026quot; use_rmcache: True size: 16 state: \u0026quot;present\u0026quot; - name: Map a SDC to volume dellemc_powerflex_volume: gateway_host: \u0026quot;{{gateway_host}}\u0026quot; username: \u0026quot;{{username}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; port: \u0026quot;{{port}}\u0026quot; vol_name: \u0026quot;sample_volume\u0026quot; allow_multiple_mappings: True sdc: - sdc_id: \u0026quot;92A304DB-EFD7-44DF-A07E-D78134CC9764\u0026quot; access_mode: \u0026quot;READ_WRITE\u0026quot; sdc_state: \u0026quot;mapped\u0026quot; state: \u0026quot;present\u0026quot; - name: Unmap a SDC to volume dellemc_powerflex_volume: gateway_host: \u0026quot;{{gateway_host}}\u0026quot; username: \u0026quot;{{username}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; port: \u0026quot;{{port}}\u0026quot; vol_name: \u0026quot;sample_volume\u0026quot; sdc: - sdc_id: \u0026quot;92A304DB-EFD7-44DF-A07E-D78134CC9764\u0026quot; sdc_state: \u0026quot;unmapped\u0026quot; state: \u0026quot;present\u0026quot; - name: Map multiple SDCs to a volume dellemc_powerflex_volume: gateway_host: \u0026quot;{{gateway_host}}\u0026quot; username: \u0026quot;{{username}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; port: \u0026quot;{{port}}\u0026quot; vol_name: \u0026quot;sample_volume\u0026quot; protection_domain_name: \u0026quot;pd_1\u0026quot; sdc: - sdc_id: \u0026quot;92A304DB-EFD7-44DF-A07E-D78134CC9764\u0026quot; access_mode: \u0026quot;READ_WRITE\u0026quot; bandwidth_limit: 2048 iops_limit: 20 - sdc_ip: \u0026quot;127.0.0.1\u0026quot; access_mode: \u0026quot;READ_ONLY\u0026quot; allow_multiple_mappings: True sdc_state: \u0026quot;mapped\u0026quot; state: \u0026quot;present\u0026quot; - name: Get the details of the volume dellemc_powerflex_volume: gateway_host: \u0026quot;{{gateway_host}}\u0026quot; username: \u0026quot;{{username}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; port: \u0026quot;{{port}}\u0026quot; vol_id: \u0026quot;fe6c8b7100000005\u0026quot; state: \u0026quot;present\u0026quot; - name: Modify the details of the Volume dellemc_powerflex_volume: gateway_host: \u0026quot;{{gateway_host}}\u0026quot; username: \u0026quot;{{username}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; port: \u0026quot;{{port}}\u0026quot; vol_name: \u0026quot;sample_volume\u0026quot; storage_pool_name: \u0026quot;pool_1\u0026quot; new_vol_name: \u0026quot;new_sample_volume\u0026quot; size: 64 state: \u0026quot;present\u0026quot; - name: Delete the Volume dellemc_powerflex_volume: gateway_host: \u0026quot;{{gateway_host}}\u0026quot; username: \u0026quot;{{username}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; port: \u0026quot;{{port}}\u0026quot; vol_name: \u0026quot;sample_volume\u0026quot; delete_snapshots: False state: \u0026quot;absent\u0026quot; - name: Delete the Volume and all its dependent snapshots dellemc_powerflex_volume: gateway_host: \u0026quot;{{gateway_host}}\u0026quot; username: \u0026quot;{{username}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; port: \u0026quot;{{port}}\u0026quot; vol_name: \u0026quot;sample_volume\u0026quot; delete_snapshots: True state: \u0026quot;absent\u0026quot; Return Values  Key Returned Description    changed  type=boolean   always  Whether or not the resource has changed      volume_details  complex   When volume exists  Details of the volume     \u0026nbsp;  id  type=string   success  The ID of the volume.     \u0026nbsp;  mappedSdcInfo  complex   success  The details of the mapped SDC     \u0026nbsp; \u0026nbsp;  accessMode  type=string   success  mapping access mode for the specified volume     \u0026nbsp; \u0026nbsp;  limitBwInMbps  type=integer   success  Bandwidth limit for the SDC     \u0026nbsp; \u0026nbsp;  limitIops  type=integer   success  IOPS limit for the SDC     \u0026nbsp; \u0026nbsp;  sdcId  type=string   success  ID of the SDC     \u0026nbsp; \u0026nbsp;  sdcIp  type=string   success  IP of the SDC     \u0026nbsp; \u0026nbsp;  sdcName  type=string   success  Name of the SDC     \u0026nbsp;  name  type=string   success  Name of the volume     \u0026nbsp;  protectionDomainId  type=string   success  ID of the protection domain in which volume resides.     \u0026nbsp;  protectionDomainName  type=string   success  Name of the protection domain in which volume resides.     \u0026nbsp;  sizeInGb  type=integer   success  Size of the volume in Gb.     \u0026nbsp;  sizeInKb  type=integer   success  Size of the volume in Kb.     \u0026nbsp;  snapshotPolicyId  type=string   success  ID of the snapshot policy associated with volume.     \u0026nbsp;  snapshotPolicyName  type=string   success  Name of the snapshot policy associated with volume.     \u0026nbsp;  snapshotsList  type=string   success  List of snapshots associated with the volume.     \u0026nbsp;  storagePoolId  type=string   success  ID of the storage pool in which volume resides.     \u0026nbsp;  storagePoolName  type=string   success  Name of the storage pool in which volume resides.      Authors  P Srinivas Rao (@srinivas-rao5) \u0026lt;ansible.team@dell.com\u0026gt;  Snapshot Module Synopsis  Managing snapshots on PowerFlex Storage System includes creating new snapshot, getting details of snapshot, mapping/unmapping snapshot to/from SDC, modifying attributes of snapshot and deleting snapshot.  Parameters  Parameter Choices/Defaults Comments    allow_multiple_mappings  type=boolean    Choices: no yes    Specifies whether to allow multiple mappings or not.     cap_unit  type=string    Choices: GB TB    The unit of the volume size. It defaults to \u0026#x27;GB\u0026#x27;, if not specified.     desired_retention  type=integer      The retention value for the Snapshot. If the desired_retention is not mentioned during creation, snapshot will be created with unlimited retention. Maximum supported desired retention is 31 days.     read_only  type=boolean    Choices: no yes    Specifies whether mapping of the created snapshot volume will have read-write access or limited to read-only access. If true, snapshot is created with read-only access. If false, snapshot is created with read-write access.     remove_mode  type=string    Choices: ONLY_ME INCLUDING_DESCENDANTS    Removal mode for the snapshot. It defaults to \u0026#x27;ONLY_ME\u0026#x27;, if not specified.     retention_unit  type=string    Choices: hours days    The unit for retention. It defaults to \u0026#x27;hours\u0026#x27;, if not specified.     sdc  type=list , elements=dictionary      Specifies SDC parameters      access_mode  type=string    Choices: READ_WRITE READ_ONLY NO_ACCESS    Define the access mode for all mappings of the snapshot.      bandwidth_limit  type=integer      Limit of snapshot network bandwidth. Need to mention in multiple of 1024 Kbps. To set no limit, 0 is to be passed.      iops_limit  type=integer      Limit of snapshot IOPS. Minimum IOPS limit is 11 and specify 0 for unlimited iops.      sdc_id  type=string      ID of the SDC. Specify either sdc_name, sdc_id or sdc_ip. Mutually exclusive with sdc_name and sdc_ip.      sdc_ip  type=string      IP of the SDC. Specify either sdc_name, sdc_id or sdc_ip. Mutually exclusive with sdc_id and sdc_ip.      sdc_name  type=string      Name of the SDC. Specify either sdc_name, sdc_id or sdc_ip. Mutually exclusive with sdc_id and sdc_ip.     sdc_state  type=string    Choices: mapped unmapped    Mapping state of the SDC.     size  type=integer      The size of the snapshot.     snapshot_id  type=string      The ID of the Snapshot.     snapshot_name  type=string      The name of the snapshot. Mandatory for create operation. Specify either snapshot name or ID (but not both) for any operation.     snapshot_new_name  type=string      New name of the snapshot. Used to rename the snapshot.     state  type=string , required=True    Choices: present absent    State of the snapshot.     vol_id  type=string      The ID of the volume.     vol_name  type=string      The name of the volume for which snapshot will be taken. Specify either vol_name or vol_id while creating snapshot.     Examples - name: Create snapshot dellemc_powerflex_snapshot: gateway_host: \u0026quot;{{gateway_host}}\u0026quot; username: \u0026quot;{{username}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; snapshot_name: \u0026quot;ansible_snapshot\u0026quot; vol_name: \u0026quot;ansible_volume\u0026quot; read_only: False desired_retention: 2 state: \u0026quot;present\u0026quot; - name: Get snapshot details using snapshot id dellemc_powerflex_snapshot: gateway_host: \u0026quot;{{gateway_host}}\u0026quot; username: \u0026quot;{{username}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; snapshot_id: \u0026quot;fe6cb28200000007\u0026quot; state: \u0026quot;present\u0026quot; - name: Map snapshot to SDC dellemc_powerflex_snapshot: gateway_host: \u0026quot;{{gateway_host}}\u0026quot; username: \u0026quot;{{username}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; snapshot_id: \u0026quot;fe6cb28200000007\u0026quot; sdc: - sdc_ip: \u0026quot;10.247.66.203\u0026quot; - sdc_id: \u0026quot;663ac0d200000001\u0026quot; allow_multiple_mappings: True sdc_state: \u0026quot;mapped\u0026quot; state: \u0026quot;present\u0026quot; - name: Modify the attributes of SDC mapped to snapshot dellemc_powerflex_snapshot: gateway_host: \u0026quot;{{gateway_host}}\u0026quot; username: \u0026quot;{{username}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; snapshot_id: \u0026quot;fe6cb28200000007\u0026quot; sdc: - sdc_ip: \u0026quot;10.247.66.203\u0026quot; iops_limit: 11 bandwidth_limit: 4096 - sdc_id: \u0026quot;663ac0d200000001\u0026quot; iops_limit: 20 bandwidth_limit: 2048 allow_multiple_mappings: True sdc_state: \u0026quot;mapped\u0026quot; state: \u0026quot;present\u0026quot; - name: Extend the size of snapshot dellemc_powerflex_snapshot: gateway_host: \u0026quot;{{gateway_host}}\u0026quot; username: \u0026quot;{{username}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; snapshot_id: \u0026quot;fe6cb28200000007\u0026quot; size: 16 state: \u0026quot;present\u0026quot; - name: Unmap SDCs from snapshot dellemc_powerflex_snapshot: gateway_host: \u0026quot;{{gateway_host}}\u0026quot; username: \u0026quot;{{username}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; snapshot_id: \u0026quot;fe6cb28200000007\u0026quot; sdc: - sdc_ip: \u0026quot;10.247.66.203\u0026quot; - sdc_id: \u0026quot;663ac0d200000001\u0026quot; sdc_state: \u0026quot;unmapped\u0026quot; state: \u0026quot;present\u0026quot; - name: Rename snapshot dellemc_powerflex_snapshot: gateway_host: \u0026quot;{{gateway_host}}\u0026quot; username: \u0026quot;{{username}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; snapshot_id: \u0026quot;fe6cb28200000007\u0026quot; snapshot_new_name: \u0026quot;ansible_renamed_snapshot_10\u0026quot; state: \u0026quot;present\u0026quot; - name: Delete snapshot dellemc_powerflex_snapshot: gateway_host: \u0026quot;{{gateway_host}}\u0026quot; username: \u0026quot;{{username}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; snapshot_id: \u0026quot;fe6cb28200000007\u0026quot; remove_mode: \u0026quot;ONLY_ME\u0026quot; state: \u0026quot;absent\u0026quot; Return Values  Key Returned Description    changed  type=boolean   always  Whether or not the resource has changed      snapshot_details  complex   When snapshot exists  Details of the snapshot     \u0026nbsp;  ancestorVolumeId  type=string   success  The of the root of the specified volume\u0026#x27;s V-Tree     \u0026nbsp;  creationTime  type=integer   success  The creation time of the snapshot     \u0026nbsp;  id  type=string   success  The ID of the snapshot     \u0026nbsp;  mappedSdcInfo  complex   success  The details of the mapped SDC     \u0026nbsp; \u0026nbsp;  accessMode  type=string   success  mapping access mode for the specified snapshot     \u0026nbsp; \u0026nbsp;  limitBwInMbps  type=integer   success  Bandwidth limit for the SDC     \u0026nbsp; \u0026nbsp;  limitIops  type=integer   success  IOPS limit for the SDC     \u0026nbsp; \u0026nbsp;  sdcId  type=string   success  ID of the SDC     \u0026nbsp; \u0026nbsp;  sdcIp  type=string   success  IP of the SDC     \u0026nbsp; \u0026nbsp;  sdcName  type=string   success  Name of the SDC     \u0026nbsp;  name  type=string   success  Name of the snapshot     \u0026nbsp;  sizeInKb  type=integer   success  Size of the snapshot     \u0026nbsp;  storagePoolId  type=string   success  Storage pool in which snapshot resides      Authors  Akash Shendge (@shenda1) \u0026lt;ansible.team@dell.com\u0026gt;  Storage Pool Module Synopsis  Dell EMC PowerFlex storage pool module includes Get the details of storage pool, Create a new storage pool, Modify the attribute of a storage pool.  Parameters  Parameter Choices/Defaults Comments    media_type  type=string    Choices: HDD SSD TRANSITIONAL    Type of devices in the storage pool.     protection_domain_id  type=string      The id of the protection domain. While creation of a pool, either protection domain name or id must be mentioned. protection_domain_name and protection_domain_id are mutually exclusive parameters.     protection_domain_name  type=string      The name of the protection domain. While creation of a pool, either protection domain name or id must be mentioned. protection_domain_name and protection_domain_name are mutually exclusive parameters.     state  type=string , required=True    Choices: present absent    State of the storage pool.     storage_pool_id  type=string      The id of the storage pool. storage_pool_id is auto generated, hence should not be provided during creation of a storage pool. storage_pool_name and storage_pool_id are mutually exclusive parameters.     storage_pool_name  type=string      The name of the storage pool. If more than one storage pool is found with the same name then protection domain id/name is required to perform the task. storage_pool_name and storage_pool_id are mutually exclusive parameters.     storage_pool_new_name  type=string      New name for the storage pool can be provided. This parameter is used for renaming the storage pool.     use_rfcache  type=boolean    Choices: no yes    Enable/Disable RFcache on a specific storage pool.     use_rmcache  type=boolean    Choices: no yes    Enable/Disable RMcache on a specific storage pool.     Notes   TRANSITIONAL media type is supported only during modification. The modules prefixed with dellemc_powerflex are built to support the Dell EMC PowerFlex storage platform.   Examples - name: Get the details of storage pool by name dellemc_powerflex_storagepool: gateway_host: \u0026quot;{{gateway_host}}\u0026quot; username: \u0026quot;{{username}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; storage_pool_name: \u0026quot;sample_pool_name\u0026quot; protection_domain_name: \u0026quot;sample_protection_domain\u0026quot; state: \u0026quot;present\u0026quot; - name: Get the details of storage pool by id dellemc_powerflex_storagepool: gateway_host: \u0026quot;{{gateway_host}}\u0026quot; username: \u0026quot;{{username}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; storage_pool_id: \u0026quot;abcd1234ab12r\u0026quot; state: \u0026quot;present\u0026quot; - name: Create a new storage pool by name dellemc_powerflex_storagepool: gateway_host: \u0026quot;{{gateway_host}}\u0026quot; username: \u0026quot;{{username}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; storage_pool_name: \u0026quot;ansible_test_pool\u0026quot; protection_domain_id: \u0026quot;1c957da800000000\u0026quot; media_type: \u0026quot;HDD\u0026quot; state: \u0026quot;present\u0026quot; - name: Modify a storage pool by name dellemc_powerflex_storagepool: gateway_host: \u0026quot;{{gateway_host}}\u0026quot; username: \u0026quot;{{username}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; storage_pool_name: \u0026quot;ansible_test_pool\u0026quot; protection_domain_id: \u0026quot;1c957da800000000\u0026quot; use_rmcache: True use_rfcache: True state: \u0026quot;present\u0026quot; - name: Rename storage pool by id dellemc_powerflex_storagepool: gateway_host: \u0026quot;{{gateway_host}}\u0026quot; username: \u0026quot;{{username}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; storage_pool_id: \u0026quot;abcd1234ab12r\u0026quot; storage_pool_new_name: \u0026quot;new_ansible_pool\u0026quot; state: \u0026quot;present\u0026quot; Return Values  Key Returned Description    changed  type=boolean   always  Whether or not the resource has changed      storage_pool_details  complex   When storage pool exists  Details of the storage pool     \u0026nbsp;  id  type=string   success  ID of the storage pool under protection domain.     \u0026nbsp;  mediaType  type=string   success  Type of devices in the storage pool     \u0026nbsp;  name  type=string   success  Name of the storage pool under protection domain.     \u0026nbsp;  protectionDomainId  type=string   success  ID of the protection domain in which pool resides.     \u0026nbsp;  protectionDomainName  type=string   success  Name of the protection domain in which pool resides.     \u0026nbsp;  useRfcache  type=boolean   success  Enable/Disable RFcache on a specific storage pool.     \u0026nbsp;  useRmcache  type=boolean   success  Enable/Disable RMcache on a specific storage pool.      Authors  Arindam Datta (@dattaarindam) \u0026lt;ansible.team@dell.com\u0026gt; P Srinivas Rao (@srinivas-rao5) \u0026lt;ansible.team@dell.com\u0026gt;  ","excerpt":"Ansible Modules for Dell EMC PowerFlex Product Guide 1.0  © 2021 Dell Inc. or its subsidiaries. All …","ref":"/ansible-docs/docs/server/platforms/powerflex/product-guide/","title":"PowerFlex Product Guide"},{"body":"Ansible Modules for Dell EMC PowerFlex Release Notes 1.0  © 2021 Dell Inc. or its subsidiaries. All rights reserved. Dell EMC, and other trademarks are trademarks of Dell Inc. or its subsidiaries. Other trademarks may be trademarks of their respective owners.\n Contents These release notes contain supplemental information about Ansible Modules for Dell EMC PowerFlex.\n Product Description New Features Known issues Limitations Distribution Documentation  Product Description The Ansible Modules for Dell EMC PowerFlex are used for managing volumes, storage pools, SDCs, and snapshots for PowerFlex storage devices. The modules use playbooks to list, show, create, delete, and modify each of the entities.\nThe Ansible Modules for Dell EMC PowerFlex supports the following features:\n Create volumes, storage pools and snapshots. Modify volumes, storage pools, SDCs and snapshots. Delete volumes and snapshots. Get details of a volumes, snapshots, SDCs and storage pool. Get entities of the PowerFlex storage device.  New Features The Ansible Modules for Dell EMC PowerFlex release 1.0 supports the following features:\n  The following are the features of the gatherfacts module:\n Get the API details of a PowerFlex storage device. Get the list of SDCs. Get the list of SDSs. Get the list of volumes. Get the list of snapshots. Get the list of storage pools. Get list of protection domains. Get list of snapshot policies.    The following are the features of the volume module:\n Get the details of a volume. Create a volume. Modify details of a volume. Delete a volume.    The following are the features of the snapshot module:\n Get the details of a snapshot. Create a snapshot. Modify details of a snapshot. Delete a snapshot.    The following are the features of the storage pools module:\n Get the details of a storage pool. Create a storage pool. Modify details of a storage pool.    The following are the features of the SDCs module:\n Get the details of the SDC. Rename a SDC.    Known issues There are no known issues.\nLimitations There are no known limitations.\nDistribution The software package is available for download from the Ansible Modules for PowerFlex GitHub page.\nDocumentation The documentation is available on Ansible Modules for PowerFlex GitHub page. It includes the following:\n README Release Notes (this document) Product Guide  ","excerpt":"Ansible Modules for Dell EMC PowerFlex Release Notes 1.0  © 2021 Dell Inc. or its subsidiaries. All …","ref":"/ansible-docs/docs/server/platforms/powerflex/release-notes/","title":"PowerFlex Release Notes"},{"body":"","excerpt":"","ref":"/ansible-docs/docs/server/platforms/powermax/","title":"PowerMax"},{"body":"","excerpt":"","ref":"/ansible-docs/docs/storage/platforms/powermax/","title":"PowerMax"},{"body":"You can upgrade the CSI Driver for Dell EMC PowerMax using Helm or Dell CSI Operator.\nUpdate Driver from v1.5 to v1.6 using Helm Steps\n Run git clone https://github.com/dell/csi-powermax.git to clone the git repository and get the v1.6 driver. Update values file as needed. Run the csi-install script with the option --upgrade by running: cd ../dell-csi-helm-installer \u0026amp;\u0026amp; ./csi-install.sh --namespace powermax --values ./my-powermax-settings.yaml --upgrade.  Update Driver from pre-v1.4 to v1.6 using Helm A rolling upgrade of the driver from an older version to v1.4 is not supported because of breaking changes in Kubernetes APIs in the migration from alpha snapshots to beta snapshots. In order to update the driver in this situation you need to remove alpha snapshot related artifacts.\nSteps\n  Delete any alpha VolumeSnapshot or VolumeSnapshotContent in the cluster.\n  Before deleting the alpha snapshot CRDs, ensure that their version is v1alpha1 by examining the output of the kubectl get crd command.\n  Delete any VolumeSnapshotClass present in the cluster.\n  Delete all the alpha snapshot CRDs from the cluster by running the following commands:\nkubectl delete crd volumesnapshotclasses.snapshot.storage.k8s.io kubectl delete crd volumesnapshotcontents.snapshot.storage.k8s.io kubectl delete crd volumesnapshots.snapshot.storage.k8s.io   Uninstall the driver using the csi-uninstall.sh script by running the command: ./csi-uninstall.sh --namespace \u0026lt;driver-namespace\u0026gt; where driver-namespace is the namespace where driver is installed.\n  Install the driver using the steps described in the Installation Using Helm section for the CSI PowerMax driver.\n  NOTE:\n If you are upgrading from a driver version which was installed using Helm v2, ensure that you install Helm3 before installing the driver. Installation of the CSI Driver for Dell EMC PowerMax version 1.6 driver is not supported on Kubernetes upstream clusters running Kubernetes version 1.17 or lower. You must upgrade your cluster to 1.18, 1.19, or 1.20 before attempting to install the new version of the driver. To update any installation parameter after the driver has been installed, change the my-powermax-settings.yaml file and run the install script with the option --upgrade, for example: ./csi-install.sh --namespace powermax --values ./my-powermax-settings.yaml –upgrade.  Upgrade using Dell CSI Operator: Follow the instructions for upgrade on the Dell CSI Operator GitHub page.\n","excerpt":"You can upgrade the CSI Driver for Dell EMC PowerMax using Helm or Dell CSI Operator.\nUpdate Driver …","ref":"/ansible-docs/v1/upgradation/drivers/powermax/","title":"PowerMax"},{"body":"You can upgrade the CSI Driver for Dell EMC PowerMax using Helm or Dell CSI Operator.\nUpdate Driver from v1.4 to v1.5 using Helm Steps\n Run git clone https://github.com/dell/csi-powermax.git to clone the git repository and get the v1.5 driver. Update values file as needed. Run the csi-install script with the option --upgrade by running: cd ../dell-csi-helm-installer \u0026amp;\u0026amp; ./csi-install.sh --namespace powermax --values ./my-powermax-settings.yaml --upgrade.  Update Driver from pre-v1.4 to v1.5 using Helm A rolling upgrade of the driver from an older version to v1.4 is not supported because of breaking changes in Kubernetes APIs in the migration from alpha snapshots to beta snapshots. In order to update the driver in this situation you need to remove alpha snapshot related artifacts.\nSteps\n  Delete any alpha VolumeSnapshot or VolumeSnapshotContent in the cluster.\n  Before deleting the alpha snapshot CRDs, ensure that their version is v1alpha1 by examining the output of the kubectl get crd command.\n  Delete any VolumeSnapshotClass present in the cluster.\n  Delete all the alpha snapshot CRDs from the cluster by running the following commands:\nkubectl delete crd volumesnapshotclasses.snapshot.storage.k8s.io kubectl delete crd volumesnapshotcontents.snapshot.storage.k8s.io kubectl delete crd volumesnapshots.snapshot.storage.k8s.io   Uninstall the driver using the csi-uninstall.sh script by running the command: ./csi-uninstall.sh --namespace \u0026lt;driver-namespace\u0026gt; where driver-namespace is the namespace where driver is installed.\n  Install the driver using the steps described in the Installation Using Helm section for the CSI PowerMax driver.\n  NOTE:\n If you are upgrading from a driver version which was installed using Helm v2, ensure that you install Helm3 before installing the driver. Installation of the CSI Driver for Dell EMC PowerMax version 1.5 driver is not supported on Kubernetes upstream clusters running Kubernetes version 1.16 or lower. You must upgrade your cluster to 1.17, 1.18, or 1.19 before attempting to install the new version of the driver. To update any installation parameter after the driver has been installed, change the my-powermax-settings.yaml file and run the install script with the option --upgrade, for example: ./csi-install.sh --namespace powermax --values ./my-powermax-settings.yaml –upgrade.  Upgrade using Dell CSI Operator: Follow the instructions for upgrade on the Dell CSI Operator GitHub page.\n","excerpt":"You can upgrade the CSI Driver for Dell EMC PowerMax using Helm or Dell CSI Operator.\nUpdate Driver …","ref":"/ansible-docs/v2/upgradation/drivers/powermax/","title":"PowerMax"},{"body":"Ansible Modules for Dell EMC PowerMax Product Guide 1.5.0  © 2021 Dell Inc. or its subsidiaries. All rights reserved. Dell EMC, and other trademarks are trademarks of Dell Inc. or its subsidiaries. Other trademarks may be trademarks of their respective owners.\n Contents  Common Parameters Gatherfacts Module  Synopsis Parameters Notes Examples Return Values Authors   Host Module  Synopsis Parameters Notes Examples Return Values Authors   Host Group Module  Synopsis Parameters Notes Examples Return Values Authors   Job Module  Synopsis Parameters Examples Return Values Authors   Masking View Module  Synopsis Parameters Examples Return Values Authors   Metro DR Module  Synopsis Parameters Examples Return Values Authors   Port Module  Synopsis Parameters Examples Return Values Authors   Port Group Module  Synopsis Parameters Examples Return Values Authors   RDF Group Module  Synopsis Parameters Examples Return Values Authors   Snapshot Module  Synopsis Parameters Notes Examples Return Values Authors   Snapshot Policy Module  Synopsis Parameters Notes Examples Return Values Authors   SRDF Module  Synopsis Parameters Examples Return Values Authors   Storage Group Module  Synopsis Parameters Examples Return Values Authors   Storage Pool Module  Synopsis Parameters Examples Return Values Authors   Volume Module  Synopsis Parameters Notes Examples Return Values Authors   Process Storage Pool Dict Module  Synopsis Parameters Examples Return Values Authors    Common Parameters These parameters are applicable to all modules, along with module-specific parameters.\nNOTE: If the parameter is mandatory, then required=true else it is an optional parameter. This is applicable to all the module specific parameters also.\n Parameter Choices/Defaults Comments    password  type=string, required=true      The password of the Unisphere host.     serial_no  type=string, required=true      The serial number of the PowerMax/VMAX array. It is a required parameter for all array-specific operations except for getting a list of arrays in the Gatherfacts module.     unispherehost  type=string, required=true      IP or FQDN of the Unisphere host     universion  type=integer    Choices: 91 92    Unisphere version, currently \u0026#x27;91\u0026#x27; and \u0026#x27;92\u0026#x27; versions are supported.     user  type=string, required=true      The username of the Unisphere host.     verifycert  type=boolean, required=true    Choices: no yes    Boolean variable to specify whether to validate SSL certificate or not. True - indicates that the SSL certificate should be verified. False - indicates that the SSL certificate should not be verified.    Gatherfacts Module Synopsis Gathers the list of specified PowerMax/VMAX storage system entities, such as the list of registered arrays, storage groups, hosts, host groups, storage groups, storage resource pools, port groups, masking views, array health status, alerts and metro DR environments, so on.\nParameters  Parameter Choices/Defaults Comments    filters  type=list elements=dictionary      List of filters to support filtered output for storage entities. Each filter is a tuple of {filter_key, filter_operator, filter_value}. Supports passing of multiple filters. The storage entities, \u0026#x27;rdf\u0026#x27;, \u0026#x27;health\u0026#x27;, \u0026#x27;snapshot_policies\u0026#x27; and \u0026#x27;metro_dr_env\u0026#x27;, does not support filters. Filters will be ignored if passed.      filter_key  type=string required=true      Name identifier of the filter.      filter_operator  type=string required=true    Choices: equal greater lesser like    Operation to be performed on filter key.      filter_value  type=string required=true      Value of the filter key.     gather_subset  type=list elements=string    Choices: alert health vol srp sg pg host hg port mv rdf metro_dr_env snapshot_policies    List of string variables to specify the PowerMax/VMAX entities for which information is required. Required only if the serial_no is present List of all PowerMax/VMAX entities supported by the module alert - gets alert summary information health - health status of a specific PowerMax array vol - volumes srp - storage resource pools sg - storage groups pg - port groups host - hosts hg - host groups port - ports mv - masking views rdf - rdf groups metro_dr_env - metro DR environments snapshot_policies - snapshot policies     serial_no  type=string      The serial number of the PowerMax/VMAX array. It is not required for getting the list of arrays.     tdev_volumes  type=boolean    Choices: no yes\u0026nbsp;\u0026larr;    Boolean variable to filter the volume list. This will have a small performance impact. By default it is set to true, only TDEV volumes will be returned. True - Will return only the TDEV volumes. False - Will return all the volumes.    Notes  Filter functionality will be supported only for the following \u0026lsquo;filter_key\u0026rsquo; against specific \u0026lsquo;gather_subset\u0026rsquo;.  vol - allocated_percent, associated, available_thin_volumes, bound_tdev, cap_cyl, cap_gb, cap_mb, cap_tb, cu_image_num, cu_image_ssid, data_volume, dld, drv, effective_wwn, emulation, encapsulated, encapsulated_wwn, gatekeeper, has_effective_wwn, mapped, mobility_id_enabled, num_of_front_end_paths, num_of_masking_views, num_of_storage_groups, oracle_instance_name, physical_name, pinned, private_volumes, rdf_group_number, reserved, split_name, status, storageGroupId, symmlun, tdev, thin_bcv, type, vdev, virtual_volumes, volume_identifier, wwn. srp - compression_state, description, effective_used_capacity_percent, emulation, num_of_disk_groups, num_of_srp_sg_demands, num_of_srp_slo_demands, rdfa_dse, reserved_cap_percent, total_allocated_cap_gb, total_srdf_dse_allocated_cap_gb, total_subscribed_cap_gb, total_usable_cap_gb. sg - base_slo_name, cap_gb, child, child_sg_name, ckd, compression, compression_ratio_to_one, fba, num_of_child_sgs, num_of_masking_views, num_of_parent_sgs, num_of_snapshots, num_of_vols, parent, parent_sg_name, slo_compliance, slo_name, srp_name, storageGroupId, tag, volumeId. pg - dir_port, fibre, iscsi, num_of_masking_views, num_of_ports host - host_group_name, num_of_host_groups, num_of_initiators, num_of_masking_views, num_of_powerpath_hosts, powerPathHostId. hg - host_name, num_of_hosts, num_of_masking_views. port - aclx, avoid_reset_broadcast, common_serial_number, director_status, disable_q_reset_on_ua, enable_auto_negotive, environ_set, hp_3000_mode, identifier, init_point_to_point, ip_list, ipv4_address, ipv6_address, iscsi_target, max_speed, negotiated_speed, neqotiate_reset, no_participating, node_wwn, num_of_cores, num_of_hypers, num_of_mapped_vols, num_of_masking_views, num_of_port_groups, port_interface, port_status, rdf_hardware_compression, rdf_hardware_compression_supported, rdf_software_compression, rdf_software_compression_supported, scsi_3, scsi_support1, siemens, soft_reset, spc2_protocol_version, sunapee, type, unique_wwn, vcm_state, vnx_attached, volume_set_addressing, wwn_node. mv - host_or_host_group_name, port_group_name, protocol_endpoint_masking_view, storage_group_name. alert - acknowledged, array, created_date, created_date_milliseconds, description, object, object_type, severity, state, type.    Examples - name: Get list of volumes with filter -- all TDEV volumes of size equal to 5GB dellemc_powermax_gatherfacts: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; universion: \u0026quot;{{universion}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; serial_no: \u0026quot;{{serial_no}}\u0026quot; gather_subset: - vol filters: - filter_key: \u0026quot;tdev\u0026quot; filter_operator: \u0026quot;equal\u0026quot; filter_value: \u0026quot;True\u0026quot; - filter_key: \u0026quot;cap_gb\u0026quot; filter_operator: \u0026quot;equal\u0026quot; filter_value: \u0026quot;5\u0026quot; - name: Get list of volumes and storage groups with filter dellemc_powermax_gatherfacts: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; universion: \u0026quot;{{universion}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; serial_no: \u0026quot;{{serial_no}}\u0026quot; gather_subset: - vol - sg filters: - filter_key: \u0026quot;tdev\u0026quot; filter_operator: \u0026quot;equal\u0026quot; filter_value: \u0026quot;True\u0026quot; - filter_key: \u0026quot;cap_gb\u0026quot; filter_operator: \u0026quot;equal\u0026quot; filter_value: \u0026quot;5\u0026quot; - name: Get list of storage groups with capacity between 2GB to 10GB dellemc_powermax_gatherfacts: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; universion: \u0026quot;{{universion}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; serial_no: \u0026quot;{{serial_no}}\u0026quot; gather_subset: - sg filters: - filter_key: \u0026quot;cap_gb\u0026quot; filter_operator: \u0026quot;greater\u0026quot; filter_value: \u0026quot;2\u0026quot; - filter_key: \u0026quot;cap_gb\u0026quot; filter_operator: \u0026quot;lesser\u0026quot; filter_value: \u0026quot;10\u0026quot; - name: Get the list of arrays for a given Unisphere host dellemc_powermax_gatherfacts: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; universion: \u0026quot;{{universion}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; register: array_list - debug: var: array_list - name: Get list of tdev-volumes dellemc_powermax_gatherfacts: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; universion: \u0026quot;{{universion}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; serial_no: \u0026quot;{{serial_no}}\u0026quot; tdev_volumes: True gather_subset: - vol - name: Get the list of arrays for a given Unisphere host dellemc_powermax_gatherfacts: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; universion: \u0026quot;{{universion}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; - name: Get array health status dellemc_powermax_gatherfacts: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; universion: \u0026quot;{{universion}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; serial_no: \u0026quot;{{serial_no}}\u0026quot; gather_subset: - health - name: Get array alerts summary dellemc_powermax_gatherfacts: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; universion: \u0026quot;{{universion}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; serial_no: \u0026quot;{{serial_no}}\u0026quot; gather_subset: - alert - name: Get the list of metro DR environments for a given Unisphere host dellemc_powermax_gatherfacts: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; universion: \u0026quot;{{universion}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; serial_no: \u0026quot;{{serial_no}}\u0026quot; gather_subset: - metro_dr_env - name: Get list of Storage groups dellemc_powermax_gatherfacts: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; universion: \u0026quot;{{universion}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; serial_no: \u0026quot;{{serial_no}}\u0026quot; gather_subset: - sg - name: Get list of Storage Resource Pools dellemc_powermax_gatherfacts: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; universion: \u0026quot;{{universion}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; serial_no: \u0026quot;{{serial_no}}\u0026quot; gather_subset: - srp - name: Get list of Ports dellemc_powermax_gatherfacts: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; universion: \u0026quot;{{universion}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; serial_no: \u0026quot;{{serial_no}}\u0026quot; gather_subset: - port - name: Get list of Port Groups dellemc_powermax_gatherfacts: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; universion: \u0026quot;{{universion}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; serial_no: \u0026quot;{{serial_no}}\u0026quot; gather_subset: - pg - name: Get list of Hosts dellemc_powermax_gatherfacts: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; universion: \u0026quot;{{universion}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; serial_no: \u0026quot;{{serial_no}}\u0026quot; gather_subset: - host - name: Get list of Host Groups dellemc_powermax_gatherfacts: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; universion: \u0026quot;{{universion}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; serial_no: \u0026quot;{{serial_no}}\u0026quot; gather_subset: - hg - name: Get list of Masking Views dellemc_powermax_gatherfacts: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; universion: \u0026quot;{{universion}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; serial_no: \u0026quot;{{serial_no}}\u0026quot; gather_subset: - mv - name: Get list of RDF Groups dellemc_powermax_gatherfacts: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; universion: \u0026quot;{{universion}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; serial_no: \u0026quot;{{serial_no}}\u0026quot; gather_subset: - rdf - name: Get list of snapshot policies dellemc_powermax_gatherfacts: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; universion: \u0026quot;{{universion}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; serial_no: \u0026quot;{{serial_no}}\u0026quot; gather_subset: - snapshot_policies Return Values The following are the fields unique to this module:\n Key Returned Description    Alerts  type=list elements=string   When the alert exists.  Alert summary of the array.     \u0026nbsp;  acknowledged  type=string   success  Whether or not this alert is acknowledged.     \u0026nbsp;  alertId  type=string   success  Unique ID of alert.     \u0026nbsp;  array  type=string   success  Array serial no.     \u0026nbsp;  created_date  type=string   success  Creation Date.     \u0026nbsp;  created_date_milliseconds  type=string   success  Creation Date in milliseconds.     \u0026nbsp;  description  type=string   success  Description about the alert     \u0026nbsp;  object  type=string   success  Object description     \u0026nbsp;  object_type  type=string   success  Resource class     \u0026nbsp;  severity  type=string   success  Severity of the alert     \u0026nbsp;  state  type=string   success  State of the alert     \u0026nbsp;  type  type=string   success  Type of the alert      Arrays  type=list elements=string   When the Unisphere exist.  List of arrays in the Unisphere.      Health  complex   When the array exist.  Health status of the array.     \u0026nbsp;  health_score_metric  type=list elements=string   success  Overall health score for the specified Symmetrix.     \u0026nbsp; \u0026nbsp;  cached_date  type=integer   success  Date Time stamp in epoch format when it was cached.     \u0026nbsp; \u0026nbsp;  data_date  type=integer   success  Date Time stamp in epoch format when it was collected.     \u0026nbsp; \u0026nbsp;  expired  type=boolean   success  Flag to indicate the expiry of the score.     \u0026nbsp; \u0026nbsp;  health_score  type=integer   success  Overall health score in numbers.     \u0026nbsp; \u0026nbsp;  instance_metrics  type=list elements=string   success  Metrics about a specific instance.     \u0026nbsp; \u0026nbsp; \u0026nbsp;  health_score_instance_metric  type=integer   success  Health score of a specific instance.     \u0026nbsp; \u0026nbsp;  metric  type=string   success  Information about which sub system , such as SYSTEM_UTILIZATION, CONFIGURATION,CAPACITY, and so on.     \u0026nbsp;  num_failed_disks  type=integer   success  Numbers of the disk failure in this system.      HostGroups  type=list elements=string   When the hostgroups exist.  List of host groups present on the array.      Hosts  type=list elements=string   When the hosts exist.  List of hosts present on the array.      MaskingViews  type=list elements=string   When the masking views exist.  List of masking views present on the array.      MetroDREnvironments  type=list elements=string   When environment exists.  List of metro DR environments on the array.      PortGroups  type=list elements=string   When the port groups exist.  List of port groups on the array.      Ports  complex   When the ports exist.  List of ports on the array.     \u0026nbsp;  directorId  type=string   success  Director ID of the port.     \u0026nbsp;  portId  type=string   success  Port number of the port.      RDFGroups  complex   When the RDF groups exist.  List of RDF groups on the array.     \u0026nbsp;  label  type=string   success  Name of the RDF group.     \u0026nbsp;  rdfgNumber  type=integer   success  Unique identifier of the RDF group.      SnapshotPolicies  type=list elements=string   When snapshot policy exists.  List of snapshot policies on the array.      StorageGroups  type=list elements=string   When the storage groups exist.  List of storage groups on the array.      StorageResourcePools  complex   When the storage pools exist.  List of storage pools on the array.     \u0026nbsp;  diskGroupId  type=list elements=string   success  ID of the disk group.     \u0026nbsp;  emulation  type=string   success  Type of volume emulation.     \u0026nbsp;  num_of_disk_groups  type=integer   success  Number of disk groups.     \u0026nbsp;  rdfa_dse  type=boolean   success  Flag for RDFA Delta Set Extension.     \u0026nbsp;  reserved_cap_percent  type=integer   success  Reserved capacity percentage.     \u0026nbsp;  srp_capacity  type=dictionary   success  Different entities to measure SRP capacity.     \u0026nbsp; \u0026nbsp;  effective_used_capacity_percent  type=integer   success  Percentage of effectively used capacity.     \u0026nbsp; \u0026nbsp;  snapshot_modified_tb  type=integer   success  Snapshot modified in TB.     \u0026nbsp; \u0026nbsp;  snapshot_total_tb  type=integer   success  Total snapshot size in TB.     \u0026nbsp; \u0026nbsp;  subscribed_allocated_tb  type=integer   success  Subscribed allocated size in TB.     \u0026nbsp; \u0026nbsp;  subscribed_total_tb  type=integer   success  Subscribed total size in TB.     \u0026nbsp; \u0026nbsp;  usable_total_tb  type=integer   success  Usable total size in TB.     \u0026nbsp; \u0026nbsp;  usable_used_tb  type=integer   success  Usable used size in TB.     \u0026nbsp;  srp_efficiency  type=dictionary   success  Different entities to measure SRP efficiency.     \u0026nbsp; \u0026nbsp;  compression_state  type=string   success  Depicts the compression state of the SRP.     \u0026nbsp; \u0026nbsp;  data_reduction_enabled_percent  type=integer   success  Percentage of data reduction enabled in the SRP.     \u0026nbsp; \u0026nbsp;  data_reduction_ratio_to_one  type=integer   success  Data reduction ratio of SRP.     \u0026nbsp; \u0026nbsp;  overall_efficiency_ratio_to_one  type=integer   success  Overall effectively ratio of SRP.     \u0026nbsp; \u0026nbsp;  snapshot_savings_ratio_to_one  type=integer   success  Snapshot savings ratio of SRP.     \u0026nbsp; \u0026nbsp;  virtual_provisioning_savings_ratio_to_one  type=integer   success  Virtual provisioning savings ratio of SRP.     \u0026nbsp;  srpId  type=string   success  Unique Identifier for SRP.     \u0026nbsp;  total_srdf_dse_allocated_cap_gb  type=integer   success  Total srdf dse allocated capacity in GB.      Volumes  type=list elements=string   When the volumes exist.  List of volumes on the array.     Authors  Arindam Datta (@dattaarindam) \u0026lt;ansible.team@dell.com\u0026gt; Rajshree Khare (@khareRajshree) \u0026lt;ansible.team@dell.com\u0026gt;  Host Module Synopsis Managing hosts on a PowerMax storage system includes creating a host with a set of initiators and host flags, adding and removing initiators to or from a host, modifying host flag values, renaming a host, and deleting a host.\nParameters  Parameter Choices/Defaults Comments    host_flags  type=dictionary      Input as a yaml dictionary List of all host_flags- 1. volume_set_addressing 2. disable_q_reset_on_ua 3. environ_set 4. avoid_reset_broadcast 5. openvms 6. scsi_3 7. spc2_protocol_version 8. scsi_support1 9. consistent_lun Possible values are true, false, unset (default state)     host_name  type=string required=true      The name of the host. No Special Character support except for _. Case sensitive for REST Calls. Creation of an empty host is allowed     host_type  type=string    Choices: default hpux    Describing the OS type (default or hpux)     initiator_state  type=string    Choices: present-in-host absent-in-host    Define whether the initiators should be present or absent on the host. present-in-host - indicates that the initiators should exist on the host absent-in-host - indicates that the initiators should not exist on the host Required when creating a host with initiators or adding and removing initiators to or from an existing host     initiators  type=list elements=string      List of Initiator WWN or IQN to be added to the host or removed from the host.     new_name  type=string      The new name of the host for the renaming function. No Special Character support except for _. Case sensitive for REST Calls     state  type=string required=true    Choices: absent present    Define whether the host should exist or not. present - indicates that the host should exist in the system absent - indicates that the host should not exist in the system    Notes  host_flags and host_type are mutually exclusive parameters.  Examples - name: Create host with host_type 'default' dellemc_powermax_host: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; universion: \u0026quot;{{universion}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; serial_no: \u0026quot;{{serial_no}}\u0026quot; host_name: \u0026quot;ansible_test_1\u0026quot; host_type: \u0026quot;default\u0026quot; state: 'present' - name: Create host with host_type 'hpux' dellemc_powermax_host: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; universion: \u0026quot;{{universion}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; serial_no: \u0026quot;{{serial_no}}\u0026quot; host_name: \u0026quot;ansible_test_2\u0026quot; host_type: \u0026quot;hpux\u0026quot; state: 'present' - name: Create host with host_flags dellemc_powermax_host: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; universion: \u0026quot;{{universion}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; serial_no: \u0026quot;{{serial_no}}\u0026quot; host_name: \u0026quot;ansible_test_3\u0026quot; initiators: - 10000090fa7b4e85 host_flags: spc2_protocol_version: true consistent_lun: true volume_set_addressing: 'unset' disable_q_reset_on_ua: false openvms: 'unset' state: 'present' initiator_state: 'present-in-host' - name: Get host details dellemc_powermax_host: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; universion: \u0026quot;{{universion}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; serial_no: \u0026quot;{{serial_no}}\u0026quot; host_name: \u0026quot;ansible_test_1\u0026quot; state: 'present' - name: Adding initiator to host dellemc_powermax_host: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; universion: \u0026quot;{{universion}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; serial_no: \u0026quot;{{serial_no}}\u0026quot; host_name: \u0026quot;ansible_test_1\u0026quot; initiators: - 10000090fa3d303e initiator_state: 'present-in-host' state: 'present' - name: Removing initiator from host dellemc_powermax_host: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; universion: \u0026quot;{{universion}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; serial_no: \u0026quot;{{serial_no}}\u0026quot; host_name: \u0026quot;ansible_test_1\u0026quot; initiators: - 10000090fa3d303e initiator_state: 'absent-in-host' state: 'present' - name: Modify host using host_type dellemc_powermax_host: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; universion: \u0026quot;{{universion}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; serial_no: \u0026quot;{{serial_no}}\u0026quot; host_name: \u0026quot;ansible_test_1\u0026quot; host_type: \u0026quot;hpux\u0026quot; state: 'present' - name: Modify host using host_flags dellemc_powermax_host: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; universion: \u0026quot;{{universion}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; serial_no: \u0026quot;{{serial_no}}\u0026quot; host_name: \u0026quot;ansible_test_1\u0026quot; host_flags: spc2_protocol_version: unset consistent_lun: unset volume_set_addressing: true disable_q_reset_on_ua: false openvms: false avoid_reset_broadcast: true state: 'present' - name: Rename host dellemc_powermax_host: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; universion: \u0026quot;{{universion}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; serial_no: \u0026quot;{{serial_no}}\u0026quot; host_name: \u0026quot;ansible_test_1\u0026quot; new_name: \u0026quot;ansible_test_1_host\u0026quot; state: 'present' - name: Delete host dellemc_powermax_host: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; universion: \u0026quot;{{universion}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; serial_no: \u0026quot;{{serial_no}}\u0026quot; host_name: \u0026quot;ansible_test_1_host\u0026quot; state: 'absent' Return Values The following are the fields unique to this module:\n Key Returned Description    changed  type=boolean   always  Whether or not the resource has changed.      host_details  complex   When host exist.  Details of the host.     \u0026nbsp;  bw_limit  type=integer   success  Bandwidth limit of the host.     \u0026nbsp;  consistent_lun  type=boolean   success  Flag for consistent LUN in host.     \u0026nbsp;  disabled_flags  type=list elements=string   success  List of any disabled port flags overridden by the initiator.     \u0026nbsp;  enabled_flags  type=list elements=string   success  List of any enabled port flags overridden by the initiator.     \u0026nbsp;  hostgroup  type=list elements=string   success  List of host groups that the host is associated with.     \u0026nbsp;  hostId  type=string   success  Host ID.     \u0026nbsp;  initiator  type=list elements=string   success  List of initiators present in the host.     \u0026nbsp;  maskingview  type=list elements=string   success  List of masking view in which the host group is present.     \u0026nbsp;  num_of_hostgroups  type=integer   success  Number of host groups associated with the host.     \u0026nbsp;  num_of_initiators  type=integer   success  Number of initiators present in the host.     \u0026nbsp;  num_of_masking_views  type=integer   success  Number of masking views associated with the host.     \u0026nbsp;  num_of_powerpath_hosts  type=integer   success  Number of PowerPath hosts associated with the host.     \u0026nbsp;  port_flags_override  type=boolean   success  Whether any of the initiator port flags are overridden.     \u0026nbsp;  type  type=string   success  Type of initiator.     Authors  Vasudevu Lakhinana (@unknown) \u0026lt;ansible.team@dell.com\u0026gt; Manisha Agrawal (@agrawm3) \u0026lt;ansible.team@dell.com\u0026gt;  Host Group Module Synopsis Managing a host group on a PowerMax storage system includes creating a host group with a set of hosts, adding or removing hosts to or from a host group, renaming a host group, modifying host flags of a host group, and deleting a host group.\nParameters  Parameter Choices/Defaults Comments    host_flags  type=dictionary      input as an yaml dictionary List of all host_flags - 1. volume_set_addressing 2. disable_q_reset_on_ua 3. environ_set 4. avoid_reset_broadcast 5. openvms 6. scsi_3 7. spc2_protocol_version 8. scsi_support1 9. consistent_lun Possible values are true, false, unset(default state)     host_state  type=string    Choices: present-in-group absent-in-group    Define whether the host should be present or absent in the host group. present-in-group - indicates that the hosts should exist in the host group absent-in-group - indicates that the hosts should not exist in the host group     host_type  type=string    Choices: default hpux    Describing the OS type (default or hpux)     hostgroup_name  type=string required=true      The name of the host group. No Special Character support except for _. Case sensitive for REST Calls.     hosts  type=list elements=string      List of host names to be added to the host group or removed from the host group. Creation of an empty host group is allowed.     new_name  type=string      The new name for the host group for the renaming function. No Special Character support except for _. Case sensitive for REST Calls     state  type=string required=true    Choices: absent present    Define whether the host group should be present or absent on the system. present - indicates that the host group should be present on the system absent - indicates that the host group should be absent on the system    Notes  In the gather facts module, empty host groups will be listed as hosts. host_flags and host_type are mutually exclusive parameters. Hostgroups with \u0026lsquo;default\u0026rsquo; host_type will have \u0026lsquo;default\u0026rsquo; hosts. Hostgroups with \u0026lsquo;hpux\u0026rsquo; host_type will have \u0026lsquo;hpux\u0026rsquo; hosts.  Examples - name: Create host group with 'default' host_type dellemc_powermax_hostgroup: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; universion: \u0026quot;{{universion}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; serial_no: \u0026quot;{{serial_no}}\u0026quot; hostgroup_name: \u0026quot;ansible_test_HG_1\u0026quot; host_type: \u0026quot;default\u0026quot; hosts: - ansible_test_1 host_state: 'present-in-group' state: 'present' - name: Create host group with 'hpux' host_type dellemc_powermax_hostgroup: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; universion: \u0026quot;{{universion}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; serial_no: \u0026quot;{{serial_no}}\u0026quot; hostgroup_name: \u0026quot;ansible_test_HG_2\u0026quot; host_type: \u0026quot;hpux\u0026quot; hosts: - ansible_test_2 host_state: 'present-in-group' state: 'present' - name: Create host group with host_flags dellemc_powermax_hostgroup: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; universion: \u0026quot;{{universion}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; serial_no: \u0026quot;{{serial_no}}\u0026quot; hostgroup_name: \u0026quot;ansible_test_HG_3\u0026quot; hosts: - ansible_test_3 state: 'present' host_state: 'present-in-group' host_flags: spc2_protocol_version: true consistent_lun: true volume_set_addressing: 'unset' disable_q_reset_on_ua: false openvms: 'unset' - name: Get host group details dellemc_powermax_hostgroup: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; universion: \u0026quot;{{universion}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; serial_no: \u0026quot;{{serial_no}}\u0026quot; hostgroup_name: \u0026quot;ansible_test_HG_1\u0026quot; state: 'present' - name: Adding host to host group dellemc_powermax_hostgroup: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; universion: \u0026quot;{{universion}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; serial_no: \u0026quot;{{serial_no}}\u0026quot; hostgroup_name: \u0026quot;ansible_test_HG_1\u0026quot; hosts: - Ansible_Testing_host2 state: 'present' host_state: 'present-in-group' - name: Removing host from host group dellemc_powermax_hostgroup: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; universion: \u0026quot;{{universion}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; serial_no: \u0026quot;{{serial_no}}\u0026quot; hostgroup_name: \u0026quot;ansible_test_HG_1\u0026quot; hosts: - Ansible_Testing_host2 state: 'present' host_state: 'absent-in-group' - name: Modify host group using host_type dellemc_powermax_hostgroup: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; universion: \u0026quot;{{universion}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; serial_no: \u0026quot;{{serial_no}}\u0026quot; hostgroup_name: \u0026quot;ansible_test_HG_1\u0026quot; host_type: \u0026quot;hpux\u0026quot; state: 'present' - name: Modify host group using host_flags dellemc_powermax_hostgroup: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; universion: \u0026quot;{{universion}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; serial_no: \u0026quot;{{serial_no}}\u0026quot; hostgroup_name: \u0026quot;ansible_test_HG_1\u0026quot; host_flags: spc2_protocol_version: unset disable_q_reset_on_ua: false openvms: false avoid_reset_broadcast: true state: 'present' - name: Rename host group dellemc_powermax_hostgroup: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; universion: \u0026quot;{{universion}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; serial_no: \u0026quot;{{serial_no}}\u0026quot; hostgroup_name: \u0026quot;ansible_test_HG_1\u0026quot; new_name: \u0026quot;ansible_test_hostgroup_1\u0026quot; state: 'present' - name: Delete host group dellemc_powermax_hostgroup: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; universion: \u0026quot;{{universion}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; serial_no: \u0026quot;{{serial_no}}\u0026quot; hostgroup_name: \u0026quot;ansible_test_hostgroup_1\u0026quot; state: 'absent' Return Values The following are the fields unique to this module:\n Key Returned Description    changed  type=boolean   always  Whether or not the resource has changed.      hostgroup_details  complex   When host group exist.  Details of the host group.     \u0026nbsp;  consistent_lun  type=boolean   success  Flag for consistent LUN in the host group.     \u0026nbsp;  disabled_flags  type=list elements=string   success  List of any disabled port flags overridden by the initiator.     \u0026nbsp;  enabled_flags  type=list elements=string   success  List of any enabled port flags overridden by the initiator.     \u0026nbsp;  host  type=list elements=string   success  List of hosts present in the host group.     \u0026nbsp; \u0026nbsp;  hostId  type=string   success  Unique identifier for the host.     \u0026nbsp; \u0026nbsp;  initiator  type=list elements=string   success  List of initiators present in the host.     \u0026nbsp;  hostGroupId  type=string   success  Host group ID.     \u0026nbsp;  maskingview  type=list elements=string   success  Masking view in which host group is present.     \u0026nbsp;  num_of_hosts  type=integer   success  Number of hosts in the host group.     \u0026nbsp;  num_of_initiators  type=integer   success  Number of initiators in the host group.     \u0026nbsp;  num_of_masking_views  type=integer   success  Number of masking views associated with the host group.     \u0026nbsp;  port_flags_override  type=boolean   success  Whether any of the initiator\u0026#x27;s port flags are overridden.     \u0026nbsp;  type  type=string   success  Type of initiator of the hosts of the host group.     Authors  Vasudevu Lakhinana (@unknown) \u0026lt;ansible.team@dell.com\u0026gt; Manisha Agrawal (@agrawm3) \u0026lt;ansible.team@dell.com\u0026gt;  Job Module Synopsis  Gets the details of a Job from a specified PowerMax/VMAX storage system. The details listed are of an asynchronous task.  Parameters  Parameter Choices/Defaults Comments    job_id  type=string required=true      Job ID of an asynchronous task, used for getting details of a job.    Examples - name: Get the details of a Job. dellemc_powermax_job: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; universion: \u0026quot;{{universion}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; job_id: \u0026quot;1570622921504\u0026quot; Return Values The following are the fields unique to this module:\n Key Returned Description    changed  type=boolean   always  Whether or not the resource has changed.      Job_details  type=dictionary   When job exist.  Details of the job.     \u0026nbsp;  completed_date_milliseconds  type=integer   success  Date of job completion in milliseconds.     \u0026nbsp;  jobId  type=string   success  Unique identifier of the job.     \u0026nbsp;  last_modified_date  type=string   success  Last modified date of job.     \u0026nbsp;  last_modified_date_milliseconds  type=integer   success  Last modified date of job in milliseconds.     \u0026nbsp;  name  type=string   success  Name of the job.     \u0026nbsp;  resourceLink  type=string   success  Resource link w.r.t Unisphere.     \u0026nbsp;  result  type=string   success  Job description     \u0026nbsp;  status  type=string   success  Status of the job.     \u0026nbsp;  task  type=list elements=string   success  Details about the job.     \u0026nbsp;  username  type=string   success  Unisphere username.     Authors  Rajshree Khare (@khareRajshree) \u0026lt;ansible.team@dell.com\u0026gt;  Masking View Module Synopsis   Managing masking views on PowerMax storage system includes, creating masking view with port group, storage group and host or host group, renaming masking view and deleting masking view.\n  For creating a masking view -\n  portgroup_name,\n  sg_name and\n  any one of host_name or hostgroup_name is required.\n    All three entities must be present on the array.\n  For renaming a masking view, the \u0026lsquo;new_mv_name\u0026rsquo; is required. After a masking view is created, only its name can be changed. No underlying entity (portgroup, storagegroup, host or hostgroup) can be changed on the masking view.\n  Parameters  Parameter Choices/Defaults Comments    host_name  type=string      The name of the existing host. This parameter is to create an exclusive or host export     hostgroup_name  type=string      The name of the existing host group. This parameter is used to create cluster export     mv_name  type=string required=true      The name of the masking view. No Special Character support except for _. Case sensitive for REST Calls.     new_mv_name  type=string      The new name for the renaming function. No Special Character support except for _. Case sensitive for REST Calls.     portgroup_name  type=string      The name of the existing port group.     sg_name  type=string      The name of the existing storage group.     state  type=string required=true    Choices: absent present    Defines whether the masking view should exist or not.    Examples - name: Create MV with hostgroup dellemc_powermax_maskingview: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; universion: \u0026quot;{{universion}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; serial_no: \u0026quot;{{serial_no}}\u0026quot; mv_name: \u0026quot;Ansible_Testing_MaskingView_HostGroup\u0026quot; portgroup_name: \u0026quot;Ansible_Testing_portgroup\u0026quot; hostgroup_name: \u0026quot;Ansible_Testing_hostgroup\u0026quot; sg_name: \u0026quot;Ansible_Testing_SG\u0026quot; state: \u0026quot;present\u0026quot; - name: Create MV with host dellemc_powermax_maskingview: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; universion: \u0026quot;{{universion}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; serial_no: \u0026quot;{{serial_no}}\u0026quot; mv_name: \u0026quot;Ansible_Testing_MaskingView_Host\u0026quot; portgroup_name: \u0026quot;Ansible_Testing_portgroup\u0026quot; host_name: \u0026quot;Ansible_Testing_host\u0026quot; sg_name: \u0026quot;Ansible_Testing_SG\u0026quot; state: \u0026quot;present\u0026quot; - name: Rename host masking view dellemc_powermax_maskingview: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; universion: \u0026quot;{{universion}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; serial_no: \u0026quot;{{serial_no}}\u0026quot; mv_name: \u0026quot;Ansible_Testing_MaskingView_Host\u0026quot; new_mv_name: \u0026quot;Ansible_Testing_mv_renamed\u0026quot; state: \u0026quot;present\u0026quot; - name: Delete host masking view dellemc_powermax_maskingview: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; universion: \u0026quot;{{universion}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; serial_no: \u0026quot;{{serial_no}}\u0026quot; mv_name: \u0026quot;Ansible_Testing_mv_renamed\u0026quot; state: \u0026quot;absent\u0026quot; Return Values The following are the fields unique to this module:\n Key Returned Description    changed  type=boolean   always  Whether or not the resource has changed.      create_mv  type=boolean   When masking view is created.  Flag sets to true when a new masking view is created.      delete_mv  type=boolean   When masking view is deleted.  Flag sets to true when a masking view is deleted.      modify_mv  type=boolean   When masking view is modified.  Flag sets to true when a masking view is modified.      mv_details  type=list elements=string   When masking view exist.  Details of masking view.     \u0026nbsp;  hostId  type=string   success  Host group present in the masking view.     \u0026nbsp;  maskingViewId  type=string   success  Masking view ID.     \u0026nbsp;  portGroupId  type=string   success  Port group present in the masking view.     \u0026nbsp;  storageGroupId  type=string   success  Storage group present in the masking view.     Authors  Vasudevu Lakhinana (@unknown) \u0026lt;ansible.team@dell.com\u0026gt; Prashant Rakheja (@prashant-dell) \u0026lt;ansible.team@dell.com\u0026gt;  Metro DR Module Synopsis Managing a metro DR environment on a PowerMax storage system includes getting details of any specific metro DR environment, creating a metro DR environment, converting an existing SG into a metro DR environment, modifying metro DR environment attributes and deleting a metro DR environment.\nParameters  Parameter Choices/Defaults Comments    dr_serial_no  type=string      Serial number of the DR array. It is required in create and convert operations.     env_name  type=string required=true      Name of the metro DR environment. Metro DR environment name will be unique across PowerMax.     metro_serial_no  type=string      Serial number of the remote metro array. It is required only in create and convert operations.     new_rdf_group_r1  type=boolean    Choices: no yes\u0026nbsp;\u0026larr;    The flag indicates whether or not to create a new RDFG for a Metro R1 array to a DR array, or to autoselect from an existing one. Used in only create operation.     new_rdf_group_r2  type=boolean    Choices: no yes\u0026nbsp;\u0026larr;    The flag indicates whether or not to create a new RDFG for a Metro R2 array to a DR array, or to autoselect from an existing one. It is used only in create operation.     remove_r1_dr_rdfg  type=boolean    Choices: no\u0026nbsp;\u0026larr; yes    The flag indicates whether or not to override default behavior and delete R11-R2 RDFG from the metro R1 side. It is used only in delete operations.     replication_mode  type=string    Choices: Asynchronous Adaptive Copy    Replication mode whose value will indicate how the data will be replicated. It is required in create and modify operations. It is a mandatory parameter in a create operation but optional in a modify operation.     serial_no  type=string required=true      Serial number of the primary metro array.     sg_name  type=string      Name of the storage group. Storage group will be present on the primary metro array and a storage group with the same name will be created on remote and DR arrays in a create operation. Storage group name is required in \u0026#x27;create metro DR environment\u0026#x27; and \u0026#x27;convert SG into metro DR environment\u0026#x27; operations.     srdf_param  type=dictionary      It contains parameters related to SRDF links. It is used only in modify operations.      dr  type=boolean    Choices: no\u0026nbsp;\u0026larr; yes    The flag indicates whether or not to direct srdf_state change towards device pairs on the disaster recovery leg of the metro DR environment.      keep_r2  type=boolean    Choices: no\u0026nbsp;\u0026larr; yes    The flag indicates whether or not in the case of srdf state suspend to make R2 data on metro available to the host.      metro  type=boolean    Choices: no\u0026nbsp;\u0026larr; yes    The flag indicates whether or not to direct srdf_state change towards the R1--R2 Metro Device leg of the metro DR environment.      srdf_state  type=string required=true    Choices: Split Restore SetMode Failback Failover Establish Suspend UpdateR1 Recover    State of the SRDF link. It is a mandatory parameter for modify operations.     state  type=string required=true    Choices: absent present    State variable to determine whether metro DR environment will exist or not.     wait_for_completion  type=boolean    Choices: no\u0026nbsp;\u0026larr; yes    The flag indicates if the operation should be run synchronously or asynchronously. True signifies synchronous execution. By default, create and convert are asynchronous operations, whereas modify is a synchronous operation.    Examples - name: Get metro environment details dellemc_powermax_metrodr: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; universion: \u0026quot;{{universion}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; serial_no: \u0026quot;{{serial_no}}\u0026quot; env_name: \u0026quot;ansible_metrodr_env\u0026quot; state: \u0026quot;present\u0026quot; - name: Convert SG to metro DR environment dellemc_powermax_metrodr: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; universion: \u0026quot;{{universion}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; sg_name: \u0026quot;ansible_sg\u0026quot; env_name: \u0026quot;ansible_metrodr_env\u0026quot; serial_no: \u0026quot;{{serial_no}}\u0026quot; metro_serial_no: \u0026quot;{{metro_serial_no}}\u0026quot; dr_serial_no: \u0026quot;{{dr_serial_no}}\u0026quot; replication_mode: \u0026quot;Asynchronous\u0026quot; wait_for_completion: False state: \u0026quot;present\u0026quot; - name: Create metro DR environment dellemc_powermax_metrodr: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; universion: \u0026quot;{{universion}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; sg_name: \u0026quot;ansible_sg\u0026quot; env_name: \u0026quot;ansible_metrodr_env\u0026quot; serial_no: \u0026quot;{{serial_no}}\u0026quot; metro_serial_no: \u0026quot;{{metro_serial_no}}\u0026quot; dr_serial_no: \u0026quot;{{dr_serial_no}}\u0026quot; replication_mode: \u0026quot;Asynchronous\u0026quot; new_rdf_group_r1: True new_rdf_group_r2: True wait_for_completion: False state: \u0026quot;present\u0026quot; - name: Modify metro DR environment dellemc_powermax_metrodr: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; universion: \u0026quot;{{universion}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; serial_no: \u0026quot;{{serial_no}}\u0026quot; env_name: \u0026quot;ansible_metrodr_env\u0026quot; srdf_param: srdf_state: \u0026quot;Suspend\u0026quot; metro: True dr: True keep_r2: True wait_for_completion: True state: \u0026quot;present\u0026quot; - name: Delete metro DR environment dellemc_powermax_metrodr: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; universion: \u0026quot;{{universion}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; serial_no: \u0026quot;{{serial_no}}\u0026quot; env_name: \u0026quot;ansible_metrodr_env\u0026quot; remove_r1_dr_rdfg: True state: 'absent' Return Values The following are the fields unique to this module:\n Key Returned Description    changed  type=boolean   always  Whether or not the resource has changed.      Job_details  type=dictionary   When job exist.  Details of the job.     \u0026nbsp;  completed_date_milliseconds  type=integer   success  Date of job completion in milliseconds.     \u0026nbsp;  jobId  type=string   success  Unique identifier of the job.     \u0026nbsp;  last_modified_date  type=string   success  Last modified date of job.     \u0026nbsp;  last_modified_date_milliseconds  type=integer   success  Last modified date of job in milliseconds.     \u0026nbsp;  name  type=string   success  Name of the job.     \u0026nbsp;  resourceLink  type=string   success  Resource link w.r.t Unisphere.     \u0026nbsp;  result  type=string   success  Job description     \u0026nbsp;  status  type=string   success  Status of the job.     \u0026nbsp;  task  type=list elements=string   success  Details about the job.     \u0026nbsp;  username  type=string   success  Unisphere username.      metrodr_env_details  type=dictionary   When environment exists.  Details of the metro DR environment link.     \u0026nbsp;  capacity_gb  type=float   success  Size of volume in GB.     \u0026nbsp;  dr_exempt  type=boolean   success  Flag to indication that if there are exempt devices (volumes) in the DR site or not.     \u0026nbsp;  dr_link_state  type=string   success  Status of DR site.     \u0026nbsp;  dr_percent_complete  type=integer   success  Percentage synchronized in DR session.     \u0026nbsp;  dr_rdf_mode  type=string   success  Replication mode with DR site.     \u0026nbsp;  dr_remain_capacity_to_copy_mb  type=integer   success  Remaining capacity to copy at DR site.     \u0026nbsp;  dr_service_state  type=string   success  The HA state of the DR session.     \u0026nbsp;  dr_state  type=string   success  The pair states of the DR session.     \u0026nbsp;  environment_exempt  type=boolean   success  Flag to indication that if there are exempt devices (volumes) in the environment or not.     \u0026nbsp;  environment_state  type=string   success  The state of the smart DR environment.     \u0026nbsp;  metro_exempt  type=boolean   success  Flag to indication that if there are exempt devices (volumes) in the DR site or not.     \u0026nbsp;  metro_link_state  type=string   success  Status of metro site.     \u0026nbsp;  metro_r1_array_health  type=string   success  Health status of metro R1 array.     \u0026nbsp;  metro_r2_array_health  type=string   success  Health status of metro R1 array.     \u0026nbsp;  metro_service_state  type=string   success  The HA state of the metro session.     \u0026nbsp;  metro_state  type=string   success  The pair states of the metro session.     \u0026nbsp;  metro_witness_state  type=string   success  The witness state of the metro session.     \u0026nbsp;  name  type=string   success  The smart DR environment name.     \u0026nbsp;  valid  type=boolean   success  Flag to indicate whether valid environment or not.     Authors  Vivek Soni (@v-soni11) \u0026lt;ansible.team@dell.com\u0026gt; Rajshree Khare (@khareRajshree) \u0026lt;ansible.team@dell.com\u0026gt;  Port Module Synopsis Managing ports on PowerMax storage system includes getting details of a port.\nParameters  Parameter Choices/Defaults Comments    ports  type=list elements=dictionary required=true      List of port director and port id    Examples - name: Get details of single/multiple ports dellemc_powermax_port: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; serial_no: \u0026quot;{{array_id}}\u0026quot; ports: - director_id: \u0026quot;FA-1D\u0026quot; port_id: \u0026quot;5\u0026quot; - director_id: \u0026quot;SE-1F\u0026quot; port_id: \u0026quot;29\u0026quot; Return Values The following are the fields unique to this module:\n Key Returned Description    changed  type=boolean   always  Whether or not the resource has changed.      port_details  type=list elements=string   When the port exist.  Details of the port.     \u0026nbsp;  symmetrixPort  type=list elements=string   success  Type of volume.     \u0026nbsp; \u0026nbsp;  aclx  type=boolean   success  Indicates whether access control logic is enabled or disabled.     \u0026nbsp; \u0026nbsp;  avoid_reset_broadcast  type=boolean   success  Indicates whether the Avoid Reset Broadcasting feature is enabled or disabled.     \u0026nbsp; \u0026nbsp;  common_serial_number  type=boolean   success  Indicates whether the Common Serial Number feature is enabled or disabled.     \u0026nbsp; \u0026nbsp;  director_status  type=string   success  Director status.     \u0026nbsp; \u0026nbsp;  disable_q_reset_on_ua  type=boolean   success  Indicates whether the Disable Q Reset on UA (Unit Attention) is enabled or disabled.     \u0026nbsp; \u0026nbsp;  enable_auto_negotiate  type=boolean   success  Indicates whether the Enable Auto Negotiate feature is enabled or disabled.     \u0026nbsp; \u0026nbsp;  environ_set  type=boolean   success  Indicates whether the environmental error reporting feature is enabled or disabled.     \u0026nbsp; \u0026nbsp;  hp_3000_mode  type=boolean   success  Indicates whether HP 3000 Mode is enabled or disabled.     \u0026nbsp; \u0026nbsp;  identifier  type=string   success  Unique identifier for port.     \u0026nbsp; \u0026nbsp;  init_point_to_point  type=boolean   success  Indicates whether Init Point to Point is enabled or disabled.     \u0026nbsp; \u0026nbsp;  iscsi_target  type=boolean   success  Indicates whether ISCSI target is enabled or disabled.     \u0026nbsp; \u0026nbsp;  maskingview  type=list elements=string   success  List of Masking views that the port is a part of.     \u0026nbsp; \u0026nbsp;  max_speed  type=string   success  Maximum port speed in GB/Second.     \u0026nbsp; \u0026nbsp;  negotiate_reset  type=boolean   success  Indicates whether the Negotiate Reset feature is enabled or disabled.     \u0026nbsp; \u0026nbsp;  negotiated_speed  type=string   success  Negotiated speed in GB/Second.     \u0026nbsp; \u0026nbsp;  no_participating  type=boolean   success  Indicates whether the No Participate feature is enabled or disabled.     \u0026nbsp; \u0026nbsp;  num_of_cores  type=integer   success  Number of cores for the director.     \u0026nbsp; \u0026nbsp;  num_of_mapped_vols  type=integer   success  Number of volumes mapped with the port.     \u0026nbsp; \u0026nbsp;  num_of_masking_views  type=integer   success  Number of masking views associated with the port.     \u0026nbsp; \u0026nbsp;  num_of_port_groups  type=integer   success  Number of port groups associated with the port.     \u0026nbsp; \u0026nbsp;  port_status  type=string   success  Port status, ON/OFF.     \u0026nbsp; \u0026nbsp;  portgroup  type=list elements=string   success  List of masking views associated with the port.     \u0026nbsp; \u0026nbsp;  scsi_3  type=boolean   success  Indicates whether the SCSI-3 protocol is enabled or disabled.     \u0026nbsp; \u0026nbsp;  scsi_support1  type=boolean   success  Indicates whether the SCSI Support1 is enabled or disabled.     \u0026nbsp; \u0026nbsp;  siemens  type=boolean   success  Indicates whether the Siemens feature is enabled or disabled.     \u0026nbsp; \u0026nbsp;  soft_reset  type=boolean   success  Indicates whether the Soft Reset feature is enabled or disabled.     \u0026nbsp; \u0026nbsp;  spc2_protocol_version  type=boolean   success  Indicates whether the SPC2 Protocol Version feature is enabled or disabled.     \u0026nbsp; \u0026nbsp;  sunapee  type=boolean   success  Indicates whether the Sunapee feature is enabled or disabled.     \u0026nbsp; \u0026nbsp;  symmetrixPortKey  type=list elements=string   success  Symmetrix system director and port in the port group.     \u0026nbsp; \u0026nbsp; \u0026nbsp;  drectorId  type=string   success  Director ID of the port.     \u0026nbsp; \u0026nbsp; \u0026nbsp;  portId  type=string   success  Port number of the port.     \u0026nbsp; \u0026nbsp;  type  type=string   success  Type of port.     \u0026nbsp; \u0026nbsp;  unique_wwn  type=boolean   success  Indicates whether the Unique WWN feature is enabled or disabled.     \u0026nbsp; \u0026nbsp;  vnx_attached  type=boolean   success  Indicates whether the VNX attached feature is enabled or disabled.     \u0026nbsp; \u0026nbsp;  volume_set_addressing  type=boolean   success  Indicates whether Volume Vet Addressing is enabled or disabled.     \u0026nbsp; \u0026nbsp;  wwn_node  type=string   success  WWN node of port.     Authors  Ashish Verma (@vermaa31) \u0026lt;ansible.team@dell.com\u0026gt;  Port Group Module Synopsis Managing port groups on a PowerMax storage system includes creating a port group with a set of ports, adding or removing single or multiple ports to or from the port group, renaming the port group and deleting the port group.\nParameters  Parameter Choices/Defaults Comments    new_name  type=string      New name of the port group while renaming. No Special Character support except for _. Case sensitive for REST Calls.     port_state  type=string    Choices: present-in-group absent-in-group    Define whether the port should be present or absent in the port group. present-in-group - indicates that the ports should be present on a port group object absent-in-group - indicates that the ports should not be present on a port group object     portgroup_name  type=string required=true      The name of the port group. No Special Character support except for _. Case sensitive for REST Calls.     ports  type=list elements=dictionary      List of directors and ports to be added or removed to or from the port group     state  type=string required=true    Choices: absent present    Define whether the port group should exist or not. present - indicates that the port group should be present on the system absent - indicates that the port group should not be present on the system    Examples - name: Create port group without ports dellemc_powermax_portgroup: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; universion: \u0026quot;{{universion}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; serial_no: \u0026quot;{{array_id}}\u0026quot; portgroup_name: \u0026quot;{{portgroup_name}}\u0026quot; state: \u0026quot;present\u0026quot; - name: Create port group with ports dellemc_powermax_portgroup: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; universion: \u0026quot;{{universion}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; serial_no: \u0026quot;{{array_id}}\u0026quot; portgroup_name: \u0026quot;{{portgroup_name}}\u0026quot; state: \u0026quot;present\u0026quot; ports: - director_id: \u0026quot;FA-1D\u0026quot; port_id: \u0026quot;5\u0026quot; - director_id: \u0026quot;FA-2D\u0026quot; port_id: \u0026quot;5\u0026quot; port_state: \u0026quot;present-in-group\u0026quot; - name: Add ports to port group dellemc_powermax_portgroup: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; universion: \u0026quot;{{universion}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; serial_no: \u0026quot;{{array_id}}\u0026quot; portgroup_name: \u0026quot;{{portgroup_name}}\u0026quot; state: \u0026quot;present\u0026quot; ports: - director_id: \u0026quot;FA-2D\u0026quot; port_id: \u0026quot;8\u0026quot; - director_id: \u0026quot;FA-2D\u0026quot; port_id: \u0026quot;9\u0026quot; port_state: \u0026quot;present-in-group\u0026quot; - name: Remove ports from port group dellemc_powermax_portgroup: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; universion: \u0026quot;{{universion}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; serial_no: \u0026quot;{{array_id}}\u0026quot; portgroup_name: \u0026quot;{{portgroup_name}}\u0026quot; state: \u0026quot;present\u0026quot; ports: - director_id: \u0026quot;FA-2D\u0026quot; port_id: \u0026quot;8\u0026quot; - director_id: \u0026quot;FA-2D\u0026quot; port_id: \u0026quot;9\u0026quot; port_state: \u0026quot;absent-in-group\u0026quot; - name: Modify port group dellemc_powermax_portgroup: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; universion: \u0026quot;{{universion}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; serial_no: \u0026quot;{{array_id}}\u0026quot; portgroup_name: \u0026quot;{{portgroup_name}}\u0026quot; state: \u0026quot;present\u0026quot; new_name: \u0026quot;{{new_name}}\u0026quot; - name: Delete port group dellemc_powermax_portgroup: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; universion: \u0026quot;{{universion}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; serial_no: \u0026quot;{{array_id}}\u0026quot; portgroup_name: \u0026quot;{{portgroup_name}}\u0026quot; state: \u0026quot;absent\u0026quot; Return Values The following are the fields unique to this module:\n Key Returned Description    changed  type=boolean   always  Whether or not the resource has changed.      portgroup_details  type=list elements=string   When the port group exist.  Details of the port group.     \u0026nbsp;  num_of_masking_views  type=integer   success  Number of masking views in where port group is associated.     \u0026nbsp;  num_of_ports  type=integer   success  Number of ports in the port group.     \u0026nbsp;  portGroupId  type=string   success  Port group ID.     \u0026nbsp;  symmetrixPortKey  type=list elements=string   success  Symmetrix system director and port in the port group.     \u0026nbsp; \u0026nbsp;  directorId  type=string   success  Director ID of the port.     \u0026nbsp; \u0026nbsp;  portId  type=string   success  Port number of the port.     \u0026nbsp;  type  type=string   success  Type of ports in port group.     Authors  Vasudevu Lakhinana (@unknown) \u0026lt;ansible.team@dell.com\u0026gt; Ashish Verma (@vermaa31) \u0026lt;ansible.team@dell.com\u0026gt; Rajshree Khare (@khareRajshree) \u0026lt;ansible.team@dell.com\u0026gt;  RDF Group Module Synopsis  Gets the details of an RDF Group from a specified PowerMax/VMAX storage system. Lists the volumes of an RDF Group from a specified PowerMax/VMAX storage system  Parameters  Parameter Choices/Defaults Comments    rdfgroup_number  type=string required=true      Identifier of an RDF Group of type string    Examples - name: Get the details of rdf group and volumes dellemc_powermax_rdfgroup: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; universion: \u0026quot;{{universion}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; serial_no: \u0026quot;{{serial_no}}\u0026quot; rdfgroup_number: \u0026quot;{{rdfgroup_id}}\u0026quot; Return Values The following are the fields unique to this module:\n Key Returned Description    changed  type=boolean   always  Whether or not the resource has changed.      RDFGroupDetails  type=list elements=string   When the RDF group exist.  Details of the RDF group.     \u0026nbsp;  async  type=boolean   success  Flag sets to true when an SRDF pair is in async mode.     \u0026nbsp;  biasConfigured  type=boolean   success  Flag for configured bias.     \u0026nbsp;  biasEffective  type=boolean   success  Flag for effective bias.     \u0026nbsp;  device_polarity  type=string   success  Type of device polarity.     \u0026nbsp;  hardware_compression  type=boolean   success  Flag for hardware compression.     \u0026nbsp;  label  type=string   success  RDF group label.     \u0026nbsp;  link_limbo  type=integer   success  The amount of time that the array\u0026#x27;s operating environment waits after the SRDF link goes down before updating the link\u0026#x27;s status. The link limbo value can be set from 0 to 120 seconds. The default value is 10 seconds.     \u0026nbsp;  localOnlinePorts  type=list elements=string   success  List of local online ports.     \u0026nbsp;  localPorts  type=list elements=string   success  List of local ports.     \u0026nbsp;  metro  type=list elements=string   success  Flag for metro configuration.     \u0026nbsp;  modes  type=string   success  Mode of the SRDF link.     \u0026nbsp;  numDevices  type=integer   success  Number of devices involved in the pairing.     \u0026nbsp;  offline  type=boolean   success  Offline flag.     \u0026nbsp;  rdfa_properties  type=list elements=string   success  Properties associated with the RDF group.     \u0026nbsp; \u0026nbsp;  average_cycle_time  type=integer   success  Average cycle time (seconds) configured for this session in seconds.     \u0026nbsp; \u0026nbsp;  consistency_exempt_volumes  type=boolean   success  Flag that indicates if consistency is exempt.     \u0026nbsp; \u0026nbsp;  cycle_number  type=integer   success  Number of cycles in seconds.     \u0026nbsp; \u0026nbsp;  dse_active  type=boolean   success  Flag for active Delta Set Extension.     \u0026nbsp; \u0026nbsp;  dse_autostart  type=string   success  Indicates DSE autostart state.     \u0026nbsp; \u0026nbsp;  dse_threshold  type=integer   success  Flag for DSE threshold.     \u0026nbsp; \u0026nbsp;  duration_of_last_cycle  type=integer   success  The cycle time (in secs) of the most recently completed cycle.     \u0026nbsp; \u0026nbsp;  duration_of_last_transmit_cycle  type=integer   success  Duration of last transmitted cycle in seconds.     \u0026nbsp; \u0026nbsp;  r1_to_r2_lag_time  type=integer   success  Time that R2 is behind R1 in seconds.     \u0026nbsp; \u0026nbsp;  session_priority  type=integer   success  Priority used to determine which RDFA sessions to drop if cache becomes full. Values range from 1 to 64, with 1 being the highest priority (last to be dropped).     \u0026nbsp; \u0026nbsp;  session_uncommitted_tracks  type=integer   success  Number of uncommitted session tracks.     \u0026nbsp; \u0026nbsp;  transmit_idle_state  type=string   success  Indicates RDFA transmit idle state.     \u0026nbsp; \u0026nbsp;  transmit_idle_time  type=integer   success  Time the transmit cycle has been idle.     \u0026nbsp; \u0026nbsp;  transmit_queue_depth  type=integer   success  The transmitted queue depth of disks.     \u0026nbsp;  rdfgNumber  type=integer   success  RDF group number on primary device.     \u0026nbsp;  RDFGroupVolumes  type=list elements=string   success  List of various properties of RDF group volume(s).     \u0026nbsp; \u0026nbsp;  largerRdfSide  type=string   success  Larger RDF side among the devices.     \u0026nbsp; \u0026nbsp;  local_wwn_external  type=integer   success  External WWN of volume at primary device.     \u0026nbsp; \u0026nbsp;  localRdfGroupNumber  type=integer   success  RDF group number at primary device.     \u0026nbsp; \u0026nbsp;  localSymmetrixId  type=integer   success  Primary device ID.     \u0026nbsp; \u0026nbsp;  localVolumeName  type=string   success  Volume name at primary device.     \u0026nbsp; \u0026nbsp;  localVolumeState  type=string   success  Volume state at primary device     \u0026nbsp; \u0026nbsp;  rdfMode  type=string   success  SRDF mode of pairing.     \u0026nbsp; \u0026nbsp;  rdfpairState  type=string   success  SRDF state of pairing.     \u0026nbsp; \u0026nbsp;  remote_wwn_external  type=integer   success  External WWN of volume at remote device.     \u0026nbsp; \u0026nbsp;  remoteRdfGroupNumber  type=integer   success  RDF group number at remote device.     \u0026nbsp; \u0026nbsp;  remoteSymmetrixId  type=integer   success  Remote device ID.     \u0026nbsp; \u0026nbsp;  remoteVolumeName  type=string   success  Volume name at remote device.     \u0026nbsp; \u0026nbsp;  remoteVolumeState  type=string   success  Volume state at remote device.     \u0026nbsp; \u0026nbsp;  volumeConfig  type=string   success  Type of volume.     \u0026nbsp;  remoteOnlinePorts  type=list elements=string   success  List of remote online ports.     \u0026nbsp;  remotePorts  type=list elements=string   success  List of remote ports.     \u0026nbsp;  remoteRdfgNumber  type=integer   success  RDF group number at remote device.     \u0026nbsp;  remoteSymmetrix  type=integer   success  Remote device ID.     \u0026nbsp;  software_compression  type=boolean   success  Flag for software compression.     \u0026nbsp;  totalDeviceCapacity  type=integer   success  Total capacity of RDF group in GB.     \u0026nbsp;  type  type=string   success  Type of RDF group.     \u0026nbsp;  vasa_group  type=boolean   success  Flag for VASA group member.     \u0026nbsp;  witness  type=boolean   success  Flag for witness.     \u0026nbsp;  witnessConfigured  type=boolean   success  Flag for configured witness.     \u0026nbsp;  witnessDegraded  type=boolean   success  Flag for degraded witness.     \u0026nbsp;  witnessEffective  type=boolean   success  Flag for effective witness.     \u0026nbsp;  witnessProtectedPhysical  type=boolean   success  Flag for physically protected witness.     \u0026nbsp;  witnessProtectedVirtual  type=boolean   success  Flag for virtually protected witness.     Authors  Arindam Datta (@dattaarindam) \u0026lt;ansible.team@dell.com\u0026gt;  Snapshot Module Synopsis Managing snapshots on a PowerMax storage system includes creating a new storage group (SG) snapshot, getting details of the SG snapshot, renaming the SG snapshot, changing the snapshot link status, and deleting an existing SG snapshot.\nParameters  Parameter Choices/Defaults Comments    generation  type=integer      The generation number of the snapshot. Generation is required for link, unlink, rename and delete operations. Optional for Get snapshot details. Create snapshot will always create a new snapshot with a generation number 0. Rename is supported only for generation number 0.     link_status  type=string    Choices: linked unlinked    Describes the link status of the snapshot.     new_snapshot_name  type=string      The new name of the snapshot.     sg_name  type=string required=true      The name of the storage group.     snapshot_id  type=integer      Unique ID of the snapshot. snapshot_id is required for link, unlink, rename and delete operations. Optional for Get snapshot details.     snapshot_name  type=string required=true      The name of the snapshot.     state  type=string required=true    Choices: absent present    Define whether the snapshot should exist or not.     target_sg_name  type=string      The target storage group.     ttl  type=string      The Time To Live (TTL) value for the snapshot. If the TTL is not specified, the storage group snap details are returned. However, to create a SG snap - TTL must be given. If the SG snap should not have any TTL - specify TTL as \u0026quot;None\u0026quot;     ttl_unit  type=string    Choices: hours days\u0026nbsp;\u0026larr;    The unit for the ttl. If no ttl_unit is specified, \u0026#x27;days\u0026#x27; is taken as default ttl_unit.    Notes  Paramters \u0026lsquo;generation\u0026rsquo; and \u0026lsquo;snapshot_id\u0026rsquo; are mutually exclusive. If \u0026lsquo;generation\u0026rsquo; or \u0026lsquo;snapshot_id\u0026rsquo; is not provided then a list of generation versus snapshot_id is returned. Use of \u0026lsquo;snapshot_id\u0026rsquo; over \u0026lsquo;generation\u0026rsquo; is preferably recommended for PowerMax microcode version 5978.669.669 and onwards.  Examples - name: Create a Snapshot for a Storage Group dellemc_powermax_snapshot: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; universion: \u0026quot;{{universion}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; serial_no: \u0026quot;{{serial_no}}\u0026quot; sg_name: \u0026quot;ansible_sg\u0026quot; snapshot_name: \u0026quot;ansible_sg_snap\u0026quot; ttl: \u0026quot;2\u0026quot; ttl_unit: \u0026quot;days\u0026quot; state: \u0026quot;present\u0026quot; - name: Get Storage Group Snapshot details dellemc_powermax_snapshot: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; universion: \u0026quot;{{universion}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; serial_no: \u0026quot;{{serial_no}}\u0026quot; sg_name: \u0026quot;ansible_sg\u0026quot; snapshot_name: \u0026quot;ansible_sg_snap\u0026quot; state: \u0026quot;present\u0026quot; - name: Get Storage Group Snapshot details using generation dellemc_powermax_snapshot: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; universion: \u0026quot;{{universion}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; serial_no: \u0026quot;{{serial_no}}\u0026quot; sg_name: \u0026quot;ansible_sg\u0026quot; snapshot_name: \u0026quot;ansible_sg_snap\u0026quot; generation: 1 state: \u0026quot;present\u0026quot; - name: Get Storage Group Snapshot details using snapshot_id dellemc_powermax_snapshot: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; universion: \u0026quot;{{universion}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; serial_no: \u0026quot;{{serial_no}}\u0026quot; sg_name: \u0026quot;ansible_sg\u0026quot; snapshot_name: \u0026quot;ansible_sg_snap\u0026quot; snapshot_id: 135023964929 state: \u0026quot;present\u0026quot; - name: Rename Storage Group Snapshot using generation dellemc_powermax_snapshot: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; universion: \u0026quot;{{universion}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; serial_no: \u0026quot;{{serial_no}}\u0026quot; sg_name: \u0026quot;ansible_sg\u0026quot; snapshot_name: \u0026quot;ansible_sg_snap\u0026quot; new_snapshot_name: \u0026quot;ansible_snap_new\u0026quot; generation: 0 state: \u0026quot;present\u0026quot; - name: Rename Storage Group Snapshot using snapshot_id dellemc_powermax_snapshot: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; universion: \u0026quot;{{universion}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; serial_no: \u0026quot;{{serial_no}}\u0026quot; sg_name: \u0026quot;ansible_sg\u0026quot; snapshot_name: \u0026quot;ansible_sg_snap\u0026quot; new_snapshot_name: \u0026quot;ansible_snap_new\u0026quot; snapshot_id: 135023964929 state: \u0026quot;present\u0026quot; - name: Change Snapshot Link Status to Linked using generation dellemc_powermax_snapshot: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; universion: \u0026quot;{{universion}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; serial_no: \u0026quot;{{serial_no}}\u0026quot; sg_name: \u0026quot;ansible_sg\u0026quot; snapshot_name: \u0026quot;ansible_snap_new\u0026quot; generation: 1 target_sg_name: \u0026quot;ansible_sg_target\u0026quot; link_status: \u0026quot;linked\u0026quot; state: \u0026quot;present\u0026quot; - name: Change Snapshot Link Status to UnLinked using generation dellemc_powermax_snapshot: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; universion: \u0026quot;{{universion}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; serial_no: \u0026quot;{{serial_no}}\u0026quot; sg_name: \u0026quot;ansible_sg\u0026quot; snapshot_name: \u0026quot;ansible_snap_new\u0026quot; generation: 1 target_sg_name: \u0026quot;ansible_sg_target\u0026quot; link_status: \u0026quot;unlinked\u0026quot; state: \u0026quot;present\u0026quot; - name: Change Snapshot Link Status to Linked using snapshot_id dellemc_powermax_snapshot: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; universion: \u0026quot;{{universion}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; serial_no: \u0026quot;{{serial_no}}\u0026quot; sg_name: \u0026quot;ansible_sg\u0026quot; snapshot_name: \u0026quot;ansible_snap_new\u0026quot; snapshot_id: 135023964515 target_sg_name: \u0026quot;ansible_sg_target\u0026quot; link_status: \u0026quot;linked\u0026quot; state: \u0026quot;present\u0026quot; - name: Change Snapshot Link Status to UnLinked using snapshot_id dellemc_powermax_snapshot: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; universion: \u0026quot;{{universion}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; serial_no: \u0026quot;{{serial_no}}\u0026quot; sg_name: \u0026quot;ansible_sg\u0026quot; snapshot_name: \u0026quot;ansible_snap_new\u0026quot; snapshot_id: 135023964515 target_sg_name: \u0026quot;ansible_sg_target\u0026quot; link_status: \u0026quot;unlinked\u0026quot; state: \u0026quot;present\u0026quot; - name: Delete Storage Group Snapshot using generation dellemc_powermax_snapshot: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; universion: \u0026quot;{{universion}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; serial_no: \u0026quot;{{serial_no}}\u0026quot; sg_name: \u0026quot;ansible_sg\u0026quot; snapshot_name: \u0026quot;ansible_sg_snap\u0026quot; generation: 1 state: \u0026quot;absent\u0026quot; - name: Delete Storage Group Snapshot using snapshot_id dellemc_powermax_snapshot: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; universion: \u0026quot;{{universion}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; serial_no: \u0026quot;{{serial_no}}\u0026quot; sg_name: \u0026quot;ansible_sg\u0026quot; snapshot_name: \u0026quot;ansible_sg_snap\u0026quot; snapshot_id: 135023964929 state: \u0026quot;absent\u0026quot; Return Values The following are the fields unique to this module:\n Key Returned Description    changed  type=boolean   always  Whether or not the resource has changed.      create_sg_snap  type=boolean   When snapshot is created.  Flag sets to true when the snapshot is created.      delete_sg_snap  type=boolean   When snapshot is deleted.  Flag sets to true when the snapshot is deleted.      rename_sg_snap  type=boolean   When snapshot is renamed.  Flag sets to true when the snapshot is renamed.      sg_snap_details  complex   When snapshot exists.  Details of the snapshot.     \u0026nbsp;  expired  type=boolean   success  Indicates whether the snapshot is expired or not.     \u0026nbsp;  generation/snapid  type=integer   success  The generation/snapshot ID of the snapshot.     \u0026nbsp;  linked  type=boolean   success  Indicates whether the snapshot is linked or not.     \u0026nbsp;  name  type=string   success  Name of the snapshot.     \u0026nbsp;  non_shared_tracks  type=integer   success  Number of non-shared tracks.     \u0026nbsp;  num_source_volumes  type=integer   success  Number of source volumes.     \u0026nbsp;  num_storage_group_volumes  type=integer   success  Number of storage group volumes.     \u0026nbsp;  restored  type=boolean   success  Indicates whether the snapshot is restored or not.     \u0026nbsp;  source_volume  type=list elements=string   success  Source volume details.     \u0026nbsp; \u0026nbsp;  capacity  type=integer   success  Volume capacity.     \u0026nbsp; \u0026nbsp;  capacity_gb  type=integer   success  Volume capacity in GB.     \u0026nbsp; \u0026nbsp;  name  type=string   success  Volume ID.     \u0026nbsp;  state  type=string   success  State of the snapshot.     \u0026nbsp;  time_to_live_expiry_date  type=string   success  Time to live expiry date.     \u0026nbsp;  timestamp  type=string   success  Snapshot time stamp.     \u0026nbsp;  timestamp_utc  type=integer   success  Snapshot time stamp specified in UTC.     \u0026nbsp;  tracks  type=integer   success  Number of tracks.     Authors  Prashant Rakheja (@prashant-dell) \u0026lt;ansible.team@dell.com\u0026gt; Rajshree Khare (@khareRajshree) \u0026lt;ansible.team@dell.com\u0026gt;  Snapshot Policy Module Synopsis Managing a snapshot policy on a PowerMax storage system includes getting details of any specific snapshot policy, creating a snapshot policy, modifying snapshot policy attributes, modifying snapshot policy state, associating or disassociating storage groups to or from snapshot policy and deleting a snapshot policy.\nParameters  Parameter Choices/Defaults Comments    compliance_count_critical  type=integer      If the number of valid snapshots falls below this number, the compliance changes to critical (red).     compliance_count_warning  type=integer      If the number of valid snapshots falls below this number, the compliance changes to warning (yellow).     interval  type=string    Choices: 10 Minutes 12 Minutes 15 Minutes 20 Minutes 30 Minutes 1 Hour 2 Hours 3 Hours 4 Hours 6 Hours 8 Hours 12 Hours 1 Day 7 Days    The value of the interval counter for snapshot policy execution.     new_snapshot_policy_name  type=string      New name of the snapshot policy.     offset_mins  type=integer      Defines when, within the interval the snapshots will be taken for a specified snapshot policy. The offset must be less than the interval of the snapshot policy. The format must be in minutes. If not specified, default value is 0.     secure  type=boolean    Choices: no yes    Secure snapshots may only be terminated after they expire or by Dell EMC support. If not specified, default value is False.     snapshot_count  type=integer      The max snapshot count of the policy. Max value is 1024.     snapshot_policy_name  type=string required=true      Name of the snapshot policy.     state  type=string required=true    Choices: present absent    Shows if the snapshot policy should be present or absent.     storage_group_state  type=string    Choices: present-in-policy absent-in-policy    The state of the storage group with regard to the snapshot policy. present-in-policy indicates associate SG to SP. absent-in-policy indicates disassociate SG from SP.     storage_groups  type=list elements=string      List of storage groups.     suspend  type=boolean    Choices: no yes    Suspend the snapshot policy. True indicates snapshot policy is in suspend state. False indicates snapshot policy is in resume state.     universion  type=integer    Choices: 92    Unisphere version, currently \u0026#x27;92\u0026#x27; version is supported.    Notes  The max number of snapshot policies on an array is limited to 20. At most four snapshot policies can be associated with a storage group. The compliance_count_warning value should be less than total_snapshot_count value of the policy. The compliance_count_critical value should be less than or equal to the compliance_count_warning value of the policy.  Examples - name: Create a snapshot policy dellemc_powermax_snapshotpolicy: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; universion: \u0026quot;{{universion}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; serial_no: \u0026quot;{{serial_no}}\u0026quot; snapshot_policy_name: \u0026quot;10min_policy_1\u0026quot; interval: \u0026quot;10 Minutes\u0026quot; secure: false snapshot_count: 10 offset_mins: 2 compliance_count_warning: 6 compliance_count_critical: 4 state: \u0026quot;present\u0026quot; - name: Create a snapshot policy and associate storage groups to it dellemc_powermax_snapshotpolicy: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; universion: \u0026quot;{{universion}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; serial_no: \u0026quot;{{serial_no}}\u0026quot; snapshot_policy_name: \u0026quot;10min_policy_2\u0026quot; interval: \u0026quot;10 Minutes\u0026quot; secure: false snapshot_count: 12 offset_mins: 5 compliance_count_warning: 8 compliance_count_critical: 4 storage_groups: - \u0026quot;11_ansible_test_1\u0026quot; - \u0026quot;11_ansible_test_2\u0026quot; storage_group_state: \u0026quot;present-in-policy\u0026quot; state: \u0026quot;present\u0026quot; - name: Get snapshot policy details dellemc_powermax_snapshotpolicy: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; universion: \u0026quot;{{universion}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; serial_no: \u0026quot;{{serial_no}}\u0026quot; snapshot_policy_name: \u0026quot;10min_policy_2\u0026quot; state: \u0026quot;present\u0026quot; - name: Modify snapshot policy attributes dellemc_powermax_snapshotpolicy: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; universion: \u0026quot;{{universion}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; serial_no: \u0026quot;{{serial_no}}\u0026quot; snapshot_policy_name: \u0026quot;10min_policy_2\u0026quot; new_snapshot_policy_name: \u0026quot;10min_policy_2_new\u0026quot; interval: \u0026quot;10 Minutes\u0026quot; snapshot_count: 16 offset_mins: 8 compliance_count_warning: 9 compliance_count_critical: 7 state: \u0026quot;present\u0026quot; - name: Modify snapshot policy, associate to storage groups dellemc_powermax_snapshotpolicy: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; universion: \u0026quot;{{universion}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; serial_no: \u0026quot;{{serial_no}}\u0026quot; snapshot_policy_name: \u0026quot;10min_policy_1\u0026quot; storage_groups: - \u0026quot;11_ansible_test_1\u0026quot; - \u0026quot;11_ansible_test_2\u0026quot; storage_group_state: \u0026quot;present-in-policy\u0026quot; state: \u0026quot;present\u0026quot; - name: Modify snapshot policy, disassociate from storage groups dellemc_powermax_snapshotpolicy: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; universion: \u0026quot;{{universion}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; serial_no: \u0026quot;{{serial_no}}\u0026quot; snapshot_policy_name: \u0026quot;10min_policy_1\u0026quot; storage_groups: - \u0026quot;11_ansible_test_1\u0026quot; - \u0026quot;11_ansible_test_2\u0026quot; storage_group_state: \u0026quot;absent-in-policy\u0026quot; state: \u0026quot;present\u0026quot; - name: Modify snapshot policy state to suspend dellemc_powermax_snapshotpolicy: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; universion: \u0026quot;{{universion}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; serial_no: \u0026quot;{{serial_no}}\u0026quot; snapshot_policy_name: \u0026quot;10min_policy_1\u0026quot; suspend: true state: \u0026quot;present\u0026quot; - name: Modify snapshot policy state to resume dellemc_powermax_snapshotpolicy: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; universion: \u0026quot;{{universion}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; serial_no: \u0026quot;{{serial_no}}\u0026quot; snapshot_policy_name: \u0026quot;10min_policy_1\u0026quot; suspend: false state: \u0026quot;present\u0026quot; - name: Delete a snapshot policy dellemc_powermax_snapshotpolicy: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; universion: \u0026quot;{{universion}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; serial_no: \u0026quot;{{serial_no}}\u0026quot; snapshot_policy_name: \u0026quot;10min_policy_1\u0026quot; state: \u0026quot;absent\u0026quot; Return Values The following are the fields unique to this module:\n Key Returned Description    changed  type=boolean   always  Whether or not the resource has changed.      snapshot_policy_details  complex   When snapshot policy exists.  Details of the snapshot policy.     \u0026nbsp;  compliance_count_critical  type=integer   success  The number of valid snapshots that have critical compliance.     \u0026nbsp;  compliance_count_warning  type=integer   success  The number of valid snapshots that have warning compliance.     \u0026nbsp;  interval_minutes  type=integer   success  The interval minutes for snapshot policy execution.     \u0026nbsp;  last_time_used  type=string   success  The timestamp indicating the last time snapshot policy was used.     \u0026nbsp;  offset_minutes  type=integer   success  It is the time in minutes within the interval when the snapshots will be taken for a specified Snapshot Policy.     \u0026nbsp;  secure  type=boolean   success  True value indicates that the secure snapshots may only be terminated after they expire or by Dell EMC support.     \u0026nbsp;  snapshot_count  type=integer   success  It is the max snapshot count of the policy.     \u0026nbsp;  snapshot_policy_name  type=string   success  Name of the snapshot policy.     \u0026nbsp;  storage_group  type=list elements=string   success  The list of storage groups associated with the snapshot policy.     \u0026nbsp;  storage_group_count  type=integer   success  The number of storage groups associated with the snapshot policy.     \u0026nbsp;  storage_group_snapshotID  type=list elements=string   success  Pair of storage group and list of snapshot IDs associated with the snapshot policy.     \u0026nbsp;  suspended  type=boolean   success  The state of the snapshot policy, true indicates policy is in suspend state.     \u0026nbsp;  symmetrixID  type=string   success  The symmetrix on which snapshot policy exists.     Authors  Rajshree Khare (@khareRajshree) \u0026lt;ansible.team@dell.com\u0026gt;  SRDF Module Synopsis Managing SRDF link on a PowerMax storage system includes creating an SRDF pair for a storage group, modifying the SRDF mode, modifying the SRDF state of an existing SRDF pair, and deleting an SRDF pair. All create and modify calls are asynchronous by default.\nParameters  Parameter Choices/Defaults Comments    job_id  type=string      Job ID of an asynchronous task. Can be used to get details of a job.     new_rdf_group  type=boolean    Choices: no yes    Overrides the SRDF group selection functionality and forces the creation of a new SRDF group. PowerMax has a limited number of RDF groups. If this flag is set to True, and the RDF groups are exhausted, then SRDF link creation will fail. If not specified, default value is \u0026#x27;false\u0026#x27;.     rdfg_no  type=integer      The RDF group number. Optional parameter for each call. For a create operation, if specified, the array will reuse the RDF group, otherwise an error is returned. For modify and delete operations, if the RFD group number is not specified, and the storage group is protected by multiple RDF groups, then an error is raised.     remote_serial_no  type=string      integer 12-digit serial number of remote PowerMax or VMAX array. Required while creating an SRDF link.     serial_no  type=string required=true      The serial number will refer to the source PowerMax/VMAX array when protecting a storage group. However srdf_state operations may be issued from primary or remote array.     sg_name  type=string      Name of storage group. SRDF pairings are managed at a storage group level. Required to identify the SRDF link.     srdf_mode  type=string    Choices: Active Adaptive Copy Synchronous Asynchronous    The replication mode of the SRDF pair. Required when creating an SRDF pair. Can be modified by providing a required value.     srdf_state  type=string    Choices: Establish Resume Restore Suspend Swap Split Failback Failover Setbias    Desired state of the SRDF pairing. While creating a new SRDF pair, allowed values are \u0026#x27;Establish\u0026#x27; and \u0026#x27;Suspend\u0026#x27;. If the state is not specified, the pair will be created in a \u0026#x27;Suspended\u0026#x27; state. When modifying the state, only certain changes are allowed.     state  type=string required=true    Choices: absent present    Define whether the SRDF pairing should exist or not. present indicates that the SRDF pairing should exist in system. absent indicates that the SRDF pairing should not exist in system.     wait_for_completion  type=boolean    Choices: no\u0026nbsp;\u0026larr; yes    Flag to indicate if the operation should be run synchronously or asynchronously. True signifies synchronous execution. By default, all create and update operations will be run asynchronously.     witness  type=boolean    Choices: no yes    Flag to specify use of Witness for a Metro configuration. Setting to True signifies to use Witness, setting it to False signifies to use Bias. It is recommended to configure a witness for SRDF Metro in a production environment, this is configured via Unisphere for PowerMax UI or REST. The flag can be set only for modifying srdf_state to either Establish, Suspend, or Restore. While creating a Metro configuration, the witness flag must be set to True.    Examples - name: Create and establish storagegroup SRDF/a pairing register: Job_details_body dellemc_powermax_srdf: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; universion: \u0026quot;{{universion}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; serial_no: \u0026quot;{{serial_no}}\u0026quot; sg_name: \u0026quot;{{sg_name}}\u0026quot; remote_serial_no: \u0026quot;{{remote_serial_no}}\u0026quot; srdf_mode: 'Asynchronous' srdf_state: 'Establish' state: 'present' - name: Create storagegroup SRDF/s pair in default suspended mode as an Synchronous task dellemc_powermax_srdf: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; universion: \u0026quot;{{universion}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; serial_no: \u0026quot;{{serial_no}}\u0026quot; sg_name: \u0026quot;{{sg_name2}}\u0026quot; remote_serial_no: \u0026quot;{{remote_serial_no}}\u0026quot; state: 'present' srdf_mode: 'Synchronous' wait_for_completion: True - name: Create storagegroup Metro SRDF pair with Witness for resiliency dellemc_powermax_srdf: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; universion: \u0026quot;{{universion}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; serial_no: \u0026quot;{{serial_no}}\u0026quot; sg_name: \u0026quot;{{sg_name}}\u0026quot; remote_serial_no: \u0026quot;{{remote_serial_no}}\u0026quot; state: 'present' srdf_mode: 'Active' wait_for_completion: True srdf_state: 'Establish' - name: Suspend storagegroup Metro SRDF pair dellemc_powermax_srdf: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; universion: \u0026quot;{{universion}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; serial_no: \u0026quot;{{serial_no}}\u0026quot; sg_name: \u0026quot;{{sg_name}}\u0026quot; remote_serial_no: \u0026quot;{{remote_serial_no}}\u0026quot; state: 'present' srdf_state: 'Suspend' - name: Establish link for storagegroup Metro SRDF pair and use Bias for resiliency dellemc_powermax_srdf: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; universion: \u0026quot;{{universion}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; serial_no: \u0026quot;{{serial_no}}\u0026quot; sg_name: \u0026quot;{{sg_name}}\u0026quot; remote_serial_no: \u0026quot;{{remote_serial_no}}\u0026quot; state: 'present' wait_for_completion: False srdf_state: 'Establish' witness: False - name: Get SRDF details dellemc_powermax_srdf: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; universion: \u0026quot;{{universion}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; serial_no: \u0026quot;{{serial_no}}\u0026quot; sg_name: \u0026quot;{{sg_name}}\u0026quot; state: 'present' - name: Modify SRDF mode dellemc_powermax_srdf: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; universion: \u0026quot;{{universion}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; serial_no: \u0026quot;{{serial_no}}\u0026quot; sg_name: \u0026quot;{{sg_name}}\u0026quot; srdf_mode: 'Synchronous' state: 'present' - name: Failover SRDF link dellemc_powermax_srdf: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; universion: \u0026quot;{{universion}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; serial_no: \u0026quot;{{serial_no}}\u0026quot; sg_name: \u0026quot;{{sg_name}}\u0026quot; srdf_state: 'Failover' state: 'present' - name: Get SRDF Job status dellemc_powermax_srdf: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; universion: \u0026quot;{{universion}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; serial_no: \u0026quot;{{serial_no}}\u0026quot; job_id: \u0026quot;{{Job_details_body.Job_details.jobId}}\u0026quot; state: 'present' - name: Establish SRDF link dellemc_powermax_srdf: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; universion: \u0026quot;{{universion}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; serial_no: \u0026quot;{{serial_no}}\u0026quot; sg_name: \u0026quot;{{sg_name2}}\u0026quot; srdf_state: 'Establish' state: 'present' - name: Suspend SRDF link dellemc_powermax_srdf: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; universion: \u0026quot;{{universion}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; serial_no: \u0026quot;{{serial_no}}\u0026quot; sg_name: \u0026quot;{{sg_name2}}\u0026quot; srdf_state: 'Suspend' state: 'present' - name: Delete SRDF link dellemc_powermax_srdf: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; universion: \u0026quot;{{universion}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; serial_no: \u0026quot;{{serial_no}}\u0026quot; sg_name: \u0026quot;{{sg_name}}\u0026quot; state: 'absent' Return Values The following are the fields unique to this module:\n Key Returned Description    changed  type=boolean   always  Whether or not the resource has changed.      Job_details  type=list elements=string   When job exist.  Details of the job.     \u0026nbsp;  completed_date_milliseconds  type=integer   success  Date of job completion in milliseconds.     \u0026nbsp;  jobId  type=string   success  Unique identifier of the job.     \u0026nbsp;  last_modified_date  type=string   success  Last modified date of job.     \u0026nbsp;  last_modified_date_milliseconds  type=integer   success  Last modified date of job in milliseconds.     \u0026nbsp;  name  type=string   success  Name of the job.     \u0026nbsp;  resourceLink  type=string   success  Resource link w.r.t Unisphere.     \u0026nbsp;  result  type=string   success  Job description     \u0026nbsp;  status  type=string   success  Status of the job.     \u0026nbsp;  task  type=list elements=string   success  Details about the job.     \u0026nbsp;  username  type=string   success  Unisphere username.      SRDF_link_details  complex   When SRDF link exists.  Details of the SRDF link.     \u0026nbsp;  hop2Modes  type=string   success  SRDF hop2 mode.     \u0026nbsp;  hop2Rdfgs  type=string   success  Hop2 RDF group number.     \u0026nbsp;  hop2States  type=string   success  SRDF hop2 state.     \u0026nbsp;  largerRdfSides  type=string   success  Larger volume side of the link.     \u0026nbsp;  localR1InvalidTracksHop1  type=integer   success  Number of invalid R1 tracks on local volume.     \u0026nbsp;  localR2InvalidTracksHop1  type=integer   success  Number of invalid R2 tracks on local volume.     \u0026nbsp;  modes  type=string   success  Mode of the SRDF pair.     \u0026nbsp;  rdfGroupNumber  type=integer   success  RDF group number of the pair.     \u0026nbsp;  remoteR1InvalidTracksHop1  type=integer   success  Number of invalid R1 tracks on remote volume.     \u0026nbsp;  remoteR2InvalidTracksHop1  type=integer   success  Number of invalid R2 tracks on remote volume.     \u0026nbsp;  remoteSymmetrix  type=string   success  Remote symmetrix ID.     \u0026nbsp;  states  type=string   success  State of the SRDF pair.     \u0026nbsp;  storageGroupName  type=string   success  Name of storage group that is SRDF protected.     \u0026nbsp;  symmetrixId  type=string   success  Primary symmetrix ID.     \u0026nbsp;  totalTracks  type=integer   success  Total number of tracks in the volume.     \u0026nbsp;  volumeRdfTypes  type=string   success  RDF type of volume.     Authors  Manisha Agrawal (@agrawm3) \u0026lt;ansible.team@dell.com\u0026gt; Rajshree Khare (@khareRajshree) \u0026lt;ansible.team@dell.com\u0026gt;  Storage Group Module Synopsis Managing storage groups on a PowerMax storage system includes listing the volumes of a storage group, creating a new storage group, deleting an existing storage group, adding existing volumes to an existing storage group, removing existing volumes from an existing storage group, creating new volumes in an existing storage group, modifying existing storage group attributes, adding child storage groups inside an existing storage group (parent), and removing a child storage group from an existing parent storage group.\nParameters  Parameter Choices/Defaults Comments    child_sg_state  type=string    Choices: present-in-group absent-in-group    Describes the state of CSG inside parent SG     child_storage_groups  type=list elements=string      This is a list of child storage groups     compression  type=boolean    Choices: no yes    compression on storage group. Compression parameter is ignored if service_level is not specified. Default is true.     new_sg_name  type=string      The new name of the storage group.     service_level  type=string      The Name of SLO.     sg_name  type=string required=true      The name of the storage group.     snapshot_policies  type=list elements=string      List of snapshot policy(s).     snapshot_policy_state  type=string    Choices: present-in-group absent-in-group    Describes the state of snapshot policy for an SG     srp  type=string      Name of the storage resource pool. This parameter is ignored if service_level is not specified. Default is to use whichever is the default SRP on the array.     state  type=string required=true    Choices: absent present    Define whether the storage group should exist or not.     vol_state  type=string    Choices: present-in-group absent-in-group    Describes the state of volumes inside the SG.     volumes  type=list elements=dictionary      This is a list of volumes. Each volume has four attributes- vol_name size cap_unit vol_id. Either the volume ID must be provided for existing volumes, or the name and size must be provided to add new volumes to SG. The unit is optional. vol_name - Represents the name of the volume size - Represents the volume size cap_unit - The unit in which size is represented. Default unit is GB. Choices are MB, GB, TB. vol_id - This is the volume ID    Examples - name: Get storage group details including volumes dellemc_powermax_storagegroup: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; universion: \u0026quot;{{universion}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; serial_no: \u0026quot;{{serial_no}}\u0026quot; sg_name: \u0026quot;ansible_sg\u0026quot; state: \u0026quot;present\u0026quot; - name: Create empty storage group dellemc_powermax_storagegroup: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; universion: \u0026quot;{{universion}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; serial_no: \u0026quot;{{serial_no}}\u0026quot; sg_name: \u0026quot;foo\u0026quot; service_level: \u0026quot;Diamond\u0026quot; srp: \u0026quot;SRP_1\u0026quot; compression: True state: \u0026quot;present\u0026quot; - name: Delete the storage Group dellemc_powermax_storagegroup: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; universion: \u0026quot;{{universion}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; serial_no: \u0026quot;{{serial_no}}\u0026quot; sg_name: \u0026quot;foo\u0026quot; state: \u0026quot;absent\u0026quot; - name: Adding existing volume(s) to existing SG dellemc_powermax_storagegroup: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; universion: \u0026quot;{{universion}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; serial_no: \u0026quot;{{serial_no}}\u0026quot; sg_name: \u0026quot;foo\u0026quot; state: \u0026quot;present\u0026quot; volumes: - vol_id: \u0026quot;00028\u0026quot; - vol_id: \u0026quot;00018\u0026quot; - vol_id: \u0026quot;00025\u0026quot; vol_state: \u0026quot;present-in-group\u0026quot; - name: Create new volumes for existing SG dellemc_powermax_storagegroup: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; universion: \u0026quot;{{universion}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; serial_no: \u0026quot;{{serial_no}}\u0026quot; sg_name: \u0026quot;foo\u0026quot; state: \u0026quot;present\u0026quot; volumes: - vol_name: \u0026quot;foo\u0026quot; size: 1 cap_unit: \u0026quot;GB\u0026quot; - vol_name: \u0026quot;bar\u0026quot; size: 1 cap_unit: \u0026quot;GB\u0026quot; vol_state: \u0026quot;present-in-group\u0026quot; - name: Remove volume(s) from existing SG dellemc_powermax_storagegroup: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; universion: \u0026quot;{{universion}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; serial_no: \u0026quot;{{serial_no}}\u0026quot; sg_name: \u0026quot;foo\u0026quot; state: \u0026quot;present\u0026quot; volumes: - vol_id: \u0026quot;00028\u0026quot; - vol_id: \u0026quot;00018\u0026quot; - vol_name: \u0026quot;ansible-vol\u0026quot; vol_state: \u0026quot;absent-in-group\u0026quot; - name: Adding child SG to parent SG dellemc_powermax_storagegroup: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; universion: \u0026quot;{{universion}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; serial_no: \u0026quot;{{serial_no}}\u0026quot; sg_name: \u0026quot;parent_sg\u0026quot; state: \u0026quot;present\u0026quot; child_storage_groups: - \u0026quot;pie\u0026quot; - \u0026quot;bar\u0026quot; child_sg_state: \u0026quot;present-in-group\u0026quot; - name: Removing child SG from parent SG dellemc_powermax_storagegroup: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; universion: \u0026quot;{{universion}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; serial_no: \u0026quot;{{serial_no}}\u0026quot; sg_name: \u0026quot;parent_sg\u0026quot; state: \u0026quot;present\u0026quot; child_storage_groups: - \u0026quot;pie\u0026quot; - \u0026quot;bar\u0026quot; child_sg_state: \u0026quot;absent-in-group\u0026quot; - name: Rename Storage Group dellemc_powermax_storagegroup: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; universion: \u0026quot;{{universion}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; serial_no: \u0026quot;{{serial_no}}\u0026quot; sg_name: \u0026quot;ansible_sg\u0026quot; new_sg_name: \u0026quot;ansible_sg_renamed\u0026quot; state: \u0026quot;present\u0026quot; - name: Create a storage group with snapshot policies dellemc_powermax_storagegroup: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; universion: \u0026quot;{{universion}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; serial_no: \u0026quot;{{serial_no}}\u0026quot; sg_name: \u0026quot;ansible_test_sg\u0026quot; service_level: \u0026quot;Diamond\u0026quot; srp: \u0026quot;SRP_1\u0026quot; compression: True snapshot_policies: - \u0026quot;10min_policy\u0026quot; - \u0026quot;30min_policy\u0026quot; snapshot_policy_state: \u0026quot;present-in-group\u0026quot; state: \u0026quot;present\u0026quot; - name: Add snapshot policy to a storage group dellemc_powermax_storagegroup: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; universion: \u0026quot;{{universion}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; serial_no: \u0026quot;{{serial_no}}\u0026quot; sg_name: \u0026quot;ansible_test_sg\u0026quot; snapshot_policies: - \u0026quot;15min_policy\u0026quot; snapshot_policy_state: \u0026quot;present-in-group\u0026quot; state: \u0026quot;present\u0026quot; - name: Remove snapshot policy from a storage group dellemc_powermax_storagegroup: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; universion: \u0026quot;{{universion}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; serial_no: \u0026quot;{{serial_no}}\u0026quot; sg_name: \u0026quot;ansible_test_sg\u0026quot; snapshot_policies: - \u0026quot;15min_policy\u0026quot; snapshot_policy_state: \u0026quot;absent-in-group\u0026quot; state: \u0026quot;present\u0026quot; Return Values The following are the fields unique to this module:\n Key Returned Description    add_child_sg  type=boolean   When value exists.  Sets to true when a child SG is added.      add_new_vols_to_sg  type=boolean   When value exists.  Sets to true when new volumes are added to the SG.      add_snapshot_policy_to_sg  type=boolean   When value exists.  Sets to true when snapshot policy(s) is added to SG.      add_vols_to_sg  type=boolean   When value exists.  Sets to true when existing volumes are added to the SG.      added_vols_details  type=list elements=string   When value exists.  Volume IDs of the volumes added.      changed  type=boolean   always  Whether or not the resource has changed.      create_sg  type=boolean   When value exists.  Sets to true when a new SG is created.      delete_sg  type=boolean   When value exists.  Sets to true when an SG is deleted.      modify_sg  type=boolean   When value exists.  Sets to true when an SG is modified.      remove_child_sg  type=boolean   When value exists.  Sets to true when a child SG is removed.      remove_snapshot_policy_to_sg  type=boolean   When value exists.  Sets to false when snapshot policy(s) is removed from SG.      remove_vols_from_sg  type=boolean   When value exists.  Sets to true when volumes are removed.      removed_vols_details  type=list elements=string   When value exists.  Volume IDs of the volumes removed.      rename_sg  type=boolean   When value exists.  Sets to true when an SG is renamed.      snapshot_policy_compliance_details  complex   When snapshot policy associated..  The compliance status of this storage group.     \u0026nbsp;  compliance  type=string   success  Compliance status     \u0026nbsp;  sl_compliance  complex   success  Compliance details     \u0026nbsp; \u0026nbsp;  compliance  type=string   success  Compliance status     \u0026nbsp; \u0026nbsp;  sl_name  type=string   success  Name of the snapshot policy     \u0026nbsp;  sl_count  type=integer   success  Number of snapshot policies associated with storage group     \u0026nbsp;  storage_group_name  type=string   success  Name of the storage group      storage_group_details  complex   When storage group exists.  Details of the storage group.     \u0026nbsp;  base_slo_name  type=string   success  Base Service Level Objective (SLO) of a storage group.     \u0026nbsp;  cap_gb  type=integer   success  Storage group capacity in GB.     \u0026nbsp;  compression  type=boolean   success  Compression flag.     \u0026nbsp;  device_emulation  type=string   success  Device emulation type.     \u0026nbsp;  num_of_child_sgs  type=integer   success  Number of child storage groups.     \u0026nbsp;  num_of_masking_views  type=integer   success  Number of masking views associated with the storage group.     \u0026nbsp;  num_of_parent_sgs  type=integer   success  Number of parent storage groups.     \u0026nbsp;  num_of_snapshots  type=integer   success  Number of snapshots for the storage group.     \u0026nbsp;  num_of_vols  type=integer   success  Number of volumes in the storage group.     \u0026nbsp;  service_level  type=string   success  Type of service level.     \u0026nbsp;  slo  type=string   success  Service level objective (SLO) type.     \u0026nbsp;  slo_compliance  type=string   success  Type of SLO compliance.     \u0026nbsp;  srp  type=string   success  Storage resource pool.     \u0026nbsp;  storageGroupId  type=string   success  Id for the storage group.     \u0026nbsp;  type  type=string   success  type of storage group.     \u0026nbsp;  unprotected  type=boolean   success  Flag for storage group protection.     \u0026nbsp;  vp_saved_percent  type=integer   success  Percentage saved for virtual pools.      storage_group_volumes  type=list elements=string   When value exists.  Volume IDs of storage group volumes.      storage_group_volumes_details  complex   When storage group volumes exists.  Details of the storage group volumes.     \u0026nbsp;  effective_wwn  type=string   success  Effective WWN of the volume.     \u0026nbsp;  type  type=string   success  Type of the volume.     \u0026nbsp;  volume_identifier  type=string   success  Name associated with the volume.     \u0026nbsp;  volumeId  type=string   success  Unique ID of the volume.     \u0026nbsp;  wwn  type=string   success  WWN of the volume.     Authors  Vasudevu Lakhinana (@unknown) \u0026lt;ansible.team@dell.com\u0026gt; Prashant Rakheja (@prashant-dell) \u0026lt;ansible.team@dell.com\u0026gt; Ambuj Dubey (@AmbujDube) \u0026lt;ansible.team@dell.com\u0026gt;  Storage Pool Module Synopsis Managing storage pools on PowerMax storage system includes getting details of storage pools.\nParameters  Parameter Choices/Defaults Comments    pool  type=string required=true      The name of the storage pool.     state  type=string required=true    Choices: absent present    State variable to determine whether storage pool will exist or not.    Examples - name: Get specific storage pool details dellemc_powermax_storagepool: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; universion: \u0026quot;{{universion}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; serial_no: \u0026quot;{{serial_no}}\u0026quot; pool: \u0026quot;SRP_1\u0026quot; state: \u0026quot;present\u0026quot; Return Values The following are the fields unique to this module:\n Key Returned Description    changed  type=boolean   always  Whether or not the resource has changed.      pool_details  complex   When storage pool exist.  Details of the storage pool.     \u0026nbsp;  serial_no  type=string   success  The PowerMax array on which storage pool resides     \u0026nbsp;  service_levels  type=list elements=string   success  The service levels supported by storage pool     \u0026nbsp;  srp_capacity  complex   success  SRP capacity details     \u0026nbsp; \u0026nbsp;  effective_used_capacity_percent  type=integer   success  The effective used capacity, expressed as a percentage     \u0026nbsp; \u0026nbsp;  usable_total_tb  type=float   success  Usable capacity of the storage pool in TB     \u0026nbsp; \u0026nbsp;  usable_used_tb  type=float   success  Used capacity of the storage pool in TB     \u0026nbsp;  srp_efficiency  complex   success  SRP efficiency details     \u0026nbsp; \u0026nbsp;  compression_state  type=string   success  Indicates whether compression is enabled or disabled for this storage resource pool.     \u0026nbsp;  srpId  type=string   success  The ID of the storage pool     \u0026nbsp;  total_free_tb  type=string   success  Free capacity of the storage pool in TB     Authors  Akash Shendge (@shenda1) \u0026lt;ansible.team@dell.com\u0026gt;  Volume Module Synopsis Managing volumes on PowerMax storage system includes creating a volume, renaming a volume, expanding a volume, and deleting a volume.\nParameters  Parameter Choices/Defaults Comments    cap_unit  type=string    Choices: MB GB TB    volume capacity units If not specified, default value is GB.     new_name  type=string      The new volume identifier for the volume.     new_sg_name  type=string      The name of the target storage group.     sg_name  type=string      The name of the storage group.     size  type=float      The new size of existing volume. Required for create and expand volume operations.     state  type=string required=true    Choices: absent present    Defines whether the volume should exist or not.     vol_id  type=string      The native id of the volume. Required for rename and delete volume operations.     vol_name  type=string      The name of the volume.     vol_wwn  type=string      The WWN of the volume.    Notes  To expand a volume, either provide vol_id or vol_name or vol_wwn and sg_name. size is required to create/expand a volume. vol_id is required to rename/delete a volume. vol_name, sg_name and new_sg_name is required to move volumes between storage groups. Deletion of volume will fail if the storage group is part of a masking view.  Examples - name: Create volume dellemc_powermax_volume: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; universion: \u0026quot;{{universion}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; serial_no: \u0026quot;{{serial_no}}\u0026quot; vol_name: \u0026quot;{{vol_name}}\u0026quot; sg_name: \u0026quot;{{sg_name}}\u0026quot; size: 1 cap_unit: \u0026quot;{{cap_unit}}\u0026quot; state: 'present' - name: Expanding volume size dellemc_powermax_volume: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; universion: \u0026quot;{{universion}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; serial_no: \u0026quot;{{serial_no}}\u0026quot; size: 3 cap_unit: \u0026quot;{{cap_unit}}\u0026quot; vol_id: \u0026quot;0059B\u0026quot; state: 'present' - name: Renaming volume dellemc_powermax_volume: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; universion: \u0026quot;{{universion}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; serial_no: \u0026quot;{{serial_no}}\u0026quot; new_name: \u0026quot;Test_GOLD_vol_Renamed\u0026quot; vol_id: \u0026quot;0059B\u0026quot; state: 'present' - name: Delete volume using volume ID dellemc_powermax_volume: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; universion: \u0026quot;{{universion}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; serial_no: \u0026quot;{{serial_no}}\u0026quot; vol_id: \u0026quot;0059B\u0026quot; state: 'absent' - name: Delete volume using volume WWN dellemc_powermax_volume: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; universion: \u0026quot;{{universion}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; serial_no: \u0026quot;{{serial_no}}\u0026quot; vol_wwn: \u0026quot;60000970000197900237533030303246\u0026quot; state: 'absent' - name: Move volume between storage group dellemc_powermax_volume: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; universion: \u0026quot;{{universion}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; serial_no: \u0026quot;{{serial_no}}\u0026quot; vol_name: \u0026quot;{{vol_name}}\u0026quot; sg_name: \u0026quot;{{sg_name}}\u0026quot; new_sg_name: \u0026quot;{{new_sg_name}}\u0026quot; state: 'present' Return Values The following are the fields unique to this module:\n Key Returned Description    changed  type=boolean   always  Whether or not the resource has changed.      volume_details  complex   When volume exists.  Details of the volume.     \u0026nbsp;  allocated_percent  type=integer   success  Allocated percentage the volume.     \u0026nbsp;  cap_cyl  type=integer   success  Number of cylinders.     \u0026nbsp;  cap_gb  type=integer   success  Volume capacity in GB.     \u0026nbsp;  cap_mb  type=integer   success  Volume capacity in MB.     \u0026nbsp;  effective_wwn  type=string   success  Effective WWN of the volume.     \u0026nbsp;  emulation  type=string   success  Volume emulation type.     \u0026nbsp;  encapsulated  type=boolean   success  Flag for encapsulation.     \u0026nbsp;  has_effective_wwn  type=string   success  Flag for effective WWN presence.     \u0026nbsp;  mobility_id_enabled  type=boolean   success  Flag for enabling mobility.     \u0026nbsp;  num_of_front_end_paths  type=integer   success  Number of front end paths in the volume.     \u0026nbsp;  num_of_storage_groups  type=integer   success  Number of storage groups in which volume is present.     \u0026nbsp;  pinned  type=boolean   success  Pinned flag.     \u0026nbsp;  rdfGroupId  type=integer   success  RDFG number for volume.     \u0026nbsp;  reserved  type=boolean   success  Reserved flag.     \u0026nbsp;  snapvx_source  type=boolean   success  Source SnapVX flag.     \u0026nbsp;  snapvx_target  type=boolean   success  Target SnapVX flag.     \u0026nbsp;  ssid  type=string   success  SSID of the volume.     \u0026nbsp;  status  type=string   success  Volume status.     \u0026nbsp;  storage_groups  type=list elements=string   success  List of storage groups for the volume.     \u0026nbsp;  storageGroupId  type=string   success  Storage group ID of the volume.     \u0026nbsp;  type  type=string   success  Type of the volume.     \u0026nbsp;  volume_identifier  type=string   success  Name identifier for the volume.     \u0026nbsp;  volumeId  type=string   success  Unique ID of the volume.     \u0026nbsp;  wwn  type=string   success  WWN of the volume.     Authors  Vasudevu Lakhinana (@unknown) \u0026lt;ansible.team@dell.com\u0026gt; Akash Shendge (@shenda1) \u0026lt;ansible.team@dell.com\u0026gt; Ambuj Dubey (@AmbujDube) \u0026lt;ansible.team@dell.com\u0026gt;  Process Storage Pool Dict Module Synopsis Process storage pools on PowerMax/VMAX storage system to find out the storage pool with maximum free storage\nParameters  Parameter Choices/Defaults Comments    pool_data  type=list elements=dictionary required=true      Storage pool details including service levels, usable total space, usable free space, total free space.     service_level  type=string      Service level of the storage group     sg_name  type=string      Name of the storage group     size  type=float required=true      Size of the storage group in GB    Examples - name: Get best suitable Pool using our python sorting module register: assigned_pool process_storage_pool_dict: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; pool_data: \u0026quot;{{ pools_list }}\u0026quot; size: 40 service_level: \u0026quot;Diamond\u0026quot; sg_name: \u0026quot;intellgent_provisioning\u0026quot; Return Values The following are the fields unique to this module:\n Key Returned Description    all_pools  type=list elements=string   when pool exists  List of all pools on unisphere     \u0026nbsp;  serial_no  type=string   when array satisfies the given criteria  The PowerMax array on which storage pool resides     \u0026nbsp;  storage_pool  type=string   when storage pool exists satisfying the given criteria  The ID of the storage pool      changed  type=boolean   always  Whether or not the resource has changed.      serial_no  type=string   when array satisfies the given criteria  The PowerMax array on which storage pool resides      storage_group  type=string   when storage group exists satisfying the given criteria  Name of the storage group      storage_pool  type=string   when storage pool exists satisfying the given criteria  The ID of the storage pool     Authors  Akash Shendge (@shenda1) \u0026lt;ansible.team@dell.com\u0026gt;  ","excerpt":"Ansible Modules for Dell EMC PowerMax Product Guide 1.5.0  © 2021 Dell Inc. or its subsidiaries. All …","ref":"/ansible-docs/docs/server/platforms/powermax/product-guide/","title":"PowerMax Product Guide"},{"body":"Ansible Modules for Dell EMC PowerMax Release Notes 1.5.0  © 2021 Dell Inc. or its subsidiaries. All rights reserved. Dell EMC, and other trademarks are trademarks of Dell Inc. or its subsidiaries. Other trademarks may be trademarks of their respective owners.\n Contents These release notes contain supplemental information about Ansible Modules for Dell EMC PowerMax.\n Revision History Product Description New Features \u0026amp; Enhancements Known issues Limitations Distribution Documentation  Revision History    Date Document revision Description of changes     May 2021 01 Ansible Modules for Dell EMC PowerMax release 1.5.0    Product Description The Ansible Modules for Dell EMC PowerMax are used for managing volumes, storage groups, ports, port groups, host, host groups, masking views, SRDF links, RDF groups, snapshots, job, snapshot policies, storage pools, role for automatic volume provisioning and Metro DR environments for PowerMax arrays. The modules use playbooks to list, show, create, delete, and modify each of the entities.\nThe Ansible Modules for Dell EMC PowerMax supports the following features:\n Create volumes, storage groups, hosts, host groups, port groups, masking views, Metro DR environments, snapshot policies, and snapshots of a storage group. Modify volumes, storage groups, hosts, host groups, Metro DR environments, snapshot policies, and port groups in the array. Delete volumes, storage groups, hosts, host groups, port groups, masking views, Metro DR environments, snapshot policies, and snapshots of a storage group. Get details of volumes, storage groups, hosts, host groups, port, port groups, masking views, Metro DR environments, Job, RDF groups, snapshot policies, storage pools, and snapshots of a storage group.  New Features \u0026amp; Enhancements The Ansible Modules for Dell EMC PowerMax release 1.5.0 supports the following features:\n The Snapshot policy module supports the following functionalities:  Create a snapshot policy. Get details of any specific snapshot policy. Modify the snapshot policy attributes. Delete a snapshot policy.  NOTE: Supports PyU4V 9.2.1.3 and above.\n    The storage pool module supports the following functionality:  Get storage pool details for a given storage pool.   The following enhancements have been made to the gatherfacts module:  Get list of snapshot policies present on the PowerMax array.  NOTE: Supports PyU4V 9.2.1.3 and above for getting snapshot policy details and PyU4V 9.2.0.8 and above for getting snapshot details.\n    The following enhancements have been made to the storage group module:  Snapshot policy can be associated/disassociated to/from a storage group.  NOTE: Supports PyU4V 9.2.1.3 and above.\n    The following enhancements have been made to the snapshot module:  New parameter \u0026lsquo;snapshot_id\u0026rsquo; has been added which indicates unique ID of snapshot. snapshot_id is required for link, unlink, rename and delete operations.It is optional for getting details of snapshot.  NOTE: Supports PyU4V 9.2.0.8 and above.\n    Following functionalities are available for ansible role for automatic volume placement:  Finding if there is enough capacity of the given service level in any array. If multiple arrays available, return which is least used as \u0026lsquo;assigned_pool\u0026rsquo;. assigned_pool includes:  serial_no srp_id sg_name (if passed)     The following enhancements have been made to the host module:  Check mode feature of ansible is enabled for host module.   The following enhancements have been made to the host group module:  Check mode feature of ansible is enabled for host group module.   The following enhancements have been made to the volume module:  Check mode feature of ansible is enabled for volume module.   Support for Unisphere 9.1 and above Support for Python version 2.8 and above Support for PyU4V python library version 9.1.2.0 and above   NOTE: Unisphere Version 9.1 is compatible with PowerMax Python library version 9.1.x.x and similarly Unisphere versions later than 9.1 will only work with Python library versions later than 9.1.x.x.\n Known issues   Modify state operation from Establish to Suspend in Adaptive Copy mode in presence of force flag is not implemented. The REST API does not support this hence Python SDK (PyU4V) has no support for this operation.\n  Task to link a snapshot to a target storage group which is already linked is not implemented. The REST API does not support this hence Python SDK (PyU4V) has no support for this operation.\n  Limitations There are no known limitations.\nDistribution The software package is available for download from the Ansible Modules for PowerMax GitHub page.\nDocumentation The documentation is available on Ansible Modules for PowerMax GitHub page. It includes the following:\n README Release Notes (this document) Product Guide  ","excerpt":"Ansible Modules for Dell EMC PowerMax Release Notes 1.5.0  © 2021 Dell Inc. or its subsidiaries. All …","ref":"/ansible-docs/docs/server/platforms/powermax/release-notes/","title":"PowerMax Release Notes"},{"body":"","excerpt":"","ref":"/ansible-docs/docs/server/platforms/powerscale/","title":"PowerScale"},{"body":"","excerpt":"","ref":"/ansible-docs/docs/storage/platforms/powerscale/","title":"PowerScale"},{"body":"You can upgrade the CSI Driver for Dell EMC Unity using Helm or Dell CSI Operator.\nUpgrade Driver from version v1.4.0 to v1.5.0 Steps\n  Verify that all pre-requisites to install CSI Driver for Dell EMC PowerScale v1.5.0 are fulfilled (including change in secret formats).\n 1.1 Delete the existing secrets (isilon-creds and isilon-certs) 1.2 Create new secrets (isilon-creds and isilon-certs-0) in the format specified by csi-powerscale 1.5.  Refer Installation section here.\n  Clone the repository https://github.com/dell/csi-powerscale , copy the helm/csi-isilon/values.yaml into a new location with name say my-isilon-settings.yaml, to customize settings for installation. Edit my-isilon-settings.yaml as per the requirements.\n  Change to directory dell-csi-helm-installer to install the Dell EMC PowerScale cd dell-csi-helm-installer\n  Upgrade the CSI Driver for Dell EMC PowerScale v1.5.0 using following command:\n./csi-install.sh --namespace isilon --values ./my-isilon-settings.yaml --upgrade\n  Upgrade using Dell CSI Operator: To upgrade the driver from csi-PowerScale v1.4 to csi-PowerScale v1.5 (OpenShift 4.6) :\n Clone operator version 1.3.0 Execute bash scripts/install.sh --upgrade .This command will install latest version of operator. Uninstall the existing driver by executing the command kubectl delete -f \u0026lt;driver.yaml\u0026gt; with appropriate yaml file used for csi-powerscale 1.4 installation. Delete the existing secrets (both isilon-creds and isilon-certs) Create new isilon-creds secret in the latest csi-PowerScale format. For additional information, refer here Create new isilon-certs secret. Make sure the name of new secret is isilon-certs-0. For additional information, refer here Furnish the sample CR yaml according to your environment. Install csi-PowerScale driver 1.5 by executing the following command: kubectl create -f \u0026lt;furnished-cr.yaml\u0026gt;  The above said steps are for Operator which was deployed in non-olm way.\nFor additional information, refer Dell CSI Operator GitHub page.\n","excerpt":"You can upgrade the CSI Driver for Dell EMC Unity using Helm or Dell CSI Operator.\nUpgrade Driver …","ref":"/ansible-docs/v1/upgradation/drivers/isilon/","title":"PowerScale"},{"body":"You can upgrade the CSI Driver for Dell EMC Unity using Helm or Dell CSI Operator.\nUpgrade Driver from version v1.3.0/v1.3.0.1 to v1.4.0 Steps\n  Verify that all pre-requisites to install CSI Driver for Dell EMC PowerScale v1.4.0 are fulfilled.\n  Clone the repository https://github.com/dell/csi-powerscale , copy the helm/csi-isilon/values.yaml into a new location with name say my-isilon-settings.yaml, to customize settings for installation. Edit my-isilon-settings.yaml as per the requirements.\n  Change to directory dell-csi-helm-installer to install the Dell EMC PowerScale cd dell-csi-helm-installer\n  Upgrade the CSI Driver for Dell EMC PowerScale v1.4.0 using following command:\n./csi-install.sh --namespace isilon --values ./my-isilon-settings.yaml --upgrade\n  Upgrade using Dell CSI Operator: Follow the instructions for upgrade on the Dell CSI Operator GitHub page.\n","excerpt":"You can upgrade the CSI Driver for Dell EMC Unity using Helm or Dell CSI Operator.\nUpgrade Driver …","ref":"/ansible-docs/v2/upgradation/drivers/isilon/","title":"PowerScale"},{"body":"Ansible Modules for Dell EMC PowerScale Product Guide 1.2.0 © 2021 Dell Inc. or its subsidiaries. All rights reserved. Dell, EMC, and other trademarks are trademarks of Dell Inc. or its subsidiaries. Other trademarks may be trademarks of their respective owners.\n Contents  LDAP Module  Synopsis Parameters Notes Examples Return Values Authors   Smart Quota Module  Synopsis Parameters Notes Examples Return Values Authors   Active Directory Module  Synopsis Parameters Examples Return Values Authors   Snapshot Schedule Module  Synopsis Parameters Examples Return Values Authors   Users Module  Synopsis Parameters Examples Return Values Authors   SMB Module  Synopsis Parameters Examples Return Values Authors   Snapshot Module  Synopsis Parameters Examples Return Values Authors   Access Zone Module  Synopsis Parameters Notes Examples Return Values Authors   NFS Module  Synopsis Parameters Examples Return Values Authors   Groups Module  Synopsis Parameters Examples Return Values Authors   File System Module  Synopsis Parameters Examples Return Values Authors     LDAP Module Manage LDAP authentication provider on PowerScale\nSynopsis Managing LDAP authentication provider on PowerScale storage system includes creating, modifying, deleting and retrieving details of LDAP provider.\nParameters   Parameter Type Required Default Choices Description   ldap_name  str   True     Specifies the name of the LDAP provider.    server_uris  list elements: str      Specifies the server URIs. This parameter is mandatory during create. Server_uris should begin with ldap:// or ldaps:// if not validation error will be displayed.    server_uri_state  str      present-in-ldap absent-in-ldap   Specifies if the server_uris need to be added or removed from the provider. This parameter is mandatory if server_uris is specified. While creating LDAP provider, this parameter value should be specified as 'present-in-ldap'.    base_dn  str      Specifies the root of the tree in which to search identities. This parameter is mandatory during create.    ldap_parameters  dict      Specify additional parameters to configure LDAP domain.    \u0026nbsp; groupnet   str      Groupnet identifier. This is an optional parameter and defaults to groupnet0.    \u0026nbsp; bind_dn   str      Specifies the distinguished name for binding to the LDAP server.    \u0026nbsp; bind_password   str      Specifies the password for the distinguished name for binding to the LDAP server.    state  str   True     absent present   The state of the LDAP provider after the task is performed. present - indicates that the LDAP provider should exist on the system. absent - indicates that the LDAP provider should not exist on the system.    onefs_host  str   True     IP address or FQDN of the PowerScale cluster.    port_no  str    8080    Port number of the PowerScale cluster.It defaults to 8080 if not specified.    verify_ssl  bool   True     True False   boolean variable to specify whether to validate SSL certificate or not. True - indicates that the SSL certificate should be verified. False - indicates that the SSL certificate should not be verified.    api_user  str   True     username of the PowerScale cluster.    api_password  str   True     the password of the PowerScale cluster.    Notes  This module does not support modification of bind_password of LDAP provider. The value specified for bind_password will be ignored during modify.  Examples - name: Add an LDAP provider dellemc_powerscale_ldap: onefs_host: \u0026quot;{{onefs_host}}\u0026quot; api_user: \u0026quot;{{api_user}}\u0026quot; api_password: \u0026quot;{{api_password}}\u0026quot; verify_ssl: \u0026quot;{{verify_ssl}}\u0026quot; ldap_name: \u0026quot;ldap_test\u0026quot; server_uris: - \u0026quot;{{server_uri_1}}\u0026quot; - \u0026quot;{{server_uri_2}}\u0026quot; server_uri_state: 'present-in-ldap' base_dn: \u0026quot;DC=ansildap,DC=com\u0026quot; ldap_parameters: groupnet: \u0026quot;groupnet_ansildap\u0026quot; bind_dn: \u0026quot;cn=admin,dc=example,dc=com\u0026quot; bind_password: \u0026quot;{{bind_password}}\u0026quot; state: \u0026quot;present\u0026quot; - name: Add server_uris to an LDAP provider dellemc_powerscale_ldap: onefs_host: \u0026quot;{{onefs_host}}\u0026quot; api_user: \u0026quot;{{api_user}}\u0026quot; api_password: \u0026quot;{{api_password}}\u0026quot; verify_ssl: \u0026quot;{{verify_ssl}}\u0026quot; ldap_name: \u0026quot;ldap_test\u0026quot; server_uris: - \u0026quot;{{server_uri_1}}\u0026quot; server_uri_state: \u0026quot;present-in-ldap\u0026quot; state: \u0026quot;present\u0026quot; - name: Remove server_uris from an LDAP provider dellemc_powerscale_ldap: onefs_host: \u0026quot;{{onefs_host}}\u0026quot; api_user: \u0026quot;{{api_user}}\u0026quot; api_password: \u0026quot;{{api_password}}\u0026quot; verify_ssl: \u0026quot;{{verify_ssl}}\u0026quot; ldap_name: \u0026quot;ldap_test\u0026quot; server_uris: - \u0026quot;{{server_uri_1}}\u0026quot; server_uri_state: \u0026quot;absent-in-ldap\u0026quot; state: \u0026quot;present\u0026quot; - name: Modify LDAP provider dellemc_powerscale_ldap: onefs_host: \u0026quot;{{onefs_host}}\u0026quot; api_user: \u0026quot;{{api_user}}\u0026quot; api_password: \u0026quot;{{api_password}}\u0026quot; verify_ssl: \u0026quot;{{verify_ssl}}\u0026quot; ldap_name: \u0026quot;ldap_test\u0026quot; base_dn: \u0026quot;DC=ansi_ldap,DC=com\u0026quot; ldap_parameters: bind_dn: \u0026quot;cn=admin,dc=test,dc=com\u0026quot; state: \u0026quot;present\u0026quot; - name: Get LDAP provider details dellemc_powerscale_ldap: onefs_host: \u0026quot;{{onefs_host}}\u0026quot; api_user: \u0026quot;{{api_user}}\u0026quot; api_password: \u0026quot;{{api_password}}\u0026quot; verify_ssl: \u0026quot;{{verify_ssl}}\u0026quot; ldap_name: \u0026quot;ldap_test\u0026quot; state: \u0026quot;present\u0026quot; - name: Delete a LDAP provider dellemc_powerscale_ldap: onefs_host: \u0026quot;{{onefs_host}}\u0026quot; api_user: \u0026quot;{{api_user}}\u0026quot; api_password: \u0026quot;{{api_password}}\u0026quot; verify_ssl: \u0026quot;{{verify_ssl}}\u0026quot; ldap_name: \u0026quot;ldap_test\u0026quot; state: \u0026quot;absent\u0026quot; Return Values   Key Type Returned Description   changed   bool   always   Whether or not the resource has changed    ldap_provider_details   complex   When LDAP provider exists   The LDAP provider details    \u0026nbsp; base_dn   str  success  Specifies the root of the tree in which to search identities.    \u0026nbsp; bind_dn   str  success  Specifies the distinguished name for binding to the LDAP server.    \u0026nbsp; groupnet   str  success  Groupnet identifier.    \u0026nbsp; linked_access_zones   list  success  List of access zones linked to the authentication provider.    \u0026nbsp; name   str  success  Specifies the name of the LDAP provider.    \u0026nbsp; server_uris   str  success  Specifies the server URIs.    \u0026nbsp; status   str  success  Specifies the status of the provider.    Authors  Jennifer John (@johnj9) ansible.team@dell.com   Smart Quota Module Manage Smart Quotas on PowerScale\nSynopsis Manages Smart Quotas on a PowerScale storage system. This includes getting details, modifying, creating and deleting Smart Quotas.\nParameters   Parameter Type Required Default Choices Description   path  str   True     The path on which the quota will be imposed. For system access zone, the path is absolute. For all other access zones, the path is a relative path from the base of the access zone.    quota_type  str   True     user group directory default-user default-group   The type of quota which will be imposed on the path.    user_name  str      The name of the user account for which quota operations will be performed.    group_name  str      The name of the group for which quota operations will be performed.    access_zone  str    system    This option mentions the zone in which the user/group exists. For a non-system access zone, the path relative to the non-system Access Zone's base directory has to be given. For a system access zone, the absolute path has to be given.    provider_type  str    local    local file ldap ads   This option defines the type which is used to authenticate the user/group. If the provider_type is 'ads' then the domain name of the Active Directory Server has to be mentioned in the user_name. The format for the user_name should be 'DOMAIN_NAME\\user_name' or \"DOMAIN_NAME\\\\user_name\". This option acts as a filter for all operations except creation.    quota  dict   True     Specifies Smart Quota parameters.    \u0026nbsp; include_snapshots   bool    False    Whether to include the snapshots in the quota or not.    \u0026nbsp; include_overheads   bool      Whether to include the data protection overheads in the quota or not. If not passed during quota creation then quota will be created excluding the overheads. This parameter is supported for SDK 8.1.1    \u0026nbsp; thresholds_on   str      app_logical_size fs_logical_size physical_size   For SDK 9.0.0 the parameter include_overheads is deprecated and thresholds_on is used.    \u0026nbsp; advisory_limit_size   int      The threshold value after which the advisory notification will be sent.    \u0026nbsp; soft_limit_size   int      Threshold value after which the soft limit exceeded notification will be sent and the soft_grace period will start. Write access will be restricted after the grace period expires. Both soft_grace_period and soft_limit_size are required to modify soft threshold for the quota.    \u0026nbsp; soft_grace_period   int      Grace Period after the soft limit for quota is exceeded. After the grace period, the write access to the quota will be restricted. Both soft_grace_period and soft_limit_size are required to modify soft threshold for the quota.    \u0026nbsp; period_unit   str      days weeks months   Unit of the time period for soft_grace_period. For months the number of days is assumed to be 30 days. This parameter is required only if the soft_grace_period, is specified.    \u0026nbsp; hard_limit_size   int      Threshold value after which a hard limit exceeded notification will be sent. Write access will be restricted after the hard limit is exceeded.    \u0026nbsp; cap_unit   str      GB TB   Unit of storage for the hard, soft and advisory limits. This parameter is required if any of the hard, soft or advisory limits is specified.    state  str   True     absent present   Define whether the Smart Quota should exist or not. present - indicates that the Smart Quota should exist on the system. absent - indicates that the Smart Quota should not exist on the system.    onefs_host  str   True     IP address or FQDN of the PowerScale cluster.    port_no  str    8080    Port number of the PowerScale cluster.It defaults to 8080 if not specified.    verify_ssl  bool   True     True False   boolean variable to specify whether to validate SSL certificate or not. True - indicates that the SSL certificate should be verified. False - indicates that the SSL certificate should not be verified.    api_user  str   True     username of the PowerScale cluster.    api_password  str   True     the password of the PowerScale cluster.    Notes  To perform any operation, path, quota_type and state are mandatory parameters. There can be two quotas for each type per directory, one with snapshots included and one without snapshots included. Once the limits are assigned, then the quota can\u0026rsquo;t be converted to accounting. Only modification to the threshold limits is permitted.  Examples  - name: Create a Quota for a User excluding snapshot. dellemc_powerscale_smartquota: onefs_host: \u0026quot;{{onefs_host}}\u0026quot; verify_ssl: \u0026quot;{{verify_ssl}}\u0026quot; api_user: \u0026quot;{{api_user}}\u0026quot; api_password: \u0026quot;{{api_password}}\u0026quot; path: \u0026quot;/sample_az/sample_fs\u0026quot; quota_type: \u0026quot;user\u0026quot; user_name: \u0026quot;{{user_name}}\u0026quot; access_zone: \u0026quot;sample-zone\u0026quot; provider_type: \u0026quot;local\u0026quot; quota: include_overheads: False advisory_limit_size: \u0026quot;{{advisory_limit_size}}\u0026quot; soft_limit_size: \u0026quot;{{soft_limit_size}}\u0026quot; soft_grace_period: \u0026quot;{{soft_grace_period}}\u0026quot; period_unit: \u0026quot;{{period_unit}}\u0026quot; hard_limit_size: \u0026quot;{{hard_limit_size}}\u0026quot; cap_unit: \u0026quot;{{cap_unit}}\u0026quot; state: \u0026quot;present\u0026quot; - name: Create a Quota for a Directory for accounting includes snapshots and data protection overheads. dellemc_powerscale_smartquota: onefs_host: \u0026quot;{{onefs_host}}\u0026quot; verify_ssl: \u0026quot;{{verify_ssl}}\u0026quot; api_user: \u0026quot;{{api_user}}\u0026quot; api_password: \u0026quot;{{api_password}}\u0026quot; path: \u0026quot;/sample_az/sample_fs\u0026quot; quota_type: \u0026quot;directory\u0026quot; quota: include_snapshots: \u0026quot;True\u0026quot; include_overheads: True state: \u0026quot;present\u0026quot; - name: Create default-user Quota for a Directory with snaps and overheads dellemc_powerscale_smartquota: onefs_host: \u0026quot;{{onefs_host}}\u0026quot; verify_ssl: \u0026quot;{{verify_ssl}}\u0026quot; api_user: \u0026quot;{{api_user}}\u0026quot; api_password: \u0026quot;{{api_password}}\u0026quot; path: \u0026quot;/sample_az/sample_fs\u0026quot; quota_type: \u0026quot;default-user\u0026quot; quota: include_snapshots: \u0026quot;True\u0026quot; include_overheads: True state: \u0026quot;present\u0026quot; - name: Get a Quota Details for a Group dellemc_powerscale_smartquota: onefs_host: \u0026quot;{{onefs_host}}\u0026quot; verify_ssl: \u0026quot;{{verify_ssl}}\u0026quot; api_user: \u0026quot;{{api_user}}\u0026quot; api_password: \u0026quot;{{api_password}}\u0026quot; path: \u0026quot;/sample_az/sample_fs\u0026quot; quota_type: \u0026quot;group\u0026quot; group_name: \u0026quot;{{user_name}}\u0026quot; access_zone: \u0026quot;sample-zone\u0026quot; provider_type: \u0026quot;local\u0026quot; quota: include_snapshots: \u0026quot;True\u0026quot; state: \u0026quot;present\u0026quot; - name: Update Quota for a User dellemc_powerscale_smartquota: onefs_host: \u0026quot;{{onefs_host}}\u0026quot; verify_ssl: \u0026quot;{{verify_ssl}}\u0026quot; api_user: \u0026quot;{{api_user}}\u0026quot; api_password: \u0026quot;{{api_password}}\u0026quot; path: \u0026quot;/sample_az/sample_fs\u0026quot; quota_type: \u0026quot;user\u0026quot; user_name: \u0026quot;{{user_name}}\u0026quot; access_zone: \u0026quot;sample-zone\u0026quot; provider_type: \u0026quot;local\u0026quot; quota: include_snapshots: \u0026quot;True\u0026quot; include_overheads: True advisory_limit_size: \u0026quot;{{new_advisory_limit_size}}\u0026quot; hard_limit_size: \u0026quot;{{new_hard_limit_size}}\u0026quot; cap_unit: \u0026quot;{{cap_unit}}\u0026quot; state: \u0026quot;present\u0026quot; - name: Modify Soft Limit and Grace period of default-user Quota dellemc_powerscale_smartquota: onefs_host: \u0026quot;{{onefs_host}}\u0026quot; verify_ssl: \u0026quot;{{verify_ssl}}\u0026quot; api_user: \u0026quot;{{api_user}}\u0026quot; api_password: \u0026quot;{{api_password}}\u0026quot; path: \u0026quot;/sample_az/sample_fs\u0026quot; quota_type: \u0026quot;default-user\u0026quot; access_zone: \u0026quot;sample-zone\u0026quot; quota: include_snapshots: \u0026quot;True\u0026quot; include_overheads: True soft_limit_size: \u0026quot;{{soft_limit_size}}\u0026quot; cap_unit: \u0026quot;{{cap_unit}}\u0026quot; soft_grace_period: \u0026quot;{{soft_grace_period}}\u0026quot; period_unit: \u0026quot;{{period_unit}}\u0026quot; state: \u0026quot;present\u0026quot; - name: Delete a Quota for a Directory dellemc_powerscale_smartquota: onefs_host: \u0026quot;{{onefs_host}}\u0026quot; verify_ssl: \u0026quot;{{verify_ssl}}\u0026quot; api_user: \u0026quot;{{api_user}}\u0026quot; api_password: \u0026quot;{{api_password}}\u0026quot; path: \u0026quot;/sample_az/sample_fs\u0026quot; quota_type: \u0026quot;directory\u0026quot; quota: include_snapshots: \u0026quot;True\u0026quot; state: \u0026quot;absent\u0026quot; - name: Delete Quota for a default-group dellemc_powerscale_smartquota: onefs_host: \u0026quot;{{onefs_host}}\u0026quot; verify_ssl: \u0026quot;{{verify_ssl}}\u0026quot; api_user: \u0026quot;{{api_user}}\u0026quot; api_password: \u0026quot;{{api_password}}\u0026quot; path: \u0026quot;/sample_az/sample_fs\u0026quot; quota_type: \u0026quot;default-group\u0026quot; quota: include_snapshots: \u0026quot;True\u0026quot; state: \u0026quot;absent\u0026quot; Return Values   Key Type Returned Description   changed   bool   always   Whether or not the resource has changed    quota_details   complex   When Quota exists.   The quota details.    \u0026nbsp; enforced   bool  success  Whether the limits are enforced on Quota or not.    \u0026nbsp; id   str  success  The ID of the Quota.    \u0026nbsp; thresholds   dict  success  Includes information about all the limits imposed on quota. The limits are mentioned in bytes and soft_grace is in seconds.    \u0026nbsp; type   str  success  The type of Quota.    \u0026nbsp; usage   dict  success  The Quota usage.    Authors  P Srinivas Rao (@srinivas-rao5) ansible.team@dell.com   Active Directory Module Manages the ADS authentication provider on PowerScale\nSynopsis Manages the Active Directory authentication provider on the PowerScale storage system. This includes creating, modifying, deleting and retreiving the details of an ADS provider.\nParameters   Parameter Type Required Default Choices Description   domain_name  str      Specifies the domain name of an Active Directory provider. This parameter is mandatory during create.    instance_name  str      Specifies the instance name of Active Directory provider. This is an optional parameter during create, and defaults to the provider name if it is not specified during the create operation. get, modify and delete operations can also be performed through instance_name. It is mutually exclusive with domain_name for get, modify and delete operations.    ads_user  str      Specifies the user name that has permission to join a machine to the given domain. This parameter is mandatory during create.    ads_password  str      Specifies the password used during domain join. This parameter is mandatory during create.    ads_parameters  dict      Specify additional parameters to configure ADS domain.    \u0026nbsp; groupnet   str      Groupnet identifier. This is an optional parameter and defaults to groupnet0.    \u0026nbsp; home_directory_template   str      Specifies the path to the home directory template. This is an optional parameter and defaults to '/ifs/home/%D/%U'.    \u0026nbsp; login_shell   str      /bin/sh /bin/csh /bin/tcsh /bin/zsh /bin/bash /bin/rbash /sbin/nologin   Specifies the login shell path. This is an optional parameter and defaults to '/bin/zsh'.    state  str   True     absent present   The state of the ads provider after the task is performed. present - indicates that the ADS provider should exist on the system. absent - indicates that the ADS provider should not exist on the system.    onefs_host  str   True     IP address or FQDN of the PowerScale cluster.    port_no  str    8080    Port number of the PowerScale cluster.It defaults to 8080 if not specified.    verify_ssl  bool   True     True False   boolean variable to specify whether to validate SSL certificate or not. True - indicates that the SSL certificate should be verified. False - indicates that the SSL certificate should not be verified.    api_user  str   True     username of the PowerScale cluster.    api_password  str   True     the password of the PowerScale cluster.    Examples - name: Add an Active Directory provider dellemc_powerscale_ads: onefs_host: \u0026quot;{{onefs_host}}\u0026quot; api_user: \u0026quot;{{api_user}}\u0026quot; api_password: \u0026quot;{{api_password}}\u0026quot; verify_ssl: \u0026quot;{{verify_ssl}}\u0026quot; domain_name: \u0026quot;ansibleneo.com\u0026quot; instance_name: \u0026quot;ansibleneo.com\u0026quot; ads_user: \u0026quot;administrator\u0026quot; ads_password: \u0026quot;Password123!\u0026quot; ads_parameters: groupnet: \u0026quot;groupnet5\u0026quot; home_directory_template: \u0026quot;/ifs/home/%D/%U\u0026quot; login_shell: \u0026quot;/bin/zsh\u0026quot; state: \u0026quot;present\u0026quot; - name: Modify an Active Directory provider with domain name dellemc_powerscale_ads: onefs_host: \u0026quot;{{onefs_host}}\u0026quot; verify_ssl: \u0026quot;{{verify_ssl}}\u0026quot; api_user: \u0026quot;{{api_user}}\u0026quot; api_password: \u0026quot;{{api_password}}\u0026quot; domain_name: \u0026quot;ansibleneo.com\u0026quot; ads_parameters: home_directory_template: \u0026quot;/ifs/usr_home/%D/%U\u0026quot; login_shell: \u0026quot;/bin/rbash\u0026quot; state: \u0026quot;present\u0026quot; - name: Modify an Active Directory provider with instance name dellemc_powerscale_ads: onefs_host: \u0026quot;{{onefs_host}}\u0026quot; verify_ssl: \u0026quot;{{verify_ssl}}\u0026quot; api_user: \u0026quot;{{api_user}}\u0026quot; api_password: \u0026quot;{{api_password}}\u0026quot; instance_name: \u0026quot;ansibleneo.com\u0026quot; ads_parameters: home_directory_template: \u0026quot;/ifs/usr_home/%D/%U\u0026quot; login_shell: \u0026quot;/bin/rbash\u0026quot; state: \u0026quot;present\u0026quot; - name: Get Active Directory provider details with domain name dellemc_powerscale_ads: onefs_host: \u0026quot;{{onefs_host}}\u0026quot; api_user: \u0026quot;{{api_user}}\u0026quot; api_password: \u0026quot;{{api_password}}\u0026quot; verify_ssl: \u0026quot;{{verify_ssl}}\u0026quot; domain_name: \u0026quot;ansibleneo.com\u0026quot; state: \u0026quot;present\u0026quot; - name: Get Active Directory provider details with instance name dellemc_powerscale_ads: onefs_host: \u0026quot;{{onefs_host}}\u0026quot; api_user: \u0026quot;{{api_user}}\u0026quot; api_password: \u0026quot;{{api_password}}\u0026quot; verify_ssl: \u0026quot;{{verify_ssl}}\u0026quot; instance_name: \u0026quot;ansibleneo.com\u0026quot; state: \u0026quot;present\u0026quot; - name: Delete an Active Directory provider with domain name dellemc_powerscale_ads: onefs_host: \u0026quot;{{onefs_host}}\u0026quot; verify_ssl: \u0026quot;{{verify_ssl}}\u0026quot; api_user: \u0026quot;{{api_user}}\u0026quot; api_password: \u0026quot;{{api_password}}\u0026quot; domain_name: \u0026quot;ansibleneo.com\u0026quot; state: \u0026quot;absent\u0026quot; - name: Delete an Active Directory provider with instance name dellemc_powerscale_ads: onefs_host: \u0026quot;{{onefs_host}}\u0026quot; verify_ssl: \u0026quot;{{verify_ssl}}\u0026quot; api_user: \u0026quot;{{api_user}}\u0026quot; api_password: \u0026quot;{{api_password}}\u0026quot; instance_name: \u0026quot;ansibleneo.com\u0026quot; state: \u0026quot;absent\u0026quot; Return Values   Key Type Returned Description   ads_provider_details   complex   When Active Directory provider exists   The Active Directory provider details    \u0026nbsp; groupnet   str  success  Groupnet identifier.    \u0026nbsp; home_directory_template   str  success  Specifies the path to the home directory template.    \u0026nbsp; id   str  success  Specifies the ID of the Active Directory provider instance.    \u0026nbsp; linked_access_zones   list  success  List of access zones linked to the authentication provider.    \u0026nbsp; login_shell   str  success  Specifies the login shell path.    \u0026nbsp; name   str  success  Specifies the Active Directory provider name.    changed   bool   always   Whether or not the resource has changed    Authors  Jennifer John (@johnj9) ansible.team@dell.com   Snapshot Schedule Module Manage snapshot schedules on Dell EMC PowerScale.\nSynopsis You can perform the following operations Managing snapshot schedules on PowerScale. Create snapshot schedule. Modify snapshot schedule. Get details of snapshot schedule. Delete snapshot schedule.\nParameters   Parameter Type Required Default Choices Description   name  str   True     The name of the snapshot schedule.    path  str      The path on which the snapshot will be taken. This path is relative to the base path of the Access Zone. For 'System' access zone, the path is absolute. This parameter is required at the time of creation. Modification of the path is not allowed through the Ansible module.    access_zone  str    System    The effective path where the snapshot is created will be determined by the base path of the Access Zone and the path provided by the user in the playbook.    new_name  str      The new name of the snapshot schedule.    desired_retention  int      The number of hours/days for which snapshots created by this snapshot schedule should be retained. If retention is not specified at the time of creation, then the snapshots created by the snapshot schedule will be retained forever. Minimum retention duration is 2 hours. For large durations (beyond days/weeks), PowerScale may round off the retention to a somewhat larger value to match a whole number of days/weeks.    retention_unit  str    hours    hours days   The retention unit for the snapshot created by this schedule.    alias  str      The alias will point to the latest snapshot created by the snapshot schedule.    pattern  str      Pattern expanded with strftime to create snapshot names. This parameter is required at the time of creation.    schedule  str      The isidate compatible natural language description of the schedule. It specifies the frequency of the schedule. This parameter is required at the time of creation.    state  str   True     absent present   Defines whether the snapshot schedule should exist or not.    onefs_host  str   True     IP address or FQDN of the PowerScale cluster.    port_no  str    8080    Port number of the PowerScale cluster.It defaults to 8080 if not specified.    verify_ssl  bool   True     True False   boolean variable to specify whether to validate SSL certificate or not. True - indicates that the SSL certificate should be verified. False - indicates that the SSL certificate should not be verified.    api_user  str   True     username of the PowerScale cluster.    api_password  str   True     the password of the PowerScale cluster.    Examples - name: Create snapshot schedule dellemc_powerscale_snapshotschedule: onefs_host: \u0026quot;{{onefs_host}}\u0026quot; verify_ssl: \u0026quot;{{verify_ssl}}\u0026quot; api_user: \u0026quot;{{api_user}}\u0026quot; api_password: \u0026quot;{{api_password}}\u0026quot; name: \u0026quot;{{name}}\u0026quot; access_zone: '{{access_zone}}' path: '{{path1}}' alias: \u0026quot;{{alias1}}\u0026quot; desired_retention: \u0026quot;{{desired_retention1}}\u0026quot; pattern: \u0026quot;{{pattern1}}\u0026quot; schedule: \u0026quot;{{schedule1}}\u0026quot; state: \u0026quot;{{state_present}}\u0026quot; - name: Get details of snapshot schedule dellemc_powerscale_snapshotschedule: onefs_host: \u0026quot;{{onefs_host}}\u0026quot; verify_ssl: \u0026quot;{{verify_ssl}}\u0026quot; api_user: \u0026quot;{{api_user}}\u0026quot; api_password: \u0026quot;{{api_password}}\u0026quot; name: \u0026quot;{{name}}\u0026quot; state: \u0026quot;{{state_present}}\u0026quot; - name: Rename snapshot schedule dellemc_powerscale_snapshotschedule: onefs_host: \u0026quot;{{onefs_host}}\u0026quot; verify_ssl: \u0026quot;{{verify_ssl}}\u0026quot; api_user: \u0026quot;{{api_user}}\u0026quot; api_password: \u0026quot;{{api_password}}\u0026quot; name: \u0026quot;{{name}}\u0026quot; new_name: \u0026quot;{{new_name}}\u0026quot; state: \u0026quot;{{state_present}}\u0026quot; - name: Modify alias of snapshot schedule dellemc_powerscale_snapshotschedule: onefs_host: \u0026quot;{{onefs_host}}\u0026quot; verify_ssl: \u0026quot;{{verify_ssl}}\u0026quot; api_user: \u0026quot;{{api_user}}\u0026quot; api_password: \u0026quot;{{api_password}}\u0026quot; name: \u0026quot;{{new_name}}\u0026quot; alias: \u0026quot;{{alias2}}\u0026quot; state: \u0026quot;{{state_present}}\u0026quot; - name: Modify pattern of snapshot schedule dellemc_powerscale_snapshotschedule: onefs_host: \u0026quot;{{onefs_host}}\u0026quot; verify_ssl: \u0026quot;{{verify_ssl}}\u0026quot; api_user: \u0026quot;{{api_user}}\u0026quot; api_password: \u0026quot;{{api_password}}\u0026quot; name: \u0026quot;{{new_name}}\u0026quot; pattern: \u0026quot;{{pattern2}}\u0026quot; state: \u0026quot;{{state_present}}\u0026quot; - name: Modify schedule of snapshot schedule dellemc_powerscale_snapshotschedule: onefs_host: \u0026quot;{{onefs_host}}\u0026quot; verify_ssl: \u0026quot;{{verify_ssl}}\u0026quot; api_user: \u0026quot;{{api_user}}\u0026quot; api_password: \u0026quot;{{api_password}}\u0026quot; name: \u0026quot;{{new_name}}\u0026quot; schedule: \u0026quot;{{schedule2}}\u0026quot; state: \u0026quot;{{state_present}}\u0026quot; - name: Modify retention of snapshot schedule dellemc_powerscale_snapshotschedule: onefs_host: \u0026quot;{{onefs_host}}\u0026quot; verify_ssl: \u0026quot;{{verify_ssl}}\u0026quot; api_user: \u0026quot;{{api_user}}\u0026quot; api_password: \u0026quot;{{api_password}}\u0026quot; name: \u0026quot;{{new_name}}\u0026quot; desired_retention: 2 retention_unit: \u0026quot;{{retention_unit_days}}\u0026quot; state: \u0026quot;{{state_present}}\u0026quot; - name: Delete snapshot schedule dellemc_powerscale_snapshotschedule: onefs_host: \u0026quot;{{onefs_host}}\u0026quot; verify_ssl: \u0026quot;{{verify_ssl}}\u0026quot; api_user: \u0026quot;{{api_user}}\u0026quot; api_password: \u0026quot;{{api_password}}\u0026quot; name: \u0026quot;{{new_name}}\u0026quot; state: \u0026quot;{{state_absent}}\u0026quot; - name: Delete snapshot schedule - Idempotency dellemc_powerscale_snapshotschedule: onefs_host: \u0026quot;{{onefs_host}}\u0026quot; verify_ssl: \u0026quot;{{verify_ssl}}\u0026quot; api_user: \u0026quot;{{api_user}}\u0026quot; api_password: \u0026quot;{{api_password}}\u0026quot; name: \u0026quot;{{new_name}}\u0026quot; state: \u0026quot;{{state_absent}}\u0026quot; Return Values   Key Type Returned Description   changed   bool   always   Whether or not the resource has changed    snapshot_schedule_details   complex   When snapshot schedule exists   Details of the snapshot schedule including snapshot details    \u0026nbsp; schedules   complex  success  Details of snapshot schedule    \u0026nbsp; \u0026nbsp; duration   int  success  Time in seconds added to creation time to construction expiration time    \u0026nbsp; \u0026nbsp; id   int  success  The system ID given to the schedule    \u0026nbsp; \u0026nbsp; next_run   int  success  Unix Epoch time of next snapshot to be created    \u0026nbsp; \u0026nbsp; next_snapshot   str  success  Formatted name of next snapshot to be created    \u0026nbsp; snapshot_list   complex  success  List of snapshots taken by this schedule    \u0026nbsp; \u0026nbsp; snapshots   complex  success  Details of snapshot    \u0026nbsp; \u0026nbsp; \u0026nbsp; created   int  success  The Unix Epoch time the snapshot was created    \u0026nbsp; \u0026nbsp; \u0026nbsp; expires   int  success  The Unix Epoch time the snapshot will expire and be eligible for automatic deletion.    \u0026nbsp; \u0026nbsp; \u0026nbsp; id   int  success  The system ID given to the snapshot.This is useful for tracking the status of delete pending snapshots    \u0026nbsp; \u0026nbsp; \u0026nbsp; name   str  success  The user or system supplied snapshot name. This will be null for snapshots pending delete    \u0026nbsp; \u0026nbsp; \u0026nbsp; size   int  success  The amount of storage in bytes used to store this snapshot    \u0026nbsp; \u0026nbsp; total   int  success  Total number of items available    Authors  Akash Shendge (@shenda1) ansible.team@dell.com   Users Module Manage users on the PowerScale Storage System\nSynopsis Managing Users on the PowerScale Storage System includes create user, delete user, update user, get user, add role and remove role.\nParameters   Parameter Type Required Default Choices Description   user_name  str      The name of the user account. Required at the time of user creation, for rest of the operations either user_name or user_id is required.    user_id  str      The user_id is auto generated at the time of creation. For all other operations either user_name or user_id is needed.    password  str      The password for the user account. Required only in the creation of a user account. If given in other operations then the password will be ignored.    access_zone  str    system    This option mentions the zone in which a user is created. For creation, access_zone acts as an attribute for the user. For all other operations access_zone acts as a filter.    provider_type  str    local    local file ldap ads   This option defines the type which will be used to authenticate the user. Creation, Modification and Deletion is allowed for local users. Adding and removing roles is allowed for all users of the system access zone. Getting user details is allowed for all users. If the provider_type is 'ads' then domain name of the Active Directory Server has to be mentioned in the user_name. The format for the user_name should be 'DOMAIN_NAME\\user_name' or \"DOMAIN_NAME\\\\user_name\". This option acts as a filter for all operations except creation.    enabled  bool      Enabled is a bool variable which is used to enable or disable the user account.    primary_group  str      A user can be member of multiple groups of which one group has to be assigned as primary group. This group will be used for access checks and can also be used when creating files. A user can be added to the group using Group Name.    home_directory  str      The path specified in this option acts as a home directory for the user. The directory which is given should not be already in use. For a user in a system access zone, the absolute path has to be given. For users in a non-system access zone, the path relative to the non-system Access Zone's base directory has to be given.    shell  str      This option is for choosing the type of shell for the user account.    full_name  str      The additional information about the user can be provided using full_name option.    email  str      The email id of the user can be added using email option. The email id can be set at the time of creation and modified later.    state  str   True     absent present   The state option is used to mention the existence of the user account.    role_name  str      The name of the role which a user will be assigned. User can be added to multiple roles.    role_state  str      present-for-user absent-for-user   The role_state option is used to mention the existence of the role for a particular user. It is required when a role is added or removed from user.    onefs_host  str   True     IP address or FQDN of the PowerScale cluster.    port_no  str    8080    Port number of the PowerScale cluster.It defaults to 8080 if not specified.    verify_ssl  bool   True     True False   boolean variable to specify whether to validate SSL certificate or not. True - indicates that the SSL certificate should be verified. False - indicates that the SSL certificate should not be verified.    api_user  str   True     username of the PowerScale cluster.    api_password  str   True     the password of the PowerScale cluster.    Examples  - name: Get User Details using user name dellemc_powerscale_user: onefs_host: \u0026quot;{{onefs_host}}\u0026quot; port_no: \u0026quot;{{port_no}}\u0026quot; api_user: \u0026quot;{{api_user}}\u0026quot; api_password: \u0026quot;{{api_password}}\u0026quot; verify_ssl: \u0026quot;{{verify_ssl}}\u0026quot; access_zone: \u0026quot;{{access_zone}}\u0026quot; provider_type: \u0026quot;{{provider_type}}\u0026quot; user_name: \u0026quot;{{account_name}}\u0026quot; state: \u0026quot;present\u0026quot; - name: Create User dellemc_powerscale_user: onefs_host: \u0026quot;{{onefs_host}}\u0026quot; port_no: \u0026quot;{{port_no}}\u0026quot; api_user: \u0026quot;{{api_user}}\u0026quot; api_password: \u0026quot;{{api_password}}\u0026quot; verify_ssl: \u0026quot;{{verify_ssl}}\u0026quot; access_zone: \u0026quot;{{access_zone}}\u0026quot; provider_type: \u0026quot;{{provider_type}}\u0026quot; user_name: \u0026quot;{{account_name}}\u0026quot; password: \u0026quot;{{account_password}}\u0026quot; primary_group: \u0026quot;{{primary_group}}\u0026quot; enabled: \u0026quot;{{enabled}}\u0026quot; email: \u0026quot;{{email}}\u0026quot; full_name: \u0026quot;{{full_name}}\u0026quot; home_directory: \u0026quot;{{home_directory}}\u0026quot; shell: \u0026quot;{{shell}}\u0026quot; role_name: \u0026quot;{{role_name}}\u0026quot; role_state: \u0026quot;present-for-user\u0026quot; state: \u0026quot;present\u0026quot; - name: Update User's Full Name and email using user name dellemc_powerscale_user: onefs_host: \u0026quot;{{onefs_host}}\u0026quot; port_no: \u0026quot;{{port_no}}\u0026quot; api_user: \u0026quot;{{api_user}}\u0026quot; api_password: \u0026quot;{{api_password}}\u0026quot; verify_ssl: \u0026quot;{{verify_ssl}}\u0026quot; access_zone: \u0026quot;{{access_zone}}\u0026quot; provider_type: \u0026quot;{{provider_type}}\u0026quot; user_name: \u0026quot;{{account_name}}\u0026quot; email: \u0026quot;{{new_email}}\u0026quot; full_name: \u0026quot;{{full_name}}\u0026quot; state: \u0026quot;present\u0026quot; - name: Disable User Account using User Id dellemc_powerscale_user: onefs_host: \u0026quot;{{onefs_host}}\u0026quot; port_no: \u0026quot;{{port_no}}\u0026quot; api_user: \u0026quot;{{api_user}}\u0026quot; api_password: \u0026quot;{{api_password}}\u0026quot; verify_ssl: \u0026quot;{{verify_ssl}}\u0026quot; access_zone: \u0026quot;{{access_zone}}\u0026quot; provider_type: \u0026quot;{{provider_type}}\u0026quot; user_id: \u0026quot;{{id}}\u0026quot; enabled: \u0026quot;False\u0026quot; state: \u0026quot;present\u0026quot; - name: Add user to a role using Username dellemc_powerscale_user: onefs_host: \u0026quot;{{onefs_host}}\u0026quot; port_no: \u0026quot;{{port_no}}\u0026quot; api_user: \u0026quot;{{api_user}}\u0026quot; api_password: \u0026quot;{{api_password}}\u0026quot; verify_ssl: \u0026quot;{{verify_ssl}}\u0026quot; user_name: \u0026quot;{{account_name}}\u0026quot; provider_type: \u0026quot;{{provider_type}}\u0026quot; role_name: \u0026quot;{{role_name}}\u0026quot; role_state: \u0026quot;present-for-user\u0026quot; state: \u0026quot;present\u0026quot; - name: Remove user from a role using User id dellemc_powerscale_user: onefs_host: \u0026quot;{{onefs_host}}\u0026quot; port_no: \u0026quot;{{port_no}}\u0026quot; api_user: \u0026quot;{{api_user}}\u0026quot; api_password: \u0026quot;{{api_password}}\u0026quot; verify_ssl: \u0026quot;{{verify_ssl}}\u0026quot; user_id: \u0026quot;{{id}}\u0026quot; role_name: \u0026quot;{{role_name}}\u0026quot; role_state: \u0026quot;absent-for-user\u0026quot; state: \u0026quot;present\u0026quot; - name: Delete User using user name dellemc_powerscale_user: onefs_host: \u0026quot;{{onefs_host}}\u0026quot; port_no: \u0026quot;{{port_no}}\u0026quot; api_user: \u0026quot;{{api_user}}\u0026quot; api_password: \u0026quot;{{api_password}}\u0026quot; verify_ssl: \u0026quot;{{verify_ssl}}\u0026quot; access_zone: \u0026quot;{{access_zone}}\u0026quot; provider_type: \u0026quot;{{provider_type}}\u0026quot; user_name: \u0026quot;{{account_name}}\u0026quot; state: \u0026quot;absent\u0026quot; Return Values   Key Type Returned Description   changed   bool   always   Whether or not the resource has changed    user_details   complex   When user exists   Details of the user.    \u0026nbsp; email   str  success  The email of the user.    \u0026nbsp; enabled   bool  success  Enabled is a bool variable which is used to enable or disable the user account.    \u0026nbsp; gecos   str  success  The full description of the user.    \u0026nbsp; gid   complex  success  The details of the primary group for the user.    \u0026nbsp; \u0026nbsp; id   str  success  The id of the primary group.    \u0026nbsp; \u0026nbsp; name   str  success  The name of the primary group.    \u0026nbsp; \u0026nbsp; type   str  success  The resource's type is mentioned.    \u0026nbsp; home_directory   str  success  The directory path acts as the home directory for the user's account.    \u0026nbsp; name   str  success  The name of the user.    \u0026nbsp; provider   str  success  The provider contains the provider type and access zone.    \u0026nbsp; roles   list  success  The list of all the roles of which user is a member.    \u0026nbsp; shell   str  success  The type of shell for the user account.    \u0026nbsp; uid   complex  success  Details about the id and name of the user.    \u0026nbsp; \u0026nbsp; id   str  success  The id of the user.    \u0026nbsp; \u0026nbsp; name   str  success  The name of the user.    \u0026nbsp; \u0026nbsp; type   str  success  The resource's type is mentioned.    Authors  P Srinivas Rao (@srinivas-rao5) ansible.team@dell.com   SMB Module Manage SMB shares on Dell EMC PowerScale. You can perform the following operations\nSynopsis Managing SMB share on PowerScale. Create a new SMB share. Modify an existing SMB share. Get details of an existing SMB share. Delete an existing SMB share.\nParameters   Parameter Type Required Default Choices Description   share_name  str   True     The name of the SMB share.    path  str      The path of the SMB share. This parameter will be mandatory only for the create operation. This is the absolute path for System Access Zone and the relative path for non-System Access Zone.    access_zone  str    System    Access zone which contains this share. If not specified it will be considered as a System Access Zone. For a non-System Access Zone the effective path where the SMB is created will be determined by the base path of the Access Zone and the path provided by the user in the playbook. For a System Access Zone the effective path will be the absolute path provided by the user in the playbook.    new_share_name  str      The new name of the SMB share.    description  str      Description about the SMB share.    permissions  list elements: dict      Specifies permission for specific user, group, or trustee. Valid options read, write, and full. This is a list of dictionaries. Each dictionry entry has 3 mandatory values- a)'user_name'/'group_name'/'wellknown' can have actual name of the trustee like 'user'/'group'/'wellknown' b)'permission' can be 'read'/''write'/'full' c)'permission_type' can be 'allow'/'deny' The fourth entry 'provider_type' is optional (default is 'local') d)'provider_type' can be 'local'/'file'/'ads'/'ldap'    access_based_enumeration  bool      Only enumerates files and folders for the requesting user has access to.    access_based_enumeration_root_only  bool      Access-based enumeration on only the root directory of the share.    browsable  bool      Share is visible in net view and the browse list.    ntfs_acl_support  bool      Support NTFS ACLs on files and directories.    directory_create_mask  str      Directory creates mask bits. Octal value for owner, group, and others vs read, write, and execute    directory_create_mode  str      Directory creates mode bits. Octal value for owner, group, and others vs read, write, and execute    file_create_mask  str      File creates mask bits. Octal value for owner, group, and others vs read, write, and execute    file_create_mode  str      File creates mode bits. Octal value for owner, group, and others vs read, write, and execute    state  str   True     absent present   Defines whether the SMB share should exist or not.    onefs_host  str   True     IP address or FQDN of the PowerScale cluster.    port_no  str    8080    Port number of the PowerScale cluster.It defaults to 8080 if not specified.    verify_ssl  bool   True     True False   boolean variable to specify whether to validate SSL certificate or not. True - indicates that the SSL certificate should be verified. False - indicates that the SSL certificate should not be verified.    api_user  str   True     username of the PowerScale cluster.    api_password  str   True     the password of the PowerScale cluster.    Examples  - name: Create SMB share for non system access zone dellemc_powerscale_smb: onefs_host: \u0026quot;{{onefs_host}}\u0026quot; verify_ssl: \u0026quot;{{verify_ssl}}\u0026quot; api_user: \u0026quot;{{api_user}}\u0026quot; api_password: \u0026quot;{{api_password}}\u0026quot; share_name: \u0026quot;{{name}}\u0026quot; path: \u0026quot;{{path}}\u0026quot; access_zone: \u0026quot;{{non_system_access_zone}}\u0026quot; state: \u0026quot;{{state_present}}\u0026quot; - name: Create SMB share for system access zone dellemc_powerscale_smb: onefs_host: \u0026quot;{{onefs_host}}\u0026quot; verify_ssl: \u0026quot;{{verify_ssl}}\u0026quot; api_user: \u0026quot;{{api_user}}\u0026quot; api_password: \u0026quot;{{api_password}}\u0026quot; share_name: \u0026quot;{{name}}\u0026quot; path: \u0026quot;{{system_az_path}}\u0026quot; description: \u0026quot;{{description}}\u0026quot; permissions: - user_name: \u0026quot;{{system_az_user}}\u0026quot; permission: \u0026quot;full\u0026quot; permission_type: \u0026quot;allow\u0026quot; - group_name: \u0026quot;{{system_az_group}}\u0026quot; permission: \u0026quot;read\u0026quot; permission_type: \u0026quot;allow\u0026quot; - wellknown: \u0026quot;everyone\u0026quot; permission: \u0026quot;read\u0026quot; permission_type: \u0026quot;allow\u0026quot; state: \u0026quot;{{state_present}}\u0026quot; - name: Modify user permission for SMB share dellemc_powerscale_smb: onefs_host: \u0026quot;{{onefs_host}}\u0026quot; verify_ssl: \u0026quot;{{verify_ssl}}\u0026quot; api_user: \u0026quot;{{api_user}}\u0026quot; api_password: \u0026quot;{{api_password}}\u0026quot; share_name: \u0026quot;{{name}}\u0026quot; path: \u0026quot;{{system_az_path}}\u0026quot; description: \u0026quot;{{description}}\u0026quot; permissions: - user_name: \u0026quot;{{system_az_user}}\u0026quot; permission: \u0026quot;full\u0026quot; permission_type: \u0026quot;allow\u0026quot; - group_name: \u0026quot;{{system_az_group}}\u0026quot; permission: \u0026quot;write\u0026quot; permission_type: \u0026quot;allow\u0026quot; - wellknown: \u0026quot;everyone\u0026quot; permission: \u0026quot;write\u0026quot; permission_type: \u0026quot;deny\u0026quot; state: \u0026quot;{{state_present}}\u0026quot; - name: Delete system access zone SMB share dellemc_powerscale_smb: onefs_host: \u0026quot;{{onefs_host}}\u0026quot; verify_ssl: \u0026quot;{{verify_ssl}}\u0026quot; api_user: \u0026quot;{{api_user}}\u0026quot; api_password: \u0026quot;{{api_password}}\u0026quot; share_name: \u0026quot;{{name}}\u0026quot; state: \u0026quot;{{state_absent}}\u0026quot; - name: Get SMB share details dellemc_powerscale_smb: onefs_host: \u0026quot;{{onefs_host}}\u0026quot; verify_ssl: \u0026quot;{{verify_ssl}}\u0026quot; api_user: \u0026quot;{{api_user}}\u0026quot; api_password: \u0026quot;{{api_password}}\u0026quot; share_name: \u0026quot;{{name}}\u0026quot; state: \u0026quot;{{state_present}}\u0026quot; - name: Create SMB share for non system access zone dellemc_powerscale_smb: onefs_host: \u0026quot;{{onefs_host}}\u0026quot; verify_ssl: \u0026quot;{{verify_ssl}}\u0026quot; api_user: \u0026quot;{{api_user}}\u0026quot; api_password: \u0026quot;{{api_password}}\u0026quot; share_name: \u0026quot;{{name}}\u0026quot; path: \u0026quot;{{non_system_az_path}}\u0026quot; access_zone: \u0026quot;{{non_system_access_zone}}\u0026quot; description: \u0026quot;{{description}}\u0026quot; permissions: - user_name: \u0026quot;{{non_system_az_user}}\u0026quot; permission: \u0026quot;full\u0026quot; permission_type: \u0026quot;allow\u0026quot; - group_name: \u0026quot;{{non_system_az_group}}\u0026quot; permission: \u0026quot;read\u0026quot; permission_type: \u0026quot;allow\u0026quot; - wellknown: \u0026quot;everyone\u0026quot; permission: \u0026quot;read\u0026quot; permission_type: \u0026quot;allow\u0026quot; state: \u0026quot;{{state_present}}\u0026quot; - name: Modify description for an non system access zone SMB share dellemc_powerscale_smb: onefs_host: \u0026quot;{{onefs_host}}\u0026quot; verify_ssl: \u0026quot;{{verify_ssl}}\u0026quot; api_user: \u0026quot;{{api_user}}\u0026quot; api_password: \u0026quot;{{api_password}}\u0026quot; share_name: \u0026quot;{{name}}\u0026quot; access_zone: \u0026quot;{{non_system_access_zone}}\u0026quot; description: \u0026quot;new description\u0026quot; state: \u0026quot;{{state_present}}\u0026quot; - name: Modify name for an existing non system access zone SMB share dellemc_powerscale_smb: onefs_host: \u0026quot;{{onefs_host}}\u0026quot; verify_ssl: \u0026quot;{{verify_ssl}}\u0026quot; api_user: \u0026quot;{{api_user}}\u0026quot; api_password: \u0026quot;{{api_password}}\u0026quot; share_name: \u0026quot;{{name}}\u0026quot; new_share_name: \u0026quot;{{new_name}}\u0026quot; access_zone: \u0026quot;{{non_system_access_zone}}\u0026quot; description: \u0026quot;new description\u0026quot; state: \u0026quot;{{state_present}}\u0026quot; Return Values   Key Type Returned Description   changed   bool   always   A boolean indicating if the task had to make changes.    smb_details   complex   always   Details of the SMB Share.    \u0026nbsp; browsable   bool  success  Share is visible in net view and the browse list    \u0026nbsp; description   str  success  Description of the SMB Share    \u0026nbsp; directory_create_mask   int  success  Directory create mask bit for SMB Share    \u0026nbsp; directory_create_mask(octal)   str  success  Directory create mask bit for SMB Share in octal format    \u0026nbsp; directory_create_mode   int  success  Directory create mode bit for SMB Share    \u0026nbsp; directory_create_mode(octal)   str  success  Directory create mode bit for SMB Share in octal format    \u0026nbsp; file_create_mask   int  success  File create mask bit for SMB Share    \u0026nbsp; file_create_mask(octal)   str  success  File create mask bit for SMB Share in octal format    \u0026nbsp; file_create_mode   int  success  File create mode bit for SMB Share    \u0026nbsp; file_create_mode(octal)   str  success  File create mode bit for SMB Share in octal format    \u0026nbsp; id   str  success  Id of the SMB Share    \u0026nbsp; name   str  success  Name of the SMB Share    \u0026nbsp; path   str  success  Path of the SMB Share    \u0026nbsp; permission   list  success  permission on the of the SMB Share for user/group/wellknown    Authors  Arindam Datta (@dattaarindam) ansible.team@dell.com   Snapshot Module Manage snapshots on Dell EMC PowerScale.\nSynopsis You can perform the following operations Managing snapshots on PowerScale. Create a filesystem snapshot. Modify a filesystem snapshot. Get details of a filesystem snapshot. Delete a filesystem snapshot.\nParameters   Parameter Type Required Default Choices Description   snapshot_name  str   True     The name of the snapshot.    path  str      Specifies the filesystem path. It is the absolute path for System access zone and it is relative if using non-System access zone. For example, if your access zone is 'Ansible' and it has a base path '/ifs/ansible' and the path specified is '/user1', then the effective path would be '/ifs/ansible/user1'. If your access zone is System, and you have 'directory1' in the access zone, the path provided should be '/ifs/directory1'.    access_zone  str    System    The effective path where the Snapshot is created will be determined by the base path of the Access Zone and the path provided by the user in the playbook.    new_snapshot_name  str      The new name of the snapshot.    expiration_timestamp  str      The timestamp on which the snapshot will expire (UTC format). Either this or desired retention can be specified, but not both.    desired_retention  str      The number of days for which the snapshot can be retained. Either this or expiration timestamp can be specified, but not both.    retention_unit  str      hours days   The retention unit for the snapshot. The default value is hours.    alias  str      The alias for the snapshot.    state  str   True     absent present   Defines whether the snapshot should exist or not.    onefs_host  str   True     IP address or FQDN of the PowerScale cluster.    port_no  str    8080    Port number of the PowerScale cluster.It defaults to 8080 if not specified.    verify_ssl  bool   True     True False   boolean variable to specify whether to validate SSL certificate or not. True - indicates that the SSL certificate should be verified. False - indicates that the SSL certificate should not be verified.    api_user  str   True     username of the PowerScale cluster.    api_password  str   True     the password of the PowerScale cluster.    Examples  - name: Create a filesystem snapshot on PowerScale dellemc_powerscale_snapshot: onefs_host: \u0026quot;{{onefs_host}}\u0026quot; verify_ssl: \u0026quot;{{verify_ssl}}\u0026quot; api_user: \u0026quot;{{api_user}}\u0026quot; api_password: \u0026quot;{{api_password}}\u0026quot; path: \u0026quot;{{ansible_path}}\u0026quot; access_zone: \u0026quot;{{access_zone}}\u0026quot; snapshot_name: \u0026quot;{{snapshot_name}}\u0026quot; desired_retention: \u0026quot;{{desired_retention}}\u0026quot; retention_unit: \u0026quot;{{retention_unit_days}}\u0026quot; alias: \u0026quot;{{ansible_snap_alias}}\u0026quot; state: \u0026quot;{{present}}\u0026quot; - name: Get details of a filesystem snapshot dellemc_powerscale_snapshot: onefs_host: \u0026quot;{{onefs_host}}\u0026quot; verify_ssl: \u0026quot;{{verify_ssl}}\u0026quot; api_user: \u0026quot;{{api_user}}\u0026quot; api_password: \u0026quot;{{api_password}}\u0026quot; snapshot_name: \u0026quot;{{snapshot_name}}\u0026quot; state: \u0026quot;{{present}}\u0026quot; - name: Modify filesystem snapshot desired retention dellemc_powerscale_snapshot: onefs_host: \u0026quot;{{onefs_host}}\u0026quot; verify_ssl: \u0026quot;{{verify_ssl}}\u0026quot; api_user: \u0026quot;{{api_user}}\u0026quot; api_password: \u0026quot;{{api_password}}\u0026quot; snapshot_name: \u0026quot;{{snapshot_name}}\u0026quot; desired_retention: \u0026quot;{{desired_retention_new}}\u0026quot; retention_unit: \u0026quot;{{retention_unit_days}}\u0026quot; state: \u0026quot;{{present}}\u0026quot; - name: Modify filesystem snapshot expiration timestamp dellemc_powerscale_snapshot: onefs_host: \u0026quot;{{onefs_host}}\u0026quot; verify_ssl: \u0026quot;{{verify_ssl}}\u0026quot; api_user: \u0026quot;{{api_user}}\u0026quot; api_password: \u0026quot;{{api_password}}\u0026quot; snapshot_name: \u0026quot;{{snapshot_name}}\u0026quot; expiration_timestamp: \u0026quot;{{expiration_timestamp_new}}\u0026quot; state: \u0026quot;{{present}}\u0026quot; - name: Modify filesystem snapshot alias dellemc_powerscale_snapshot: onefs_host: \u0026quot;{{onefs_host}}\u0026quot; verify_ssl: \u0026quot;{{verify_ssl}}\u0026quot; api_user: \u0026quot;{{api_user}}\u0026quot; api_password: \u0026quot;{{api_password}}\u0026quot; snapshot_name: \u0026quot;{{snapshot_name}}\u0026quot; alias: \u0026quot;{{ansible_snap_alias_new}}\u0026quot; state: \u0026quot;{{present}}\u0026quot; - name: Delete snapshot alias dellemc_powerscale_snapshot: onefs_host: \u0026quot;{{onefs_host}}\u0026quot; verify_ssl: \u0026quot;{{verify_ssl}}\u0026quot; api_user: \u0026quot;{{api_user}}\u0026quot; api_password: \u0026quot;{{api_password}}\u0026quot; snapshot_name: \u0026quot;{{snapshot_name}}\u0026quot; alias: \u0026quot;\u0026quot; state: \u0026quot;{{present}}\u0026quot; - name: Rename filesystem snapshot dellemc_powerscale_snapshot: onefs_host: \u0026quot;{{onefs_host}}\u0026quot; verify_ssl: \u0026quot;{{verify_ssl}}\u0026quot; api_user: \u0026quot;{{api_user}}\u0026quot; api_password: \u0026quot;{{api_password}}\u0026quot; snapshot_name: \u0026quot;{{snapshot_name}}\u0026quot; new_snapshot_name: \u0026quot;{{new_snapshot_name}}\u0026quot; state: \u0026quot;{{present}}\u0026quot; - name: Delete filesystem snapshot dellemc_powerscale_snapshot: onefs_host: \u0026quot;{{onefs_host}}\u0026quot; verify_ssl: \u0026quot;{{verify_ssl}}\u0026quot; api_user: \u0026quot;{{api_user}}\u0026quot; api_password: \u0026quot;{{api_password}}\u0026quot; snapshot_name: \u0026quot;{{new_snapshot_name}}\u0026quot; state: \u0026quot;{{absent}}\u0026quot; Return Values   Key Type Returned Description   changed   bool   always   Whether or not the resource has changed    snapshot_details   complex   When snapshot exists.   The snapshot details.    \u0026nbsp; alias   str  success  Snapshot alias.    \u0026nbsp; created   int  success  The creation timestamp.    \u0026nbsp; expires   int  success  The expiration timestamp.    \u0026nbsp; has_locks   bool  success  Whether the snapshot has locks.    \u0026nbsp; id   int  success  The snapshot ID.    \u0026nbsp; name   str  success  The name of the snapshot.    \u0026nbsp; path   str  success  The directory path whose snapshot has been taken.    \u0026nbsp; pct_filesystem   float  success  The percentage of filesystem used.    \u0026nbsp; pct_reserve   float  success  The percentage of filesystem reserved.    \u0026nbsp; size   int  success  The snapshot size.    \u0026nbsp; state   str  success  The state of the snapshot.    \u0026nbsp; target_id   int  success  target ID of snapshot whose alias it is.    \u0026nbsp; target_name   str  success  target name of snapshot whose alias it is.    Authors  Prashant Rakheja (@prashant-dell) ansible.team@dell.com   Access Zone Module Manages access zones on PowerScale\nSynopsis Managing access zones on the PowerScale storage system includes getting details of the access zone and modifying the smb and nfs settings.\nParameters   Parameter Type Required Default Choices Description   az_name  str   True     The name of the access zone.    smb  dict      Specifies the default SMB setting parameters of access zone.    \u0026nbsp; create_permissions   str    default acl    default acl Inherit mode bits Use create mask and mode   Sets the default source permissions to apply when a file or directory is created.    \u0026nbsp; directory_create_mask   str      Specifies the UNIX mask bits(octal) that are removed when a directory is created, restricting permissions. Mask bits are applied before mode bits are applied.    \u0026nbsp; directory_create_mode   str      Specifies the UNIX mode bits(octal) that are added when a directory is created, enabling permissions.    \u0026nbsp; file_create_mask   str      Specifies the UNIX mask bits(octal) that are removed when a file is created, restricting permissions.    \u0026nbsp; file_create_mode   str      Specifies the UNIX mode bits(octal) that are added when a file is created, enabling permissions.    \u0026nbsp; access_based_enumeration   bool      Allows access based enumeration only on the files and folders that the requesting user can access.    \u0026nbsp; access_based_enumeration_root_only   bool      Access-based enumeration on only the root directory of the share.    \u0026nbsp; ntfs_acl_support   bool      Allows ACLs to be stored and edited from SMB clients.    \u0026nbsp; oplocks   bool      An oplock allows clients to provide performance improvements by using locally-cached information.    nfs  dict      Specifies the default NFS setting parameters of access zone.    \u0026nbsp; commit_asynchronous   bool      Set to True if NFS commit requests execute asynchronously.    \u0026nbsp; nfsv4_domain   str      Specifies the domain or realm through which users and groups are associated.    \u0026nbsp; nfsv4_allow_numeric_ids   bool      If true, sends owners and groups as UIDs and GIDs when look up fails or if the 'nfsv4_no_name' property is set to 1.    \u0026nbsp; nfsv4_no_domain   bool      If true, sends owners and groups without a domain name.    \u0026nbsp; nfsv4_no_domain_uids   bool      If true, sends UIDs and GIDs without a domain name.    \u0026nbsp; nfsv4_no_names   bool      If true, sends owners and groups as UIDs and GIDs.    provider_state  str      add remove   Defines whether the auth providers should be added or removed from access zone. If auth_providers are given, then provider_state should also be specified. add - indicates that the auth providers should be added to the access zone. remove - indicates that auth providers should be removed from the access zone.    auth_providers  list elements: dict      Specifies the auth providers which needs to be added or removed from access zone. If auth_providers are given, then provider_state should also be specified.    \u0026nbsp; provider_name   str   True     Specifies the auth provider name which needs to be added or removed from access zone.    \u0026nbsp; provider_type   str   True     local file ldap ads   Specifies the auth provider type which needs to be added or removed from access zone.    state  str   True     present absent   Defines whether the access zone should exist or not. present - indicates that the access zone should exist on the system. absent - indicates that the access zone should not exist on the system.    onefs_host  str   True     IP address or FQDN of the PowerScale cluster.    port_no  str    8080    Port number of the PowerScale cluster.It defaults to 8080 if not specified.    verify_ssl  bool   True     True False   boolean variable to specify whether to validate SSL certificate or not. True - indicates that the SSL certificate should be verified. False - indicates that the SSL certificate should not be verified.    api_user  str   True     username of the PowerScale cluster.    api_password  str   True     the password of the PowerScale cluster.    Notes  Creation/Deletion of access zone is not allowed through the Ansible module.  Examples - name: Get details of access zone including smb and nfs settings dellemc_powerscale_accesszone: onefs_host: \u0026quot;{{onefs_host}}\u0026quot; api_user: \u0026quot;{{api_user}}\u0026quot; api_password: \u0026quot;{{api_password}}\u0026quot; verify_ssl: \u0026quot;{{verify_ssl}}\u0026quot; az_name: \u0026quot;{{access zone}}\u0026quot; state: \u0026quot;present\u0026quot; - name: Modify smb settings of access zone dellemc_powerscale_accesszone: onefs_host: \u0026quot;{{onefs_host}}\u0026quot; api_user: \u0026quot;{{api_user}}\u0026quot; api_password: \u0026quot;{{api_password}}\u0026quot; verify_ssl: \u0026quot;{{verify_ssl}}\u0026quot; az_name: \u0026quot;{{access zone}}\u0026quot; state: \u0026quot;present\u0026quot; smb: create_permissions: 'default acl' directory_create_mask: '777' directory_create_mode: '700' file_create_mask: '700' file_create_mode: '100' access_based_enumeration: true access_based_enumeration_root_only: false ntfs_acl_support: true oplocks: true - name: Modify nfs settings of access zone dellemc_powerscale_accesszone: onefs_host: \u0026quot;{{onefs_host}}\u0026quot; api_user: \u0026quot;{{api_user}}\u0026quot; api_password: \u0026quot;{{api_password}}\u0026quot; verify_ssl: \u0026quot;{{verify_ssl}}\u0026quot; az_name: \u0026quot;{{access zone}}\u0026quot; state: \u0026quot;present\u0026quot; nfs: commit_asynchronous: false nfsv4_allow_numeric_ids: false nfsv4_domain: 'localhost' nfsv4_no_domain: false nfsv4_no_domain_uids: false nfsv4_no_names: false - name: Modify smb and nfs settings of access zone dellemc_powerscale_accesszone: onefs_host: \u0026quot;{{onefs_host}}\u0026quot; api_user: \u0026quot;{{api_user}}\u0026quot; api_password: \u0026quot;{{api_password}}\u0026quot; verify_ssl: \u0026quot;{{verify_ssl}}\u0026quot; az_name: \u0026quot;{{access zone}}\u0026quot; state: \u0026quot;present\u0026quot; smb: create_permissions: 'default acl' directory_create_mask: '777' directory_create_mode: '700' file_create_mask: '700' file_create_mode: '100' access_based_enumeration: true access_based_enumeration_root_only: false ntfs_acl_support: true oplocks: true nfs: commit_asynchronous: false nfsv4_allow_numeric_ids: false nfsv4_domain: 'localhost' nfsv4_no_domain: false nfsv4_no_domain_uids: false nfsv4_no_names: false - name: Add Auth Providers to the access zone dellemc_powerscale_accesszone: onefs_host: \u0026quot;{{onefs_host}}\u0026quot; api_user: \u0026quot;{{api_user}}\u0026quot; api_password: \u0026quot;{{api_password}}\u0026quot; verify_ssl: \u0026quot;{{verify_ssl}}\u0026quot; az_name: \u0026quot;{{access zone}}\u0026quot; provider_state: \u0026quot;add\u0026quot; auth_providers: - provider_name: \u0026quot;System\u0026quot; provider_type: \u0026quot;file\u0026quot; - provider_name: \u0026quot;ldap-prashant\u0026quot; provider_type: \u0026quot;ldap\u0026quot; state: \u0026quot;present\u0026quot; - name: Remove Auth Providers from the access zone dellemc_powerscale_accesszone: onefs_host: \u0026quot;{{onefs_host}}\u0026quot; api_user: \u0026quot;{{api_user}}\u0026quot; api_password: \u0026quot;{{api_password}}\u0026quot; verify_ssl: \u0026quot;{{verify_ssl}}\u0026quot; az_name: \u0026quot;{{access zone}}\u0026quot; provider_state: \u0026quot;remove\u0026quot; auth_providers: - provider_name: \u0026quot;System\u0026quot; provider_type: \u0026quot;file\u0026quot; state: \u0026quot;present\u0026quot; Return Values   Key Type Returned Description   access_zone_details   complex   When access zone exists   The access zone details    \u0026nbsp; nfs_settings   complex  success  NFS settings of access zone    \u0026nbsp; \u0026nbsp; export_settings   complex  success  Default values for NFS exports    \u0026nbsp; \u0026nbsp; \u0026nbsp; commit_asynchronous   bool  success  Set to True if NFS commit requests execute asynchronously    \u0026nbsp; \u0026nbsp; zone_settings   complex  success  NFS server settings for this zone    \u0026nbsp; \u0026nbsp; \u0026nbsp; nfsv4_allow_numeric_ids   bool  success  If true, sends owners and groups as UIDs and GIDs when look up fails or if the 'nfsv4_no_name' property is set to 1    \u0026nbsp; \u0026nbsp; \u0026nbsp; nfsv4_domain   str  success  Specifies the domain or realm through which users and groups are associated    \u0026nbsp; \u0026nbsp; \u0026nbsp; nfsv4_no_domain   bool  success  If true, sends owners and groups without a domain name    \u0026nbsp; \u0026nbsp; \u0026nbsp; nfsv4_no_domain_uids   bool  success  If true, sends UIDs and GIDs without a domain name    \u0026nbsp; \u0026nbsp; \u0026nbsp; nfsv4_no_names   bool  success  If true, sends owners and groups as UIDs and GIDs    \u0026nbsp; smb_settings   complex  success  SMB settings of access zone    \u0026nbsp; \u0026nbsp; directory_create_mask(octal)   str  success  UNIX mask bits for directory in octal format    \u0026nbsp; \u0026nbsp; directory_create_mode(octal)   str  success  UNIX mode bits for directory in octal format    \u0026nbsp; \u0026nbsp; file_create_mask(octal)   str  success  UNIX mask bits for file in octal format    \u0026nbsp; \u0026nbsp; file_create_mode(octal)   str  success  UNIX mode bits for file in octal format    access_zone_modify_flag   bool   on success   Whether auth providers linked to access zone has changed    changed   bool   always   Whether or not the resource has changed    nfs_modify_flag   bool   on success   Whether or not the default NFS settings of access zone has changed    smb_modify_flag   bool   on success   Whether or not the default SMB settings of access zone has changed    Authors  Akash Shendge (@shenda1) ansible.team@dell.com   NFS Module Manage NFS exports on a DellEMC PowerScale system\nSynopsis Managing NFS exports on an PowerScale system includes creating NFS export for a directory in an access zone, adding or removing clients, modifying different parameters of the export and deleting export.\nParameters   Parameter Type Required Default Choices Description   path  str   True     Specifies the filesystem path. It is the absolute path for System access zone and it is relative if using non-system access zone. For example, if your access zone is 'Ansible' and it has a base path '/ifs/ansible' and the path specified is '/user1', then the effective path would be '/ifs/ansible/user1'. If your access zone is System, and you have 'directory1' in the access zone, the path provided should be '/ifs/directory1'. The directory on the path must exist - the NFS module will not create the directory. Ansible module will only support exports with a unique path. If there are multiple exports present with the same path, fetching details, creation, modification or deletion of such exports will fail.    access_zone  str    System    Specifies the zone in which the export is valid. Access zone once set cannot be changed.    clients  list elements: str      Specifies the clients to the export. The type of access to clients in this list is determined by the 'read_only' parameter. This list can be changed anytime during the lifetime of the NFS export.    root_clients  list elements: str      Specifies the clients with root access to the export. This list can be changed anytime during the lifetime of the NFS export.    read_only_clients  list elements: str      Specifies the clients with read-only access to the export, even when the export is read/write. This list can be changed anytime during the lifetime of the NFS export.    read_write_clients  list elements: str      Specifies the clients with both read and write access to the export, even when the export is set to read-only. This list can be changed anytime during the lifetime of the NFS export.    read_only  bool      Specifies whether the export is read-only or read-write. This parameter only has effect on the 'clients' list and not the other three types of clients. This setting can be modified any time. If it is not set at the time of creation, the export will be of type read/write.    sub_directories_mountable  bool      True if all directories under the specified paths are mountable. If not set, sub-directories will not be mountable. This setting can be modified any time.    description  str      Optional description field for the NFS export. Can be modified by passing a new value.    state  str   True     absent present   Defines whether the NFS export should exist or not. present indicates that the NFS export should exist in system. absent indicates that the NFS export should not exist in system.    client_state  str      present-in-export absent-in-export   Defines whether the clients can access the NFS export. present-in-export indicates that the clients can access the NFS export. absent-in-export indicates that the client cannot access the NFS export. Required when adding or removing access of clients from the export. While removing clients, only the specified clients will be removed from the export, others will remain as is.    onefs_host  str   True     IP address or FQDN of the PowerScale cluster.    port_no  str    8080    Port number of the PowerScale cluster.It defaults to 8080 if not specified.    verify_ssl  bool   True     True False   boolean variable to specify whether to validate SSL certificate or not. True - indicates that the SSL certificate should be verified. False - indicates that the SSL certificate should not be verified.    api_user  str   True     username of the PowerScale cluster.    api_password  str   True     the password of the PowerScale cluster.    Examples  - name: Create NFS Export dellemc_powerscale_nfs: onefs_host: \u0026quot;{{onefs_host}}\u0026quot; api_user: \u0026quot;{{api_user}}\u0026quot; api_password: \u0026quot;{{api_password}}\u0026quot; verify_ssl: \u0026quot;{{verify_ssl}}\u0026quot; path: \u0026quot;{{path}}\u0026quot; access_zone: \u0026quot;{{access_zone}}\u0026quot; read_only_clients: - \u0026quot;{{client1}}\u0026quot; - \u0026quot;{{client2}}\u0026quot; read_only: True clients: [\u0026quot;{{client3}}\u0026quot;] client_state: 'present-in-export' state: 'present' - name: Get NFS Export dellemc_powerscale_nfs: onefs_host: \u0026quot;{{onefs_host}}\u0026quot; api_user: \u0026quot;{{api_user}}\u0026quot; api_password: \u0026quot;{{api_password}}\u0026quot; verify_ssl: \u0026quot;{{verify_ssl}}\u0026quot; path: \u0026quot;{{path}}\u0026quot; access_zone: \u0026quot;{{access_zone}}\u0026quot; state: 'present' - name: Add a root client dellemc_powerscale_nfs: onefs_host: \u0026quot;{{onefs_host}}\u0026quot; api_user: \u0026quot;{{api_user}}\u0026quot; api_password: \u0026quot;{{api_password}}\u0026quot; verify_ssl: \u0026quot;{{verify_ssl}}\u0026quot; path: \u0026quot;{{path}}\u0026quot; access_zone: \u0026quot;{{access_zone}}\u0026quot; root_clients: - \u0026quot;{{client4}}\u0026quot; client_state: 'present-in-export' state: 'present' - name: Set sub_directories_mountable flag to True dellemc_powerscale_nfs: onefs_host: \u0026quot;{{onefs_host}}\u0026quot; api_user: \u0026quot;{{api_user}}\u0026quot; api_password: \u0026quot;{{api_password}}\u0026quot; verify_ssl: \u0026quot;{{verify_ssl}}\u0026quot; path: \u0026quot;{{path}}\u0026quot; access_zone: \u0026quot;{{access_zone}}\u0026quot; sub_directories_mountable: True state: 'present' - name: Remove a root client dellemc_powerscale_nfs: onefs_host: \u0026quot;{{onefs_host}}\u0026quot; api_user: \u0026quot;{{api_user}}\u0026quot; api_password: \u0026quot;{{api_password}}\u0026quot; verify_ssl: \u0026quot;{{verify_ssl}}\u0026quot; path: \u0026quot;{{path}}\u0026quot; access_zone: \u0026quot;{{access_zone}}\u0026quot; root_clients: - \u0026quot;{{client4}}\u0026quot; client_state: 'absent-in-export' state: 'present' - name: Modify description dellemc_powerscale_nfs: onefs_host: \u0026quot;{{onefs_host}}\u0026quot; api_user: \u0026quot;{{api_user}}\u0026quot; api_password: \u0026quot;{{api_password}}\u0026quot; verify_ssl: \u0026quot;{{verify_ssl}}\u0026quot; path: \u0026quot;{{path}}\u0026quot; access_zone: \u0026quot;{{access_zone}}\u0026quot; description: \u0026quot;new description\u0026quot; state: 'present' - name: Set read_only flag to False dellemc_powerscale_nfs: onefs_host: \u0026quot;{{onefs_host}}\u0026quot; api_user: \u0026quot;{{api_user}}\u0026quot; api_password: \u0026quot;{{api_password}}\u0026quot; verify_ssl: \u0026quot;{{verify_ssl}}\u0026quot; path: \u0026quot;{{path}}\u0026quot; access_zone: \u0026quot;{{access_zone}}\u0026quot; read_only: False state: 'present' - name: Delete NFS Export dellemc_powerscale_nfs: onefs_host: \u0026quot;{{onefs_host}}\u0026quot; api_user: \u0026quot;{{api_user}}\u0026quot; api_password: \u0026quot;{{api_password}}\u0026quot; verify_ssl: \u0026quot;{{verify_ssl}}\u0026quot; path: \u0026quot;{{path}}\u0026quot; access_zone: \u0026quot;{{access_zone}}\u0026quot; state: 'absent' Return Values   Key Type Returned Description   NFS_export_details   complex   always   The updated NFS Export details.    \u0026nbsp; all_dirs   bool  success  sub_directories_mountable flag value.    \u0026nbsp; clients   list  success  The list of clients for the NFS Export.    \u0026nbsp; description   str  success  Description for the export.    \u0026nbsp; id   int  success  The ID of the NFS Export, generated by the array.    \u0026nbsp; paths   list  success  The filesystem path.    \u0026nbsp; read_only   bool  success  Specifies whether the export is read-only or read-write.    \u0026nbsp; read_only_clients   list  success  The list of read only clients for the NFS Export.    \u0026nbsp; read_write_clients   list  success  The list of read write clients for the NFS Export.    \u0026nbsp; root_clients   list  success  The list of root clients for the NFS Export.    \u0026nbsp; zone   str  success  Specifies the zone in which the export is valid.    changed   bool   always   A boolean indicating if the task had to make changes.    Authors  Manisha Agrawal(@agrawm3) ansible.team@dell.com   Groups Module Manage Groups on the PowerScale Storage System\nSynopsis Managing Groups on the PowerScale Storage System includes create group, delete group, get group, add users and remove users.\nParameters   Parameter Type Required Default Choices Description   group_name  str      The name of the group. Required at the time of group creation, for the rest of the operations either group_name or group_id is required.    group_id  str      The group_id is auto generated at the time of creation. For all other operations either group_name or group_id is needed.    access_zone  str    system    This option mentions the zone in which a group is created. For creation, access_zone acts as an attribute for the group. For all other operations access_zone acts as a filter.    provider_type  str    local    local file ldap ads   This option defines the type which will be used to authenticate the group members. Creation, Deletion and Modification is allowed only for local group. Details of groups of all provider types can be fetched. If the provider_type is 'ads' then the domain name of the Active Directory Server has to be mentioned in the group_name. The format for the group_name should be 'DOMAIN_NAME\\group_name' or \"DOMAIN_NAME\\\\group_name\". This option acts as a filter for all operations except creation.    state  str   True     absent present   The state option is used to determine whether the group will exist or not.    users  list elements: dict      Either user_name or user_id is needed to add or remove the user from the group. users can be part of multiple groups.    user_state  str      present-in-group absent-in-group   The user_state option is used to determine whether the users will exist for a particular group or not. It is required when users are added or removed from a group.    onefs_host  str   True     IP address or FQDN of the PowerScale cluster.    port_no  str    8080    Port number of the PowerScale cluster.It defaults to 8080 if not specified.    verify_ssl  bool   True     True False   boolean variable to specify whether to validate SSL certificate or not. True - indicates that the SSL certificate should be verified. False - indicates that the SSL certificate should not be verified.    api_user  str   True     username of the PowerScale cluster.    api_password  str   True     the password of the PowerScale cluster.    Examples  - name: Create a Group dellemc_powerscale_group: onefs_host: \u0026quot;{{onefs_host}}\u0026quot; api_user: \u0026quot;{{api_user}}\u0026quot; api_password: \u0026quot;{{api_password}}\u0026quot; verify_ssl: \u0026quot;{{verify_ssl}}\u0026quot; access_zone: \u0026quot;{{access_zone}}\u0026quot; provider_type: \u0026quot;{{provider_type}}\u0026quot; group_name: \u0026quot;{{group_name}}\u0026quot; state: \u0026quot;present\u0026quot; - name: Create Group with Users dellemc_powerscale_group: onefs_host: \u0026quot;{{onefs_host}}\u0026quot; api_user: \u0026quot;{{api_user}}\u0026quot; api_password: \u0026quot;{{api_password}}\u0026quot; verify_ssl: \u0026quot;{{verify_ssl}}\u0026quot; provider_type: \u0026quot;{{provider_type}}\u0026quot; access_zone: \u0026quot;{{access_zone}}\u0026quot; group_name: \u0026quot;{{group_name}}\u0026quot; users: - user_name: \u0026quot;{{user_name}}\u0026quot; - user_id: \u0026quot;{{user_id}}\u0026quot; - user_name: \u0026quot;{{user_name_2}}\u0026quot; user_state: \u0026quot;present-in-group\u0026quot; state: \u0026quot;present\u0026quot; - name: Get Details of the Group using Group Id dellemc_powerscale_group: onefs_host: \u0026quot;{{onefs_host}}\u0026quot; api_user: \u0026quot;{{api_user}}\u0026quot; api_password: \u0026quot;{{api_password}}\u0026quot; verify_ssl: \u0026quot;{{verify_ssl}}\u0026quot; provider_type: \u0026quot;{{provider_type}}\u0026quot; access_zone: \u0026quot;{{access_zone}}\u0026quot; group_id: \u0026quot;{{group_id}}\u0026quot; state: \u0026quot;present\u0026quot; - name: Delete the Group using Group Name dellemc_powerscale_group: onefs_host: \u0026quot;{{onefs_host}}\u0026quot; api_user: \u0026quot;{{api_user}}\u0026quot; api_password: \u0026quot;{{api_password}}\u0026quot; verify_ssl: \u0026quot;{{verify_ssl}}\u0026quot; provider_type: \u0026quot;{{provider_type}}\u0026quot; access_zone: \u0026quot;{{access_zone}}\u0026quot; group_name: \u0026quot;{{group_name}}\u0026quot; state: \u0026quot;absent\u0026quot; - name: Add Users to a Group dellemc_powerscale_group: onefs_host: \u0026quot;{{onefs_host}}\u0026quot; api_user: \u0026quot;{{api_user}}\u0026quot; api_password: \u0026quot;{{api_password}}\u0026quot; verify_ssl: \u0026quot;{{verify_ssl}}\u0026quot; provider_type: \u0026quot;{{provider_type}}\u0026quot; access_zone: \u0026quot;{{access_zone}}\u0026quot; group_id: \u0026quot;{{group_id}}\u0026quot; users: - user_name: \u0026quot;{{user_name}}\u0026quot; - user_id: \u0026quot;{{user_id}}\u0026quot; - user_name: \u0026quot;{{user_name_2}}\u0026quot; user_state: \u0026quot;present-in-group\u0026quot; state: \u0026quot;present\u0026quot; - name: Remove Users from a Group dellemc_powerscale_group: onefs_host: \u0026quot;{{onefs_host}}\u0026quot; api_user: \u0026quot;{{api_user}}\u0026quot; api_password: \u0026quot;{{api_password}}\u0026quot; verify_ssl: \u0026quot;{{verify_ssl}}\u0026quot; provider_type: \u0026quot;{{provider_type}}\u0026quot; access_zone: \u0026quot;{{access_zone}}\u0026quot; group_id: \u0026quot;{{group_id}}\u0026quot; users: - user_name: \u0026quot;{{user_name_1}}\u0026quot; - user_id: \u0026quot;{{user_id}}\u0026quot; - user_name: \u0026quot;{{user_name_2}}\u0026quot; user_state: \u0026quot;absent-in-group\u0026quot; state: \u0026quot;present\u0026quot; Return Values   Key Type Returned Description   changed   bool   always   Whether or not the resource has changed    group_details   complex   When group exists   Details of the group    \u0026nbsp; gid   complex  success  The details of the primary group for the user.    \u0026nbsp; \u0026nbsp; id   str  success  The id of the group.    \u0026nbsp; \u0026nbsp; name   str  success  The name of the group.    \u0026nbsp; \u0026nbsp; type_of_resource   str  success  The resource's type is mentioned.    \u0026nbsp; members   complex  success  The list of sid's the members of group.    \u0026nbsp; \u0026nbsp; sid   complex  success  The details of the associated resource.    \u0026nbsp; \u0026nbsp; \u0026nbsp; id   str  success  The unique security identifier of the resource.    \u0026nbsp; \u0026nbsp; \u0026nbsp; name   str  success  The name of the resource.    \u0026nbsp; \u0026nbsp; \u0026nbsp; type_of_resource   str  success  The resource's type is mentioned.    \u0026nbsp; name   str  success  The name of the group.    \u0026nbsp; provider   str  success  The provider contains the provider type and access zone.    Authors  P Srinivas Rao (@srinivas-rao5) ansible.team@dell.com   File System Module Manage Filesystems on PowerScale\nSynopsis Managing Filesystems on PowerScale Storage System includes Create a new Filesystem, Delete a Filesystem, Get details of a filesystem, Modify a Filesystem (Quota, ACLs).\nParameters   Parameter Type Required Default Choices Description   path  str   True     This is the directory path. It is the absolute path for System access zone and is relative if using a non-System access zone. For example, if your access zone is 'Ansible' and it has a base path '/ifs/ansible' and the path specified is '/user1', then the effective path would be '/ifs/ansible/user1'. If your access zone is System, and you have 'directory1' in the access zone, the path provided should be '/ifs/directory1'.    access_zone  str    System    The access zone. If no Access Zone is specified, the 'System' access zone would be taken by default.    owner  dict      The owner of the Filesystem. This parameter is required while creating a Filesystem. The following sub-options are supported for Owner. - name(str), - provider_type(str). If you specify owner, then the corresponding name is mandatory. The provider_type is optional and it defaults to 'local'. The supported values for provider_type are 'local', 'file', 'ldap' and 'ads'.    group  dict      The group of the Filesystem. The following sub-options are supported for Group. - name(str), - provider_type(str). If you specify a group, then the corresponding name is mandatory. The provider_type is optional, it defaults to 'local'. The supported values for provider_type are 'local', 'file', 'ldap' and 'ads'.    access_control  str      The ACL value for the directory. At the time of creation, users can either provide input such as 'private_read' , 'private' , 'public_read', 'public_read_write', 'public' or in POSIX format (eg 0700). Modification of ACL is only supported from POSIX to POSIX mode.    recursive  bool    True    Creates intermediate folders recursively when set to true.    quota  dict      The Smart Quota for the filesystem. Only directory Quotas are supported. The following sub-options are supported for Quota. - include_snap_data(boolean), - include_data_protection_overhead(boolean), - thresholds_on(app_logical_size, fs_logical_size, physical_size) - advisory_limit_size(int), - soft_limit_size(int), - hard_limit_size(int), - cap_unit (MB, GB or TB), - quota_state (present or absent). The default grace period is 7 days. Modification of grace period is not supported. The default capacity unit is GB. The parameter include_data_protection_overhead is supported for SDK 8.1.1 For SDK 9.0.0 the parameter include_data_protection_overhead is deprecated and thresholds_on is used.    state  str   True     absent present   Defines whether the Filesystem should exist or not. A filesystem with NFS exports or SMB shares cannot be deleted. Any Quotas on the Filesystem need to be removed before deleting the filesystem.    list_snapshots  bool    False    If set to true, the filesystem's snapshots are returned.    onefs_host  str   True     IP address or FQDN of the PowerScale cluster.    port_no  str    8080    Port number of the PowerScale cluster.It defaults to 8080 if not specified.    verify_ssl  bool   True     True False   boolean variable to specify whether to validate SSL certificate or not. True - indicates that the SSL certificate should be verified. False - indicates that the SSL certificate should not be verified.    api_user  str   True     username of the PowerScale cluster.    api_password  str   True     the password of the PowerScale cluster.    Examples  - name: Create Filesystem with Quota in given access zone dellemc_powerscale_filesystem: onefs_host: \u0026quot;{{powerscalehost}}\u0026quot; port: \u0026quot;{{powerscaleport}}\u0026quot; verify_ssl: \u0026quot;{{verify_ssl}}\u0026quot; username: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; path: \u0026quot;{{path}}\u0026quot; access_zone: \u0026quot;{{access_zone}}\u0026quot; owner: name: 'ansible_user' provider_type: 'ldap' group: name: 'ansible_group' provider_type: 'ldap' access_control: \u0026quot;{{access_control}}\u0026quot; quota: include_snap_data: False include_data_protection_overhead: False advisory_limit_size: 2 soft_limit_size: 5 hard_limit_size: 10 cap_unit: \u0026quot;GB\u0026quot; quota_state: \u0026quot;present\u0026quot; recursive: \u0026quot;{{recursive}}\u0026quot; state: \u0026quot;{{state_present}}\u0026quot; - name: Create Filesystem in default (system) access zone, without Quota dellemc_powerscale_filesystem: onefs_host: \u0026quot;{{powerscalehost}}\u0026quot; port: \u0026quot;{{powerscaleport}}\u0026quot; verify_ssl: \u0026quot;{{verify_ssl}}\u0026quot; username: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; path: \u0026quot;{{path}}\u0026quot; owner: name: 'ansible_user' provider_type: 'ldap' state: \u0026quot;{{state_present}}\u0026quot; - name: Get filesystem details dellemc_powerscale_filesystem: onefs_host: \u0026quot;{{powerscalehost}}\u0026quot; port: \u0026quot;{{powerscaleport}}\u0026quot; verify_ssl: \u0026quot;{{verify_ssl}}\u0026quot; username: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; access_zone: \u0026quot;{{access_zone}}\u0026quot; path: \u0026quot;{{path}}\u0026quot; state: \u0026quot;{{state_present}}\u0026quot; - name: Get filesystem details with snapshots dellemc_powerscale_filesystem: onefs_host: \u0026quot;{{powerscalehost}}\u0026quot; port: \u0026quot;{{powerscaleport}}\u0026quot; verify_ssl: \u0026quot;{{verify_ssl}}\u0026quot; username: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; access_zone: \u0026quot;{{access_zone}}\u0026quot; path: \u0026quot;{{path}}\u0026quot; list_snapshots: \u0026quot;{{list_snapshots_true}}\u0026quot; state: \u0026quot;{{state_present}}\u0026quot; - name: Modify Filesystem Hard Quota dellemc_powerscale_filesystem: onefs_host: \u0026quot;{{powerscalehost}}\u0026quot; port: \u0026quot;{{powerscaleport}}\u0026quot; verify_ssl: \u0026quot;{{verify_ssl}}\u0026quot; username: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; path: \u0026quot;{{path}}\u0026quot; access_zone: \u0026quot;{{access_zone}}\u0026quot; quota: hard_limit_size: 15 cap_unit: \u0026quot;GB\u0026quot; quota_state: \u0026quot;present\u0026quot; state: \u0026quot;{{state_present}}\u0026quot; - name: Modify Filesystem Owner, Group and ACL dellemc_powerscale_filesystem: onefs_host: \u0026quot;{{powerscalehost}}\u0026quot; port: \u0026quot;{{powerscaleport}}\u0026quot; verify_ssl: \u0026quot;{{verify_ssl}}\u0026quot; username: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; path: \u0026quot;{{path}}\u0026quot; access_zone: \u0026quot;{{access_zone}}\u0026quot; owner: name: 'ansible_user' provider_type: 'ldap' group: name: 'ansible_group' provider_type: 'ldap' access_control: \u0026quot;{{new_access_control}}\u0026quot; state: \u0026quot;{{state_present}}\u0026quot; - name: Remove Quota from FS dellemc_powerscale_filesystem: onefs_host: \u0026quot;{{onefs_host}}\u0026quot; verify_ssl: \u0026quot;{{verify_ssl}}\u0026quot; api_user: \u0026quot;{{api_user}}\u0026quot; api_password: \u0026quot;{{api_password}}\u0026quot; path: \u0026quot;{{path}}\u0026quot; access_zone: \u0026quot;{{access_zone}}\u0026quot; quota: quota_state: \u0026quot;absent\u0026quot; state: \u0026quot;{{state_present}}\u0026quot; - name: Delete filesystem dellemc_powerscale_filesystem: onefs_host: \u0026quot;{{powerscalehost}}\u0026quot; port: \u0026quot;{{powerscaleport}}\u0026quot; verify_ssl: \u0026quot;{{verify_ssl}}\u0026quot; username: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; access_zone: \u0026quot;{{access_zone}}\u0026quot; path: \u0026quot;{{path}}\u0026quot; state: \u0026quot;{{state_absent}}\u0026quot; Return Values   Key Type Returned Description   changed   bool   always   Whether or not the resource has changed    filesystem_details   complex   When Filesystem exists.   The filesystem details.    \u0026nbsp; attrs   dict  success  The attributes of the filesystem.    filesystem_snapshots   complex   When list_snapshots is True.   The filesystem snapshot details.    \u0026nbsp; created   int  success  The creation timestamp.    \u0026nbsp; expires   int  success  The expiration timestamp.    \u0026nbsp; id   int  success  The id of the snapshot.    \u0026nbsp; name   str  success  The name of the snapshot.    \u0026nbsp; path   str  success  The path of the snapshot.    quota_details   complex   When Quota exists.   The quota details.    \u0026nbsp; enforced   bool  success  Whether the Quota is enforced.    \u0026nbsp; id   str  success  The ID of the Quota.    \u0026nbsp; type   str  success  The type of Quota.    \u0026nbsp; usage   dict  success  The Quota usage.    Authors  Prashant Rakheja (@prashant-dell) ansible.team@dell.com   ","excerpt":"Ansible Modules for Dell EMC PowerScale Product Guide 1.2.0 © 2021 Dell Inc. or its subsidiaries. …","ref":"/ansible-docs/docs/server/platforms/powerscale/product_guide/","title":"PowerScale Product Guide"},{"body":"Ansible Modules for Dell EMC PowerScale Release Notes 1.2.0  © 2021 Dell Inc. or its subsidiaries. All rights reserved. Dell, EMC, and other trademarks are trademarks of Dell Inc. or its subsidiaries. Other trademarks may be trademarks of their respective owners.\n Content These release notes contain supplemental information about Ansible Modules for Dell EMC PowerScale.\n Revision History Product Description Features Known Problem and limitations Software media, organization, and files Additional resources  Revision history The table in this section lists the revision history of this document.\nTable 1. Revision history\n   Revision Date Description     01 Jun 2021 Ansible Modules for Dell EMC PowerScale 1.2.0    Product Description This section describes the Ansible Modules for Dell EMC PowerScale. The Ansible Modules for Dell EMC PowerScale allow Data Center and IT administrators to use RedHat Ansible to automate and orchestrate the configuration and management of Dell EMC PowerScale arrays.\nThe Ansible Modules for Dell EMC PowerScale support the following features:\n Create user, groups, filesystem, NFS export, smart quotas, SMB share, snapshot and snapshot schedule of a filesystem. Modify user, groups, filesystem, access zone, NFS export, smart quotas, SMB share, snapshot and snapshot schedule of a filesystem. Delete user, groups, filesystem, NFS export, smart quotas, SMB share, snapshot and snapshot schedule of a filesystem. Get details of user, groups, node, filesystem, access zone, NFS export, smart quotas, SMB share, snapshot and snapshot schedule of a filesystem. Add, modify and remove Active Directory and LDAP to Authentication providers list. Map or unmap Active Directory and LDAP Authentication providers to Access zone. Get attributes and entities of the array.  The Ansible modules use playbooks, written in yaml syntax, to list, show, create, delete, and modify each of these entities.\nFeatures This section describes the features of the Ansible Modules for Dell EMC PowerScale for this release.\nThe Ansible Modules for Dell EMC PowerScale release 1.2.0 supports the following features:\n  Idempotency\n Has been handled in all modules. Allows the playbook to be run multiple times . Avoids the need for complex rollbacks.    Access Zones\n PowerScale has a concept of access zones. These are to partition the cluster into multiple isolated sections. Ansible modules support access zone operations that can also operate on the default (system) access zone. Users and Groups can be specific to a particular access zone. For non-system access zones, the path provided by the playbook is a relative path. Absolute path = Access zone base path + relative path provided by the user.    MODULES\n  The Access Zone module has the following enhancements:\n Map or unmap authentication providers to/from an access zone.    The File System module is enhanced to support the following functionality:\n Create a filesystem is updated to support both isi_sdk_8_1_1 and isi_sdk_9_0_0. Update a filesystem is updated to support both isi_sdk_8_1_1 and isi_sdk_9_0_0.    The ADS module supports the following functionality:\n Add Active Directory provider to authentication providers. Modify Active Directory provider parameters. Remove Active Directory provider from authentication providers. Retrieve details of Active Directory provider.    The LDAP module supports the following functionality:\n Add LDAP provider to authentication providers. Modify LDAP provider parameters. Remove LDAP provider from authentication providers. Retrieve details of LDAP provider.    The Node module supports the following functionality:\n Get Node details of Dell EMC PowerScale storage    The Gather Facts module is enhanced to support the following functionality:\n  Get details of the any entity listed below:\n Nodes Nfs exports Smb shares Active clients      The Smart Quotas module is enhanced to support the following functionality:\n Create a default-user/default-group quota. Modify the attributes of quota like include_overheads(8_1_1)/thresholds_on(9_0_0), soft_grace_period, hard_limit_size. Updated code to support both isi_sdk_8_1_1 and isi_sdk_9_0_0. Get details of the default-user/default-group quota. Delete the default-user/default-group quota.    Known issues Known problems in this release are listed.\n  Snapshot schedule\n If the playbook has a desired_retention field, running same the playbook again returns the changed as True (Idempotency does not work).    Filesystem Creation\n  Creation of a filesystem can fail when api_user: \u0026ldquo;admin\u0026rdquo; because it is possible that the admin user may not have privileges to set an ACLs.\n  In that case, create a filesystem with api_user: \u0026ldquo;root\u0026rdquo;.\n    Snapshot creation with alias name\n Alias name attribute remains null in spite of creating snapshot with alias name. This is an issue with PowerScale rest API. Alias name is not getting appended to the attribute in response.    Limitations This section lists the limitations in this release of Ansible Modules for Dell EMC PowerScale.\n  Gatherfacts\n Getting the list of users and groups with very long names may fail.    Users and Groups\n Only local users and groups can be created. Operations on users and groups with very long names may fail.    Access Zone\n Creation and deletion of access zones is not supported.    Filesystems\n ACLs can only be modified from POSIX to POSIX mode. Only directory quotas are supported but not user or group quotas. Modification of include_snap_data flag is not supported.    NFS Export\n If there multiple exports present with the same path in an access zone, operations on such exports fail.    Smart Quota\n Once the limits are assigned to the quota, then the quota can\u0026rsquo;t be converted to accounting. Only modification to the threshold limits is permitted. Its mandatory to pass \u0026lsquo;quota\u0026rsquo; parameter for create and modify operations for any quota type.    No support for advanced PowerScale features\n Advanced PowerScale features include SyncIQ, tiering, replication, and so on.     Software media, organization, and files The software package is available for download from the Ansible Modules for PowerScale GitHub page.\nAdditional resources This section provides more information about the product, how to get support, and provide feedback.\nDocumentation This section lists the related documentation for Ansible Modules for Dell EMC PowerScale. The documentation is available on the Ansible Modules for PowerScale GitHub page. The documentation includes the following:\n Ansible Modules for Dell EMC PowerScale Release Notes (this document). Ansible Modules for Dell EMC PowerScale Product Guide  Troubleshooting and support The Dell Container Community provides your primary source of support services.\nFor any setup, configuration issues, questions or feedback, join the Dell EMC Container community at https://www.dell.com/community/ Containers/bd-p/Containers.\n  Technical support\n  Dell EMC Online Support also provides technical support services. To open a service request, you must have a valid support agreement.\n  To get a valid support agreement or for other questions about your account, contact your Dell EMC sales representative.\n  For documentation, release notes, software updates, and other information about Dell EMC products, go to Dell EMC Online Support.\n    Support   Use the resources in this topic to get help and support.\n  The source code available on Github is unsupported and provided solely under the terms of the license attached to the source code.\n  For clarity, Dell EMC does not provide support for any source code modifications.\n  For any Ansible module setup, configuration issues, questions or feedback, join the Dell EMC Automation community at https:// www.dell.com/community/Automation/bd-p/Automation?ref=lithium_menu\n  For any Dell EMC storage issues, please contact Dell support at: https://www.dell.com/support.\n  ","excerpt":"Ansible Modules for Dell EMC PowerScale Release Notes 1.2.0  © 2021 Dell Inc. or its subsidiaries. …","ref":"/ansible-docs/docs/server/platforms/powerscale/release-notes/","title":"PowerScale Release Notes"},{"body":"","excerpt":"","ref":"/ansible-docs/docs/server/platforms/powerstore/","title":"PowerStore"},{"body":"","excerpt":"","ref":"/ansible-docs/docs/storage/platforms/powerstore/","title":"PowerStore"},{"body":"You can upgrade the CSI Driver for Dell EMC PowerStore using Helm or Dell CSI Operator.\nUpdate Driver from v1.2 to v1.3 using Helm Steps\n  Run git clone https://github.com/dell/csi-powerstore.git to clone the git repository and get the v1.3 driver.\n  Edit helm/config.yaml file and configure connection information for your PowerStore arrays changing following parameters:\n endpoint: defines the full URL path to the PowerStore API. username, password: defines credentials for connecting to array. insecure: defines if we should use insecure connection or not. default: defines if we should treat the current array as a default. block-protocol: defines what SCSI transport protocol we must use (FC, ISCSI, None, or auto). nas-name: defines what NAS should be used for NFS volumes.  Add more blocks similar to above for each PowerStore array if necessary.\n  (optional) create new storage classes using ones from helm/samples/storageclass folder as an example and apply them to the Kubernetes cluster by running kubectl create -f \u0026lt;path_to_storageclass_file\u0026gt;\n Storage classes created by v1.2 driver will not be deleted, v1.3 driver will use default array to manage volumes provisioned with old storage classes. Thus, if you still have volumes provisioned by v1.2 in your cluster then be sure to include same array you have used for v1.2 driver and make it default in config.yaml file.\n   Create the secret by running sed \u0026quot;s/CONFIG_YAML/`cat helm/config.yaml | base64 -w0`/g\u0026quot; helm/secret.yaml | kubectl apply -f -\n  Update values file as needed.\n  Run the csi-install script with the option --upgrade by running: cd ../dell-csi-helm-installer \u0026amp;\u0026amp; ./csi-install.sh --namespace csi-powerstore --values ./my-powerstore-settings.yaml --upgrade.\n  Upgrade using Dell CSI Operator: Follow the instructions for upgrade on the Dell CSI Operator GitHub page.\n","excerpt":"You can upgrade the CSI Driver for Dell EMC PowerStore using Helm or Dell CSI Operator.\nUpdate …","ref":"/ansible-docs/v1/upgradation/drivers/powerstore/","title":"PowerStore"},{"body":"You can upgrade the CSI Driver for Dell EMC PowerStore using Helm or Dell CSI Operator.\nUpdate Driver from v1.1 to v1.2 using Helm Steps\n Run git clone https://github.com/dell/csi-powerstore.git to clone the git repository and get the v1.2 driver. Update values file as needed. Run the csi-install script with the option --upgrade by running: cd ../dell-csi-helm-installer \u0026amp;\u0026amp; ./csi-install.sh --namespace csi-powerstore --values ./my-powerstore-settings.yaml --upgrade.  Upgrade using Dell CSI Operator: Follow the instructions for upgrade on the Dell CSI Operator GitHub page.\n","excerpt":"You can upgrade the CSI Driver for Dell EMC PowerStore using Helm or Dell CSI Operator.\nUpdate …","ref":"/ansible-docs/v2/upgradation/drivers/powerstore/","title":"PowerStore"},{"body":"Ansible Modules for Dell EMC PowerStore Product Guide 1.2 © 2021 Dell Inc. or its subsidiaries. All rights reserved. Dell, EMC, and other trademarks are trademarks of Dell Inc. or its subsidiaries. Other trademarks may be trademarks of their respective owners.\n Contents  File System Module  Synopsis Parameters Notes Examples Return Values Authors   Volume Module  Synopsis Parameters Notes Examples Return Values Authors   Quota Module  Synopsis Parameters Notes Examples Return Values Authors   Host Module  Synopsis Parameters Examples Return Values Authors   Snapshot Rule Module  Synopsis Parameters Examples Return Values Authors   Gatherfacts Module  Synopsis Parameters Examples Return Values Authors   Replication Session Module  Synopsis Parameters Notes Examples Return Values Authors   Host Group Module  Synopsis Parameters Examples Return Values Authors   NFS Module  Synopsis Parameters Examples Return Values Authors   Volume Group Module  Synopsis Parameters Notes Examples Return Values Authors   NAS Server Module  Synopsis Parameters Examples Return Values Authors   SMB Share Module  Synopsis Parameters Notes Examples Return Values Authors   Snapshot Module  Synopsis Parameters Examples Return Values Authors   Replication Rule Module  Synopsis Parameters Examples Return Values Authors   Protection Policy Module  Synopsis Parameters Notes Examples Return Values Authors   Filesystem Snapshot Module  Synopsis Parameters Examples Return Values Authors     File System Module Filesystem operations on PowerStore Storage system\nSynopsis Supports the provisioning operations on a filesystem such as create, modify, delete and get the details of a filesystem.\nParameters   Parameter Type Required Default Choices Description   filesystem_name  str      Name of the file system. Mutually exclusive with filesystem_id. Mandatory only for create operation.    filesystem_id  str      Unique id of the file system. Mutually exclusive with filesystem_name.    description  str      Description of the file system.    nas_server  str      Name or ID of the NAS Server on which the file system is created. Mandatory parameter whenever filesystem_name is provided, since filesystem names are unique only within a NAS server    size  int      Size that the file system presents to the host or end user. Mandatory only for create operation.    cap_unit  str      GB TB   capacity unit for the size. It defaults to 'GB', if not specified.    access_policy  str      NATIVE UNIX WINDOWS   File system security access policies.    locking_policy  str      ADVISORY MANDATORY   File system locking policies. ADVISORY- No lock checking for NFS and honor SMB lock range only for SMB. MANDATORY- Honor SMB and NFS lock range.    folder_rename_policy  str      ALL_ALLOWED SMB_FORBIDDEN ALL_FORBIDDEN   File system folder rename policies for the file system with multi-protocol access enabled. ALL_ALLOWED - All protocols are allowed to rename directories without any restrictions. SMB_FORBIDDEN - A directory rename from the SMB protocol will be denied if at least one file is opened in the directory or in one of its child directories. All_FORBIDDEN - Any directory rename request will be denied regardless of the protocol used, if at least one file is opened in the directory or in one of its child directories.    smb_properties  dict      Advance settings for SMB. It contains below optional candidate variables    \u0026nbsp; is_smb_sync_writes_enabled   bool   False     Indicates whether the synchronous writes option is enabled on the file system.    \u0026nbsp; is_smb_no_notify_enabled   bool   False     Indicates whether notifications of changes to directory file structure are enabled.    \u0026nbsp; is_smb_op_locks_enabled   bool   False     Indicates whether opportunistic file locking is enabled on the file system.    \u0026nbsp; is_smb_notify_on_access_enabled   bool   False     Indicates whether file access notifications are enabled on the file system.    \u0026nbsp; is_smb_notify_on_write_enabled   bool   False     Indicates whether file write notifications are enabled on the file system    \u0026nbsp; smb_notify_on_change_dir_depth   int   False     Integer variable , determines the lowest directory level to which the enabled notifications apply. minimum value is 1.    protection_policy  str      Name or ID of the protection policy applied to the file system. Specifying \"\" (empty string) removes the existing protection policy from file system.    quota_defaults  dict      Contains the default attributes for a filesystem quota.It contains below optional candidate variables.    \u0026nbsp; grace_period   int   False     Grace period of soft limit.    \u0026nbsp; grace_period_unit   str   False     days weeks months   Unit of the grace period of soft limit.    \u0026nbsp; default_hard_limit   int   False     Default hard limit of user quotas and tree quotas.    \u0026nbsp; default_soft_limit   int   False     Default soft limit of user quotas and tree quotas.    \u0026nbsp; cap_unit   str   False     GB TB   Capacity unit for default hard \u0026 soft limit.    state  str   True     absent present   Define whether the filesystem should exist or not.    array_ip  str   True     IP or FQDN of the PowerStore management system.    verifycert  bool   True     True False   Boolean variable to specify whether to validate SSL certificate or not. True - indicates that the SSL certificate should be verified. Set the environment variable REQUESTS_CA_BUNDLE to the path of the SSL certificate. False - indicates that the SSL certificate should not be verified.    user  str   True     The username of the PowerStore host.    password  str   True     The password of the PowerStore host.    Notes  It is recommended to remove the protection policy before deleting the filesystem.  Examples  - name: Create FileSystem by Name register: result_fs dellemc_powerstore_filesystem: array_ip: \u0026quot;{{array_ip}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; filesystem_name: \u0026quot;{{filesystem_name}}\u0026quot; description: \u0026quot;{{description}}\u0026quot; nas_server: \u0026quot;{{nas_server_id}}\u0026quot; size: \u0026quot;5\u0026quot; cap_unit: \u0026quot;GB\u0026quot; access_policy: \u0026quot;UNIX\u0026quot; locking_policy: \u0026quot;MANDATORY\u0026quot; smb_properties: is_smb_no_notify_enabled: True is_smb_notify_on_access_enabled: True quota_defaults: grace_period: 1 grace_period_unit: 'days' default_hard_limit: 3 default_soft_limit: 2 protection_policy: \u0026quot;{{protection_policy_id}}\u0026quot; state: \u0026quot;present\u0026quot; - name: Modify File System by id dellemc_powerstore_filesystem: array_ip: \u0026quot;{{array_ip}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; filesystem_id: \u0026quot;{{fs_id}}\u0026quot; folder_rename_policy: \u0026quot;ALL_ALLOWED\u0026quot; smb_properties: is_smb_op_locks_enabled: True smb_notify_on_change_dir_depth: 3 quota_defaults: grace_period: 2 grace_period_unit: 'weeks' default_hard_limit: 2 default_soft_limit: 1 state: \u0026quot;present\u0026quot; - name: Get File System details by id dellemc_powerstore_filesystem: array_ip: \u0026quot;{{array_ip}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; filesystem_id: \u0026quot;{{result_fs.filesystem_details.id}}\u0026quot; state: \u0026quot;present\u0026quot; - name: Delete File System by id dellemc_powerstore_filesystem: array_ip: \u0026quot;{{array_ip}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; filesystem_id: \u0026quot;{{result_fs.filesystem_details.id}}\u0026quot; state: \u0026quot;absent\u0026quot; Return Values   Key Type Returned Description   changed   bool   always   Whether or not the resource has changed    filesystem_details   complex   When filesystem exists   Details of the filesystem    \u0026nbsp; access_policy   str  success  Access policy about the filesystem.    \u0026nbsp; default_hard_limit   int  success  Default hard limit period for a filesystem quota in byte.    \u0026nbsp; default_soft_limit   int  success  Default soft limit period for a filesystem quota in byte.    \u0026nbsp; description   str  success  The description about the filesystem.    \u0026nbsp; grace_period   int  success  Default grace period for a filesystem quota in second.    \u0026nbsp; id   str  success  The system generated ID given to the filesystem.    \u0026nbsp; is_smb_no_notify_enabled   bool  success  Whether smb notify policy is enabled for a filesystem.    \u0026nbsp; is_smb_notify_on_access_enabled   bool  success  Whether smb on access notify policy is enabled.    \u0026nbsp; is_smb_op_locks_enabled   bool  success  Whether smb op lock is enabled.    \u0026nbsp; locking_policy   str  success  Locking policy about the filesystem.    \u0026nbsp; name   str  success  Name of the filesystem.    \u0026nbsp; nas_server   dict  success  Id and name of the nas server to which the filesystem belongs.    \u0026nbsp; protection_policy   dict  success  Id and name of the protection policy associated with the filesystem.    \u0026nbsp; size_total   int  success  Total size of the filesystem in bytes.    \u0026nbsp; size_used   int  success  Used size of the filesystem in bytes.    \u0026nbsp; snapshots   list  success  Id and name of the snapshots of a filesystem.    \u0026nbsp; total_size_with_unit   str  success  Total size of the filesystem with appropriate unit.    \u0026nbsp; used_size_with_unit   str  success  Used size of the filesystem with appropriate unit.    Authors  Arindam Datta (@dattaarindam) ansible.team@dell.com   Volume Module Manage volumes on a PowerStore storage system.\nSynopsis Managing volume on PowerStore storage system includes create volume, get details of volume, modify name, size, description, protection policy, performance policy, map or unmap volume to host/host group, and delete volume.\nParameters   Parameter Type Required Default Choices Description   vol_name  str      Unique name of the volume. This value must contain 128 or fewer printable unicode characters. Required when creating a volume. All other functionalities on a volume are supported using volume name or ID.    vg_name  str      The name of the volume group. A volume can optionally be assigned to a volume group at the time of creation. Use the Volume Group Module for modification of the assignment.    vol_id  str      The 36 character long ID of the volume, automatically generated when a volume is created. Cannot be used while creating a volume. All other functionalities on a volume are supported using volume name or ID.    size  float      Size of the volume. Minimum volume size is 1MB. Maximum volume size is 256TB. Size must be a multiple of 8192. Required in case of create and expand volume.    cap_unit  str      MB GB TB   Volume size unit. Used to signify unit of the size provided for creation and expansion of volume. It defaults to 'GB', if not specified.    new_name  str      The new volume name for the volume, used in case of rename functionality.    description  str      Description for the volume. Optional parameter when creating a volume. To modify, pass the new value in description field.    protection_policy  str      The protection_policy of the volume. To represent policy, both name or ID can be used interchangably. The module will detect both. A volume can be assigned a protection policy at the time of creation of volume or later as well. The policy can also be changed for a given volume by simply passing the new value. The policy can be removed by passing an empty string. Check examples for more clarity.    performance_policy  str      high medium low   The performance_policy for the volume. A volume can be assigned a performance policy at the time of creation of the volume, or later as well. The policy can also be changed for a given volume, by simply passing the new value. Check examples for more clarity. If not given, performance policy will be 'medium'.    host  str      Host to be mapped/unmapped to a volume. If not specified, an unmapped volume is created. Only one of the host or host group can be supplied in one call. To represent host, both name or ID can be used interchangeably. The module will detect both.    hostgroup  str      Hostgroup to be mapped/unmapped to a volume. If not specified, an unmapped volume is created. Only one of the host or host group can be mapped in one call. To represent a hostgroup, both name or ID can be used interchangeably. The module will detect both.    mapping_state  str      mapped unmapped   Define whether the volume should be mapped to a host or hostgroup. mapped - indicates that the volume should be mapped to the host or host group. unmapped - indicates that the volume should not be mapped to the host or host group. Only one of a host or host group can be supplied in one call.    hlu  int      Logical unit number for the host/host group volume access. Optional parameter when mapping a volume to host/host group. HLU modification is not supported.    state  str   True     absent present   Define whether the volume should exist or not. present - indicates that the volume should exist on the system. absent - indicates that the volume should not exist on the system.    array_ip  str   True     IP or FQDN of the PowerStore management system.    verifycert  bool   True     True False   Boolean variable to specify whether to validate SSL certificate or not. True - indicates that the SSL certificate should be verified. Set the environment variable REQUESTS_CA_BUNDLE to the path of the SSL certificate. False - indicates that the SSL certificate should not be verified.    user  str   True     The username of the PowerStore host.    password  str   True     The password of the PowerStore host.    Notes  To create a new volume, vol_name and size is required. cap_unit, description, vg_name, performance_policy, and protection_policy are optional. new_name should not be provided when creating a new volume. size is a required parameter for expand volume. Clones or Snapshots of a deleted production volume or a clone are not deleted. A volume that is attached to a host/host group, or that is part of a volume group cannot be deleted.  Examples - name: Create stand-alone volume dellemc_powerstore_volume: array_ip: \u0026quot;{{array_ip}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; vol_name: \u0026quot;{{vol_name}}\u0026quot; size: 1 cap_unit: \u0026quot;{{cap_unit}}\u0026quot; state: 'present' - name: Create stand-alone volume with performance and protection policy dellemc_powerstore_volume: array_ip: \u0026quot;{{array_ip}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; vol_name: \u0026quot;{{vol_name}}\u0026quot; size: 5 cap_unit: \u0026quot;{{cap_unit}}\u0026quot; state: 'present' description: 'Description' performance_policy: 'low' protection_policy: 'protection_policy_name' - name: Create volume and assign to a volume group dellemc_powerstore_volume: array_ip: \u0026quot;{{array_ip}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; vol_name: \u0026quot;{{vol_name}}\u0026quot; vg_name: \u0026quot;{{vg_name}}\u0026quot; size: 1 cap_unit: \u0026quot;{{cap_unit}}\u0026quot; state: 'present' - name: Create volume and map it to a host dellemc_powerstore_volume: array_ip: \u0026quot;{{array_ip}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; vol_name: \u0026quot;{{vol_name}}\u0026quot; size: 1 cap_unit: \u0026quot;{{cap_unit}}\u0026quot; mapping_state: 'mapped' host: \u0026quot;{{host_name}}\u0026quot; state: 'present' - name: Get volume details using ID dellemc_powerstore_volume: array_ip: \u0026quot;{{array_ip}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; vol_id: \u0026quot;{{result.volume_details.id}}\u0026quot; state: \u0026quot;present\u0026quot; - name: Get volume details using name dellemc_powerstore_volume: array_ip: \u0026quot;{{array_ip}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; vol_name: \u0026quot;{{vol_name}}\u0026quot; state: \u0026quot;present\u0026quot; - name: Modify volume size, name, description and performance policy dellemc_powerstore_volume: array_ip: \u0026quot;{{array_ip}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; new_name: \u0026quot;{{new_name}}\u0026quot; vol_name: \u0026quot;{{vol_name}}\u0026quot; state: \u0026quot;present\u0026quot; size: 2 performance_policy: 'high' description: 'new description' - name: Remove protection policy from Volume dellemc_powerstore_volume: array_ip: \u0026quot;{{array_ip}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; new_name: \u0026quot;{{new_name}}\u0026quot; vol_name: \u0026quot;{{vol_name}}\u0026quot; state: \u0026quot;present\u0026quot; protection_policy: \u0026quot;\u0026quot; - name: Map volume to a host with HLU dellemc_powerstore_volume: array_ip: \u0026quot;{{array_ip}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; vol_name: \u0026quot;{{vol_name}}\u0026quot; state: 'present' mapping_state: 'mapped' host: 'host1' hlu: 12 - name: Map volume to a host without HLU dellemc_powerstore_volume: array_ip: \u0026quot;{{array_ip}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; vol_name: \u0026quot;{{vol_name}}\u0026quot; state: 'present' mapping_state: 'mapped' host: 'host2' - name: Delete volume dellemc_powerstore_volume: array_ip: \u0026quot;{{array_ip}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; vol_id: \u0026quot;{{result.volume_details.id}}\u0026quot; state: \u0026quot;absent\u0026quot; Return Values   Key Type Returned Description   add_vols_to_vg   bool   When value exists   A boolean flag to indicate whether volume/s got added to volume group    changed   bool   always   Whether or not the resource has changed    create_vg   bool   When value exists   A boolean flag to indicate whether volume group got created    delete_vg   bool   When value exists   A boolean flag to indicate whether volume group got deleted    modify_vg   bool   When value exists   A boolean flag to indicate whether volume group got modified    remove_vols_from_vg   bool   When value exists   A boolean flag to indicate whether volume/s got removed from volume group    volume_group_details   complex   When volume group exists   Details of the volume group    \u0026nbsp; description   str  success  ['description about the volume group']    \u0026nbsp; id   str  success  ['The system generated ID given to the volume group']    \u0026nbsp; is_write_order_consistent   bool  success  ['A boolean flag to indicate whether snapshot sets of the volume group will be write-order consistent']    \u0026nbsp; name   str  success  ['Name of the volume group']    \u0026nbsp; protection_policy_id   str  success  ['The protection policy of the volume group']    \u0026nbsp; type   str  success  ['The type of the volume group']    \u0026nbsp; volumes   complex  success  ['The volumes details of the volume group']    \u0026nbsp; \u0026nbsp; id   str  success  ['The system generated ID given to the volume associated with the volume group']    \u0026nbsp; \u0026nbsp; name   str  success  ['The name of the volume associated with the volume group.']    Authors  Ambuj Dubey (@AmbujDube) ansible.team@dell.com Manisha Agrawal (@agrawm3) ansible.team@dell.com   Quota Module Manage Tree Quotas and User Quotas on PowerStore.\nSynopsis Managing Quotas on Powerstore storage system includes getting details, modifying, creating and deleting Quotas.\nParameters   Parameter Type Required Default Choices Description   path  str      The path on which the quota will be imposed. Path is relative to the root of the filesystem. For user quota, if path is not specified, quota will be created at the root of the filesystem.    quota_type  str      user tree   The type of quota which will be imposed.    quota_id  str      Id of the user/tree quota. If quota_id is mentioned, then path/nas_server/file_system/quota_type is not required.    filesystem  str      The ID/Name of the filesystem for which the Tree/User Quota will be created. If filesystem name is specified, then nas_server is required to uniquely identify the filesystem.    nas_server  str      The NAS server. This could be the name or ID of the NAS server.    description  str      Additional information that can be mentioned for a Tree Quota. Description parameter can only be used when quota_type is 'tree'    unix_name  str      The name of the unix user account for which quota operations will be performed. Any one among uid/unix_name/windows_name/windows_sid is required when quota_type is 'user'.    windows_name  str      The name of the Windows User for which quota operations will be performed. The name should be mentioned along with Domain Name as 'DOMAIN_NAME\\user_name' or as \"DOMAIN_NAME\\\\user_name\". Any one among uid/unix_name/windows_name/windows_sid is required when quota_type is 'user'.    uid  int      The ID of the unix user account for which quota operations will be performed. Any one among uid/unix_name/windows_name/windows_sid is required when quota_type is 'user'.    windows_sid  str      The SID of the Windows User account for which quota operations will be performed. Any one among uid/unix_name/windows_name/windows_sid is required when quota_type is 'user'.    quota  dict      Specifies Quota parameters.    \u0026nbsp; soft_limit   int      Soft limit of the User/Tree quota. No Soft limit when set to 0.    \u0026nbsp; hard_limit   int      Hard limit of the user quota. No hard limit when set to 0.    \u0026nbsp; cap_unit   str    GB    GB TB   Unit of storage for the hard and soft limits. This parameter is required if limit is specified.    state  str   True     absent present   Define whether the Quota should exist or not. present indicates that the Quota should exist on the system. absent indicates that the Quota should not exist on the system.    array_ip  str   True     IP or FQDN of the PowerStore management system.    verifycert  bool   True     True False   Boolean variable to specify whether to validate SSL certificate or not. True - indicates that the SSL certificate should be verified. Set the environment variable REQUESTS_CA_BUNDLE to the path of the SSL certificate. False - indicates that the SSL certificate should not be verified.    user  str   True     The username of the PowerStore host.    password  str   True     The password of the PowerStore host.    Notes  Tree quota can not be created at the root of the filesystem. When the ID of the filesystem is passed then nas_server is not required. If passed, then filesystem should exist for the nas_server, else the task will fail. If a primary directory of the current directory or a subordinate directory of the path is having a Tree Quota configured, then the quota for that path can\u0026rsquo;t be created. Hierarchical tree quotas are not allowed. When the first quota is created for a directory/user in a filesystem then the quotas will be enabled for that filesystem automatically. If a user quota is to be created on a tree quota, then the user quotas will be enabled automatically in a tree quota. Delete User Quota operation is not supported.  Examples  - name: Create a Quota for a User using unix name dellemc_powerstore_quota: array_ip: \u0026quot;{{array_ip}}\u0026quot; verifycert: \u0026quot;{{verify_cert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; quota_type: \u0026quot;user\u0026quot; unix_name: \u0026quot;{{unix_name}}\u0026quot; filesystem: \u0026quot;sample_fs\u0026quot; nas_server: \u0026quot;{{nas_server_id}}\u0026quot; quota: soft_limit: 5 hard_limit: 10 cap_unit: \u0026quot;TB\u0026quot; state: \u0026quot;present\u0026quot; - name: Create a Tree Quota dellemc_powerstore_quota: array_ip: \u0026quot;{{array_ip}}\u0026quot; verifycert: \u0026quot;{{verify_cert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; quota_type: \u0026quot;tree\u0026quot; path: \u0026quot;/home\u0026quot; filesystem: \u0026quot;sample_fs\u0026quot; nas_server: \u0026quot;sample_nas_server\u0026quot; quota: soft_limit: 5 hard_limit: 10 cap_unit: \u0026quot;TB\u0026quot; state: \u0026quot;present\u0026quot; - name: Modify attributes for Tree Quota dellemc_powerstore_quota: array_ip: \u0026quot;{{array_ip}}\u0026quot; verifycert: \u0026quot;{{verify_cert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; quota_id: \u0026quot;{{quota_id}}\u0026quot; quota: soft_limit: 10 hard_limit: 15 cap_unit: \u0026quot;TB\u0026quot; state: \u0026quot;present\u0026quot; - name: Get details of User Quota dellemc_powerstore_quota: array_ip: \u0026quot;{{array_ip}}\u0026quot; verifycert: \u0026quot;{{verify_cert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; quota_type: \u0026quot;user\u0026quot; uid: 100 path: \u0026quot;/home\u0026quot; filesystem: \u0026quot;{{filesystem_id}}\u0026quot; state: \u0026quot;present\u0026quot; - name: Get details of Tree Quota dellemc_powerstore_quota: array_ip: \u0026quot;{{array_ip}}\u0026quot; verifycert: \u0026quot;{{verify_cert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; quota_id: \u0026quot;{{quota_id}}\u0026quot; state: \u0026quot;present\u0026quot; - name: Delete a Tree Quota dellemc_powerstore_quota: array_ip: \u0026quot;{{array_ip}}\u0026quot; verifycert: \u0026quot;{{verify_cert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; quota_type: \u0026quot;tree\u0026quot; path: \u0026quot;/home\u0026quot; filesystem: \u0026quot;sample_fs\u0026quot; nas_server: \u0026quot;sample_nas_server\u0026quot; state: \u0026quot;absent\u0026quot; Return Values   Key Type Returned Description   changed   bool   always   Whether or not the resource has changed    quota_details   complex   When Quota exists.   The quota details.    \u0026nbsp; description   str  success  ['Additional information about the tree quota.', 'Only applicable for Tree Quotas.']    \u0026nbsp; file_system   complex  success  Includes ID and Name of filesystem and nas server for which smb share exists.    \u0026nbsp; \u0026nbsp; filesystem_type   str  success  Type of filesystem.    \u0026nbsp; \u0026nbsp; id   str  success  ID of filesystem.    \u0026nbsp; \u0026nbsp; name   str  success  Name of filesystem.    \u0026nbsp; \u0026nbsp; nas_server   dict  success  nas_server of filesystem.    \u0026nbsp; hard_limit(cap_unit)   int  success  Value of the Hard Limit imposed on the quota.    \u0026nbsp; id   str  success  The ID of the Quota.    \u0026nbsp; remaining_grace_period   int  success  The time period remaining after which the grace period will expire.    \u0026nbsp; size_used   int  success  Size currently consumed by Tree/User on the filesystem.    \u0026nbsp; soft_limit(cap_unit)   int  success  Value of the Soft Limit imposed on the quota.    \u0026nbsp; state   str  success  ['State of the user quota or tree quota record period.', 'OK means No quota limits are exceeded.', 'Soft_Exceeded means Soft limit is exceeded, and grace period is not expired.', 'Soft_Exceeded_And_Expired means Soft limit is exceeded, and grace period is expired.', 'Hard_Reached means Hard limit is reached.']    \u0026nbsp; state_l10n   str  success  Localized message string corresponding to state.    \u0026nbsp; tree_quota_for_user_quota   complex  success  ['Additional Information of Tree Quota limits on which user quota exists.', 'Only applicable for User Quotas']    \u0026nbsp; \u0026nbsp; description   str  success  Description of Tree Quota for user quota.    \u0026nbsp; \u0026nbsp; hard_limit(cap_unit)   int  success  Value of the Hard Limit imposed on the quota.    \u0026nbsp; \u0026nbsp; path   str  success  The path on which the quota will be imposed.    \u0026nbsp; tree_quota_id   str  success  ['ID of the Tree Quota on which the specific User Quota exists.', 'Only applicable for user quotas.']    \u0026nbsp; uid   int  success  ['The ID of the unix host for which user quota exists.', 'Only applicable for user quotas.']    \u0026nbsp; unix_name   str  success  ['The Name of the unix host for which user quota exists.', 'Only applicable for user quotas.']    \u0026nbsp; windows_name   str  success  ['The Name of the Windows host for which user quota exists.', 'Only applicable for user quotas.']    \u0026nbsp; windows_sid   str  success  ['The SID of the windows host for which user quota exists.', 'Only applicable for user quotas.']    Authors  P Srinivas Rao (@srinivas-rao5) ansible.team@dell.com   Host Module Manage host on PowerStore storage system.\nSynopsis Managing host on PowerStore storage system includes create host with a set of initiators, add/remove initiators from host, rename host and delete host.\nParameters   Parameter Type Required Default Choices Description   host_name  str      The host name. This value must contain 128 or fewer printable Unicode characters. Creation of an empty host is not allowed. Required when creating a host. Use either host_id or host_name for modify and delete tasks.    host_id  str      The 36 character long host id automatically generated when a host is created. Use either host_id or host_name for modify and delete tasks. host_id cannot be used while creating host, as it is generated by the array after creation of host.    os_type  str      Windows Linux ESXi AIX HP-UX Solaris   Operating system of the host. Required when creating a host OS type cannot be modified for a given host.    initiators  list elements: str      List of Initiator WWN or IQN to be added or removed from the host. Subordinate initiators in a host can only be of one type, either FC or iSCSI. Required when creating a host.    state  str   True     absent present   Define whether the host should exist or not. present - indicates that the host should exist in system. absent - indicates that the host should not exist in system.    initiator_state  str      present-in-host absent-in-host   Define whether the initiators should be present or absent in host. present-in-host - indicates that the initiators should exist on host. absent-in-host - indicates that the initiators should not exist on host. Required when creating a host with initiators or adding/removing initiators to/from existing host.    new_name  str      The new name of host for renaming function. This value must contain 128 or fewer printable Unicode characters. Cannot be specified when creating a host.    array_ip  str   True     IP or FQDN of the PowerStore management system.    verifycert  bool   True     True False   Boolean variable to specify whether to validate SSL certificate or not. True - indicates that the SSL certificate should be verified. Set the environment variable REQUESTS_CA_BUNDLE to the path of the SSL certificate. False - indicates that the SSL certificate should not be verified.    user  str   True     The username of the PowerStore host.    password  str   True     The password of the PowerStore host.    Examples  - name: Create host dellemc_powerstore_host: array_ip: \u0026quot;{{array_ip}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; host_name: \u0026quot;{{host_name}}\u0026quot; os_type: 'Windows' initiators: -21:00:00:24:ff:31:e9:fc state: 'present' initiator_state: 'present-in-host' - name: Get host details by name dellemc_powerstore_host: array_ip: \u0026quot;{{array_ip}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; host_name: \u0026quot;{{host_name}}\u0026quot; state: 'present' - name: Get host details by id dellemc_powerstore_host: array_ip: \u0026quot;{{array_ip}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; host_id: \u0026quot;{{host_id}}\u0026quot; state: 'present' - name: Add initiators to host dellemc_powerstore_host: array_ip: \u0026quot;{{array_ip}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; host_name: \u0026quot;{{host_name}}\u0026quot; initiators: -21:00:00:24:ff:31:e9:ee initiator_state: 'present-in-host' state: 'present' - name: Remove initiators from host dellemc_powerstore_host: array_ip: \u0026quot;{{array_ip}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; host_name: \u0026quot;{{host_name}}\u0026quot; initiators: -21:00:00:24:ff:31:e9:ee initiator_state: 'absent-in-host' state: 'present' - name: Rename host dellemc_powerstore_host: array_ip: \u0026quot;{{array_ip}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; host_name: \u0026quot;{{host_name}}\u0026quot; new_name: \u0026quot;{{new_host_name}}\u0026quot; state: 'present' - name: Delete host dellemc_powerstore_host: array_ip: \u0026quot;{{array_ip}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; host_name: \u0026quot;{{new_host_name}}\u0026quot; state: 'absent' Return Values   Key Type Returned Description   changed   bool   always   Whether or not the resource has changed    hostgroup_details   complex   When host group exists   Details of the host group    \u0026nbsp; description   str  success  Description about the host group    \u0026nbsp; hosts   complex  success  The hosts details which are part of this host group    \u0026nbsp; \u0026nbsp; id   str  success  The ID of the host    \u0026nbsp; \u0026nbsp; name   str  success  The name of the host    \u0026nbsp; id   str  success  The system generated ID given to the host group    \u0026nbsp; name   str  success  Name of the host group    Authors  Manisha Agrawal (@agrawm3) ansible.team@dell.com   Snapshot Rule Module SnapshotRule operations on a PowerStore storage system.\nSynopsis Performs all snapshot rule operations on PowerStore Storage System. This modules supports get details of an existing snapshot rule, create new Snapshot Rule with Interval, create new Snapshot Rule with specific time and days_of_week with all supported. parameters. Modify Snapshot Rule with supported parameters. Delete a specific Snapshot Rule.\nParameters   Parameter Type Required Default Choices Description   name  str      String variable. Indicates the name of the Snapshot rule.    snapshotrule_id  str      String variable. Indicates the ID of the Snapshot rule.    new_name  str      String variable. Indicates the new name of the Snapshot rule. Used for renaming operation    days_of_week  list elements: str      Monday Tuesday Wednesday Thursday Friday Saturday Sunday   List of strings to specify days of the week on which the Snapshot rule. should be applied. Must be applied for Snapshot rules where the 'time_of_day' parameter is set. Optional for the Snapshot rule created with an interval. When 'days_of_week' is not specified for a new Snapshot rule, the rule is applied on every day of the week.    interval  str      Five_Minutes Fifteen_Minutes Thirty_Minutes One_Hour Two_Hours Three_Hours Four_Hours Six_Hours Eight_Hours Twelve_Hours One_Day   String variable. Indicates the interval between Snapshots. When creating a Snapshot rule, specify either \"interval\" or \"time_of_day\", but not both.    desired_retention  int      Integer variable. Indicates the desired Snapshot retention period. It is required when creating a new Snapshot rule.    time_of_day  str      String variable. Indicates the time of the day to take a daily Snapshot, with the format \"hh:mm\" in 24 hour time format When creating a Snapshot rule, specify either \"interval\"or \"time_of_day\" but not both.    delete_snaps  bool      Boolean variable to specify whether all Snapshots previously created by this rule should also be deleted when this rule is removed. True specifies to delete all previously created Snapshots by this rule while deleting this rule. False specifies to retain all previously created Snapshots while deleting this rule    state  str   True     present absent   String variable indicates the state of Snapshot rule. For \"Delete\" operation only, it should be set to \"absent\". For all Create, Modify or Get details operation it should be set to \"present\".    array_ip  str   True     IP or FQDN of the PowerStore management system.    verifycert  bool   True     True False   Boolean variable to specify whether to validate SSL certificate or not. True - indicates that the SSL certificate should be verified. Set the environment variable REQUESTS_CA_BUNDLE to the path of the SSL certificate. False - indicates that the SSL certificate should not be verified.    user  str   True     The username of the PowerStore host.    password  str   True     The password of the PowerStore host.    Examples - name: Get details of an existing snapshot rule by name dellemc_powerstore_snapshotrule: array_ip: \u0026quot;{{array_ip}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; name: \u0026quot;{{name}}\u0026quot; state: \u0026quot;present\u0026quot; - name: Get details of an existing snapshot rule by id dellemc_powerstore_snapshotrule: array_ip: \u0026quot;{{array_ip}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; snapshotrule_id: \u0026quot;{{snapshotrule_id}}\u0026quot; state: \u0026quot;present\u0026quot; - name: Create new snapshot rule by interval dellemc_powerstore_snapshotrule: array_ip: \u0026quot;{{array_ip}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; name: \u0026quot;{{name}}\u0026quot; interval: \u0026quot;{{interval}}\u0026quot; days_of_week: - Monday desired_retention: \u0026quot;{{desired_retention}}\u0026quot; state: \u0026quot;present\u0026quot; - name: Create new snapshot rule by time_of_day and days_of_week dellemc_powerstore_snapshotrule: array_ip: \u0026quot;{{array_ip}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; name: \u0026quot;{{name}}\u0026quot; desired_retention: \u0026quot;{{desired_retention}}\u0026quot; days_of_week: - Monday - Wednesday - Friday time_of_day: \u0026quot;{{time_of_day}}\u0026quot; state: \u0026quot;present\u0026quot; - name: Modify existing snapshot rule to time_of_day and days_of_week dellemc_powerstore_snapshotrule: array_ip: \u0026quot;{{array_ip}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; name: \u0026quot;{{name}}\u0026quot; days_of_week: - Monday - Wednesday - Friday - Sunday time_of_day: \u0026quot;{{time_of_day}}\u0026quot; state: \u0026quot;present\u0026quot; - name: Modify existing snapshot rule to interval dellemc_powerstore_snapshotrule: array_ip: \u0026quot;{{array_ip}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; name: \u0026quot;{{name}}\u0026quot; interval: \u0026quot;{{interval}}\u0026quot; state: \u0026quot;present\u0026quot; - name: Delete an existing snapshot rule by name dellemc_powerstore_snapshotrule: array_ip: \u0026quot;{{array_ip}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; name: \u0026quot;{{name}}\u0026quot; state: \u0026quot;absent\u0026quot; Return Values   Key Type Returned Description   changed   bool   always   Whether or not the resource has changed    snapshotrule_details   complex   When snapshot rule exists   Details of the snapshot rule    \u0026nbsp; days_of_week   list  success  List of string to specify days of the week on which the rule should be applied    \u0026nbsp; desired_retention   int  success  Desired snapshot retention period    \u0026nbsp; id   str  success  The system generated ID given to the snapshot rule    \u0026nbsp; interval   str  success  The interval between snapshots    \u0026nbsp; name   str  success  Name of the snapshot rule    \u0026nbsp; policies   complex  success  The protection policies details of the snapshot rule    \u0026nbsp; \u0026nbsp; id   str  success  The protection policy ID in which the snapshot rule is selected    \u0026nbsp; \u0026nbsp; name   str  success  Name of the protection policy in which the snapshot rule is selected    \u0026nbsp; time_of_day   str  success  The time of the day to take a daily snapshot    Authors  Arindam Datta (@dattaarindam) ansible.team@dell.com   Gatherfacts Module Gathers information about PowerStore Storage entities\nSynopsis Gathers the list of specified PowerStore Storage System entities, such as the list of cluster nodes, volumes, volume groups, hosts, host groups, snapshot rules, protection policies, NAS servers, NFS exports, SMB shares, tree quotas, user quotas, and file systems.\nParameters   Parameter Type Required Default Choices Description   gather_subset  list elements: str   True     vol vg host hg node protection_policy snapshot_rule nas_server nfs_export smb_share tree_quota user_quota file_system replication_rule replication_session remote_system   A list of string variables which specify the PowerStore system entities requiring information.information. vol - volumes node - all the nodes vg - volume groups protection_policy - protection policy host - hosts hg - host groups snapshot_rule - snapshot rule nas_server - NAS servers nfs_export - NFS exports smb_share - SMB shares tree_quota - tree quotas user_quota - user quotas file_system - file systems replication_rule - replication rules replication_session - replication sessions remote_system - remote systems    filters  list elements: dict      A list of filters to support filtered output for storage entities. Each filter is a list of filter_key, filter_operator, filter_value. Supports passing of multiple filters.    \u0026nbsp; filter_key   str   True     Name identifier of the filter.    \u0026nbsp; filter_operator   str   True     equal greater lesser like notequal   Operation to be performed on the filter key.    \u0026nbsp; filter_value   str   True     Value of the filter key.    all_pages  bool    False    Indicates whether to return all available entities on the storage system. If set to True, the Gather Facts module will implement pagination and return all entities. Otherwise, a maximum of the first 100 entities of any type will be returned.    array_ip  str   True     IP or FQDN of the PowerStore management system.    verifycert  bool   True     True False   Boolean variable to specify whether to validate SSL certificate or not. True - indicates that the SSL certificate should be verified. Set the environment variable REQUESTS_CA_BUNDLE to the path of the SSL certificate. False - indicates that the SSL certificate should not be verified.    user  str   True     The username of the PowerStore host.    password  str   True     The password of the PowerStore host.    Examples - name: Get list of volumes, volume groups, hosts, host groups and node dellemc_powerstore_gatherfacts: array_ip: \u0026quot;{{array_ip}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; gather_subset: - vol - vg - host - hg - node - name: Get list of replication related entities dellemc_powerstore_gatherfacts: array_ip: \u0026quot;{{array_ip}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; gather_subset: - replication_rule - replication_session - remote_system - name: Get list of volumes whose state notequal to ready dellemc_powerstore_gatherfacts: array_ip: \u0026quot;{{array_ip}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; gather_subset: - vol filters: - filter_key: \u0026quot;state\u0026quot; filter_operator: \u0026quot;notequal\u0026quot; filter_value: \u0026quot;ready\u0026quot; - name: Get list of protection policies and snapshot rules dellemc_powerstore_gatherfacts: array_ip: \u0026quot;{{array_ip}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; gather_subset: - protection_policy - snapshot_rule - name: Get list of snapshot rules whose desired_retention between 101-499 dellemc_powerstore_gatherfacts: array_ip: \u0026quot;{{array_ip}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; gather_subset: - snapshot_rule filters: - filter_key: \u0026quot;desired_retention\u0026quot; filter_operator: \u0026quot;greater\u0026quot; filter_value: \u0026quot;100\u0026quot; - filter_key: \u0026quot;desired_retention\u0026quot; filter_operator: \u0026quot;lesser\u0026quot; filter_value: \u0026quot;500\u0026quot; - name: Get list of nas server, nfs_export and smb share dellemc_powerstore_gatherfacts: array_ip: \u0026quot;{{array_ip}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; gather_subset: - nas_server - nfs_export - smb_share - name: Get list of tree quota, user quota and file system dellemc_powerstore_gatherfacts: array_ip: \u0026quot;{{array_ip}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; gather_subset: - tree_quota - user_quota - file_system - name: Get list of nas server whose name equal to 'nas_server' dellemc_powerstore_gatherfacts: array_ip: \u0026quot;{{array_ip}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; gather_subset: - nas_server filters: - filter_key: \u0026quot;name\u0026quot; filter_operator: \u0026quot;equal\u0026quot; filter_value: \u0026quot;nas_server\u0026quot; - name: Get list of smb share whose name contains 'share' dellemc_powerstore_gatherfacts: array_ip: \u0026quot;{{array_ip}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; gather_subset: - nas_server filters: - filter_key: \u0026quot;name\u0026quot; filter_operator: \u0026quot;like\u0026quot; filter_value: \u0026quot;*share*\u0026quot; Return Values   Key Type Returned Description   changed   bool   always   Shows whether or not the resource has changed    subset_result   complex   always   Provides details of all given subsets.    \u0026nbsp; Cluster   list  success  Provides details of all clusters.    \u0026nbsp; \u0026nbsp; id   str  success  cluster id    \u0026nbsp; \u0026nbsp; name   str  success  cluster name    \u0026nbsp; FileSystems   list  success  Provides details of all filesystems.    \u0026nbsp; \u0026nbsp; id   str  success  filesystem id    \u0026nbsp; \u0026nbsp; name   str  success  filesystem name    \u0026nbsp; HostGroups   list  success  Provides details of all hostgroups.    \u0026nbsp; \u0026nbsp; id   str  success  hostgroup id    \u0026nbsp; \u0026nbsp; name   str  success  hostgroup name    \u0026nbsp; Hosts   list  success  Provides details of all hosts.    \u0026nbsp; \u0026nbsp; id   str  success  host id    \u0026nbsp; \u0026nbsp; name   str  success  host name    \u0026nbsp; NASServers   list  success  Provides details of all nas servers.    \u0026nbsp; \u0026nbsp; id   str  success  nas server id    \u0026nbsp; \u0026nbsp; name   str  success  nas server name    \u0026nbsp; NFSExports   list  success  Provides details of all nfs exports.    \u0026nbsp; \u0026nbsp; id   str  success  nfs export id    \u0026nbsp; \u0026nbsp; name   str  success  nfs export name    \u0026nbsp; Nodes   list  success  Provides details of all nodes.    \u0026nbsp; \u0026nbsp; id   str  success  node id    \u0026nbsp; \u0026nbsp; name   str  success  node name    \u0026nbsp; ProtectionPolicies   list  success  Provides details of all protectionpolicies.    \u0026nbsp; \u0026nbsp; id   str  success  protectionpolicy id    \u0026nbsp; \u0026nbsp; name   str  success  protectionpolicy name    \u0026nbsp; RemoteSystems   list  success  Provides details of all remote systems.    \u0026nbsp; \u0026nbsp; id   str  success  remote system id    \u0026nbsp; \u0026nbsp; name   str  success  remote system name    \u0026nbsp; ReplicationRules   list  success  Provides details of all replication rules.    \u0026nbsp; \u0026nbsp; id   str  success  replication rule id    \u0026nbsp; \u0026nbsp; name   str  success  replication rule name    \u0026nbsp; ReplicationSession   list  success  details of all replication sessions    \u0026nbsp; \u0026nbsp; id   str  success  replication session id    \u0026nbsp; SMBShares   list  success  Provides details of all smb shares.    \u0026nbsp; \u0026nbsp; id   str  success  smb share id    \u0026nbsp; \u0026nbsp; name   str  success  smb share name    \u0026nbsp; SnapshotRules   list  success  Provides details of all snapshot rules.    \u0026nbsp; \u0026nbsp; id   str  success  snapshot rule id    \u0026nbsp; \u0026nbsp; name   str  success  snapshot rule name    \u0026nbsp; TreeQuotas   list  success  Provides details of all tree quotas.    \u0026nbsp; \u0026nbsp; id   str  success  tree quota id    \u0026nbsp; \u0026nbsp; path   str  success  tree quota path    \u0026nbsp; UserQuotas   list  success  Provides details of all user quotas    \u0026nbsp; \u0026nbsp; id   str  success  user quota id    \u0026nbsp; VolumeGroups   list  success  Provides details of all volumegroups.    \u0026nbsp; \u0026nbsp; id   str  success  volumegroup id    \u0026nbsp; \u0026nbsp; name   str  success  volumegroup name    \u0026nbsp; Volumes   list  success  Provides details of all volumes.    \u0026nbsp; \u0026nbsp; id   str  success  volume id    \u0026nbsp; \u0026nbsp; name   str  success  volume name    Authors  Arindam Datta (@dattaarindam) ansible.team@dell.com Vivek Soni (@v-soni11) ansible.team@dell.com   Replication Session Module Replication session operations on a PowerStore storage system.\nSynopsis Performs all replication session state change operations on a PowerStore Storage System. This module supports get details of an existing replication session. Updating the state of the replication session.\nParameters   Parameter Type Required Default Choices Description   volume_group  str      Name/ID of the volume group for which a replication session exists. volume_group, volume, and session_id are mutually exclusive.    volume  str      Name/ID of the volume for which replication session exists. volume_group, volume, and session_id are mutually exclusive.    session_id  str      ID of the replication session. volume_group, volume, and session_id are mutually exclusive.    session_state  str      failed_over paused synchronizing   State in which the replication session is present after performing the task.    array_ip  str   True     IP or FQDN of the PowerStore management system.    verifycert  bool   True     True False   Boolean variable to specify whether to validate SSL certificate or not. True - indicates that the SSL certificate should be verified. Set the environment variable REQUESTS_CA_BUNDLE to the path of the SSL certificate. False - indicates that the SSL certificate should not be verified.    user  str   True     The username of the PowerStore host.    password  str   True     The password of the PowerStore host.    Notes  Manual synchronization for a replication session is not supported through the Ansible module. When the current state of the replication session is \u0026lsquo;OK\u0026rsquo; and in the playbook task \u0026lsquo;synchronizing\u0026rsquo;, then it will return \u0026ldquo;changed\u0026rdquo; as False. This is because there is a scheduled synchronization in place with the associated replication rule\u0026rsquo;s RPO in the protection policy.  Examples - name: Pause a replication session dellemc_powerstore_replicationsession: array_ip: \u0026quot;{{array_ip}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; volume: \u0026quot;sample_volume_1\u0026quot; session_state: \u0026quot;paused\u0026quot; - name: Synchronize a replication session dellemc_powerstore_replicationsession: array_ip: \u0026quot;{{array_ip}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; volume: \u0026quot;sample_volume_1\u0026quot; session_state: \u0026quot;synchronizing\u0026quot; - name: Get details of a replication session dellemc_powerstore_replicationsession: array_ip: \u0026quot;{{array_ip}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; volume: \u0026quot;sample_volume_1\u0026quot; - name: Fail over a replication session dellemc_powerstore_replicationsession: array_ip: \u0026quot;{{array_ip}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; volume: \u0026quot;sample_volume_1\u0026quot; session_state: \u0026quot;failed_over\u0026quot; Return Values   Key Type Returned Description   changed   bool   always   Whether or not the resource has changed    replication_session_details   complex   When replication session exists   Details of the replication session    \u0026nbsp; estimated_completion_timestamp   str  success  Estimated completion time of the current replication operation.    \u0026nbsp; id   str  success  ['The system generated ID of the replication session.', 'Unique across source and destination roles.']    \u0026nbsp; last_sync_timestamp   str  success  Time of last successful synchronization.    \u0026nbsp; local_resource_id   str  success  Unique identifier of the local storage resource for the replication session.    \u0026nbsp; name   str  success  Name of the replication rule.    \u0026nbsp; progress_percentage   int  success  Progress of the current replication operation.    \u0026nbsp; remote_resource_id   str  success  Unique identifier of the remote storage resource for the replication session.    \u0026nbsp; remote_system_id   str  success  Unique identifier of the remote system instance.    \u0026nbsp; replication_rule_id   str  success  Associated replication rule instance if created by policy engine.    \u0026nbsp; resource_type   str  success  ['Storage resource type eligible for replication protection.', 'volume - Replication session created on a volume.', 'volume_group - Replication session created on a volume group.']    \u0026nbsp; role   str  success  ['Role of the replication session.', 'Source - The local resource is the source of the remote replication session.', 'Destination - The local resource is the destination of the remote replication session.']    \u0026nbsp; state   str  success  State of the replication session.    Authors  P Srinivas Rao (@srinivas-rao5) ansible.team@dell.com   Host Group Module Manage host group on PowerStore Storage System.\nSynopsis Managing host group on PowerStore storage system includes create host group with a set of hosts, add/remove hosts from host group, rename host group, and delete host group. Deletion of a host group results in deletion of the containing hosts as well. Remove hosts from the host group first to retain them.\nParameters   Parameter Type Required Default Choices Description   hostgroup_name  str      The host group name. This value must contain 128 or fewer printable Unicode characters. Creation of an empty host group is not allowed. Required when creating a host group. Use either hostgroup_id or hostgroup_name for modify and delete tasks.    hostgroup_id  str      The 36-character long host group id, automatically generated when a host group is created. Use either hostgroup_id or hostgroup_name for modify and delete tasks. hostgroup_id cannot be used while creating host group, as it is generated by the array after creation of host group.    hosts  list elements: str      List of hosts to be added or removed from the host group. Subordinate hosts in a host group can only be of one type, either FC or iSCSI. Required when creating a host group. To represent host, both name or ID can be used interchangeably. The module will detect both.    state  str   True     absent present   Define whether the host group should exist or not. present - indicates that the host group should exist on the system. absent - indicates that the host group should not exist on the system. Deletion of a host group results in deletion of the containing hosts as well. Remove hosts from the host group first to retain them.    host_state  str      present-in-group absent-in-group   Define whether the hosts should be present or absent in host group. present-in-group - indicates that the hosts should exist on the host group. absent-in-group - indicates that the hosts should not exist on the host group. Required when creating a host group with hosts or adding/removing hosts from existing host group.    new_name  str      The new name for host group renaming function. This value must contain 128 or fewer printable Unicode characters.    array_ip  str   True     IP or FQDN of the PowerStore management system.    verifycert  bool   True     True False   Boolean variable to specify whether to validate SSL certificate or not. True - indicates that the SSL certificate should be verified. Set the environment variable REQUESTS_CA_BUNDLE to the path of the SSL certificate. False - indicates that the SSL certificate should not be verified.    user  str   True     The username of the PowerStore host.    password  str   True     The password of the PowerStore host.    Examples  - name: Create host group with hosts using host name dellemc_powerstore_hostgroup: array_ip: \u0026quot;{{array_ip}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; hostgroup_name: \u0026quot;{{hostgroup_name}}\u0026quot; hosts: - host1 - host2 state: 'present' host_state: 'present-in-group' - name: Create host group with hosts using host ID dellemc_powerstore_hostgroup: array_ip: \u0026quot;{{array_ip}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; hostgroup_name: \u0026quot;{{hostgroup_name}}\u0026quot; hosts: - c17fc987-bf82-480c-af31-9307b89923c3 state: 'present' host_state: 'present-in-group' - name: Get host group details dellemc_powerstore_hostgroup: array_ip: \u0026quot;{{array_ip}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; hostgroup_name: \u0026quot;{{hostgroup_name}}\u0026quot; state: 'present' - name: Get host group details using ID dellemc_powerstore_hostgroup: array_ip: \u0026quot;{{array_ip}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; hostgroup_id: \u0026quot;{{host group_id}}\u0026quot; state: 'present' - name: Add hosts to host group dellemc_powerstore_hostgroup: array_ip: \u0026quot;{{array_ip}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; hostgroup_name: \u0026quot;{{hostgroup_name}}\u0026quot; hosts: - host3 host_state: 'present-in-group' state: 'present' - name: Remove hosts from host group dellemc_powerstore_hostgroup: array_ip: \u0026quot;{{array_ip}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; hostgroup_name: \u0026quot;{{hostgroup_name}}\u0026quot; hosts: - host3 host_state: 'absent-in-group' state: 'present' - name: Rename host group dellemc_powerstore_hostgroup: array_ip: \u0026quot;{{array_ip}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; hostgroup_name: \u0026quot;{{hostgroup_name}}\u0026quot; new_name: \u0026quot;{{new_hostgroup_name}}\u0026quot; state: 'present' - name: Delete host group dellemc_powerstore_hostgroup: array_ip: \u0026quot;{{array_ip}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; hostgroup_name: \u0026quot;{{hostgroup_name}}\u0026quot; state: 'absent' Return Values   Key Type Returned Description   changed   bool   always   Whether or not the resource has changed    hostgroup_details   complex   When host group exists   Details of the host group    \u0026nbsp; description   str  success  Description about the host group    \u0026nbsp; hosts   complex  success  The hosts details which are part of this host group    \u0026nbsp; \u0026nbsp; id   str  success  The ID of the host    \u0026nbsp; \u0026nbsp; name   str  success  The name of the host    \u0026nbsp; id   str  success  The system generated ID given to the host group    \u0026nbsp; name   str  success  Name of the host group    Authors  Manisha Agrawal (@agrawm3) ansible.team@dell.com   NFS Module Manage NFS exports on Dell EMC PowerStore.\nSynopsis Managing NFS exports on PowerStore Storage System includes creating new NFS Export, getting details of NFS export, modifying attributes of NFS export, and deleting NFS export.\nParameters   Parameter Type Required Default Choices Description   nfs_export_name  str      The name of the NFS export. Mandatory for create operation. Specify either nfs_export_name or nfs_export_id(but not both) for any operation.    nfs_export_id  str      The ID of the NFS export.    filesystem  str      The ID/Name of the filesystem for which the NFS export will be created. Either filesystem or snapshot is required for creation of the NFS Export. If filesystem name is specified, then nas_server is required to uniquely identify the filesystem. If filesystem parameter is provided, then snapshot cannot be specified.    snapshot  str      The ID/Name of the Snapshot for which NFS export will be created. Either filesystem or snapshot is required for creation of the NFS Export. If snapshot name is specified, then nas_server is required to uniquely identify the snapshot. If snapshot parameter is provided, then filesystem cannot be specified. NFS export can be created only if access type of snapshot is \"protocol\".    nas_server  str      The NAS server. This could be the name or ID of the NAS server.    path  str      Local path to export relative to the NAS server root. With NFS, each export of a file_system or file_snap must have a unique local path. Mandatory while creating NFS export.    description  str      The description for the NFS export.    default_access  str      NO_ACCESS READ_ONLY READ_WRITE ROOT READ_ONLY_ROOT   Default access level for all hosts that can access the Export. For hosts that need different access than the default, they can be configured by adding to the list. If default_access is not mentioned during creation, then NFS export will be created with No_Access.    no_access_hosts  list elements: str      Hosts with no access to the NFS export.    read_only_hosts  list elements: str      Hosts with read-only access to the NFS export.    read_only_root_hosts  list elements: str      Hosts with read-only access for root user to the NFS export.    read_write_hosts  list elements: str      Hosts with read and write access to the NFS export.    read_write_root_hosts  list elements: str      Hosts with read and write access for root user to the NFS export.    min_security  str      SYS KERBEROS KERBEROS_WITH_INTEGRITY KERBEROS_WITH_ENCRYPTION   NFS enforced security type for users accessing an NFS export. If not specified at the time of creation, it will be set to SYS.    anonymous_uid  int      Specifies the user ID of the anonymous account. If not specified at the time of creation, it will be set to -2.    anonymous_gid  int      Specifies the group ID of the anonymous account. If not specified at the time of creation, it will be set to -2.    is_no_suid  bool      If set, do not allow access to set SUID. Otherwise, allow access. If not specified at the time of creation, it will be set to False.    host_state  str      present-in-export absent-in-export   Define whether the hosts can access the NFS export. Required when adding or removing host access from the export.    state  str   True     absent present   Define whether the NFS export should exist or not.    array_ip  str   True     IP or FQDN of the PowerStore management system.    verifycert  bool   True     True False   Boolean variable to specify whether to validate SSL certificate or not. True - indicates that the SSL certificate should be verified. Set the environment variable REQUESTS_CA_BUNDLE to the path of the SSL certificate. False - indicates that the SSL certificate should not be verified.    user  str   True     The username of the PowerStore host.    password  str   True     The password of the PowerStore host.    Examples - name: Create NFS export (filesystem) dellemc_powerstore_nfs: array_ip: \u0026quot;{{array_ip}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; nfs_export_name: \u0026quot;{{export_name1}}\u0026quot; filesystem: \u0026quot;{{filesystem}}\u0026quot; nas_server: \u0026quot;{{nas_server}}\u0026quot; path: \u0026quot;{{path1}}\u0026quot; description: \u0026quot;sample description\u0026quot; default_access: \u0026quot;NO_ACCESS\u0026quot; no_access_hosts: - \u0026quot;{{host5}}\u0026quot; read_only_hosts: - \u0026quot;{{host1}}\u0026quot; read_only_root_hosts: - \u0026quot;{{host2}}\u0026quot; read_write_hosts: - \u0026quot;{{host3}}\u0026quot; read_write_root_hosts: - \u0026quot;{{host4}}\u0026quot; min_security: \u0026quot;SYS\u0026quot; anonymous_uid: 1000 anonymous_gid: 1000 is_no_suid: True host_state: \u0026quot;present-in-export\u0026quot; state: \u0026quot;present\u0026quot; - name: Create NFS export Create NFS export for filesystem snapshot with mandatory parameters dellemc_powerstore_nfs: array_ip: \u0026quot;{{array_ip}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; nfs_export_name: \u0026quot;{{export_name2}}\u0026quot; snapshot: \u0026quot;{{snapshot}}\u0026quot; nas_server: \u0026quot;{{nas_server}}\u0026quot; path: \u0026quot;{{path2}}\u0026quot; state: \u0026quot;present\u0026quot; - name: Get NFS export details using ID dellemc_powerstore_nfs: array_ip: \u0026quot;{{array_ip}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; nfs_export_id: \u0026quot;{{export_id}}\u0026quot; state: \u0026quot;present\u0026quot; - name: Add Read-Only and Read-Write hosts to NFS export dellemc_powerstore_nfs: array_ip: \u0026quot;{{array_ip}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; nfs_export_id: \u0026quot;{{export_id}}\u0026quot; read_only_hosts: - \u0026quot;{{host5}}\u0026quot; read_write_hosts: - \u0026quot;{{host6}}\u0026quot; host_state: \u0026quot;present-in-export\u0026quot; state: \u0026quot;present\u0026quot; - name: Remove Read-Only and Read-Write hosts from NFS export dellemc_powerstore_nfs: array_ip: \u0026quot;{{array_ip}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; nfs_export_id: \u0026quot;{{export_id}}\u0026quot; read_only_hosts: - \u0026quot;{{host1}}\u0026quot; read_write_hosts: - \u0026quot;{{host3}}\u0026quot; host_state: \u0026quot;absent-in-export\u0026quot; state: \u0026quot;present\u0026quot; - name: Modify the attributes of NFS export dellemc_powerstore_nfs: array_ip: \u0026quot;{{array_ip}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; nfs_export_id: \u0026quot;{{export_id}}\u0026quot; description: \u0026quot;modify description\u0026quot; default_access: \u0026quot;ROOT\u0026quot; state: \u0026quot;present\u0026quot; - name: Delete NFS export using name dellemc_powerstore_nfs: array_ip: \u0026quot;{{array_ip}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; nfs_export_name: \u0026quot;{{export_name}}\u0026quot; nas_server: \u0026quot;{{nas_server}}\u0026quot; state: \u0026quot;absent\u0026quot; Return Values   Key Type Returned Description   changed   bool   always   Whether or not the resource has changed    nfs_export_details   complex   When NFS export exists.   The NFS export details.    \u0026nbsp; anonymous_GID   int  success  The group ID of the anonymous account.    \u0026nbsp; anonymous_UID   int  success  The user ID of the anonymous account.    \u0026nbsp; default_access   str  success  Default access level for all hosts that can access the export.    \u0026nbsp; description   str  success  The description for the NFS export.    \u0026nbsp; file_system   complex  success  Details of filesystem and NAS server on which NFS export is present.    \u0026nbsp; \u0026nbsp; filesystem_type   str  success  The type of the filesystem.    \u0026nbsp; \u0026nbsp; id   str  success  The ID of the filesystem.    \u0026nbsp; \u0026nbsp; name   str  success  The name of the filesystem.    \u0026nbsp; \u0026nbsp; nas_server   complex  success  Details of NAS server.    \u0026nbsp; \u0026nbsp; \u0026nbsp; id   str  success  The ID of the NAS server.    \u0026nbsp; \u0026nbsp; \u0026nbsp; name   str  success  The name of the NAS server.    \u0026nbsp; id   str  success  The ID of the NFS export.    \u0026nbsp; is_no_SUID   bool  success  If set, do not allow access to set SUID. Otherwise, allow access.    \u0026nbsp; min_security   str  success  NFS enforced security type for users accessing an NFS export.    \u0026nbsp; name   str  success  The name of the NFS export.    \u0026nbsp; no_access_hosts   list  success  Hosts with no access to the NFS export.    \u0026nbsp; path   str  success  Local path to a location within the file system.    \u0026nbsp; read_only_hosts   list  success  Hosts with read-only access to the NFS export.    \u0026nbsp; read_only_root_hosts   list  success  Hosts with read-only for root user access to the NFS export.    \u0026nbsp; read_write_hosts   list  success  Hosts with read and write access to the NFS export.    \u0026nbsp; read_write_root_hosts   list  success  Hosts with read and write for root user access to the NFS export.    Authors  Akash Shendge (@shenda1) ansible.team@dell.com   Volume Group Module Manage volume groups on a PowerStore Storage System\nSynopsis Managing volume group on PowerStore Storage System includes creating new volume group, adding volumes to volume group, removing volumes from volume group, renaming volume group, modifying volume group, and deleting volume group.\nParameters   Parameter Type Required Default Choices Description   vg_name  str      The name of the volume group.    vg_id  str      The id of the volume group. It can be used only for Modify, Add/Remove, or Delete operation.    volumes  list elements: str      This is a list of volumes. Either the volume ID or name must be provided for adding/removing existing volumes from a volume group. If volumes are given, then vol_state should also be specified.    vol_state  str      present-in-group absent-in-group   String variable. Describes the state of volumes inside a volume group. If volume is given, then vol_state should also be specified.    new_vg_name  str      The new name of the volume group.    description  str      Description about the volume group.    protection_policy  str      String variable. Represents Protection policy id or name used for volume group. Specifying an empty string or \"\" removes the existing protection policy from volume group.    is_write_order_consistent  bool      A boolean flag to indicate whether Snapshot sets of the volume group will be write-order consistent. If this parameter is not specified, the array by default sets it to true.    state  str   True     absent present   Define whether the volume group should exist or not.    array_ip  str   True     IP or FQDN of the PowerStore management system.    verifycert  bool   True     True False   Boolean variable to specify whether to validate SSL certificate or not. True - indicates that the SSL certificate should be verified. Set the environment variable REQUESTS_CA_BUNDLE to the path of the SSL certificate. False - indicates that the SSL certificate should not be verified.    user  str   True     The username of the PowerStore host.    password  str   True     The password of the PowerStore host.    Notes  vol_state is mandatory if volumes are provided. A protection policy can be specified either for an volume group, or for the individual volumes inside the volume group. A volume can be a member of at most one volume group. Specifying \u0026ldquo;protection_policy\u0026rdquo; as empty string or \u0026quot;\u0026rdquo; removes the existing protection policy from a volume group.  Examples - name: Create volume group without protection policy dellemc_powerstore_volumegroup: array_ip: \u0026quot;{{array_ip}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; vg_name: \u0026quot;{{vg_name}}\u0026quot; description: \u0026quot;This volume group is for ansible\u0026quot; state: \u0026quot;present\u0026quot; - name: Get details of volume group dellemc_powerstore_volumegroup: array_ip: \u0026quot;{{array_ip}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; vg_name: \u0026quot;{{vg_name}}\u0026quot; state: \u0026quot;present\u0026quot; - name: Add volumes to volume group dellemc_powerstore_volumegroup: array_ip: \u0026quot;{{array_ip}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; vg_name: \u0026quot;{{vg_name}}\u0026quot; state: \u0026quot;present\u0026quot; volumes: - \u0026quot;7f879569-676c-4749-a06f-c2c30e09b295\u0026quot; - \u0026quot;68e4dad5-5de5-4644-a98f-6d4fb916e169\u0026quot; - \u0026quot;Ansible_Testing\u0026quot; vol_state: \u0026quot;present-in-group\u0026quot; - name: Remove volumes from volume group dellemc_powerstore_volumegroup: array_ip: \u0026quot;{{array_ip}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; vg_name: \u0026quot;{{vg_name}}\u0026quot; state: \u0026quot;present\u0026quot; volumes: - \u0026quot;7f879569-676c-4749-a06f-c2c30e09b295\u0026quot; - \u0026quot;Ansible_Testing\u0026quot; vol_state: \u0026quot;absent-in-group\u0026quot; - name: Rename volume group and change is_write_order_consistent flag dellemc_powerstore_volumegroup: array_ip: \u0026quot;{{array_ip}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; vg_name: \u0026quot;{{vg_name}}\u0026quot; new_vg_name: \u0026quot;{{new_vg_name}}\u0026quot; is_write_order_consistent: False state: \u0026quot;present\u0026quot; - name: Get details of volume group by ID dellemc_powerstore_volumegroup: array_ip: \u0026quot;{{array_ip}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; vg_id: \u0026quot;{{vg_id}}\u0026quot; state: \u0026quot;present\u0026quot; - name: Delete volume group dellemc_powerstore_volumegroup: array_ip: \u0026quot;{{array_ip}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; name: \u0026quot;{{new_vg_name}}\u0026quot; state: \u0026quot;absent\u0026quot; Return Values   Key Type Returned Description   add_vols_to_vg   bool   When value exists   A boolean flag to indicate whether volume/s got added to volume group    changed   bool   always   Whether or not the resource has changed    create_vg   bool   When value exists   A boolean flag to indicate whether volume group got created    delete_vg   bool   When value exists   A boolean flag to indicate whether volume group got deleted    modify_vg   bool   When value exists   A boolean flag to indicate whether volume group got modified    remove_vols_from_vg   bool   When value exists   A boolean flag to indicate whether volume/s got removed from volume group    volume_group_details   complex   When volume group exists   Details of the volume group    \u0026nbsp; description   str  success  ['description about the volume group']    \u0026nbsp; id   str  success  ['The system generated ID given to the volume group']    \u0026nbsp; is_write_order_consistent   bool  success  ['A boolean flag to indicate whether snapshot sets of the volume group will be write-order consistent']    \u0026nbsp; name   str  success  ['Name of the volume group']    \u0026nbsp; protection_policy_id   str  success  ['The protection policy of the volume group']    \u0026nbsp; type   str  success  ['The type of the volume group']    \u0026nbsp; volumes   complex  success  ['The volumes details of the volume group']    \u0026nbsp; \u0026nbsp; id   str  success  ['The system generated ID given to the volume associated with the volume group']    \u0026nbsp; \u0026nbsp; name   str  success  ['The name of the volume associated with the volume group.']    Authors  Akash Shendge (@shenda1) ansible.team@dell.com Arindam Datta (@dattaarindam) ansible.team@dell.com   NAS Server Module NAS Server operations on PowerStore Storage system.\nSynopsis Supports getting the details and modifying the attributes of a NAS server.\nParameters   Parameter Type Required Default Choices Description   nas_server_name  str      Name of the NAS server. Mutually exclusive with nas_server_id.    nas_server_id  str      Unique id of the NAS server. Mutually exclusive with nas_server_name.    description  str      Description of the NAS server.    nas_server_new_name  str      New name of the NAS server for a rename operation.    current_node  str      Unique identifier or name of the node on which the NAS server is running.    preferred_node  str      Unique identifier or name of the preferred node for the NAS server. The initial value (on NAS server create) is taken from the current node.    current_unix_directory_service  str      NIS LDAP LOCAL_FILES LOCAL_THEN_NIS LOCAL_THEN_LDAP   Define the Unix directory service used for looking up identity information for Unix such as UIDs, GIDs, net groups, and so on.    default_unix_user  str      Default Unix user name used for granting access in case of Windows to Unix user mapping failure. When empty, access in such case is denied.    default_windows_user  str      Default Windows user name used for granting access in case of Unix to Windows user mapping failure. When empty, access in such case is denied.    state  str   True     absent present   Define whether the nas server should exist or not.    array_ip  str   True     IP or FQDN of the PowerStore management system.    verifycert  bool   True     True False   Boolean variable to specify whether to validate SSL certificate or not. True - indicates that the SSL certificate should be verified. Set the environment variable REQUESTS_CA_BUNDLE to the path of the SSL certificate. False - indicates that the SSL certificate should not be verified.    user  str   True     The username of the PowerStore host.    password  str   True     The password of the PowerStore host.    Examples  - name: Get details of NAS Server by name dellemc_powerstore_nasserver: array_ip: \u0026quot;{{array_ip}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; nas_server_name: \u0026quot;{{nas_server_name}}\u0026quot; state: \u0026quot;present\u0026quot; - name: Get Details of NAS Server by ID dellemc_powerstore_nasserver: array_ip: \u0026quot;{{array_ip}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; nas_server_id: \u0026quot;{{nas_id}}\u0026quot; state: \u0026quot;present\u0026quot; - name: Rename NAS Server by Name dellemc_powerstore_nasserver: array_ip: \u0026quot;{{array_ip}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; nas_server_name: \u0026quot;{{nas_server_name}}\u0026quot; nas_server_new_name : \u0026quot;{{nas_server_new_name}}\u0026quot; state: \u0026quot;present\u0026quot; - name: Modify NAS Server attributes by ID dellemc_powerstore_nasserver: array_ip: \u0026quot;{{array_ip}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; nas_server_id: \u0026quot;{{nas_id}}\u0026quot; current_unix_directory_service: \u0026quot;LOCAL_FILES\u0026quot; current_node: \u0026quot;{{cur_node_n1}}\u0026quot; preferred_node: \u0026quot;{{prefered_node}}\u0026quot; state: \u0026quot;present\u0026quot; Return Values   Key Type Returned Description   changed   bool   always   Whether or not the resource has changed    nasserver_details   complex   When nas server exists   Details about the nas server    \u0026nbsp; backup_IPv4_interface_id   str  success  Unique identifier of the preferred IPv4 backup interface.    \u0026nbsp; backup_IPv6_interface_id   str  success  Unique identifier of the preferred IPv6 backup interface.    \u0026nbsp; current_node   dict  success  Unique identifier and name of the node on which the NAS server is running.    \u0026nbsp; current_unix_directory_service   str  success  Define the Unix directory service used for looking up identity information for Unix such as UIDs, GIDs, net groups, and so on.    \u0026nbsp; default_unix_user   str  success  Default Unix user name used for granting access in case of Windows to Unix user mapping failure.    \u0026nbsp; description   str  success  Additional information about the nas server.    \u0026nbsp; file_interfaces   dict  success  This is the inverse of the resource type file_interface association.Will return the id,name \u0026 ip_address of the associated file interface    \u0026nbsp; file_ldaps   str  success  This is the inverse of the resource type file_ldap association.    \u0026nbsp; file_systems   dict  success  This is the inverse of the resource type file_system association.    \u0026nbsp; id   str  success  The system generated ID given to the nas server    \u0026nbsp; is_username_translation_enabled   bool  success  Enable the possibility to match a windows account to a Unix account with different names.    \u0026nbsp; name   str  success  Name of the nas server    \u0026nbsp; nfs_servers   str  success  This is the inverse of the resource type nfs_server association.    \u0026nbsp; operational_status   str  success  NAS server operational status.    \u0026nbsp; preferred_node   dict  success  Unique identifier and name of the preferred node for the NAS server.    \u0026nbsp; production_IPv4_interface_id   str  success  Unique identifier of the preferred IPv4 production interface.    \u0026nbsp; production_IPv6_interface_id   str  success  Unique identifier of the preferred IPv6 production interface.    \u0026nbsp; smb_servers   str  success  This is the inverse of the resource type smb_server association.    Authors  Arindam Datta (@dattaarindam) ansible.team@dell.com   SMB Share Module Manage SMB shares on a PowerStore storage system.\nSynopsis Managing SMB Shares on PowerStore storage system includes create, get, modify, and delete the SMB shares.\nParameters   Parameter Type Required Default Choices Description   share_name  str      Name of the SMB share. Required during creation of the SMB share. For all other operations either share_name or share_id is required.    share_id  str      ID of the SMB share. Should not be specified during creation. ID is auto generated. For all other operations either share_name or share_id is required. If share_id is used then no need to pass nas_server/filesystem/snapshot/ path.    path  str      Local path to the file system/Snapshot or any existing sub-folder of the file system/Snapshot that is shared over the network. Path is relative to the base of the NAS server and must start with the name of the filesystem. Required for creation of the SMB share.    filesystem  str      The ID/Name of the File System. Either filesystem or snapshot is required for creation of the SMB share. If filesystem name is specified, then nas_server is required to uniquely identify the filesystem. If filesystem parameter is provided, then snapshot cannot be specified.    snapshot  str      The ID/Name of the Snapshot. Either filesystem or snapshot is required for creation of the SMB share. If snapshot name is specified, then nas_server is required to uniquely identify the snapshot. If snapshot parameter is provided, then filesystem cannot be specified. SMB share can be created only if access type of snapshot is \"protocol\".    nas_server  str      The ID/Name of the NAS Server. It is not required if share_id is used.    description  str      Description for the SMB share. Optional parameter when creating a share. To modify, pass the new value in description field.    is_abe_enabled  bool      Indicates whether Access-based Enumeration (ABE) for SMB share is enabled. During creation, if not mentioned, then the default is False.    is_branch_cache_enabled  bool      Indicates whether Branch Cache optimization for SMB share is enabled. During creation, if not mentioned then default is False.    is_continuous_availability_enabled  bool      Indicates whether continuous availability for SMB 3.0 is enabled. During creation, if not mentioned, then the default is False.    is_encryption_enabled  bool      Indicates whether encryption for SMB 3.0 is enabled at the shared folder level. During creation, if not mentioned then default is False.    offline_availability  str      MANUAL DOCUMENTS PROGRAMS NONE   Defines valid states of Offline Availability. MANUAL- Only specified files will be available offline. DOCUMENTS- All files that users open will be available offline. PROGRAMS- Program will preferably run from the offline cache even when connected to the network. All files that users open will be available offline. NONE- Prevents clients from storing documents and programs in offline cache.    umask  str      The default UNIX umask for new files created on the SMB Share. During creation, if not mentioned, then the default is \"022\". For all other operations, the default is None.    state  str   True     absent present   Define whether the SMB share should exist or not. present indicates that the share should exist on the system. absent indicates that the share should not exist on the system.    array_ip  str   True     IP or FQDN of the PowerStore management system.    verifycert  bool   True     True False   Boolean variable to specify whether to validate SSL certificate or not. True - indicates that the SSL certificate should be verified. Set the environment variable REQUESTS_CA_BUNDLE to the path of the SSL certificate. False - indicates that the SSL certificate should not be verified.    user  str   True     The username of the PowerStore host.    password  str   True     The password of the PowerStore host.    Notes  When the ID of the filesystem/snapshot is passed then nas_server is not required. If passed, then the filesystem/snapshot should exist for the nas_server, else the task will fail. Multiple SMB shares can be created for the same local path.  Examples - name: Create SMB share for a filesystem dellemc_powerstore_smbshare: array_ip: \u0026quot;{{array_ip}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; share_name: \u0026quot;sample_smb_share\u0026quot; filesystem: \u0026quot;sample_fs\u0026quot; nas_server: \u0026quot;{{nas_server_id}}\u0026quot; path: \u0026quot;{{path}}\u0026quot; description: \u0026quot;Sample SMB share created\u0026quot; is_abe_enabled: True is_branch_cache_enabled: True offline_availability: \u0026quot;DOCUMENTS\u0026quot; is_continuous_availability_enabled: True is_encryption_enabled: True state: \u0026quot;present\u0026quot; - name: Modify Attributes of SMB share for a filesystem dellemc_powerstore_smbshare: array_ip: \u0026quot;{{array_ip}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; share_name: \u0026quot;sample_smb_share\u0026quot; nas_server: \u0026quot;sample_nas_server\u0026quot; description: \u0026quot;Sample SMB share attributes updated\u0026quot; is_abe_enabled: False is_branch_cache_enabled: False offline_availability: \u0026quot;MANUAL\u0026quot; is_continuous_availability_enabled: False is_encryption_enabled: False umask: \u0026quot;022\u0026quot; state: \u0026quot;present\u0026quot; - name: Create SMB share for a snapshot dellemc_powerstore_smbshare: array_ip: \u0026quot;{{array_ip}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; share_name: \u0026quot;sample_snap_smb_share\u0026quot; snapshot: \u0026quot;sample_snapshot\u0026quot; nas_server: \u0026quot;{{nas_server_id}}\u0026quot; path: \u0026quot;{{path}}\u0026quot; description: \u0026quot;Sample SMB share created for snapshot\u0026quot; is_abe_enabled: True is_branch_cache_enabled: True is_continuous_availability_enabled: True state: \u0026quot;present\u0026quot; - name: Modify Attributes of SMB share for a snapshot dellemc_powerstore_smbshare: array_ip: \u0026quot;{{array_ip}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; share_name: \u0026quot;sample_snap_smb_share\u0026quot; nas_server: \u0026quot;sample_nas_server\u0026quot; description: \u0026quot;Sample SMB share attributes updated for snapshot\u0026quot; is_abe_enabled: False is_branch_cache_enabled: False offline_availability: \u0026quot;MANUAL\u0026quot; is_continuous_availability_enabled: False umask: \u0026quot;022\u0026quot; state: \u0026quot;present\u0026quot; - name: Get details of SMB share dellemc_powerstore_smbshare: array_ip: \u0026quot;{{array_ip}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; share_id: \u0026quot;{{smb_share_id}}\u0026quot; state: \u0026quot;present\u0026quot; - name: Delete SMB share dellemc_powerstore_smbshare: array_ip: \u0026quot;{{array_ip}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; share_id: \u0026quot;{{smb_share_id}}\u0026quot; state: \u0026quot;absent\u0026quot; Return Values   Key Type Returned Description   changed   bool   always   Whether or not the resource has changed    smb_share_details   complex   When share exists.   The SMB share details.    \u0026nbsp; description   str  success  Additional information about the share.    \u0026nbsp; file_system   complex  success  Includes ID and Name of filesystem and nas server for which smb share exists.    \u0026nbsp; \u0026nbsp; filesystem_type   str  success  Type of filesystem.    \u0026nbsp; \u0026nbsp; id   str  success  ID of filesystem.    \u0026nbsp; \u0026nbsp; name   str  success  Name of filesystem.    \u0026nbsp; \u0026nbsp; nas_server   dict  success  nas_server of filesystem.    \u0026nbsp; id   str  success  The ID of the SMB share.    \u0026nbsp; is_ABE_enabled   bool  success  Whether Access Based enumeration is enforced or not    \u0026nbsp; is_branch_cache_enabled   bool  success  Whether branch cache is enabled or not.    \u0026nbsp; is_continuous_availability_enabled   bool  success  Whether the share will be available continuously or not    \u0026nbsp; is_encryption_enabled   bool  success  Whether encryption is enabled or not    \u0026nbsp; name   str  success  Name of the SMB share.    Authors  P Srinivas Rao (@srinivas-rao5) ansible.team@dell.com   Snapshot Module Manage Snapshots on Dell EMC PowerStore.\nSynopsis Managing Snapshots on PowerStore. Create a new Volume Group Snapshot. Get details of Volume Group Snapshot. Modify Volume Group Snapshot Delete an existing Volume Group Snapshot. Create a new Volume Snapshot. Get details of Volume Snapshot. Modify Volume Snapshot. Delete an existing Volume Snapshot.\nParameters   Parameter Type Required Default Choices Description   snapshot_name  str      The name of the Snapshot. Either snapshot name or ID is required.    snapshot_id  str      The ID of the Snapshot. Either snapshot ID or Snapshot name is required.    volume  str      The volume. This could be the volume name or ID.    volume_group  str      The volume group. This could be the volume group name or ID.    new_snapshot_name  str      The new name of the Snapshot.    desired_retention  str      The retention value for the Snapshot. If the retention value is not specified, the Snapshot details would be returned. To create a Snapshot, either a retention or expiration timestamp must be given. If the Snapshot does not have any retention value - specify it as 'None'.    retention_unit  str      hours days   The unit for retention. If this unit is not specified, 'hours' is taken as default retention_unit. If desired_retention is specified, expiration_timestamp cannot be specified.    expiration_timestamp  str      The expiration timestamp of the Snapshot. This should be provided in UTC format, e.g 2019-07-24T10:54:54Z.    description  str      The description for the Snapshot.    state  str   True     absent present   Defines whether the Snapshot should exist or not.    array_ip  str   True     IP or FQDN of the PowerStore management system.    verifycert  bool   True     True False   Boolean variable to specify whether to validate SSL certificate or not. True - indicates that the SSL certificate should be verified. Set the environment variable REQUESTS_CA_BUNDLE to the path of the SSL certificate. False - indicates that the SSL certificate should not be verified.    user  str   True     The username of the PowerStore host.    password  str   True     The password of the PowerStore host.    Examples  - name: Create a volume snapshot on PowerStore dellemc_powerstore_snapshot: array_ip: \u0026quot;{{mgmt_ip}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; snapshot_name: \u0026quot;{{snapshot_name}}\u0026quot; volume: \u0026quot;{{volume}}\u0026quot; description: \u0026quot;{{description}}\u0026quot; desired_retention: \u0026quot;{{desired_retention}}\u0026quot; retention_unit: \u0026quot;{{retention_unit_days}}\u0026quot; state: \u0026quot;{{state_present}}\u0026quot; - name: Get details of a volume snapshot dellemc_powerstore_snapshot: array_ip: \u0026quot;{{mgmt_ip}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; snapshot_name: \u0026quot;{{snapshot_name}}\u0026quot; volume: \u0026quot;{{volume}}\u0026quot; state: \u0026quot;{{state_present}}\u0026quot; - name: Rename volume snapshot dellemc_powerstore_snapshot: array_ip: \u0026quot;{{mgmt_ip}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; snapshot_name: \u0026quot;{{snapshot_name}}\u0026quot; new_snapshot_name: \u0026quot;{{new_snapshot_name}}\u0026quot; volume: \u0026quot;{{volume}}\u0026quot; state: \u0026quot;{{state_present}}\u0026quot; - name: Delete volume snapshot dellemc_powerstore_snapshot: array_ip: \u0026quot;{{mgmt_ip}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; snapshot_name: \u0026quot;{{new_snapshot_name}}\u0026quot; volume: \u0026quot;{{volume}}\u0026quot; state: \u0026quot;{{state_absent}}\u0026quot; - name: Create a volume group snapshot on PowerStore dellemc_powerstore_snapshot: array_ip: \u0026quot;{{mgmt_ip}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; snapshot_name: \u0026quot;{{snapshot_name}}\u0026quot; volume_group: \u0026quot;{{volume_group}}\u0026quot; description: \u0026quot;{{description}}\u0026quot; expiration_timestamp: \u0026quot;{{expiration_timestamp}}\u0026quot; state: \u0026quot;{{state_present}}\u0026quot; - name: Get details of a volume group snapshot dellemc_powerstore_snapshot: array_ip: \u0026quot;{{mgmt_ip}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; snapshot_name: \u0026quot;{{snapshot_name}}\u0026quot; volume_group: \u0026quot;{{volume_group}}\u0026quot; state: \u0026quot;{{state_present}}\u0026quot; - name: Modify volume group snapshot expiration timestamp dellemc_powerstore_snapshot: array_ip: \u0026quot;{{mgmt_ip}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; snapshot_name: \u0026quot;{{snapshot_name}}\u0026quot; volume_group: \u0026quot;{{volume_group}}\u0026quot; description: \u0026quot;{{description}}\u0026quot; expiration_timestamp: \u0026quot;{{expiration_timestamp_new}}\u0026quot; state: \u0026quot;{{state_present}}\u0026quot; - name: Rename volume group snapshot dellemc_powerstore_snapshot: array_ip: \u0026quot;{{mgmt_ip}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; snapshot_name: \u0026quot;{{snapshot_name}}\u0026quot; new_snapshot_name: \u0026quot;{{new_snapshot_name}}\u0026quot; volume_group: \u0026quot;{{volume_group}}\u0026quot; state: \u0026quot;{{state_present}}\u0026quot; - name: Delete volume group snapshot dellemc_powerstore_snapshot: array_ip: \u0026quot;{{mgmt_ip}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; snapshot_name: \u0026quot;{{new_snapshot_name}}\u0026quot; volume_group: \u0026quot;{{volume_group}}\u0026quot; state: \u0026quot;{{state_absent}}\u0026quot; Return Values   Key Type Returned Description   changed   bool   always   Whether or not the resource has changed    create_vg_snap   bool   When value exists   A boolean flag to indicate whether volume group snapshot got created    create_vol_snap   bool   When value exists   A boolean flag to indicate whether volume snapshot got created    delete_vg_snap   bool   When value exists   A boolean flag to indicate whether volume group snapshot got deleted    delete_vol_snap   bool   When value exists   A boolean flag to indicate whether volume snapshot got deleted    modify_vg_snap   bool   When value exists   A boolean flag to indicate whether volume group snapshot got modified    modify_vol_snap   bool   When value exists   A boolean flag to indicate whether volume snapshot got modified    snap_details   complex   When snapshot exists   Details of the snapshot    \u0026nbsp; creation_timestamp   str  success  The creation timestamp of the snapshot    \u0026nbsp; description   str  success  Description about the snapshot    \u0026nbsp; id   str  success  The system generated ID given to the snapshot    \u0026nbsp; name   str  success  Name of the snapshot    \u0026nbsp; performance_policy_id   str  success  The performance policy for the snapshot    \u0026nbsp; protection_data   complex  success  The protection data of the snapshot    \u0026nbsp; \u0026nbsp; expiration_timestamp   str  success  The expiration timestamp of the snapshot    \u0026nbsp; protection_policy_id   str  success  The protection policy of the snapshot    \u0026nbsp; size   int  success  Size of the snapshot    \u0026nbsp; state   str  success  The state of the snapshot    \u0026nbsp; type   str  success  The type of the snapshot    \u0026nbsp; volumes   complex  success  The volumes details of the volume group snapshot    \u0026nbsp; \u0026nbsp; id   str  success  The system generated ID given to the volume associated with the volume group    Authors  Rajshree Khare (@khareRajshree) ansible.team@dell.com Prashant Rakheja (@prashant-dell) ansible.team@dell.com   Replication Rule Module Replication rule operations on a PowerStore storage system.\nSynopsis Performs all replication rule operations on a PowerStore Storage System. This module supports get details of an existing replication rule. Create new replication rule for all supported parameters. Modify replication rule with supported parameters. Delete a specific replication rule.\nParameters   Parameter Type Required Default Choices Description   replication_rule_name  str      Name of the replication rule. Required during creation of a replication rule. replication_rule_name and replication_rule_id are mutually exclusive.    replication_rule_id  str      ID of the replication rule. ID for the rule is autogenerated, cannot be passed during creation of a replication rule. replication_rule_name and replication_rule_id are mutually exclusive.    new_name  str      New name of the replication rule. Used for renaming a replication rule.    rpo  str      Five_Minutes Fifteen_Minutes Thirty_Minutes One_Hour Six_Hours Twelve_Hours One_Day   Recovery point objective (RPO), which is the acceptable amount of data, measured in units of time, that may be lost in case of a failure.    alert_threshold  int      Acceptable delay between the expected and actual replication sync intervals. The system generates an alert if the delay between the expected and actual sync exceeds this threshold. During creation, if not passed, then by default one RPO in minutes will be passed. The range of integers supported are in between 0 and 1440 (inclusive of both).    remote_system  str      ID or name of the remote system to which this rule will replicate the associated resources.    state  str   True     present absent   The state of the replication rule after the task is performed. For Delete operation only, it should be set to \"absent\". For all Create, Modify or Get details operations it should be set to \"present\".    array_ip  str   True     IP or FQDN of the PowerStore management system.    verifycert  bool   True     True False   Boolean variable to specify whether to validate SSL certificate or not. True - indicates that the SSL certificate should be verified. Set the environment variable REQUESTS_CA_BUNDLE to the path of the SSL certificate. False - indicates that the SSL certificate should not be verified.    user  str   True     The username of the PowerStore host.    password  str   True     The password of the PowerStore host.    Examples - name: Create new replication rule dellemc_powerstore_replicationrule: array_ip: \u0026quot;{{array_ip}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; replication_rule_name: \u0026quot;sample_replication_rule\u0026quot; rpo: \u0026quot;Five_Minutes\u0026quot; alert_threshold: \u0026quot;15\u0026quot; remote_system: \u0026quot;WN-D8877\u0026quot; state: \u0026quot;present\u0026quot; - name: Modify existing replication rule dellemc_powerstore_replicationrule: array_ip: \u0026quot;{{array_ip}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; replication_rule_name: \u0026quot;sample_replication_rule\u0026quot; new_name: \u0026quot;new_sample_replication_rule\u0026quot; rpo: \u0026quot;One_Hour\u0026quot; alert_threshold: \u0026quot;60\u0026quot; remote_system: \u0026quot;WN-D0517\u0026quot; state: \u0026quot;present\u0026quot; - name: Get details of replication rule dellemc_powerstore_replicationrule: array_ip: \u0026quot;{{array_ip}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; replication_rule_id: \u0026quot;{{id}}\u0026quot; state: \u0026quot;present\u0026quot; - name: Delete an existing replication rule dellemc_powerstore_replicationrule: array_ip: \u0026quot;{{array_ip}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; replication_rule_name: \u0026quot;new_sample_replication_rule\u0026quot; state: \u0026quot;absent\u0026quot; Return Values   Key Type Returned Description   changed   bool   always   Whether or not the resource has changed    replication_rule_details   complex   When replication rule exists   Details of the replication rule    \u0026nbsp; alert_threshold   int  success  Acceptable delay in minutes between the expected and actual replication sync intervals.    \u0026nbsp; id   str  success  The system generated ID of the replication rule    \u0026nbsp; name   str  success  Name of the replication rule    \u0026nbsp; remote_system_id   str  success  Unique identifier of the remote system to which this rule will replicate the associated resources.    \u0026nbsp; remote_system_name   str  success  Name of the remote system to which this rule will replicate the associated resources.    \u0026nbsp; rpo   str  success  Recovery point objective (RPO), which is the acceptable amount of data, measured in units of time, that may be lost in case of a failure.    Authors  P Srinivas Rao (@srinivas-rao5) ansible.team@dell.com   Protection Policy Module Perform Protection policy operations on PowerStore storage system\nSynopsis Performs all protection policy operations on PowerStore Storage System. This modules supports get details of an existing protection policy. Create new protection policy with existing Snapshot Rule or replication rule. Modify protection policy to change the name and description, and add or remove existing snapshot rules/ replication rule. Delete an existing protection policy.\nParameters   Parameter Type Required Default Choices Description   name  str      String variable. Indicates the name of the protection policy.    protectionpolicy_id  str      String variable. Indicates the id of the protection policy.    new_name  str      String variable. Indicates the new name of the protection policy. Used for renaming operation    snapshotrules  list elements: str      List of strings to specify the name or ids of snapshot rules which are to be added or removed, to or from, the protection policy.    replicationrule  str      The name or ids of the replcation rule which is to be added to the protection policy. To remove the replication rule, an empty string has to be passed.    description  str      String variable. Indicates the description of the protection policy.    state  str   True     present absent   String variable. Indicates the state of protection policy. For Delete operation only, it should be set to \"absent\" For all other operations like Create, Modify or Get details, it should be set to \"present\"    snapshotrule_state  str      present-in-policy absent-in-policy   String variable. Indicates the state of a snapshotrule in a protection policy. When snapshot rules are specified, this variable is required. present-in-policy indicates to add to protection policy. absent-in-policy indicates to remove from protection policy.    array_ip  str   True     IP or FQDN of the PowerStore management system.    verifycert  bool   True     True False   Boolean variable to specify whether to validate SSL certificate or not. True - indicates that the SSL certificate should be verified. Set the environment variable REQUESTS_CA_BUNDLE to the path of the SSL certificate. False - indicates that the SSL certificate should not be verified.    user  str   True     The username of the PowerStore host.    password  str   True     The password of the PowerStore host.    Notes  Before deleting a protection policy, the replication rule has to be removed from the protection policy.  Examples - name: Create a protection policy with snapshot rule and replication rule dellemc_powerstore_protectionpolicy: array_ip: \u0026quot;{{array_ip}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; name: \u0026quot;{{name}}\u0026quot; description: \u0026quot;{{description}}\u0026quot; snapshotrules: - \u0026quot;Ansible_test_snap_rule_1\u0026quot; replicationrule: \u0026quot;ansible_replication_rule_1\u0026quot; snapshotrule_state: \u0026quot;present-in-policy\u0026quot; state: \u0026quot;present\u0026quot; - name : Modify protection policy, change name dellemc_powerstore_protectionpolicy: array_ip: \u0026quot;{{array_ip}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; name: \u0026quot;{{name}}\u0026quot; new_name: \u0026quot;{{new_name}}\u0026quot; state: \u0026quot;present\u0026quot; - name : Modify protection policy, add snapshot rule dellemc_powerstore_protectionpolicy: array_ip: \u0026quot;{{array_ip}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; name: \u0026quot;{{name}}\u0026quot; snapshotrules: - \u0026quot;Ansible_test_snaprule_1\u0026quot; snapshotrule_state: \u0026quot;present-in-policy\u0026quot; state: \u0026quot;present\u0026quot; - name : Modify protection policy, remove snapshot rule, replication rule dellemc_powerstore_protectionpolicy: array_ip: \u0026quot;{{array_ip}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; name: \u0026quot;{{name}}\u0026quot; snapshotrules: - \u0026quot;Ansible_test_to_be_removed\u0026quot; replicationrule: \u0026quot;\u0026quot; snapshotrule_state: \u0026quot;absent-in-policy\u0026quot; state: \u0026quot;present\u0026quot; - name : Get details of protection policy by name dellemc_powerstore_protectionpolicy: array_ip: \u0026quot;{{array_ip}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; name: \u0026quot;{{name}}\u0026quot; state: \u0026quot;present\u0026quot; - name : Get details of protection policy by ID dellemc_powerstore_protectionpolicy: array_ip: \u0026quot;{{array_ip}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; protectionpolicy_id: \u0026quot;{{protectionpolicy_id}}\u0026quot; state: \u0026quot;present\u0026quot; - name : Delete protection policy dellemc_powerstore_protectionpolicy: array_ip: \u0026quot;{{array_ip}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; name: \u0026quot;{{name}}\u0026quot; state: \u0026quot;absent\u0026quot; Return Values   Key Type Returned Description   changed   bool   always   Whether or not the resource has changed    protectionpolicy_details   complex   When protection policy exists   Details of the protection policy    \u0026nbsp; description   str  success  description about the protection policy    \u0026nbsp; id   str  success  The system generated ID given to the protection policy    \u0026nbsp; name   str  success  Name of the protection policy    \u0026nbsp; replication_rules   complex  success  The replication rule details of the protection policy    \u0026nbsp; \u0026nbsp; id   str  success  The replication rule ID of the protection policy    \u0026nbsp; \u0026nbsp; name   str  success  The replication rule name of the protection policy    \u0026nbsp; snapshot_rules   complex  success  The snapshot rules details of the protection policy    \u0026nbsp; \u0026nbsp; id   str  success  The snapshot rule ID of the protection policy    \u0026nbsp; \u0026nbsp; name   str  success  The snapshot rule name of the protection policy    \u0026nbsp; type   str  success  The type for the protection policy    Authors  Arindam Datta (@dattaarindam) ansible.team@dell.com P Srinivas Rao (@srinivas-rao5) ansible.team@dell.com   Filesystem Snapshot Module Manage Filesystem Snapshots on Dell EMC PowerStore\nSynopsis Managing filesystem snapshots on PowerStore Storage System includes creating new filesystem snapshot, getting details of filesystem snapshot, modifying attributes of filesystem snapshot and deleting filesystem snapshot.\nParameters   Parameter Type Required Default Choices Description   snapshot_name  str      The name of the filesystem snapshot. Mandatory for create operation. Specify either snapshot name or ID (but not both) for any operation.    snapshot_id  str      The ID of the Snapshot.    filesystem  str      The ID/Name of the filesystem for which snapshot will be taken. If filesystem name is specified, then nas_server is required to uniquely identify the filesystem. Mandatory for create operation.    nas_server  str      The NAS server, this could be the name or ID of the NAS server.    description  str      The description for the filesystem snapshot.    desired_retention  int      The retention value for the Snapshot. If the desired_retention/expiration_timestamp is not mentioned during creation, snapshot will be created with unlimited retention. Maximum supported desired retention is 31 days.    retention_unit  str    hours    hours days   The unit for retention.    expiration_timestamp  str      The expiration timestamp of the snapshot. This should be provided in UTC format, e.g 2020-07-24T10:54:54Z. To remove the expiration timestamp, specify it as an empty string.    access_type  str      SNAPSHOT PROTOCOL   Specifies whether the snapshot directory or protocol access is granted to the filesystem snapshot. For create operation, if access_type is not specified, snapshot will be created with 'SNAPSHOT' access type.    state  str   True     absent present   Define whether the filesystem snapshot should exist or not.    array_ip  str   True     IP or FQDN of the PowerStore management system.    verifycert  bool   True     True False   Boolean variable to specify whether to validate SSL certificate or not. True - indicates that the SSL certificate should be verified. Set the environment variable REQUESTS_CA_BUNDLE to the path of the SSL certificate. False - indicates that the SSL certificate should not be verified.    user  str   True     The username of the PowerStore host.    password  str   True     The password of the PowerStore host.    Examples - name: Create filesystem snapshot dellemc_powerstore_filesystem_snapshot: array_ip: \u0026quot;{{array_ip}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; snapshot_name: \u0026quot;sample_filesystem_snapshot\u0026quot; nas_server: \u0026quot;ansible_nas_server\u0026quot; filesystem: \u0026quot;sample_filesystem\u0026quot; desired_retention: 20 retention_unit: \u0026quot;days\u0026quot; state: \u0026quot;present\u0026quot; - name: Get the details of filesystem snapshot dellemc_powerstore_filesystem_snapshot: array_ip: \u0026quot;{{array_ip}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; snapshot_id: \u0026quot;{{fs_snapshot_id}}\u0026quot; state: \u0026quot;present\u0026quot; - name: Modify the filesystem snapshot dellemc_powerstore_filesystem_snapshot: array_ip: \u0026quot;{{array_ip}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; snapshot_name: \u0026quot;sample_filesystem_snapshot\u0026quot; nas_server: \u0026quot;ansible_nas_server\u0026quot; description: \u0026quot;modify description\u0026quot; expiration_timestamp: \u0026quot;\u0026quot; state: \u0026quot;present\u0026quot; - name: Delete filesystem snapshot dellemc_powerstore_filesystem_snapshot: array_ip: \u0026quot;{{array_ip}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; snapshot_id: \u0026quot;{{fs_snapshot_id}}\u0026quot; state: \u0026quot;absent\u0026quot; Return Values   Key Type Returned Description   changed   bool   always   Whether or not the resource has changed    create_fs_snap   bool   always   Whether or not the resource has created    delete_fs_snap   bool   always   Whether or not the resource has deleted    filesystem_snap_details   dict   When snapshot exists.   Details of the snapshot.    \u0026nbsp; access_type   str  success  Displays the type of access allowed to the snapshot.    \u0026nbsp; creation_timestamp   str  success  The date and time the snapshot was created.    \u0026nbsp; description   str  success  Description of the filesystem snapshot.    \u0026nbsp; expiration_timestamp   str  success  The date and time the snapshot is due to be automatically deleted by the system.    \u0026nbsp; id   str  success  Unique identifier of the filesystem snapshot instance.    \u0026nbsp; name   str  success  The name of the snapshot.    \u0026nbsp; nas_server   dict  success  Details of NAS server on which snapshot is present.    \u0026nbsp; \u0026nbsp; id   str  success  ID of the NAS server.    \u0026nbsp; \u0026nbsp; name   str  success  Name of the NAS server    \u0026nbsp; parent_id   str  success  ID of the filesystem on which snapshot is taken.    \u0026nbsp; parent_name   str  success  Name of the filesystem on which snapshot is taken.    modify_fs_snap   bool   always   Whether or not the resource has modified    Authors  Akash Shendge (@shenda1) ansible.team@dell.com   ","excerpt":"Ansible Modules for Dell EMC PowerStore Product Guide 1.2 © 2021 Dell Inc. or its subsidiaries. All …","ref":"/ansible-docs/docs/server/platforms/powerstore/product-guide/","title":"PowerStore Product Guide"},{"body":"Ansible Modules for Dell EMC PowerStore Release Notes 1.2.0  © 2021 Dell Inc. or its subsidiaries. All rights reserved. Dell, EMC, and other trademarks are trademarks of Dell Inc. or its subsidiaries. Other trademarks may be trademarks of their respective owners.\n Content These release notes contain supplemental information about Ansible Modules for Dell EMC PowerStore.\n Revision History Product Description New Features \u0026amp; Enhancements Known Issues Limitations Distribution Documentation  Revision history The table in this section lists the revision history of this document.\nTable 1. Revision history\n   Revision Date Description     01 June 2021 Current release of Ansible Modules for Dell EMC PowerStore 1.2.0    Product Description The Ansible modules for Dell EMC PowerStore are used to automate and orchestrate the deployment, configuration, and management of Dell EMC PowerStore storage systems. The capabilities of Ansible modules are managing Volumes, Volume groups, Hosts, Host groups, Protection policies, Replication rules, Replication sessions, NFS exports, SMB shares, NAS server, File systems, File system snapshots, Quota tree, Quotas for filesystem and obtaining PowerStore system information. The options available for each capability are list, show, create, delete, and modify. The only exception is for NAS server for which the options available are list \u0026amp; modify.\nNew features \u0026amp; enhancements Along with the previous release deliverables, this release supports the following features -\n  Replication rule module supports the following functionalities:\n Create a replication rule Get replication rule details Modify attributes of replication rule Delete replication rule    Replication session module supports the following functionalities:\n Get replication session details Modify the state of the replication session    Protection policy module has the following enhancements:\n Add a replication rule to protection policy Remove a replication rule from protection policy    Gather Facts Module has the following enhancements:\n List of remote systems List of replication sessions List of replication rules    Known issues There are no known issues.\nLimitations There are no known limitations.\nDistribution The software package is available for download from the Ansible Modules for PowerStore GitHub page.\nDocumentation The documentation is available on Ansible Modules for PowerStore GitHub page. It includes the following:\n README Release Notes (this document) Product Guide  ","excerpt":"Ansible Modules for Dell EMC PowerStore Release Notes 1.2.0  © 2021 Dell Inc. or its subsidiaries. …","ref":"/ansible-docs/docs/server/platforms/powerstore/release-notes/","title":"PowerStore Release Notes"},{"body":"Ansible Modules for Dell EMC PowerFlex Product Guide 1.0  © 2021 Dell Inc. or its subsidiaries. All rights reserved. Dell EMC, and other trademarks are trademarks of Dell Inc. or its subsidiaries. Other trademarks may be trademarks of their respective owners.\n Contents   Common access parameters\n  Gatherfacts module\n Synopsis Parameters Examples Return Values Authors    SDC module\n Synopsis Parameters Examples Return Values Authors    Volume module\n Synopsis Parameters Examples Return Values Authors    Snapshot module\n Synopsis Parameters Examples Return Values Authors    Storage pool module\n Synopsis Parameters Examples Return Values Authors    Common access parameters These parameters are applicable to all modules, along with module-specific parameters.\nNOTE: If the parameter is mandatory, then required=True else it is an optional parameter. This is applicable to all the module specific parameters also.\n Parameter Choices/Defaults Comments    gateway_host  type=string, required=True      IP or FQDN of the PowerFlex gateway host.     port  type=integer    Default:443   Port number through which communication happens with PowerFlex gateway host.     username  type=string , required=True      The username of the PowerFlex gateway host.     password  type=string , required=True      The password of the PowerFlex gateway host.     verifycert  type=boolean    Choices: no yes\u0026nbsp;\u0026larr;    Boolean variable to specify whether or not to validate SSL certificate. True - Indicates that the SSL certificate should be verified. False - Indicates that the SSL certificate should not be verified.     Gather Facts Module Synopsis  Gathering information about Dell EMC PowerFlex storage system includes Get the API details of a PowerFlex array, Get list of volumes in PowerFlex array, Get list of SDSs in a PowerFlex array, Get list of SDCs in a PowerFlex array, Get list of storage pools in PowerFlex array, Get list of protection domains in a PowerFlex array, Get list of snapshot policies in a PowerFlex array.  Parameters  Parameter Choices/Defaults Comments    filters  type=list , elements=dictionary      List of filters to support filtered output for storage entities. Each filter is a list of filter_key, filter_operator, filter_value. Supports passing of multiple filters.      filter_key  type=string , required=True      Name identifier of the filter.      filter_operator  type=string , required=True    Choices: equal    Operation to be performed on filter key.      filter_value  type=string , required=True      Value of the filter key.     gather_subset  type=list , elements=string    Choices: vol storage_pool protection_domain sdc sds snapshot_policy    List of string variables to specify the Powerflex storage system entities for which information is required. vol storage_pool protection_domain sdc sds snapshot_policy     Examples - name: Get detailed list of PowerFlex entities. dellemc_powerflex_gatherfacts: gateway_host: \u0026quot;{{gateway_host}}\u0026quot; username: \u0026quot;{{username}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; gather_subset: - vol - storage_pool - protection_domain - sdc - sds - snapshot_policy - name: Get a subset list of PowerFlex volumes. dellemc_powerflex_gatherfacts: gateway_host: \u0026quot;{{gateway_host}}\u0026quot; username: \u0026quot;{{username}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; gather_subset: - vol filters: - filter_key: \u0026quot;name\u0026quot; filter_operator: \u0026quot;equal\u0026quot; filter_value: \u0026quot;ansible_test\u0026quot; Return Values  Key Returned Description    API_Version  type=string   always  API version of PowerFlex API Gateway.      Array_Details  type=list , elements=string   always  System entities of PowerFlex storage array.     \u0026nbsp;  id  type=string   success  The ID of the system     \u0026nbsp;  installId  type=string   success  installation Id     \u0026nbsp;  mdmSecurityPolicy  type=string   success  mdm security policy     \u0026nbsp;  systemVersionName  type=string   success  system version and name      changed  type=boolean   always  Whether or not the resource has changed      Protection_Domains  complex   always  Details of all protection domains     \u0026nbsp;  id  type=string   success  protection domain id     \u0026nbsp;  name  type=string   success  protection domain name      SDCs  complex   always  Details of storage data clients     \u0026nbsp;  id  type=string   success  storage data client id     \u0026nbsp;  name  type=string   success  storage data client name      SDSs  complex   always  Details of storage data servers     \u0026nbsp;  id  type=string   success  storage data server id     \u0026nbsp;  name  type=string   success  storage data server name      Snapshot_Policies  complex   always  Details of snapshot policies     \u0026nbsp;  id  type=string   success  snapshot policy id     \u0026nbsp;  name  type=string   success  snapshot policy name      Storage_Pools  complex   always  Details of storage pools     \u0026nbsp;  id  type=string   success  storage pool id     \u0026nbsp;  name  type=string   success  storage pool name      Volumes  complex   always  Details of volumes     \u0026nbsp;  id  type=string   success  volume id     \u0026nbsp;  name  type=string   success  volume name      Authors  Arindam Datta (@dattaarindam) \u0026lt;ansible.team@dell.com\u0026gt;  SDC Module Synopsis  Managing SDC\u0026rsquo;s on PowerFlex storage system includes getting details of SDC and renaming SDC.  Parameters  Parameter Choices/Defaults Comments    sdc_id  type=string      ID of the SDC. Specify either sdc_name, sdc_id or sdc_ip for get/rename operation. Mutually exclusive with sdc_name and sdc_ip.     sdc_ip  type=string      IP of the SDC. Specify either sdc_name, sdc_id or sdc_ip for get/rename operation. Mutually exclusive with sdc_id and sdc_name.     sdc_name  type=string      Name of the SDC. Specify either sdc_name, sdc_id or sdc_ip for get/rename operation. Mutually exclusive with sdc_id and sdc_ip.     sdc_new_name  type=string      New name of the SDC. Used to rename the SDC.     state  type=string , required=True    Choices: present absent    State of the storage pool.     Examples - name: Get SDC details using SDC ip dellemc_powerflex_sdc: gateway_host: \u0026quot;{{gateway_host}}\u0026quot; username: \u0026quot;{{username}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; sdc_ip: \u0026quot;{{sdc_ip}}\u0026quot; state: \u0026quot;present\u0026quot; - name: Rename SDC using SDC name dellemc_powerflex_sdc: gateway_host: \u0026quot;{{gateway_host}}\u0026quot; username: \u0026quot;{{username}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; sdc_name: \u0026quot;centos_sdc\u0026quot; sdc_new_name: \u0026quot;centos_sdc_renamed\u0026quot; state: \u0026quot;present\u0026quot; Return Values  Key Returned Description    changed  type=boolean   always  Whether or not the resource has changed      sdc_details  complex   When SDC exists  Details of the SDC     \u0026nbsp;  id  type=string   success  The ID of the SDC     \u0026nbsp;  mapped_volumes  type=list , elements=string   success  The details of the mapped volumes     \u0026nbsp; \u0026nbsp;  id  type=string   success  The ID of the volume     \u0026nbsp; \u0026nbsp;  name  type=string   success  The name of the volume     \u0026nbsp; \u0026nbsp;  volumeType  type=string   success  Type of the volume     \u0026nbsp;  name  type=string   success  Name of the SDC     \u0026nbsp;  osType  type=string   success  OS type of the SDC     \u0026nbsp;  sdcApproved  type=boolean   success  Indicates whether an SDC has approved access to the system     \u0026nbsp;  sdcIp  type=string   success  IP of the SDC      Authors  Akash Shendge (@shenda1) \u0026lt;ansible.team@dell.com\u0026gt;  Volume Module Synopsis  Managing volumes on PowerFlex storage system includes creating new volume, getting details of volume, adding/removing snapshot policy to/from volume, mapping/unmapping volume to/from SDC, listing snapshots associated with a volume, modifying attributes of volume and deleting volume.  Parameters  Parameter Choices/Defaults Comments    allow_multiple_mappings  type=boolean    Choices: no yes    Specifies whether to allow multiple mappings or not. If the volume is mapped to one SDC then for every new mapping allow_multiple_mappings has to be passed as True.     auto_snap_remove_type  type=string    Choices: remove detach    Whether to remove or detach the snapshot policy. To remove/detach snapshot policy, empty snapshot_policy_id/snapshot_policy_name is to be passed along with auto_snap_remove_type. If the snapshot policy name/id is passed empty then auto_snap_remove_type is defaulted to \u0026#x27;detach\u0026#x27;.     cap_unit  type=string    Choices: GB TB    The unit of the volume size. It defaults to \u0026#x27;GB\u0026#x27;.     compression_type  type=string    Choices: NORMAL NONE    Type of the compression method.     delete_snapshots  type=boolean    Choices: no yes    If True, the volume and all its dependent snapshots will be deleted. If False, only the volume will be deleted. delete_snapshots parameter can be specified only when the state is absent. delete_snapshots defaults to False.     protection_domain_id  type=string      The id of the protection domain. While creation of a volume, if more than one storage pool exists with the same name then either protection domain name or id must be mentioned along with it. protection_domain_name and protection_domain_id are mutually exclusive parameters.     protection_domain_name  type=string      The name of the protection domain. While creation of a volume, if more than one storage pool exists with the same name then either protection domain name or id must be mentioned along with it. protection_domain_name and protection_domain_name are mutually exclusive parameters.     sdc  type=list , elements=dictionary      Specifies SDC parameters      access_mode  type=string    Choices: READ_WRITE READ_ONLY NO_ACCESS    Define the access mode for all mappings of the volume.      bandwidth_limit  type=integer      Limit of volume network bandwidth. Need to mention in multiple of 1024 Kbps. To set no limit, 0 is to be passed.      iops_limit  type=integer      Limit of volume IOPS. Minimum IOPS limit is 11 and specify 0 for unlimited iops.      sdc_id  type=string      ID of the SDC. Specify either sdc_name, sdc_id or sdc_ip. Mutually exclusive with sdc_name and sdc_ip.      sdc_ip  type=string      IP of the SDC. Specify either sdc_name, sdc_id or sdc_ip. Mutually exclusive with sdc_id and sdc_ip.      sdc_name  type=string      Name of the SDC. Specify either sdc_name, sdc_id or sdc_ip. Mutually exclusive with sdc_id and sdc_ip.     sdc_state  type=string    Choices: mapped unmapped    Mapping state of the SDC.     size  type=integer      The size of the volume. Size of the volume will be assigned as higher multiple of 8 GB.     snapshot_policy_id  type=string      ID of the snapshot policy. To remove/detach snapshot policy, empty snapshot_policy_id/snapshot_policy_name is to be passed along with auto_snap_remove_type.     snapshot_policy_name  type=string      Name of the snapshot policy. To remove/detach snapshot policy, empty snapshot_policy_id/snapshot_policy_name is to be passed along with auto_snap_remove_type.     state  type=string , required=True    Choices: present absent    State of the volume.     storage_pool_id  type=string      The id of the storage pool. Either name or the id of the storage pool is required for creating a volume. storage_pool_name and storage_pool_id are mutually exclusive parameters.     storage_pool_name  type=string      The name of the storage pool. Either name or the id of the storage pool is required for creating a volume. During creation, If storage pool name is provided then either protection domain name or id must be mentioned along with it. storage_pool_name and storage_pool_id are mutually exclusive parameters.     use_rmcache  type=boolean    Choices: no yes    Whether to use RM Cache or not.     vol_id  type=string      The ID of the volume. Except create operation, all other operations can be performed using vol_id. vol_name and vol_id are mutually exclusive parameters.     vol_name  type=string      The name of the volume. Mandatory for create operation. vol_name is unique across the PowerFlex array. vol_name and vol_id are mutually exclusive parameters.     vol_new_name  type=string      New name of the volume. Used to rename the volume.     vol_type  type=string    Choices: THICK_PROVISIONED THIN_PROVISIONED    The type of volume provisioning.     Examples - name: Create a volume dellemc_powerflex_volume: gateway_host: \u0026quot;{{gateway_host}}\u0026quot; username: \u0026quot;{{username}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; port: \u0026quot;{{port}}\u0026quot; vol_name: \u0026quot;sample_volume\u0026quot; storage_pool_name: \u0026quot;pool_1\u0026quot; protection_domain_name: \u0026quot;pd_1\u0026quot; vol_type: \u0026quot;THICK_PROVISIONED\u0026quot; compression_type: \u0026quot;NORMAL\u0026quot; use_rmcache: True size: 16 state: \u0026quot;present\u0026quot; - name: Map a SDC to volume dellemc_powerflex_volume: gateway_host: \u0026quot;{{gateway_host}}\u0026quot; username: \u0026quot;{{username}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; port: \u0026quot;{{port}}\u0026quot; vol_name: \u0026quot;sample_volume\u0026quot; allow_multiple_mappings: True sdc: - sdc_id: \u0026quot;92A304DB-EFD7-44DF-A07E-D78134CC9764\u0026quot; access_mode: \u0026quot;READ_WRITE\u0026quot; sdc_state: \u0026quot;mapped\u0026quot; state: \u0026quot;present\u0026quot; - name: Unmap a SDC to volume dellemc_powerflex_volume: gateway_host: \u0026quot;{{gateway_host}}\u0026quot; username: \u0026quot;{{username}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; port: \u0026quot;{{port}}\u0026quot; vol_name: \u0026quot;sample_volume\u0026quot; sdc: - sdc_id: \u0026quot;92A304DB-EFD7-44DF-A07E-D78134CC9764\u0026quot; sdc_state: \u0026quot;unmapped\u0026quot; state: \u0026quot;present\u0026quot; - name: Map multiple SDCs to a volume dellemc_powerflex_volume: gateway_host: \u0026quot;{{gateway_host}}\u0026quot; username: \u0026quot;{{username}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; port: \u0026quot;{{port}}\u0026quot; vol_name: \u0026quot;sample_volume\u0026quot; protection_domain_name: \u0026quot;pd_1\u0026quot; sdc: - sdc_id: \u0026quot;92A304DB-EFD7-44DF-A07E-D78134CC9764\u0026quot; access_mode: \u0026quot;READ_WRITE\u0026quot; bandwidth_limit: 2048 iops_limit: 20 - sdc_ip: \u0026quot;127.0.0.1\u0026quot; access_mode: \u0026quot;READ_ONLY\u0026quot; allow_multiple_mappings: True sdc_state: \u0026quot;mapped\u0026quot; state: \u0026quot;present\u0026quot; - name: Get the details of the volume dellemc_powerflex_volume: gateway_host: \u0026quot;{{gateway_host}}\u0026quot; username: \u0026quot;{{username}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; port: \u0026quot;{{port}}\u0026quot; vol_id: \u0026quot;fe6c8b7100000005\u0026quot; state: \u0026quot;present\u0026quot; - name: Modify the details of the Volume dellemc_powerflex_volume: gateway_host: \u0026quot;{{gateway_host}}\u0026quot; username: \u0026quot;{{username}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; port: \u0026quot;{{port}}\u0026quot; vol_name: \u0026quot;sample_volume\u0026quot; storage_pool_name: \u0026quot;pool_1\u0026quot; new_vol_name: \u0026quot;new_sample_volume\u0026quot; size: 64 state: \u0026quot;present\u0026quot; - name: Delete the Volume dellemc_powerflex_volume: gateway_host: \u0026quot;{{gateway_host}}\u0026quot; username: \u0026quot;{{username}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; port: \u0026quot;{{port}}\u0026quot; vol_name: \u0026quot;sample_volume\u0026quot; delete_snapshots: False state: \u0026quot;absent\u0026quot; - name: Delete the Volume and all its dependent snapshots dellemc_powerflex_volume: gateway_host: \u0026quot;{{gateway_host}}\u0026quot; username: \u0026quot;{{username}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; port: \u0026quot;{{port}}\u0026quot; vol_name: \u0026quot;sample_volume\u0026quot; delete_snapshots: True state: \u0026quot;absent\u0026quot; Return Values  Key Returned Description    changed  type=boolean   always  Whether or not the resource has changed      volume_details  complex   When volume exists  Details of the volume     \u0026nbsp;  id  type=string   success  The ID of the volume.     \u0026nbsp;  mappedSdcInfo  complex   success  The details of the mapped SDC     \u0026nbsp; \u0026nbsp;  accessMode  type=string   success  mapping access mode for the specified volume     \u0026nbsp; \u0026nbsp;  limitBwInMbps  type=integer   success  Bandwidth limit for the SDC     \u0026nbsp; \u0026nbsp;  limitIops  type=integer   success  IOPS limit for the SDC     \u0026nbsp; \u0026nbsp;  sdcId  type=string   success  ID of the SDC     \u0026nbsp; \u0026nbsp;  sdcIp  type=string   success  IP of the SDC     \u0026nbsp; \u0026nbsp;  sdcName  type=string   success  Name of the SDC     \u0026nbsp;  name  type=string   success  Name of the volume     \u0026nbsp;  protectionDomainId  type=string   success  ID of the protection domain in which volume resides.     \u0026nbsp;  protectionDomainName  type=string   success  Name of the protection domain in which volume resides.     \u0026nbsp;  sizeInGb  type=integer   success  Size of the volume in Gb.     \u0026nbsp;  sizeInKb  type=integer   success  Size of the volume in Kb.     \u0026nbsp;  snapshotPolicyId  type=string   success  ID of the snapshot policy associated with volume.     \u0026nbsp;  snapshotPolicyName  type=string   success  Name of the snapshot policy associated with volume.     \u0026nbsp;  snapshotsList  type=string   success  List of snapshots associated with the volume.     \u0026nbsp;  storagePoolId  type=string   success  ID of the storage pool in which volume resides.     \u0026nbsp;  storagePoolName  type=string   success  Name of the storage pool in which volume resides.      Authors  P Srinivas Rao (@srinivas-rao5) \u0026lt;ansible.team@dell.com\u0026gt;  Snapshot Module Synopsis  Managing snapshots on PowerFlex Storage System includes creating new snapshot, getting details of snapshot, mapping/unmapping snapshot to/from SDC, modifying attributes of snapshot and deleting snapshot.  Parameters  Parameter Choices/Defaults Comments    allow_multiple_mappings  type=boolean    Choices: no yes    Specifies whether to allow multiple mappings or not.     cap_unit  type=string    Choices: GB TB    The unit of the volume size. It defaults to \u0026#x27;GB\u0026#x27;, if not specified.     desired_retention  type=integer      The retention value for the Snapshot. If the desired_retention is not mentioned during creation, snapshot will be created with unlimited retention. Maximum supported desired retention is 31 days.     read_only  type=boolean    Choices: no yes    Specifies whether mapping of the created snapshot volume will have read-write access or limited to read-only access. If true, snapshot is created with read-only access. If false, snapshot is created with read-write access.     remove_mode  type=string    Choices: ONLY_ME INCLUDING_DESCENDANTS    Removal mode for the snapshot. It defaults to \u0026#x27;ONLY_ME\u0026#x27;, if not specified.     retention_unit  type=string    Choices: hours days    The unit for retention. It defaults to \u0026#x27;hours\u0026#x27;, if not specified.     sdc  type=list , elements=dictionary      Specifies SDC parameters      access_mode  type=string    Choices: READ_WRITE READ_ONLY NO_ACCESS    Define the access mode for all mappings of the snapshot.      bandwidth_limit  type=integer      Limit of snapshot network bandwidth. Need to mention in multiple of 1024 Kbps. To set no limit, 0 is to be passed.      iops_limit  type=integer      Limit of snapshot IOPS. Minimum IOPS limit is 11 and specify 0 for unlimited iops.      sdc_id  type=string      ID of the SDC. Specify either sdc_name, sdc_id or sdc_ip. Mutually exclusive with sdc_name and sdc_ip.      sdc_ip  type=string      IP of the SDC. Specify either sdc_name, sdc_id or sdc_ip. Mutually exclusive with sdc_id and sdc_ip.      sdc_name  type=string      Name of the SDC. Specify either sdc_name, sdc_id or sdc_ip. Mutually exclusive with sdc_id and sdc_ip.     sdc_state  type=string    Choices: mapped unmapped    Mapping state of the SDC.     size  type=integer      The size of the snapshot.     snapshot_id  type=string      The ID of the Snapshot.     snapshot_name  type=string      The name of the snapshot. Mandatory for create operation. Specify either snapshot name or ID (but not both) for any operation.     snapshot_new_name  type=string      New name of the snapshot. Used to rename the snapshot.     state  type=string , required=True    Choices: present absent    State of the snapshot.     vol_id  type=string      The ID of the volume.     vol_name  type=string      The name of the volume for which snapshot will be taken. Specify either vol_name or vol_id while creating snapshot.     Examples - name: Create snapshot dellemc_powerflex_snapshot: gateway_host: \u0026quot;{{gateway_host}}\u0026quot; username: \u0026quot;{{username}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; snapshot_name: \u0026quot;ansible_snapshot\u0026quot; vol_name: \u0026quot;ansible_volume\u0026quot; read_only: False desired_retention: 2 state: \u0026quot;present\u0026quot; - name: Get snapshot details using snapshot id dellemc_powerflex_snapshot: gateway_host: \u0026quot;{{gateway_host}}\u0026quot; username: \u0026quot;{{username}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; snapshot_id: \u0026quot;fe6cb28200000007\u0026quot; state: \u0026quot;present\u0026quot; - name: Map snapshot to SDC dellemc_powerflex_snapshot: gateway_host: \u0026quot;{{gateway_host}}\u0026quot; username: \u0026quot;{{username}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; snapshot_id: \u0026quot;fe6cb28200000007\u0026quot; sdc: - sdc_ip: \u0026quot;10.247.66.203\u0026quot; - sdc_id: \u0026quot;663ac0d200000001\u0026quot; allow_multiple_mappings: True sdc_state: \u0026quot;mapped\u0026quot; state: \u0026quot;present\u0026quot; - name: Modify the attributes of SDC mapped to snapshot dellemc_powerflex_snapshot: gateway_host: \u0026quot;{{gateway_host}}\u0026quot; username: \u0026quot;{{username}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; snapshot_id: \u0026quot;fe6cb28200000007\u0026quot; sdc: - sdc_ip: \u0026quot;10.247.66.203\u0026quot; iops_limit: 11 bandwidth_limit: 4096 - sdc_id: \u0026quot;663ac0d200000001\u0026quot; iops_limit: 20 bandwidth_limit: 2048 allow_multiple_mappings: True sdc_state: \u0026quot;mapped\u0026quot; state: \u0026quot;present\u0026quot; - name: Extend the size of snapshot dellemc_powerflex_snapshot: gateway_host: \u0026quot;{{gateway_host}}\u0026quot; username: \u0026quot;{{username}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; snapshot_id: \u0026quot;fe6cb28200000007\u0026quot; size: 16 state: \u0026quot;present\u0026quot; - name: Unmap SDCs from snapshot dellemc_powerflex_snapshot: gateway_host: \u0026quot;{{gateway_host}}\u0026quot; username: \u0026quot;{{username}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; snapshot_id: \u0026quot;fe6cb28200000007\u0026quot; sdc: - sdc_ip: \u0026quot;10.247.66.203\u0026quot; - sdc_id: \u0026quot;663ac0d200000001\u0026quot; sdc_state: \u0026quot;unmapped\u0026quot; state: \u0026quot;present\u0026quot; - name: Rename snapshot dellemc_powerflex_snapshot: gateway_host: \u0026quot;{{gateway_host}}\u0026quot; username: \u0026quot;{{username}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; snapshot_id: \u0026quot;fe6cb28200000007\u0026quot; snapshot_new_name: \u0026quot;ansible_renamed_snapshot_10\u0026quot; state: \u0026quot;present\u0026quot; - name: Delete snapshot dellemc_powerflex_snapshot: gateway_host: \u0026quot;{{gateway_host}}\u0026quot; username: \u0026quot;{{username}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; snapshot_id: \u0026quot;fe6cb28200000007\u0026quot; remove_mode: \u0026quot;ONLY_ME\u0026quot; state: \u0026quot;absent\u0026quot; Return Values  Key Returned Description    changed  type=boolean   always  Whether or not the resource has changed      snapshot_details  complex   When snapshot exists  Details of the snapshot     \u0026nbsp;  ancestorVolumeId  type=string   success  The of the root of the specified volume\u0026#x27;s V-Tree     \u0026nbsp;  creationTime  type=integer   success  The creation time of the snapshot     \u0026nbsp;  id  type=string   success  The ID of the snapshot     \u0026nbsp;  mappedSdcInfo  complex   success  The details of the mapped SDC     \u0026nbsp; \u0026nbsp;  accessMode  type=string   success  mapping access mode for the specified snapshot     \u0026nbsp; \u0026nbsp;  limitBwInMbps  type=integer   success  Bandwidth limit for the SDC     \u0026nbsp; \u0026nbsp;  limitIops  type=integer   success  IOPS limit for the SDC     \u0026nbsp; \u0026nbsp;  sdcId  type=string   success  ID of the SDC     \u0026nbsp; \u0026nbsp;  sdcIp  type=string   success  IP of the SDC     \u0026nbsp; \u0026nbsp;  sdcName  type=string   success  Name of the SDC     \u0026nbsp;  name  type=string   success  Name of the snapshot     \u0026nbsp;  sizeInKb  type=integer   success  Size of the snapshot     \u0026nbsp;  storagePoolId  type=string   success  Storage pool in which snapshot resides      Authors  Akash Shendge (@shenda1) \u0026lt;ansible.team@dell.com\u0026gt;  Storage Pool Module Synopsis  Dell EMC PowerFlex storage pool module includes Get the details of storage pool, Create a new storage pool, Modify the attribute of a storage pool.  Parameters  Parameter Choices/Defaults Comments    media_type  type=string    Choices: HDD SSD TRANSITIONAL    Type of devices in the storage pool.     protection_domain_id  type=string      The id of the protection domain. While creation of a pool, either protection domain name or id must be mentioned. protection_domain_name and protection_domain_id are mutually exclusive parameters.     protection_domain_name  type=string      The name of the protection domain. While creation of a pool, either protection domain name or id must be mentioned. protection_domain_name and protection_domain_name are mutually exclusive parameters.     state  type=string , required=True    Choices: present absent    State of the storage pool.     storage_pool_id  type=string      The id of the storage pool. storage_pool_id is auto generated, hence should not be provided during creation of a storage pool. storage_pool_name and storage_pool_id are mutually exclusive parameters.     storage_pool_name  type=string      The name of the storage pool. If more than one storage pool is found with the same name then protection domain id/name is required to perform the task. storage_pool_name and storage_pool_id are mutually exclusive parameters.     storage_pool_new_name  type=string      New name for the storage pool can be provided. This parameter is used for renaming the storage pool.     use_rfcache  type=boolean    Choices: no yes    Enable/Disable RFcache on a specific storage pool.     use_rmcache  type=boolean    Choices: no yes    Enable/Disable RMcache on a specific storage pool.     Notes   TRANSITIONAL media type is supported only during modification. The modules prefixed with dellemc_powerflex are built to support the Dell EMC PowerFlex storage platform.   Examples - name: Get the details of storage pool by name dellemc_powerflex_storagepool: gateway_host: \u0026quot;{{gateway_host}}\u0026quot; username: \u0026quot;{{username}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; storage_pool_name: \u0026quot;sample_pool_name\u0026quot; protection_domain_name: \u0026quot;sample_protection_domain\u0026quot; state: \u0026quot;present\u0026quot; - name: Get the details of storage pool by id dellemc_powerflex_storagepool: gateway_host: \u0026quot;{{gateway_host}}\u0026quot; username: \u0026quot;{{username}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; storage_pool_id: \u0026quot;abcd1234ab12r\u0026quot; state: \u0026quot;present\u0026quot; - name: Create a new storage pool by name dellemc_powerflex_storagepool: gateway_host: \u0026quot;{{gateway_host}}\u0026quot; username: \u0026quot;{{username}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; storage_pool_name: \u0026quot;ansible_test_pool\u0026quot; protection_domain_id: \u0026quot;1c957da800000000\u0026quot; media_type: \u0026quot;HDD\u0026quot; state: \u0026quot;present\u0026quot; - name: Modify a storage pool by name dellemc_powerflex_storagepool: gateway_host: \u0026quot;{{gateway_host}}\u0026quot; username: \u0026quot;{{username}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; storage_pool_name: \u0026quot;ansible_test_pool\u0026quot; protection_domain_id: \u0026quot;1c957da800000000\u0026quot; use_rmcache: True use_rfcache: True state: \u0026quot;present\u0026quot; - name: Rename storage pool by id dellemc_powerflex_storagepool: gateway_host: \u0026quot;{{gateway_host}}\u0026quot; username: \u0026quot;{{username}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; storage_pool_id: \u0026quot;abcd1234ab12r\u0026quot; storage_pool_new_name: \u0026quot;new_ansible_pool\u0026quot; state: \u0026quot;present\u0026quot; Return Values  Key Returned Description    changed  type=boolean   always  Whether or not the resource has changed      storage_pool_details  complex   When storage pool exists  Details of the storage pool     \u0026nbsp;  id  type=string   success  ID of the storage pool under protection domain.     \u0026nbsp;  mediaType  type=string   success  Type of devices in the storage pool     \u0026nbsp;  name  type=string   success  Name of the storage pool under protection domain.     \u0026nbsp;  protectionDomainId  type=string   success  ID of the protection domain in which pool resides.     \u0026nbsp;  protectionDomainName  type=string   success  Name of the protection domain in which pool resides.     \u0026nbsp;  useRfcache  type=boolean   success  Enable/Disable RFcache on a specific storage pool.     \u0026nbsp;  useRmcache  type=boolean   success  Enable/Disable RMcache on a specific storage pool.      Authors  Arindam Datta (@dattaarindam) \u0026lt;ansible.team@dell.com\u0026gt; P Srinivas Rao (@srinivas-rao5) \u0026lt;ansible.team@dell.com\u0026gt;  ","excerpt":"Ansible Modules for Dell EMC PowerFlex Product Guide 1.0  © 2021 Dell Inc. or its subsidiaries. All …","ref":"/ansible-docs/docs/storage/platforms/powerflex/product-guide/","title":"Product Guide"},{"body":"Ansible Modules for Dell EMC PowerMax Product Guide 1.5.0  © 2021 Dell Inc. or its subsidiaries. All rights reserved. Dell EMC, and other trademarks are trademarks of Dell Inc. or its subsidiaries. Other trademarks may be trademarks of their respective owners.\n Contents  Common Parameters Gatherfacts Module  Synopsis Parameters Notes Examples Return Values Authors   Host Module  Synopsis Parameters Notes Examples Return Values Authors   Host Group Module  Synopsis Parameters Notes Examples Return Values Authors   Job Module  Synopsis Parameters Examples Return Values Authors   Masking View Module  Synopsis Parameters Examples Return Values Authors   Metro DR Module  Synopsis Parameters Examples Return Values Authors   Port Module  Synopsis Parameters Examples Return Values Authors   Port Group Module  Synopsis Parameters Examples Return Values Authors   RDF Group Module  Synopsis Parameters Examples Return Values Authors   Snapshot Module  Synopsis Parameters Notes Examples Return Values Authors   Snapshot Policy Module  Synopsis Parameters Notes Examples Return Values Authors   SRDF Module  Synopsis Parameters Examples Return Values Authors   Storage Group Module  Synopsis Parameters Examples Return Values Authors   Storage Pool Module  Synopsis Parameters Examples Return Values Authors   Volume Module  Synopsis Parameters Notes Examples Return Values Authors   Process Storage Pool Dict Module  Synopsis Parameters Examples Return Values Authors    Common Parameters These parameters are applicable to all modules, along with module-specific parameters.\nNOTE: If the parameter is mandatory, then required=true else it is an optional parameter. This is applicable to all the module specific parameters also.\n Parameter Choices/Defaults Comments    password  type=string, required=true      The password of the Unisphere host.     serial_no  type=string, required=true      The serial number of the PowerMax/VMAX array. It is a required parameter for all array-specific operations except for getting a list of arrays in the Gatherfacts module.     unispherehost  type=string, required=true      IP or FQDN of the Unisphere host     universion  type=integer    Choices: 91 92    Unisphere version, currently \u0026#x27;91\u0026#x27; and \u0026#x27;92\u0026#x27; versions are supported.     user  type=string, required=true      The username of the Unisphere host.     verifycert  type=boolean, required=true    Choices: no yes    Boolean variable to specify whether to validate SSL certificate or not. True - indicates that the SSL certificate should be verified. False - indicates that the SSL certificate should not be verified.    Gatherfacts Module Synopsis Gathers the list of specified PowerMax/VMAX storage system entities, such as the list of registered arrays, storage groups, hosts, host groups, storage groups, storage resource pools, port groups, masking views, array health status, alerts and metro DR environments, so on.\nParameters  Parameter Choices/Defaults Comments    filters  type=list elements=dictionary      List of filters to support filtered output for storage entities. Each filter is a tuple of {filter_key, filter_operator, filter_value}. Supports passing of multiple filters. The storage entities, \u0026#x27;rdf\u0026#x27;, \u0026#x27;health\u0026#x27;, \u0026#x27;snapshot_policies\u0026#x27; and \u0026#x27;metro_dr_env\u0026#x27;, does not support filters. Filters will be ignored if passed.      filter_key  type=string required=true      Name identifier of the filter.      filter_operator  type=string required=true    Choices: equal greater lesser like    Operation to be performed on filter key.      filter_value  type=string required=true      Value of the filter key.     gather_subset  type=list elements=string    Choices: alert health vol srp sg pg host hg port mv rdf metro_dr_env snapshot_policies    List of string variables to specify the PowerMax/VMAX entities for which information is required. Required only if the serial_no is present List of all PowerMax/VMAX entities supported by the module alert - gets alert summary information health - health status of a specific PowerMax array vol - volumes srp - storage resource pools sg - storage groups pg - port groups host - hosts hg - host groups port - ports mv - masking views rdf - rdf groups metro_dr_env - metro DR environments snapshot_policies - snapshot policies     serial_no  type=string      The serial number of the PowerMax/VMAX array. It is not required for getting the list of arrays.     tdev_volumes  type=boolean    Choices: no yes\u0026nbsp;\u0026larr;    Boolean variable to filter the volume list. This will have a small performance impact. By default it is set to true, only TDEV volumes will be returned. True - Will return only the TDEV volumes. False - Will return all the volumes.    Notes  Filter functionality will be supported only for the following \u0026lsquo;filter_key\u0026rsquo; against specific \u0026lsquo;gather_subset\u0026rsquo;.  vol - allocated_percent, associated, available_thin_volumes, bound_tdev, cap_cyl, cap_gb, cap_mb, cap_tb, cu_image_num, cu_image_ssid, data_volume, dld, drv, effective_wwn, emulation, encapsulated, encapsulated_wwn, gatekeeper, has_effective_wwn, mapped, mobility_id_enabled, num_of_front_end_paths, num_of_masking_views, num_of_storage_groups, oracle_instance_name, physical_name, pinned, private_volumes, rdf_group_number, reserved, split_name, status, storageGroupId, symmlun, tdev, thin_bcv, type, vdev, virtual_volumes, volume_identifier, wwn. srp - compression_state, description, effective_used_capacity_percent, emulation, num_of_disk_groups, num_of_srp_sg_demands, num_of_srp_slo_demands, rdfa_dse, reserved_cap_percent, total_allocated_cap_gb, total_srdf_dse_allocated_cap_gb, total_subscribed_cap_gb, total_usable_cap_gb. sg - base_slo_name, cap_gb, child, child_sg_name, ckd, compression, compression_ratio_to_one, fba, num_of_child_sgs, num_of_masking_views, num_of_parent_sgs, num_of_snapshots, num_of_vols, parent, parent_sg_name, slo_compliance, slo_name, srp_name, storageGroupId, tag, volumeId. pg - dir_port, fibre, iscsi, num_of_masking_views, num_of_ports host - host_group_name, num_of_host_groups, num_of_initiators, num_of_masking_views, num_of_powerpath_hosts, powerPathHostId. hg - host_name, num_of_hosts, num_of_masking_views. port - aclx, avoid_reset_broadcast, common_serial_number, director_status, disable_q_reset_on_ua, enable_auto_negotive, environ_set, hp_3000_mode, identifier, init_point_to_point, ip_list, ipv4_address, ipv6_address, iscsi_target, max_speed, negotiated_speed, neqotiate_reset, no_participating, node_wwn, num_of_cores, num_of_hypers, num_of_mapped_vols, num_of_masking_views, num_of_port_groups, port_interface, port_status, rdf_hardware_compression, rdf_hardware_compression_supported, rdf_software_compression, rdf_software_compression_supported, scsi_3, scsi_support1, siemens, soft_reset, spc2_protocol_version, sunapee, type, unique_wwn, vcm_state, vnx_attached, volume_set_addressing, wwn_node. mv - host_or_host_group_name, port_group_name, protocol_endpoint_masking_view, storage_group_name. alert - acknowledged, array, created_date, created_date_milliseconds, description, object, object_type, severity, state, type.    Examples - name: Get list of volumes with filter -- all TDEV volumes of size equal to 5GB dellemc_powermax_gatherfacts: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; universion: \u0026quot;{{universion}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; serial_no: \u0026quot;{{serial_no}}\u0026quot; gather_subset: - vol filters: - filter_key: \u0026quot;tdev\u0026quot; filter_operator: \u0026quot;equal\u0026quot; filter_value: \u0026quot;True\u0026quot; - filter_key: \u0026quot;cap_gb\u0026quot; filter_operator: \u0026quot;equal\u0026quot; filter_value: \u0026quot;5\u0026quot; - name: Get list of volumes and storage groups with filter dellemc_powermax_gatherfacts: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; universion: \u0026quot;{{universion}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; serial_no: \u0026quot;{{serial_no}}\u0026quot; gather_subset: - vol - sg filters: - filter_key: \u0026quot;tdev\u0026quot; filter_operator: \u0026quot;equal\u0026quot; filter_value: \u0026quot;True\u0026quot; - filter_key: \u0026quot;cap_gb\u0026quot; filter_operator: \u0026quot;equal\u0026quot; filter_value: \u0026quot;5\u0026quot; - name: Get list of storage groups with capacity between 2GB to 10GB dellemc_powermax_gatherfacts: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; universion: \u0026quot;{{universion}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; serial_no: \u0026quot;{{serial_no}}\u0026quot; gather_subset: - sg filters: - filter_key: \u0026quot;cap_gb\u0026quot; filter_operator: \u0026quot;greater\u0026quot; filter_value: \u0026quot;2\u0026quot; - filter_key: \u0026quot;cap_gb\u0026quot; filter_operator: \u0026quot;lesser\u0026quot; filter_value: \u0026quot;10\u0026quot; - name: Get the list of arrays for a given Unisphere host dellemc_powermax_gatherfacts: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; universion: \u0026quot;{{universion}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; register: array_list - debug: var: array_list - name: Get list of tdev-volumes dellemc_powermax_gatherfacts: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; universion: \u0026quot;{{universion}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; serial_no: \u0026quot;{{serial_no}}\u0026quot; tdev_volumes: True gather_subset: - vol - name: Get the list of arrays for a given Unisphere host dellemc_powermax_gatherfacts: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; universion: \u0026quot;{{universion}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; - name: Get array health status dellemc_powermax_gatherfacts: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; universion: \u0026quot;{{universion}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; serial_no: \u0026quot;{{serial_no}}\u0026quot; gather_subset: - health - name: Get array alerts summary dellemc_powermax_gatherfacts: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; universion: \u0026quot;{{universion}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; serial_no: \u0026quot;{{serial_no}}\u0026quot; gather_subset: - alert - name: Get the list of metro DR environments for a given Unisphere host dellemc_powermax_gatherfacts: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; universion: \u0026quot;{{universion}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; serial_no: \u0026quot;{{serial_no}}\u0026quot; gather_subset: - metro_dr_env - name: Get list of Storage groups dellemc_powermax_gatherfacts: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; universion: \u0026quot;{{universion}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; serial_no: \u0026quot;{{serial_no}}\u0026quot; gather_subset: - sg - name: Get list of Storage Resource Pools dellemc_powermax_gatherfacts: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; universion: \u0026quot;{{universion}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; serial_no: \u0026quot;{{serial_no}}\u0026quot; gather_subset: - srp - name: Get list of Ports dellemc_powermax_gatherfacts: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; universion: \u0026quot;{{universion}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; serial_no: \u0026quot;{{serial_no}}\u0026quot; gather_subset: - port - name: Get list of Port Groups dellemc_powermax_gatherfacts: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; universion: \u0026quot;{{universion}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; serial_no: \u0026quot;{{serial_no}}\u0026quot; gather_subset: - pg - name: Get list of Hosts dellemc_powermax_gatherfacts: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; universion: \u0026quot;{{universion}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; serial_no: \u0026quot;{{serial_no}}\u0026quot; gather_subset: - host - name: Get list of Host Groups dellemc_powermax_gatherfacts: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; universion: \u0026quot;{{universion}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; serial_no: \u0026quot;{{serial_no}}\u0026quot; gather_subset: - hg - name: Get list of Masking Views dellemc_powermax_gatherfacts: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; universion: \u0026quot;{{universion}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; serial_no: \u0026quot;{{serial_no}}\u0026quot; gather_subset: - mv - name: Get list of RDF Groups dellemc_powermax_gatherfacts: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; universion: \u0026quot;{{universion}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; serial_no: \u0026quot;{{serial_no}}\u0026quot; gather_subset: - rdf - name: Get list of snapshot policies dellemc_powermax_gatherfacts: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; universion: \u0026quot;{{universion}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; serial_no: \u0026quot;{{serial_no}}\u0026quot; gather_subset: - snapshot_policies Return Values The following are the fields unique to this module:\n Key Returned Description    Alerts  type=list elements=string   When the alert exists.  Alert summary of the array.     \u0026nbsp;  acknowledged  type=string   success  Whether or not this alert is acknowledged.     \u0026nbsp;  alertId  type=string   success  Unique ID of alert.     \u0026nbsp;  array  type=string   success  Array serial no.     \u0026nbsp;  created_date  type=string   success  Creation Date.     \u0026nbsp;  created_date_milliseconds  type=string   success  Creation Date in milliseconds.     \u0026nbsp;  description  type=string   success  Description about the alert     \u0026nbsp;  object  type=string   success  Object description     \u0026nbsp;  object_type  type=string   success  Resource class     \u0026nbsp;  severity  type=string   success  Severity of the alert     \u0026nbsp;  state  type=string   success  State of the alert     \u0026nbsp;  type  type=string   success  Type of the alert      Arrays  type=list elements=string   When the Unisphere exist.  List of arrays in the Unisphere.      Health  complex   When the array exist.  Health status of the array.     \u0026nbsp;  health_score_metric  type=list elements=string   success  Overall health score for the specified Symmetrix.     \u0026nbsp; \u0026nbsp;  cached_date  type=integer   success  Date Time stamp in epoch format when it was cached.     \u0026nbsp; \u0026nbsp;  data_date  type=integer   success  Date Time stamp in epoch format when it was collected.     \u0026nbsp; \u0026nbsp;  expired  type=boolean   success  Flag to indicate the expiry of the score.     \u0026nbsp; \u0026nbsp;  health_score  type=integer   success  Overall health score in numbers.     \u0026nbsp; \u0026nbsp;  instance_metrics  type=list elements=string   success  Metrics about a specific instance.     \u0026nbsp; \u0026nbsp; \u0026nbsp;  health_score_instance_metric  type=integer   success  Health score of a specific instance.     \u0026nbsp; \u0026nbsp;  metric  type=string   success  Information about which sub system , such as SYSTEM_UTILIZATION, CONFIGURATION,CAPACITY, and so on.     \u0026nbsp;  num_failed_disks  type=integer   success  Numbers of the disk failure in this system.      HostGroups  type=list elements=string   When the hostgroups exist.  List of host groups present on the array.      Hosts  type=list elements=string   When the hosts exist.  List of hosts present on the array.      MaskingViews  type=list elements=string   When the masking views exist.  List of masking views present on the array.      MetroDREnvironments  type=list elements=string   When environment exists.  List of metro DR environments on the array.      PortGroups  type=list elements=string   When the port groups exist.  List of port groups on the array.      Ports  complex   When the ports exist.  List of ports on the array.     \u0026nbsp;  directorId  type=string   success  Director ID of the port.     \u0026nbsp;  portId  type=string   success  Port number of the port.      RDFGroups  complex   When the RDF groups exist.  List of RDF groups on the array.     \u0026nbsp;  label  type=string   success  Name of the RDF group.     \u0026nbsp;  rdfgNumber  type=integer   success  Unique identifier of the RDF group.      SnapshotPolicies  type=list elements=string   When snapshot policy exists.  List of snapshot policies on the array.      StorageGroups  type=list elements=string   When the storage groups exist.  List of storage groups on the array.      StorageResourcePools  complex   When the storage pools exist.  List of storage pools on the array.     \u0026nbsp;  diskGroupId  type=list elements=string   success  ID of the disk group.     \u0026nbsp;  emulation  type=string   success  Type of volume emulation.     \u0026nbsp;  num_of_disk_groups  type=integer   success  Number of disk groups.     \u0026nbsp;  rdfa_dse  type=boolean   success  Flag for RDFA Delta Set Extension.     \u0026nbsp;  reserved_cap_percent  type=integer   success  Reserved capacity percentage.     \u0026nbsp;  srp_capacity  type=dictionary   success  Different entities to measure SRP capacity.     \u0026nbsp; \u0026nbsp;  effective_used_capacity_percent  type=integer   success  Percentage of effectively used capacity.     \u0026nbsp; \u0026nbsp;  snapshot_modified_tb  type=integer   success  Snapshot modified in TB.     \u0026nbsp; \u0026nbsp;  snapshot_total_tb  type=integer   success  Total snapshot size in TB.     \u0026nbsp; \u0026nbsp;  subscribed_allocated_tb  type=integer   success  Subscribed allocated size in TB.     \u0026nbsp; \u0026nbsp;  subscribed_total_tb  type=integer   success  Subscribed total size in TB.     \u0026nbsp; \u0026nbsp;  usable_total_tb  type=integer   success  Usable total size in TB.     \u0026nbsp; \u0026nbsp;  usable_used_tb  type=integer   success  Usable used size in TB.     \u0026nbsp;  srp_efficiency  type=dictionary   success  Different entities to measure SRP efficiency.     \u0026nbsp; \u0026nbsp;  compression_state  type=string   success  Depicts the compression state of the SRP.     \u0026nbsp; \u0026nbsp;  data_reduction_enabled_percent  type=integer   success  Percentage of data reduction enabled in the SRP.     \u0026nbsp; \u0026nbsp;  data_reduction_ratio_to_one  type=integer   success  Data reduction ratio of SRP.     \u0026nbsp; \u0026nbsp;  overall_efficiency_ratio_to_one  type=integer   success  Overall effectively ratio of SRP.     \u0026nbsp; \u0026nbsp;  snapshot_savings_ratio_to_one  type=integer   success  Snapshot savings ratio of SRP.     \u0026nbsp; \u0026nbsp;  virtual_provisioning_savings_ratio_to_one  type=integer   success  Virtual provisioning savings ratio of SRP.     \u0026nbsp;  srpId  type=string   success  Unique Identifier for SRP.     \u0026nbsp;  total_srdf_dse_allocated_cap_gb  type=integer   success  Total srdf dse allocated capacity in GB.      Volumes  type=list elements=string   When the volumes exist.  List of volumes on the array.     Authors  Arindam Datta (@dattaarindam) \u0026lt;ansible.team@dell.com\u0026gt; Rajshree Khare (@khareRajshree) \u0026lt;ansible.team@dell.com\u0026gt;  Host Module Synopsis Managing hosts on a PowerMax storage system includes creating a host with a set of initiators and host flags, adding and removing initiators to or from a host, modifying host flag values, renaming a host, and deleting a host.\nParameters  Parameter Choices/Defaults Comments    host_flags  type=dictionary      Input as a yaml dictionary List of all host_flags- 1. volume_set_addressing 2. disable_q_reset_on_ua 3. environ_set 4. avoid_reset_broadcast 5. openvms 6. scsi_3 7. spc2_protocol_version 8. scsi_support1 9. consistent_lun Possible values are true, false, unset (default state)     host_name  type=string required=true      The name of the host. No Special Character support except for _. Case sensitive for REST Calls. Creation of an empty host is allowed     host_type  type=string    Choices: default hpux    Describing the OS type (default or hpux)     initiator_state  type=string    Choices: present-in-host absent-in-host    Define whether the initiators should be present or absent on the host. present-in-host - indicates that the initiators should exist on the host absent-in-host - indicates that the initiators should not exist on the host Required when creating a host with initiators or adding and removing initiators to or from an existing host     initiators  type=list elements=string      List of Initiator WWN or IQN to be added to the host or removed from the host.     new_name  type=string      The new name of the host for the renaming function. No Special Character support except for _. Case sensitive for REST Calls     state  type=string required=true    Choices: absent present    Define whether the host should exist or not. present - indicates that the host should exist in the system absent - indicates that the host should not exist in the system    Notes  host_flags and host_type are mutually exclusive parameters.  Examples - name: Create host with host_type 'default' dellemc_powermax_host: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; universion: \u0026quot;{{universion}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; serial_no: \u0026quot;{{serial_no}}\u0026quot; host_name: \u0026quot;ansible_test_1\u0026quot; host_type: \u0026quot;default\u0026quot; state: 'present' - name: Create host with host_type 'hpux' dellemc_powermax_host: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; universion: \u0026quot;{{universion}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; serial_no: \u0026quot;{{serial_no}}\u0026quot; host_name: \u0026quot;ansible_test_2\u0026quot; host_type: \u0026quot;hpux\u0026quot; state: 'present' - name: Create host with host_flags dellemc_powermax_host: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; universion: \u0026quot;{{universion}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; serial_no: \u0026quot;{{serial_no}}\u0026quot; host_name: \u0026quot;ansible_test_3\u0026quot; initiators: - 10000090fa7b4e85 host_flags: spc2_protocol_version: true consistent_lun: true volume_set_addressing: 'unset' disable_q_reset_on_ua: false openvms: 'unset' state: 'present' initiator_state: 'present-in-host' - name: Get host details dellemc_powermax_host: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; universion: \u0026quot;{{universion}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; serial_no: \u0026quot;{{serial_no}}\u0026quot; host_name: \u0026quot;ansible_test_1\u0026quot; state: 'present' - name: Adding initiator to host dellemc_powermax_host: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; universion: \u0026quot;{{universion}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; serial_no: \u0026quot;{{serial_no}}\u0026quot; host_name: \u0026quot;ansible_test_1\u0026quot; initiators: - 10000090fa3d303e initiator_state: 'present-in-host' state: 'present' - name: Removing initiator from host dellemc_powermax_host: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; universion: \u0026quot;{{universion}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; serial_no: \u0026quot;{{serial_no}}\u0026quot; host_name: \u0026quot;ansible_test_1\u0026quot; initiators: - 10000090fa3d303e initiator_state: 'absent-in-host' state: 'present' - name: Modify host using host_type dellemc_powermax_host: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; universion: \u0026quot;{{universion}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; serial_no: \u0026quot;{{serial_no}}\u0026quot; host_name: \u0026quot;ansible_test_1\u0026quot; host_type: \u0026quot;hpux\u0026quot; state: 'present' - name: Modify host using host_flags dellemc_powermax_host: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; universion: \u0026quot;{{universion}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; serial_no: \u0026quot;{{serial_no}}\u0026quot; host_name: \u0026quot;ansible_test_1\u0026quot; host_flags: spc2_protocol_version: unset consistent_lun: unset volume_set_addressing: true disable_q_reset_on_ua: false openvms: false avoid_reset_broadcast: true state: 'present' - name: Rename host dellemc_powermax_host: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; universion: \u0026quot;{{universion}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; serial_no: \u0026quot;{{serial_no}}\u0026quot; host_name: \u0026quot;ansible_test_1\u0026quot; new_name: \u0026quot;ansible_test_1_host\u0026quot; state: 'present' - name: Delete host dellemc_powermax_host: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; universion: \u0026quot;{{universion}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; serial_no: \u0026quot;{{serial_no}}\u0026quot; host_name: \u0026quot;ansible_test_1_host\u0026quot; state: 'absent' Return Values The following are the fields unique to this module:\n Key Returned Description    changed  type=boolean   always  Whether or not the resource has changed.      host_details  complex   When host exist.  Details of the host.     \u0026nbsp;  bw_limit  type=integer   success  Bandwidth limit of the host.     \u0026nbsp;  consistent_lun  type=boolean   success  Flag for consistent LUN in host.     \u0026nbsp;  disabled_flags  type=list elements=string   success  List of any disabled port flags overridden by the initiator.     \u0026nbsp;  enabled_flags  type=list elements=string   success  List of any enabled port flags overridden by the initiator.     \u0026nbsp;  hostgroup  type=list elements=string   success  List of host groups that the host is associated with.     \u0026nbsp;  hostId  type=string   success  Host ID.     \u0026nbsp;  initiator  type=list elements=string   success  List of initiators present in the host.     \u0026nbsp;  maskingview  type=list elements=string   success  List of masking view in which the host group is present.     \u0026nbsp;  num_of_hostgroups  type=integer   success  Number of host groups associated with the host.     \u0026nbsp;  num_of_initiators  type=integer   success  Number of initiators present in the host.     \u0026nbsp;  num_of_masking_views  type=integer   success  Number of masking views associated with the host.     \u0026nbsp;  num_of_powerpath_hosts  type=integer   success  Number of PowerPath hosts associated with the host.     \u0026nbsp;  port_flags_override  type=boolean   success  Whether any of the initiator port flags are overridden.     \u0026nbsp;  type  type=string   success  Type of initiator.     Authors  Vasudevu Lakhinana (@unknown) \u0026lt;ansible.team@dell.com\u0026gt; Manisha Agrawal (@agrawm3) \u0026lt;ansible.team@dell.com\u0026gt;  Host Group Module Synopsis Managing a host group on a PowerMax storage system includes creating a host group with a set of hosts, adding or removing hosts to or from a host group, renaming a host group, modifying host flags of a host group, and deleting a host group.\nParameters  Parameter Choices/Defaults Comments    host_flags  type=dictionary      input as an yaml dictionary List of all host_flags - 1. volume_set_addressing 2. disable_q_reset_on_ua 3. environ_set 4. avoid_reset_broadcast 5. openvms 6. scsi_3 7. spc2_protocol_version 8. scsi_support1 9. consistent_lun Possible values are true, false, unset(default state)     host_state  type=string    Choices: present-in-group absent-in-group    Define whether the host should be present or absent in the host group. present-in-group - indicates that the hosts should exist in the host group absent-in-group - indicates that the hosts should not exist in the host group     host_type  type=string    Choices: default hpux    Describing the OS type (default or hpux)     hostgroup_name  type=string required=true      The name of the host group. No Special Character support except for _. Case sensitive for REST Calls.     hosts  type=list elements=string      List of host names to be added to the host group or removed from the host group. Creation of an empty host group is allowed.     new_name  type=string      The new name for the host group for the renaming function. No Special Character support except for _. Case sensitive for REST Calls     state  type=string required=true    Choices: absent present    Define whether the host group should be present or absent on the system. present - indicates that the host group should be present on the system absent - indicates that the host group should be absent on the system    Notes  In the gather facts module, empty host groups will be listed as hosts. host_flags and host_type are mutually exclusive parameters. Hostgroups with \u0026lsquo;default\u0026rsquo; host_type will have \u0026lsquo;default\u0026rsquo; hosts. Hostgroups with \u0026lsquo;hpux\u0026rsquo; host_type will have \u0026lsquo;hpux\u0026rsquo; hosts.  Examples - name: Create host group with 'default' host_type dellemc_powermax_hostgroup: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; universion: \u0026quot;{{universion}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; serial_no: \u0026quot;{{serial_no}}\u0026quot; hostgroup_name: \u0026quot;ansible_test_HG_1\u0026quot; host_type: \u0026quot;default\u0026quot; hosts: - ansible_test_1 host_state: 'present-in-group' state: 'present' - name: Create host group with 'hpux' host_type dellemc_powermax_hostgroup: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; universion: \u0026quot;{{universion}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; serial_no: \u0026quot;{{serial_no}}\u0026quot; hostgroup_name: \u0026quot;ansible_test_HG_2\u0026quot; host_type: \u0026quot;hpux\u0026quot; hosts: - ansible_test_2 host_state: 'present-in-group' state: 'present' - name: Create host group with host_flags dellemc_powermax_hostgroup: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; universion: \u0026quot;{{universion}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; serial_no: \u0026quot;{{serial_no}}\u0026quot; hostgroup_name: \u0026quot;ansible_test_HG_3\u0026quot; hosts: - ansible_test_3 state: 'present' host_state: 'present-in-group' host_flags: spc2_protocol_version: true consistent_lun: true volume_set_addressing: 'unset' disable_q_reset_on_ua: false openvms: 'unset' - name: Get host group details dellemc_powermax_hostgroup: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; universion: \u0026quot;{{universion}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; serial_no: \u0026quot;{{serial_no}}\u0026quot; hostgroup_name: \u0026quot;ansible_test_HG_1\u0026quot; state: 'present' - name: Adding host to host group dellemc_powermax_hostgroup: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; universion: \u0026quot;{{universion}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; serial_no: \u0026quot;{{serial_no}}\u0026quot; hostgroup_name: \u0026quot;ansible_test_HG_1\u0026quot; hosts: - Ansible_Testing_host2 state: 'present' host_state: 'present-in-group' - name: Removing host from host group dellemc_powermax_hostgroup: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; universion: \u0026quot;{{universion}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; serial_no: \u0026quot;{{serial_no}}\u0026quot; hostgroup_name: \u0026quot;ansible_test_HG_1\u0026quot; hosts: - Ansible_Testing_host2 state: 'present' host_state: 'absent-in-group' - name: Modify host group using host_type dellemc_powermax_hostgroup: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; universion: \u0026quot;{{universion}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; serial_no: \u0026quot;{{serial_no}}\u0026quot; hostgroup_name: \u0026quot;ansible_test_HG_1\u0026quot; host_type: \u0026quot;hpux\u0026quot; state: 'present' - name: Modify host group using host_flags dellemc_powermax_hostgroup: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; universion: \u0026quot;{{universion}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; serial_no: \u0026quot;{{serial_no}}\u0026quot; hostgroup_name: \u0026quot;ansible_test_HG_1\u0026quot; host_flags: spc2_protocol_version: unset disable_q_reset_on_ua: false openvms: false avoid_reset_broadcast: true state: 'present' - name: Rename host group dellemc_powermax_hostgroup: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; universion: \u0026quot;{{universion}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; serial_no: \u0026quot;{{serial_no}}\u0026quot; hostgroup_name: \u0026quot;ansible_test_HG_1\u0026quot; new_name: \u0026quot;ansible_test_hostgroup_1\u0026quot; state: 'present' - name: Delete host group dellemc_powermax_hostgroup: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; universion: \u0026quot;{{universion}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; serial_no: \u0026quot;{{serial_no}}\u0026quot; hostgroup_name: \u0026quot;ansible_test_hostgroup_1\u0026quot; state: 'absent' Return Values The following are the fields unique to this module:\n Key Returned Description    changed  type=boolean   always  Whether or not the resource has changed.      hostgroup_details  complex   When host group exist.  Details of the host group.     \u0026nbsp;  consistent_lun  type=boolean   success  Flag for consistent LUN in the host group.     \u0026nbsp;  disabled_flags  type=list elements=string   success  List of any disabled port flags overridden by the initiator.     \u0026nbsp;  enabled_flags  type=list elements=string   success  List of any enabled port flags overridden by the initiator.     \u0026nbsp;  host  type=list elements=string   success  List of hosts present in the host group.     \u0026nbsp; \u0026nbsp;  hostId  type=string   success  Unique identifier for the host.     \u0026nbsp; \u0026nbsp;  initiator  type=list elements=string   success  List of initiators present in the host.     \u0026nbsp;  hostGroupId  type=string   success  Host group ID.     \u0026nbsp;  maskingview  type=list elements=string   success  Masking view in which host group is present.     \u0026nbsp;  num_of_hosts  type=integer   success  Number of hosts in the host group.     \u0026nbsp;  num_of_initiators  type=integer   success  Number of initiators in the host group.     \u0026nbsp;  num_of_masking_views  type=integer   success  Number of masking views associated with the host group.     \u0026nbsp;  port_flags_override  type=boolean   success  Whether any of the initiator\u0026#x27;s port flags are overridden.     \u0026nbsp;  type  type=string   success  Type of initiator of the hosts of the host group.     Authors  Vasudevu Lakhinana (@unknown) \u0026lt;ansible.team@dell.com\u0026gt; Manisha Agrawal (@agrawm3) \u0026lt;ansible.team@dell.com\u0026gt;  Job Module Synopsis  Gets the details of a Job from a specified PowerMax/VMAX storage system. The details listed are of an asynchronous task.  Parameters  Parameter Choices/Defaults Comments    job_id  type=string required=true      Job ID of an asynchronous task, used for getting details of a job.    Examples - name: Get the details of a Job. dellemc_powermax_job: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; universion: \u0026quot;{{universion}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; job_id: \u0026quot;1570622921504\u0026quot; Return Values The following are the fields unique to this module:\n Key Returned Description    changed  type=boolean   always  Whether or not the resource has changed.      Job_details  type=dictionary   When job exist.  Details of the job.     \u0026nbsp;  completed_date_milliseconds  type=integer   success  Date of job completion in milliseconds.     \u0026nbsp;  jobId  type=string   success  Unique identifier of the job.     \u0026nbsp;  last_modified_date  type=string   success  Last modified date of job.     \u0026nbsp;  last_modified_date_milliseconds  type=integer   success  Last modified date of job in milliseconds.     \u0026nbsp;  name  type=string   success  Name of the job.     \u0026nbsp;  resourceLink  type=string   success  Resource link w.r.t Unisphere.     \u0026nbsp;  result  type=string   success  Job description     \u0026nbsp;  status  type=string   success  Status of the job.     \u0026nbsp;  task  type=list elements=string   success  Details about the job.     \u0026nbsp;  username  type=string   success  Unisphere username.     Authors  Rajshree Khare (@khareRajshree) \u0026lt;ansible.team@dell.com\u0026gt;  Masking View Module Synopsis   Managing masking views on PowerMax storage system includes, creating masking view with port group, storage group and host or host group, renaming masking view and deleting masking view.\n  For creating a masking view -\n  portgroup_name,\n  sg_name and\n  any one of host_name or hostgroup_name is required.\n    All three entities must be present on the array.\n  For renaming a masking view, the \u0026lsquo;new_mv_name\u0026rsquo; is required. After a masking view is created, only its name can be changed. No underlying entity (portgroup, storagegroup, host or hostgroup) can be changed on the masking view.\n  Parameters  Parameter Choices/Defaults Comments    host_name  type=string      The name of the existing host. This parameter is to create an exclusive or host export     hostgroup_name  type=string      The name of the existing host group. This parameter is used to create cluster export     mv_name  type=string required=true      The name of the masking view. No Special Character support except for _. Case sensitive for REST Calls.     new_mv_name  type=string      The new name for the renaming function. No Special Character support except for _. Case sensitive for REST Calls.     portgroup_name  type=string      The name of the existing port group.     sg_name  type=string      The name of the existing storage group.     state  type=string required=true    Choices: absent present    Defines whether the masking view should exist or not.    Examples - name: Create MV with hostgroup dellemc_powermax_maskingview: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; universion: \u0026quot;{{universion}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; serial_no: \u0026quot;{{serial_no}}\u0026quot; mv_name: \u0026quot;Ansible_Testing_MaskingView_HostGroup\u0026quot; portgroup_name: \u0026quot;Ansible_Testing_portgroup\u0026quot; hostgroup_name: \u0026quot;Ansible_Testing_hostgroup\u0026quot; sg_name: \u0026quot;Ansible_Testing_SG\u0026quot; state: \u0026quot;present\u0026quot; - name: Create MV with host dellemc_powermax_maskingview: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; universion: \u0026quot;{{universion}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; serial_no: \u0026quot;{{serial_no}}\u0026quot; mv_name: \u0026quot;Ansible_Testing_MaskingView_Host\u0026quot; portgroup_name: \u0026quot;Ansible_Testing_portgroup\u0026quot; host_name: \u0026quot;Ansible_Testing_host\u0026quot; sg_name: \u0026quot;Ansible_Testing_SG\u0026quot; state: \u0026quot;present\u0026quot; - name: Rename host masking view dellemc_powermax_maskingview: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; universion: \u0026quot;{{universion}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; serial_no: \u0026quot;{{serial_no}}\u0026quot; mv_name: \u0026quot;Ansible_Testing_MaskingView_Host\u0026quot; new_mv_name: \u0026quot;Ansible_Testing_mv_renamed\u0026quot; state: \u0026quot;present\u0026quot; - name: Delete host masking view dellemc_powermax_maskingview: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; universion: \u0026quot;{{universion}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; serial_no: \u0026quot;{{serial_no}}\u0026quot; mv_name: \u0026quot;Ansible_Testing_mv_renamed\u0026quot; state: \u0026quot;absent\u0026quot; Return Values The following are the fields unique to this module:\n Key Returned Description    changed  type=boolean   always  Whether or not the resource has changed.      create_mv  type=boolean   When masking view is created.  Flag sets to true when a new masking view is created.      delete_mv  type=boolean   When masking view is deleted.  Flag sets to true when a masking view is deleted.      modify_mv  type=boolean   When masking view is modified.  Flag sets to true when a masking view is modified.      mv_details  type=list elements=string   When masking view exist.  Details of masking view.     \u0026nbsp;  hostId  type=string   success  Host group present in the masking view.     \u0026nbsp;  maskingViewId  type=string   success  Masking view ID.     \u0026nbsp;  portGroupId  type=string   success  Port group present in the masking view.     \u0026nbsp;  storageGroupId  type=string   success  Storage group present in the masking view.     Authors  Vasudevu Lakhinana (@unknown) \u0026lt;ansible.team@dell.com\u0026gt; Prashant Rakheja (@prashant-dell) \u0026lt;ansible.team@dell.com\u0026gt;  Metro DR Module Synopsis Managing a metro DR environment on a PowerMax storage system includes getting details of any specific metro DR environment, creating a metro DR environment, converting an existing SG into a metro DR environment, modifying metro DR environment attributes and deleting a metro DR environment.\nParameters  Parameter Choices/Defaults Comments    dr_serial_no  type=string      Serial number of the DR array. It is required in create and convert operations.     env_name  type=string required=true      Name of the metro DR environment. Metro DR environment name will be unique across PowerMax.     metro_serial_no  type=string      Serial number of the remote metro array. It is required only in create and convert operations.     new_rdf_group_r1  type=boolean    Choices: no yes\u0026nbsp;\u0026larr;    The flag indicates whether or not to create a new RDFG for a Metro R1 array to a DR array, or to autoselect from an existing one. Used in only create operation.     new_rdf_group_r2  type=boolean    Choices: no yes\u0026nbsp;\u0026larr;    The flag indicates whether or not to create a new RDFG for a Metro R2 array to a DR array, or to autoselect from an existing one. It is used only in create operation.     remove_r1_dr_rdfg  type=boolean    Choices: no\u0026nbsp;\u0026larr; yes    The flag indicates whether or not to override default behavior and delete R11-R2 RDFG from the metro R1 side. It is used only in delete operations.     replication_mode  type=string    Choices: Asynchronous Adaptive Copy    Replication mode whose value will indicate how the data will be replicated. It is required in create and modify operations. It is a mandatory parameter in a create operation but optional in a modify operation.     serial_no  type=string required=true      Serial number of the primary metro array.     sg_name  type=string      Name of the storage group. Storage group will be present on the primary metro array and a storage group with the same name will be created on remote and DR arrays in a create operation. Storage group name is required in \u0026#x27;create metro DR environment\u0026#x27; and \u0026#x27;convert SG into metro DR environment\u0026#x27; operations.     srdf_param  type=dictionary      It contains parameters related to SRDF links. It is used only in modify operations.      dr  type=boolean    Choices: no\u0026nbsp;\u0026larr; yes    The flag indicates whether or not to direct srdf_state change towards device pairs on the disaster recovery leg of the metro DR environment.      keep_r2  type=boolean    Choices: no\u0026nbsp;\u0026larr; yes    The flag indicates whether or not in the case of srdf state suspend to make R2 data on metro available to the host.      metro  type=boolean    Choices: no\u0026nbsp;\u0026larr; yes    The flag indicates whether or not to direct srdf_state change towards the R1--R2 Metro Device leg of the metro DR environment.      srdf_state  type=string required=true    Choices: Split Restore SetMode Failback Failover Establish Suspend UpdateR1 Recover    State of the SRDF link. It is a mandatory parameter for modify operations.     state  type=string required=true    Choices: absent present    State variable to determine whether metro DR environment will exist or not.     wait_for_completion  type=boolean    Choices: no\u0026nbsp;\u0026larr; yes    The flag indicates if the operation should be run synchronously or asynchronously. True signifies synchronous execution. By default, create and convert are asynchronous operations, whereas modify is a synchronous operation.    Examples - name: Get metro environment details dellemc_powermax_metrodr: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; universion: \u0026quot;{{universion}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; serial_no: \u0026quot;{{serial_no}}\u0026quot; env_name: \u0026quot;ansible_metrodr_env\u0026quot; state: \u0026quot;present\u0026quot; - name: Convert SG to metro DR environment dellemc_powermax_metrodr: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; universion: \u0026quot;{{universion}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; sg_name: \u0026quot;ansible_sg\u0026quot; env_name: \u0026quot;ansible_metrodr_env\u0026quot; serial_no: \u0026quot;{{serial_no}}\u0026quot; metro_serial_no: \u0026quot;{{metro_serial_no}}\u0026quot; dr_serial_no: \u0026quot;{{dr_serial_no}}\u0026quot; replication_mode: \u0026quot;Asynchronous\u0026quot; wait_for_completion: False state: \u0026quot;present\u0026quot; - name: Create metro DR environment dellemc_powermax_metrodr: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; universion: \u0026quot;{{universion}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; sg_name: \u0026quot;ansible_sg\u0026quot; env_name: \u0026quot;ansible_metrodr_env\u0026quot; serial_no: \u0026quot;{{serial_no}}\u0026quot; metro_serial_no: \u0026quot;{{metro_serial_no}}\u0026quot; dr_serial_no: \u0026quot;{{dr_serial_no}}\u0026quot; replication_mode: \u0026quot;Asynchronous\u0026quot; new_rdf_group_r1: True new_rdf_group_r2: True wait_for_completion: False state: \u0026quot;present\u0026quot; - name: Modify metro DR environment dellemc_powermax_metrodr: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; universion: \u0026quot;{{universion}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; serial_no: \u0026quot;{{serial_no}}\u0026quot; env_name: \u0026quot;ansible_metrodr_env\u0026quot; srdf_param: srdf_state: \u0026quot;Suspend\u0026quot; metro: True dr: True keep_r2: True wait_for_completion: True state: \u0026quot;present\u0026quot; - name: Delete metro DR environment dellemc_powermax_metrodr: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; universion: \u0026quot;{{universion}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; serial_no: \u0026quot;{{serial_no}}\u0026quot; env_name: \u0026quot;ansible_metrodr_env\u0026quot; remove_r1_dr_rdfg: True state: 'absent' Return Values The following are the fields unique to this module:\n Key Returned Description    changed  type=boolean   always  Whether or not the resource has changed.      Job_details  type=dictionary   When job exist.  Details of the job.     \u0026nbsp;  completed_date_milliseconds  type=integer   success  Date of job completion in milliseconds.     \u0026nbsp;  jobId  type=string   success  Unique identifier of the job.     \u0026nbsp;  last_modified_date  type=string   success  Last modified date of job.     \u0026nbsp;  last_modified_date_milliseconds  type=integer   success  Last modified date of job in milliseconds.     \u0026nbsp;  name  type=string   success  Name of the job.     \u0026nbsp;  resourceLink  type=string   success  Resource link w.r.t Unisphere.     \u0026nbsp;  result  type=string   success  Job description     \u0026nbsp;  status  type=string   success  Status of the job.     \u0026nbsp;  task  type=list elements=string   success  Details about the job.     \u0026nbsp;  username  type=string   success  Unisphere username.      metrodr_env_details  type=dictionary   When environment exists.  Details of the metro DR environment link.     \u0026nbsp;  capacity_gb  type=float   success  Size of volume in GB.     \u0026nbsp;  dr_exempt  type=boolean   success  Flag to indication that if there are exempt devices (volumes) in the DR site or not.     \u0026nbsp;  dr_link_state  type=string   success  Status of DR site.     \u0026nbsp;  dr_percent_complete  type=integer   success  Percentage synchronized in DR session.     \u0026nbsp;  dr_rdf_mode  type=string   success  Replication mode with DR site.     \u0026nbsp;  dr_remain_capacity_to_copy_mb  type=integer   success  Remaining capacity to copy at DR site.     \u0026nbsp;  dr_service_state  type=string   success  The HA state of the DR session.     \u0026nbsp;  dr_state  type=string   success  The pair states of the DR session.     \u0026nbsp;  environment_exempt  type=boolean   success  Flag to indication that if there are exempt devices (volumes) in the environment or not.     \u0026nbsp;  environment_state  type=string   success  The state of the smart DR environment.     \u0026nbsp;  metro_exempt  type=boolean   success  Flag to indication that if there are exempt devices (volumes) in the DR site or not.     \u0026nbsp;  metro_link_state  type=string   success  Status of metro site.     \u0026nbsp;  metro_r1_array_health  type=string   success  Health status of metro R1 array.     \u0026nbsp;  metro_r2_array_health  type=string   success  Health status of metro R1 array.     \u0026nbsp;  metro_service_state  type=string   success  The HA state of the metro session.     \u0026nbsp;  metro_state  type=string   success  The pair states of the metro session.     \u0026nbsp;  metro_witness_state  type=string   success  The witness state of the metro session.     \u0026nbsp;  name  type=string   success  The smart DR environment name.     \u0026nbsp;  valid  type=boolean   success  Flag to indicate whether valid environment or not.     Authors  Vivek Soni (@v-soni11) \u0026lt;ansible.team@dell.com\u0026gt; Rajshree Khare (@khareRajshree) \u0026lt;ansible.team@dell.com\u0026gt;  Port Module Synopsis Managing ports on PowerMax storage system includes getting details of a port.\nParameters  Parameter Choices/Defaults Comments    ports  type=list elements=dictionary required=true      List of port director and port id    Examples - name: Get details of single/multiple ports dellemc_powermax_port: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; serial_no: \u0026quot;{{array_id}}\u0026quot; ports: - director_id: \u0026quot;FA-1D\u0026quot; port_id: \u0026quot;5\u0026quot; - director_id: \u0026quot;SE-1F\u0026quot; port_id: \u0026quot;29\u0026quot; Return Values The following are the fields unique to this module:\n Key Returned Description    changed  type=boolean   always  Whether or not the resource has changed.      port_details  type=list elements=string   When the port exist.  Details of the port.     \u0026nbsp;  symmetrixPort  type=list elements=string   success  Type of volume.     \u0026nbsp; \u0026nbsp;  aclx  type=boolean   success  Indicates whether access control logic is enabled or disabled.     \u0026nbsp; \u0026nbsp;  avoid_reset_broadcast  type=boolean   success  Indicates whether the Avoid Reset Broadcasting feature is enabled or disabled.     \u0026nbsp; \u0026nbsp;  common_serial_number  type=boolean   success  Indicates whether the Common Serial Number feature is enabled or disabled.     \u0026nbsp; \u0026nbsp;  director_status  type=string   success  Director status.     \u0026nbsp; \u0026nbsp;  disable_q_reset_on_ua  type=boolean   success  Indicates whether the Disable Q Reset on UA (Unit Attention) is enabled or disabled.     \u0026nbsp; \u0026nbsp;  enable_auto_negotiate  type=boolean   success  Indicates whether the Enable Auto Negotiate feature is enabled or disabled.     \u0026nbsp; \u0026nbsp;  environ_set  type=boolean   success  Indicates whether the environmental error reporting feature is enabled or disabled.     \u0026nbsp; \u0026nbsp;  hp_3000_mode  type=boolean   success  Indicates whether HP 3000 Mode is enabled or disabled.     \u0026nbsp; \u0026nbsp;  identifier  type=string   success  Unique identifier for port.     \u0026nbsp; \u0026nbsp;  init_point_to_point  type=boolean   success  Indicates whether Init Point to Point is enabled or disabled.     \u0026nbsp; \u0026nbsp;  iscsi_target  type=boolean   success  Indicates whether ISCSI target is enabled or disabled.     \u0026nbsp; \u0026nbsp;  maskingview  type=list elements=string   success  List of Masking views that the port is a part of.     \u0026nbsp; \u0026nbsp;  max_speed  type=string   success  Maximum port speed in GB/Second.     \u0026nbsp; \u0026nbsp;  negotiate_reset  type=boolean   success  Indicates whether the Negotiate Reset feature is enabled or disabled.     \u0026nbsp; \u0026nbsp;  negotiated_speed  type=string   success  Negotiated speed in GB/Second.     \u0026nbsp; \u0026nbsp;  no_participating  type=boolean   success  Indicates whether the No Participate feature is enabled or disabled.     \u0026nbsp; \u0026nbsp;  num_of_cores  type=integer   success  Number of cores for the director.     \u0026nbsp; \u0026nbsp;  num_of_mapped_vols  type=integer   success  Number of volumes mapped with the port.     \u0026nbsp; \u0026nbsp;  num_of_masking_views  type=integer   success  Number of masking views associated with the port.     \u0026nbsp; \u0026nbsp;  num_of_port_groups  type=integer   success  Number of port groups associated with the port.     \u0026nbsp; \u0026nbsp;  port_status  type=string   success  Port status, ON/OFF.     \u0026nbsp; \u0026nbsp;  portgroup  type=list elements=string   success  List of masking views associated with the port.     \u0026nbsp; \u0026nbsp;  scsi_3  type=boolean   success  Indicates whether the SCSI-3 protocol is enabled or disabled.     \u0026nbsp; \u0026nbsp;  scsi_support1  type=boolean   success  Indicates whether the SCSI Support1 is enabled or disabled.     \u0026nbsp; \u0026nbsp;  siemens  type=boolean   success  Indicates whether the Siemens feature is enabled or disabled.     \u0026nbsp; \u0026nbsp;  soft_reset  type=boolean   success  Indicates whether the Soft Reset feature is enabled or disabled.     \u0026nbsp; \u0026nbsp;  spc2_protocol_version  type=boolean   success  Indicates whether the SPC2 Protocol Version feature is enabled or disabled.     \u0026nbsp; \u0026nbsp;  sunapee  type=boolean   success  Indicates whether the Sunapee feature is enabled or disabled.     \u0026nbsp; \u0026nbsp;  symmetrixPortKey  type=list elements=string   success  Symmetrix system director and port in the port group.     \u0026nbsp; \u0026nbsp; \u0026nbsp;  drectorId  type=string   success  Director ID of the port.     \u0026nbsp; \u0026nbsp; \u0026nbsp;  portId  type=string   success  Port number of the port.     \u0026nbsp; \u0026nbsp;  type  type=string   success  Type of port.     \u0026nbsp; \u0026nbsp;  unique_wwn  type=boolean   success  Indicates whether the Unique WWN feature is enabled or disabled.     \u0026nbsp; \u0026nbsp;  vnx_attached  type=boolean   success  Indicates whether the VNX attached feature is enabled or disabled.     \u0026nbsp; \u0026nbsp;  volume_set_addressing  type=boolean   success  Indicates whether Volume Vet Addressing is enabled or disabled.     \u0026nbsp; \u0026nbsp;  wwn_node  type=string   success  WWN node of port.     Authors  Ashish Verma (@vermaa31) \u0026lt;ansible.team@dell.com\u0026gt;  Port Group Module Synopsis Managing port groups on a PowerMax storage system includes creating a port group with a set of ports, adding or removing single or multiple ports to or from the port group, renaming the port group and deleting the port group.\nParameters  Parameter Choices/Defaults Comments    new_name  type=string      New name of the port group while renaming. No Special Character support except for _. Case sensitive for REST Calls.     port_state  type=string    Choices: present-in-group absent-in-group    Define whether the port should be present or absent in the port group. present-in-group - indicates that the ports should be present on a port group object absent-in-group - indicates that the ports should not be present on a port group object     portgroup_name  type=string required=true      The name of the port group. No Special Character support except for _. Case sensitive for REST Calls.     ports  type=list elements=dictionary      List of directors and ports to be added or removed to or from the port group     state  type=string required=true    Choices: absent present    Define whether the port group should exist or not. present - indicates that the port group should be present on the system absent - indicates that the port group should not be present on the system    Examples - name: Create port group without ports dellemc_powermax_portgroup: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; universion: \u0026quot;{{universion}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; serial_no: \u0026quot;{{array_id}}\u0026quot; portgroup_name: \u0026quot;{{portgroup_name}}\u0026quot; state: \u0026quot;present\u0026quot; - name: Create port group with ports dellemc_powermax_portgroup: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; universion: \u0026quot;{{universion}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; serial_no: \u0026quot;{{array_id}}\u0026quot; portgroup_name: \u0026quot;{{portgroup_name}}\u0026quot; state: \u0026quot;present\u0026quot; ports: - director_id: \u0026quot;FA-1D\u0026quot; port_id: \u0026quot;5\u0026quot; - director_id: \u0026quot;FA-2D\u0026quot; port_id: \u0026quot;5\u0026quot; port_state: \u0026quot;present-in-group\u0026quot; - name: Add ports to port group dellemc_powermax_portgroup: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; universion: \u0026quot;{{universion}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; serial_no: \u0026quot;{{array_id}}\u0026quot; portgroup_name: \u0026quot;{{portgroup_name}}\u0026quot; state: \u0026quot;present\u0026quot; ports: - director_id: \u0026quot;FA-2D\u0026quot; port_id: \u0026quot;8\u0026quot; - director_id: \u0026quot;FA-2D\u0026quot; port_id: \u0026quot;9\u0026quot; port_state: \u0026quot;present-in-group\u0026quot; - name: Remove ports from port group dellemc_powermax_portgroup: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; universion: \u0026quot;{{universion}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; serial_no: \u0026quot;{{array_id}}\u0026quot; portgroup_name: \u0026quot;{{portgroup_name}}\u0026quot; state: \u0026quot;present\u0026quot; ports: - director_id: \u0026quot;FA-2D\u0026quot; port_id: \u0026quot;8\u0026quot; - director_id: \u0026quot;FA-2D\u0026quot; port_id: \u0026quot;9\u0026quot; port_state: \u0026quot;absent-in-group\u0026quot; - name: Modify port group dellemc_powermax_portgroup: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; universion: \u0026quot;{{universion}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; serial_no: \u0026quot;{{array_id}}\u0026quot; portgroup_name: \u0026quot;{{portgroup_name}}\u0026quot; state: \u0026quot;present\u0026quot; new_name: \u0026quot;{{new_name}}\u0026quot; - name: Delete port group dellemc_powermax_portgroup: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; universion: \u0026quot;{{universion}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; serial_no: \u0026quot;{{array_id}}\u0026quot; portgroup_name: \u0026quot;{{portgroup_name}}\u0026quot; state: \u0026quot;absent\u0026quot; Return Values The following are the fields unique to this module:\n Key Returned Description    changed  type=boolean   always  Whether or not the resource has changed.      portgroup_details  type=list elements=string   When the port group exist.  Details of the port group.     \u0026nbsp;  num_of_masking_views  type=integer   success  Number of masking views in where port group is associated.     \u0026nbsp;  num_of_ports  type=integer   success  Number of ports in the port group.     \u0026nbsp;  portGroupId  type=string   success  Port group ID.     \u0026nbsp;  symmetrixPortKey  type=list elements=string   success  Symmetrix system director and port in the port group.     \u0026nbsp; \u0026nbsp;  directorId  type=string   success  Director ID of the port.     \u0026nbsp; \u0026nbsp;  portId  type=string   success  Port number of the port.     \u0026nbsp;  type  type=string   success  Type of ports in port group.     Authors  Vasudevu Lakhinana (@unknown) \u0026lt;ansible.team@dell.com\u0026gt; Ashish Verma (@vermaa31) \u0026lt;ansible.team@dell.com\u0026gt; Rajshree Khare (@khareRajshree) \u0026lt;ansible.team@dell.com\u0026gt;  RDF Group Module Synopsis  Gets the details of an RDF Group from a specified PowerMax/VMAX storage system. Lists the volumes of an RDF Group from a specified PowerMax/VMAX storage system  Parameters  Parameter Choices/Defaults Comments    rdfgroup_number  type=string required=true      Identifier of an RDF Group of type string    Examples - name: Get the details of rdf group and volumes dellemc_powermax_rdfgroup: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; universion: \u0026quot;{{universion}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; serial_no: \u0026quot;{{serial_no}}\u0026quot; rdfgroup_number: \u0026quot;{{rdfgroup_id}}\u0026quot; Return Values The following are the fields unique to this module:\n Key Returned Description    changed  type=boolean   always  Whether or not the resource has changed.      RDFGroupDetails  type=list elements=string   When the RDF group exist.  Details of the RDF group.     \u0026nbsp;  async  type=boolean   success  Flag sets to true when an SRDF pair is in async mode.     \u0026nbsp;  biasConfigured  type=boolean   success  Flag for configured bias.     \u0026nbsp;  biasEffective  type=boolean   success  Flag for effective bias.     \u0026nbsp;  device_polarity  type=string   success  Type of device polarity.     \u0026nbsp;  hardware_compression  type=boolean   success  Flag for hardware compression.     \u0026nbsp;  label  type=string   success  RDF group label.     \u0026nbsp;  link_limbo  type=integer   success  The amount of time that the array\u0026#x27;s operating environment waits after the SRDF link goes down before updating the link\u0026#x27;s status. The link limbo value can be set from 0 to 120 seconds. The default value is 10 seconds.     \u0026nbsp;  localOnlinePorts  type=list elements=string   success  List of local online ports.     \u0026nbsp;  localPorts  type=list elements=string   success  List of local ports.     \u0026nbsp;  metro  type=list elements=string   success  Flag for metro configuration.     \u0026nbsp;  modes  type=string   success  Mode of the SRDF link.     \u0026nbsp;  numDevices  type=integer   success  Number of devices involved in the pairing.     \u0026nbsp;  offline  type=boolean   success  Offline flag.     \u0026nbsp;  rdfa_properties  type=list elements=string   success  Properties associated with the RDF group.     \u0026nbsp; \u0026nbsp;  average_cycle_time  type=integer   success  Average cycle time (seconds) configured for this session in seconds.     \u0026nbsp; \u0026nbsp;  consistency_exempt_volumes  type=boolean   success  Flag that indicates if consistency is exempt.     \u0026nbsp; \u0026nbsp;  cycle_number  type=integer   success  Number of cycles in seconds.     \u0026nbsp; \u0026nbsp;  dse_active  type=boolean   success  Flag for active Delta Set Extension.     \u0026nbsp; \u0026nbsp;  dse_autostart  type=string   success  Indicates DSE autostart state.     \u0026nbsp; \u0026nbsp;  dse_threshold  type=integer   success  Flag for DSE threshold.     \u0026nbsp; \u0026nbsp;  duration_of_last_cycle  type=integer   success  The cycle time (in secs) of the most recently completed cycle.     \u0026nbsp; \u0026nbsp;  duration_of_last_transmit_cycle  type=integer   success  Duration of last transmitted cycle in seconds.     \u0026nbsp; \u0026nbsp;  r1_to_r2_lag_time  type=integer   success  Time that R2 is behind R1 in seconds.     \u0026nbsp; \u0026nbsp;  session_priority  type=integer   success  Priority used to determine which RDFA sessions to drop if cache becomes full. Values range from 1 to 64, with 1 being the highest priority (last to be dropped).     \u0026nbsp; \u0026nbsp;  session_uncommitted_tracks  type=integer   success  Number of uncommitted session tracks.     \u0026nbsp; \u0026nbsp;  transmit_idle_state  type=string   success  Indicates RDFA transmit idle state.     \u0026nbsp; \u0026nbsp;  transmit_idle_time  type=integer   success  Time the transmit cycle has been idle.     \u0026nbsp; \u0026nbsp;  transmit_queue_depth  type=integer   success  The transmitted queue depth of disks.     \u0026nbsp;  rdfgNumber  type=integer   success  RDF group number on primary device.     \u0026nbsp;  RDFGroupVolumes  type=list elements=string   success  List of various properties of RDF group volume(s).     \u0026nbsp; \u0026nbsp;  largerRdfSide  type=string   success  Larger RDF side among the devices.     \u0026nbsp; \u0026nbsp;  local_wwn_external  type=integer   success  External WWN of volume at primary device.     \u0026nbsp; \u0026nbsp;  localRdfGroupNumber  type=integer   success  RDF group number at primary device.     \u0026nbsp; \u0026nbsp;  localSymmetrixId  type=integer   success  Primary device ID.     \u0026nbsp; \u0026nbsp;  localVolumeName  type=string   success  Volume name at primary device.     \u0026nbsp; \u0026nbsp;  localVolumeState  type=string   success  Volume state at primary device     \u0026nbsp; \u0026nbsp;  rdfMode  type=string   success  SRDF mode of pairing.     \u0026nbsp; \u0026nbsp;  rdfpairState  type=string   success  SRDF state of pairing.     \u0026nbsp; \u0026nbsp;  remote_wwn_external  type=integer   success  External WWN of volume at remote device.     \u0026nbsp; \u0026nbsp;  remoteRdfGroupNumber  type=integer   success  RDF group number at remote device.     \u0026nbsp; \u0026nbsp;  remoteSymmetrixId  type=integer   success  Remote device ID.     \u0026nbsp; \u0026nbsp;  remoteVolumeName  type=string   success  Volume name at remote device.     \u0026nbsp; \u0026nbsp;  remoteVolumeState  type=string   success  Volume state at remote device.     \u0026nbsp; \u0026nbsp;  volumeConfig  type=string   success  Type of volume.     \u0026nbsp;  remoteOnlinePorts  type=list elements=string   success  List of remote online ports.     \u0026nbsp;  remotePorts  type=list elements=string   success  List of remote ports.     \u0026nbsp;  remoteRdfgNumber  type=integer   success  RDF group number at remote device.     \u0026nbsp;  remoteSymmetrix  type=integer   success  Remote device ID.     \u0026nbsp;  software_compression  type=boolean   success  Flag for software compression.     \u0026nbsp;  totalDeviceCapacity  type=integer   success  Total capacity of RDF group in GB.     \u0026nbsp;  type  type=string   success  Type of RDF group.     \u0026nbsp;  vasa_group  type=boolean   success  Flag for VASA group member.     \u0026nbsp;  witness  type=boolean   success  Flag for witness.     \u0026nbsp;  witnessConfigured  type=boolean   success  Flag for configured witness.     \u0026nbsp;  witnessDegraded  type=boolean   success  Flag for degraded witness.     \u0026nbsp;  witnessEffective  type=boolean   success  Flag for effective witness.     \u0026nbsp;  witnessProtectedPhysical  type=boolean   success  Flag for physically protected witness.     \u0026nbsp;  witnessProtectedVirtual  type=boolean   success  Flag for virtually protected witness.     Authors  Arindam Datta (@dattaarindam) \u0026lt;ansible.team@dell.com\u0026gt;  Snapshot Module Synopsis Managing snapshots on a PowerMax storage system includes creating a new storage group (SG) snapshot, getting details of the SG snapshot, renaming the SG snapshot, changing the snapshot link status, and deleting an existing SG snapshot.\nParameters  Parameter Choices/Defaults Comments    generation  type=integer      The generation number of the snapshot. Generation is required for link, unlink, rename and delete operations. Optional for Get snapshot details. Create snapshot will always create a new snapshot with a generation number 0. Rename is supported only for generation number 0.     link_status  type=string    Choices: linked unlinked    Describes the link status of the snapshot.     new_snapshot_name  type=string      The new name of the snapshot.     sg_name  type=string required=true      The name of the storage group.     snapshot_id  type=integer      Unique ID of the snapshot. snapshot_id is required for link, unlink, rename and delete operations. Optional for Get snapshot details.     snapshot_name  type=string required=true      The name of the snapshot.     state  type=string required=true    Choices: absent present    Define whether the snapshot should exist or not.     target_sg_name  type=string      The target storage group.     ttl  type=string      The Time To Live (TTL) value for the snapshot. If the TTL is not specified, the storage group snap details are returned. However, to create a SG snap - TTL must be given. If the SG snap should not have any TTL - specify TTL as \u0026quot;None\u0026quot;     ttl_unit  type=string    Choices: hours days\u0026nbsp;\u0026larr;    The unit for the ttl. If no ttl_unit is specified, \u0026#x27;days\u0026#x27; is taken as default ttl_unit.    Notes  Paramters \u0026lsquo;generation\u0026rsquo; and \u0026lsquo;snapshot_id\u0026rsquo; are mutually exclusive. If \u0026lsquo;generation\u0026rsquo; or \u0026lsquo;snapshot_id\u0026rsquo; is not provided then a list of generation versus snapshot_id is returned. Use of \u0026lsquo;snapshot_id\u0026rsquo; over \u0026lsquo;generation\u0026rsquo; is preferably recommended for PowerMax microcode version 5978.669.669 and onwards.  Examples - name: Create a Snapshot for a Storage Group dellemc_powermax_snapshot: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; universion: \u0026quot;{{universion}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; serial_no: \u0026quot;{{serial_no}}\u0026quot; sg_name: \u0026quot;ansible_sg\u0026quot; snapshot_name: \u0026quot;ansible_sg_snap\u0026quot; ttl: \u0026quot;2\u0026quot; ttl_unit: \u0026quot;days\u0026quot; state: \u0026quot;present\u0026quot; - name: Get Storage Group Snapshot details dellemc_powermax_snapshot: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; universion: \u0026quot;{{universion}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; serial_no: \u0026quot;{{serial_no}}\u0026quot; sg_name: \u0026quot;ansible_sg\u0026quot; snapshot_name: \u0026quot;ansible_sg_snap\u0026quot; state: \u0026quot;present\u0026quot; - name: Get Storage Group Snapshot details using generation dellemc_powermax_snapshot: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; universion: \u0026quot;{{universion}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; serial_no: \u0026quot;{{serial_no}}\u0026quot; sg_name: \u0026quot;ansible_sg\u0026quot; snapshot_name: \u0026quot;ansible_sg_snap\u0026quot; generation: 1 state: \u0026quot;present\u0026quot; - name: Get Storage Group Snapshot details using snapshot_id dellemc_powermax_snapshot: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; universion: \u0026quot;{{universion}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; serial_no: \u0026quot;{{serial_no}}\u0026quot; sg_name: \u0026quot;ansible_sg\u0026quot; snapshot_name: \u0026quot;ansible_sg_snap\u0026quot; snapshot_id: 135023964929 state: \u0026quot;present\u0026quot; - name: Rename Storage Group Snapshot using generation dellemc_powermax_snapshot: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; universion: \u0026quot;{{universion}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; serial_no: \u0026quot;{{serial_no}}\u0026quot; sg_name: \u0026quot;ansible_sg\u0026quot; snapshot_name: \u0026quot;ansible_sg_snap\u0026quot; new_snapshot_name: \u0026quot;ansible_snap_new\u0026quot; generation: 0 state: \u0026quot;present\u0026quot; - name: Rename Storage Group Snapshot using snapshot_id dellemc_powermax_snapshot: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; universion: \u0026quot;{{universion}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; serial_no: \u0026quot;{{serial_no}}\u0026quot; sg_name: \u0026quot;ansible_sg\u0026quot; snapshot_name: \u0026quot;ansible_sg_snap\u0026quot; new_snapshot_name: \u0026quot;ansible_snap_new\u0026quot; snapshot_id: 135023964929 state: \u0026quot;present\u0026quot; - name: Change Snapshot Link Status to Linked using generation dellemc_powermax_snapshot: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; universion: \u0026quot;{{universion}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; serial_no: \u0026quot;{{serial_no}}\u0026quot; sg_name: \u0026quot;ansible_sg\u0026quot; snapshot_name: \u0026quot;ansible_snap_new\u0026quot; generation: 1 target_sg_name: \u0026quot;ansible_sg_target\u0026quot; link_status: \u0026quot;linked\u0026quot; state: \u0026quot;present\u0026quot; - name: Change Snapshot Link Status to UnLinked using generation dellemc_powermax_snapshot: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; universion: \u0026quot;{{universion}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; serial_no: \u0026quot;{{serial_no}}\u0026quot; sg_name: \u0026quot;ansible_sg\u0026quot; snapshot_name: \u0026quot;ansible_snap_new\u0026quot; generation: 1 target_sg_name: \u0026quot;ansible_sg_target\u0026quot; link_status: \u0026quot;unlinked\u0026quot; state: \u0026quot;present\u0026quot; - name: Change Snapshot Link Status to Linked using snapshot_id dellemc_powermax_snapshot: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; universion: \u0026quot;{{universion}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; serial_no: \u0026quot;{{serial_no}}\u0026quot; sg_name: \u0026quot;ansible_sg\u0026quot; snapshot_name: \u0026quot;ansible_snap_new\u0026quot; snapshot_id: 135023964515 target_sg_name: \u0026quot;ansible_sg_target\u0026quot; link_status: \u0026quot;linked\u0026quot; state: \u0026quot;present\u0026quot; - name: Change Snapshot Link Status to UnLinked using snapshot_id dellemc_powermax_snapshot: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; universion: \u0026quot;{{universion}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; serial_no: \u0026quot;{{serial_no}}\u0026quot; sg_name: \u0026quot;ansible_sg\u0026quot; snapshot_name: \u0026quot;ansible_snap_new\u0026quot; snapshot_id: 135023964515 target_sg_name: \u0026quot;ansible_sg_target\u0026quot; link_status: \u0026quot;unlinked\u0026quot; state: \u0026quot;present\u0026quot; - name: Delete Storage Group Snapshot using generation dellemc_powermax_snapshot: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; universion: \u0026quot;{{universion}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; serial_no: \u0026quot;{{serial_no}}\u0026quot; sg_name: \u0026quot;ansible_sg\u0026quot; snapshot_name: \u0026quot;ansible_sg_snap\u0026quot; generation: 1 state: \u0026quot;absent\u0026quot; - name: Delete Storage Group Snapshot using snapshot_id dellemc_powermax_snapshot: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; universion: \u0026quot;{{universion}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; serial_no: \u0026quot;{{serial_no}}\u0026quot; sg_name: \u0026quot;ansible_sg\u0026quot; snapshot_name: \u0026quot;ansible_sg_snap\u0026quot; snapshot_id: 135023964929 state: \u0026quot;absent\u0026quot; Return Values The following are the fields unique to this module:\n Key Returned Description    changed  type=boolean   always  Whether or not the resource has changed.      create_sg_snap  type=boolean   When snapshot is created.  Flag sets to true when the snapshot is created.      delete_sg_snap  type=boolean   When snapshot is deleted.  Flag sets to true when the snapshot is deleted.      rename_sg_snap  type=boolean   When snapshot is renamed.  Flag sets to true when the snapshot is renamed.      sg_snap_details  complex   When snapshot exists.  Details of the snapshot.     \u0026nbsp;  expired  type=boolean   success  Indicates whether the snapshot is expired or not.     \u0026nbsp;  generation/snapid  type=integer   success  The generation/snapshot ID of the snapshot.     \u0026nbsp;  linked  type=boolean   success  Indicates whether the snapshot is linked or not.     \u0026nbsp;  name  type=string   success  Name of the snapshot.     \u0026nbsp;  non_shared_tracks  type=integer   success  Number of non-shared tracks.     \u0026nbsp;  num_source_volumes  type=integer   success  Number of source volumes.     \u0026nbsp;  num_storage_group_volumes  type=integer   success  Number of storage group volumes.     \u0026nbsp;  restored  type=boolean   success  Indicates whether the snapshot is restored or not.     \u0026nbsp;  source_volume  type=list elements=string   success  Source volume details.     \u0026nbsp; \u0026nbsp;  capacity  type=integer   success  Volume capacity.     \u0026nbsp; \u0026nbsp;  capacity_gb  type=integer   success  Volume capacity in GB.     \u0026nbsp; \u0026nbsp;  name  type=string   success  Volume ID.     \u0026nbsp;  state  type=string   success  State of the snapshot.     \u0026nbsp;  time_to_live_expiry_date  type=string   success  Time to live expiry date.     \u0026nbsp;  timestamp  type=string   success  Snapshot time stamp.     \u0026nbsp;  timestamp_utc  type=integer   success  Snapshot time stamp specified in UTC.     \u0026nbsp;  tracks  type=integer   success  Number of tracks.     Authors  Prashant Rakheja (@prashant-dell) \u0026lt;ansible.team@dell.com\u0026gt; Rajshree Khare (@khareRajshree) \u0026lt;ansible.team@dell.com\u0026gt;  Snapshot Policy Module Synopsis Managing a snapshot policy on a PowerMax storage system includes getting details of any specific snapshot policy, creating a snapshot policy, modifying snapshot policy attributes, modifying snapshot policy state, associating or disassociating storage groups to or from snapshot policy and deleting a snapshot policy.\nParameters  Parameter Choices/Defaults Comments    compliance_count_critical  type=integer      If the number of valid snapshots falls below this number, the compliance changes to critical (red).     compliance_count_warning  type=integer      If the number of valid snapshots falls below this number, the compliance changes to warning (yellow).     interval  type=string    Choices: 10 Minutes 12 Minutes 15 Minutes 20 Minutes 30 Minutes 1 Hour 2 Hours 3 Hours 4 Hours 6 Hours 8 Hours 12 Hours 1 Day 7 Days    The value of the interval counter for snapshot policy execution.     new_snapshot_policy_name  type=string      New name of the snapshot policy.     offset_mins  type=integer      Defines when, within the interval the snapshots will be taken for a specified snapshot policy. The offset must be less than the interval of the snapshot policy. The format must be in minutes. If not specified, default value is 0.     secure  type=boolean    Choices: no yes    Secure snapshots may only be terminated after they expire or by Dell EMC support. If not specified, default value is False.     snapshot_count  type=integer      The max snapshot count of the policy. Max value is 1024.     snapshot_policy_name  type=string required=true      Name of the snapshot policy.     state  type=string required=true    Choices: present absent    Shows if the snapshot policy should be present or absent.     storage_group_state  type=string    Choices: present-in-policy absent-in-policy    The state of the storage group with regard to the snapshot policy. present-in-policy indicates associate SG to SP. absent-in-policy indicates disassociate SG from SP.     storage_groups  type=list elements=string      List of storage groups.     suspend  type=boolean    Choices: no yes    Suspend the snapshot policy. True indicates snapshot policy is in suspend state. False indicates snapshot policy is in resume state.     universion  type=integer    Choices: 92    Unisphere version, currently \u0026#x27;92\u0026#x27; version is supported.    Notes  The max number of snapshot policies on an array is limited to 20. At most four snapshot policies can be associated with a storage group. The compliance_count_warning value should be less than total_snapshot_count value of the policy. The compliance_count_critical value should be less than or equal to the compliance_count_warning value of the policy.  Examples - name: Create a snapshot policy dellemc_powermax_snapshotpolicy: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; universion: \u0026quot;{{universion}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; serial_no: \u0026quot;{{serial_no}}\u0026quot; snapshot_policy_name: \u0026quot;10min_policy_1\u0026quot; interval: \u0026quot;10 Minutes\u0026quot; secure: false snapshot_count: 10 offset_mins: 2 compliance_count_warning: 6 compliance_count_critical: 4 state: \u0026quot;present\u0026quot; - name: Create a snapshot policy and associate storage groups to it dellemc_powermax_snapshotpolicy: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; universion: \u0026quot;{{universion}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; serial_no: \u0026quot;{{serial_no}}\u0026quot; snapshot_policy_name: \u0026quot;10min_policy_2\u0026quot; interval: \u0026quot;10 Minutes\u0026quot; secure: false snapshot_count: 12 offset_mins: 5 compliance_count_warning: 8 compliance_count_critical: 4 storage_groups: - \u0026quot;11_ansible_test_1\u0026quot; - \u0026quot;11_ansible_test_2\u0026quot; storage_group_state: \u0026quot;present-in-policy\u0026quot; state: \u0026quot;present\u0026quot; - name: Get snapshot policy details dellemc_powermax_snapshotpolicy: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; universion: \u0026quot;{{universion}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; serial_no: \u0026quot;{{serial_no}}\u0026quot; snapshot_policy_name: \u0026quot;10min_policy_2\u0026quot; state: \u0026quot;present\u0026quot; - name: Modify snapshot policy attributes dellemc_powermax_snapshotpolicy: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; universion: \u0026quot;{{universion}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; serial_no: \u0026quot;{{serial_no}}\u0026quot; snapshot_policy_name: \u0026quot;10min_policy_2\u0026quot; new_snapshot_policy_name: \u0026quot;10min_policy_2_new\u0026quot; interval: \u0026quot;10 Minutes\u0026quot; snapshot_count: 16 offset_mins: 8 compliance_count_warning: 9 compliance_count_critical: 7 state: \u0026quot;present\u0026quot; - name: Modify snapshot policy, associate to storage groups dellemc_powermax_snapshotpolicy: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; universion: \u0026quot;{{universion}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; serial_no: \u0026quot;{{serial_no}}\u0026quot; snapshot_policy_name: \u0026quot;10min_policy_1\u0026quot; storage_groups: - \u0026quot;11_ansible_test_1\u0026quot; - \u0026quot;11_ansible_test_2\u0026quot; storage_group_state: \u0026quot;present-in-policy\u0026quot; state: \u0026quot;present\u0026quot; - name: Modify snapshot policy, disassociate from storage groups dellemc_powermax_snapshotpolicy: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; universion: \u0026quot;{{universion}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; serial_no: \u0026quot;{{serial_no}}\u0026quot; snapshot_policy_name: \u0026quot;10min_policy_1\u0026quot; storage_groups: - \u0026quot;11_ansible_test_1\u0026quot; - \u0026quot;11_ansible_test_2\u0026quot; storage_group_state: \u0026quot;absent-in-policy\u0026quot; state: \u0026quot;present\u0026quot; - name: Modify snapshot policy state to suspend dellemc_powermax_snapshotpolicy: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; universion: \u0026quot;{{universion}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; serial_no: \u0026quot;{{serial_no}}\u0026quot; snapshot_policy_name: \u0026quot;10min_policy_1\u0026quot; suspend: true state: \u0026quot;present\u0026quot; - name: Modify snapshot policy state to resume dellemc_powermax_snapshotpolicy: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; universion: \u0026quot;{{universion}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; serial_no: \u0026quot;{{serial_no}}\u0026quot; snapshot_policy_name: \u0026quot;10min_policy_1\u0026quot; suspend: false state: \u0026quot;present\u0026quot; - name: Delete a snapshot policy dellemc_powermax_snapshotpolicy: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; universion: \u0026quot;{{universion}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; serial_no: \u0026quot;{{serial_no}}\u0026quot; snapshot_policy_name: \u0026quot;10min_policy_1\u0026quot; state: \u0026quot;absent\u0026quot; Return Values The following are the fields unique to this module:\n Key Returned Description    changed  type=boolean   always  Whether or not the resource has changed.      snapshot_policy_details  complex   When snapshot policy exists.  Details of the snapshot policy.     \u0026nbsp;  compliance_count_critical  type=integer   success  The number of valid snapshots that have critical compliance.     \u0026nbsp;  compliance_count_warning  type=integer   success  The number of valid snapshots that have warning compliance.     \u0026nbsp;  interval_minutes  type=integer   success  The interval minutes for snapshot policy execution.     \u0026nbsp;  last_time_used  type=string   success  The timestamp indicating the last time snapshot policy was used.     \u0026nbsp;  offset_minutes  type=integer   success  It is the time in minutes within the interval when the snapshots will be taken for a specified Snapshot Policy.     \u0026nbsp;  secure  type=boolean   success  True value indicates that the secure snapshots may only be terminated after they expire or by Dell EMC support.     \u0026nbsp;  snapshot_count  type=integer   success  It is the max snapshot count of the policy.     \u0026nbsp;  snapshot_policy_name  type=string   success  Name of the snapshot policy.     \u0026nbsp;  storage_group  type=list elements=string   success  The list of storage groups associated with the snapshot policy.     \u0026nbsp;  storage_group_count  type=integer   success  The number of storage groups associated with the snapshot policy.     \u0026nbsp;  storage_group_snapshotID  type=list elements=string   success  Pair of storage group and list of snapshot IDs associated with the snapshot policy.     \u0026nbsp;  suspended  type=boolean   success  The state of the snapshot policy, true indicates policy is in suspend state.     \u0026nbsp;  symmetrixID  type=string   success  The symmetrix on which snapshot policy exists.     Authors  Rajshree Khare (@khareRajshree) \u0026lt;ansible.team@dell.com\u0026gt;  SRDF Module Synopsis Managing SRDF link on a PowerMax storage system includes creating an SRDF pair for a storage group, modifying the SRDF mode, modifying the SRDF state of an existing SRDF pair, and deleting an SRDF pair. All create and modify calls are asynchronous by default.\nParameters  Parameter Choices/Defaults Comments    job_id  type=string      Job ID of an asynchronous task. Can be used to get details of a job.     new_rdf_group  type=boolean    Choices: no yes    Overrides the SRDF group selection functionality and forces the creation of a new SRDF group. PowerMax has a limited number of RDF groups. If this flag is set to True, and the RDF groups are exhausted, then SRDF link creation will fail. If not specified, default value is \u0026#x27;false\u0026#x27;.     rdfg_no  type=integer      The RDF group number. Optional parameter for each call. For a create operation, if specified, the array will reuse the RDF group, otherwise an error is returned. For modify and delete operations, if the RFD group number is not specified, and the storage group is protected by multiple RDF groups, then an error is raised.     remote_serial_no  type=string      integer 12-digit serial number of remote PowerMax or VMAX array. Required while creating an SRDF link.     serial_no  type=string required=true      The serial number will refer to the source PowerMax/VMAX array when protecting a storage group. However srdf_state operations may be issued from primary or remote array.     sg_name  type=string      Name of storage group. SRDF pairings are managed at a storage group level. Required to identify the SRDF link.     srdf_mode  type=string    Choices: Active Adaptive Copy Synchronous Asynchronous    The replication mode of the SRDF pair. Required when creating an SRDF pair. Can be modified by providing a required value.     srdf_state  type=string    Choices: Establish Resume Restore Suspend Swap Split Failback Failover Setbias    Desired state of the SRDF pairing. While creating a new SRDF pair, allowed values are \u0026#x27;Establish\u0026#x27; and \u0026#x27;Suspend\u0026#x27;. If the state is not specified, the pair will be created in a \u0026#x27;Suspended\u0026#x27; state. When modifying the state, only certain changes are allowed.     state  type=string required=true    Choices: absent present    Define whether the SRDF pairing should exist or not. present indicates that the SRDF pairing should exist in system. absent indicates that the SRDF pairing should not exist in system.     wait_for_completion  type=boolean    Choices: no\u0026nbsp;\u0026larr; yes    Flag to indicate if the operation should be run synchronously or asynchronously. True signifies synchronous execution. By default, all create and update operations will be run asynchronously.     witness  type=boolean    Choices: no yes    Flag to specify use of Witness for a Metro configuration. Setting to True signifies to use Witness, setting it to False signifies to use Bias. It is recommended to configure a witness for SRDF Metro in a production environment, this is configured via Unisphere for PowerMax UI or REST. The flag can be set only for modifying srdf_state to either Establish, Suspend, or Restore. While creating a Metro configuration, the witness flag must be set to True.    Examples - name: Create and establish storagegroup SRDF/a pairing register: Job_details_body dellemc_powermax_srdf: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; universion: \u0026quot;{{universion}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; serial_no: \u0026quot;{{serial_no}}\u0026quot; sg_name: \u0026quot;{{sg_name}}\u0026quot; remote_serial_no: \u0026quot;{{remote_serial_no}}\u0026quot; srdf_mode: 'Asynchronous' srdf_state: 'Establish' state: 'present' - name: Create storagegroup SRDF/s pair in default suspended mode as an Synchronous task dellemc_powermax_srdf: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; universion: \u0026quot;{{universion}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; serial_no: \u0026quot;{{serial_no}}\u0026quot; sg_name: \u0026quot;{{sg_name2}}\u0026quot; remote_serial_no: \u0026quot;{{remote_serial_no}}\u0026quot; state: 'present' srdf_mode: 'Synchronous' wait_for_completion: True - name: Create storagegroup Metro SRDF pair with Witness for resiliency dellemc_powermax_srdf: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; universion: \u0026quot;{{universion}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; serial_no: \u0026quot;{{serial_no}}\u0026quot; sg_name: \u0026quot;{{sg_name}}\u0026quot; remote_serial_no: \u0026quot;{{remote_serial_no}}\u0026quot; state: 'present' srdf_mode: 'Active' wait_for_completion: True srdf_state: 'Establish' - name: Suspend storagegroup Metro SRDF pair dellemc_powermax_srdf: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; universion: \u0026quot;{{universion}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; serial_no: \u0026quot;{{serial_no}}\u0026quot; sg_name: \u0026quot;{{sg_name}}\u0026quot; remote_serial_no: \u0026quot;{{remote_serial_no}}\u0026quot; state: 'present' srdf_state: 'Suspend' - name: Establish link for storagegroup Metro SRDF pair and use Bias for resiliency dellemc_powermax_srdf: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; universion: \u0026quot;{{universion}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; serial_no: \u0026quot;{{serial_no}}\u0026quot; sg_name: \u0026quot;{{sg_name}}\u0026quot; remote_serial_no: \u0026quot;{{remote_serial_no}}\u0026quot; state: 'present' wait_for_completion: False srdf_state: 'Establish' witness: False - name: Get SRDF details dellemc_powermax_srdf: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; universion: \u0026quot;{{universion}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; serial_no: \u0026quot;{{serial_no}}\u0026quot; sg_name: \u0026quot;{{sg_name}}\u0026quot; state: 'present' - name: Modify SRDF mode dellemc_powermax_srdf: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; universion: \u0026quot;{{universion}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; serial_no: \u0026quot;{{serial_no}}\u0026quot; sg_name: \u0026quot;{{sg_name}}\u0026quot; srdf_mode: 'Synchronous' state: 'present' - name: Failover SRDF link dellemc_powermax_srdf: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; universion: \u0026quot;{{universion}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; serial_no: \u0026quot;{{serial_no}}\u0026quot; sg_name: \u0026quot;{{sg_name}}\u0026quot; srdf_state: 'Failover' state: 'present' - name: Get SRDF Job status dellemc_powermax_srdf: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; universion: \u0026quot;{{universion}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; serial_no: \u0026quot;{{serial_no}}\u0026quot; job_id: \u0026quot;{{Job_details_body.Job_details.jobId}}\u0026quot; state: 'present' - name: Establish SRDF link dellemc_powermax_srdf: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; universion: \u0026quot;{{universion}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; serial_no: \u0026quot;{{serial_no}}\u0026quot; sg_name: \u0026quot;{{sg_name2}}\u0026quot; srdf_state: 'Establish' state: 'present' - name: Suspend SRDF link dellemc_powermax_srdf: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; universion: \u0026quot;{{universion}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; serial_no: \u0026quot;{{serial_no}}\u0026quot; sg_name: \u0026quot;{{sg_name2}}\u0026quot; srdf_state: 'Suspend' state: 'present' - name: Delete SRDF link dellemc_powermax_srdf: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; universion: \u0026quot;{{universion}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; serial_no: \u0026quot;{{serial_no}}\u0026quot; sg_name: \u0026quot;{{sg_name}}\u0026quot; state: 'absent' Return Values The following are the fields unique to this module:\n Key Returned Description    changed  type=boolean   always  Whether or not the resource has changed.      Job_details  type=list elements=string   When job exist.  Details of the job.     \u0026nbsp;  completed_date_milliseconds  type=integer   success  Date of job completion in milliseconds.     \u0026nbsp;  jobId  type=string   success  Unique identifier of the job.     \u0026nbsp;  last_modified_date  type=string   success  Last modified date of job.     \u0026nbsp;  last_modified_date_milliseconds  type=integer   success  Last modified date of job in milliseconds.     \u0026nbsp;  name  type=string   success  Name of the job.     \u0026nbsp;  resourceLink  type=string   success  Resource link w.r.t Unisphere.     \u0026nbsp;  result  type=string   success  Job description     \u0026nbsp;  status  type=string   success  Status of the job.     \u0026nbsp;  task  type=list elements=string   success  Details about the job.     \u0026nbsp;  username  type=string   success  Unisphere username.      SRDF_link_details  complex   When SRDF link exists.  Details of the SRDF link.     \u0026nbsp;  hop2Modes  type=string   success  SRDF hop2 mode.     \u0026nbsp;  hop2Rdfgs  type=string   success  Hop2 RDF group number.     \u0026nbsp;  hop2States  type=string   success  SRDF hop2 state.     \u0026nbsp;  largerRdfSides  type=string   success  Larger volume side of the link.     \u0026nbsp;  localR1InvalidTracksHop1  type=integer   success  Number of invalid R1 tracks on local volume.     \u0026nbsp;  localR2InvalidTracksHop1  type=integer   success  Number of invalid R2 tracks on local volume.     \u0026nbsp;  modes  type=string   success  Mode of the SRDF pair.     \u0026nbsp;  rdfGroupNumber  type=integer   success  RDF group number of the pair.     \u0026nbsp;  remoteR1InvalidTracksHop1  type=integer   success  Number of invalid R1 tracks on remote volume.     \u0026nbsp;  remoteR2InvalidTracksHop1  type=integer   success  Number of invalid R2 tracks on remote volume.     \u0026nbsp;  remoteSymmetrix  type=string   success  Remote symmetrix ID.     \u0026nbsp;  states  type=string   success  State of the SRDF pair.     \u0026nbsp;  storageGroupName  type=string   success  Name of storage group that is SRDF protected.     \u0026nbsp;  symmetrixId  type=string   success  Primary symmetrix ID.     \u0026nbsp;  totalTracks  type=integer   success  Total number of tracks in the volume.     \u0026nbsp;  volumeRdfTypes  type=string   success  RDF type of volume.     Authors  Manisha Agrawal (@agrawm3) \u0026lt;ansible.team@dell.com\u0026gt; Rajshree Khare (@khareRajshree) \u0026lt;ansible.team@dell.com\u0026gt;  Storage Group Module Synopsis Managing storage groups on a PowerMax storage system includes listing the volumes of a storage group, creating a new storage group, deleting an existing storage group, adding existing volumes to an existing storage group, removing existing volumes from an existing storage group, creating new volumes in an existing storage group, modifying existing storage group attributes, adding child storage groups inside an existing storage group (parent), and removing a child storage group from an existing parent storage group.\nParameters  Parameter Choices/Defaults Comments    child_sg_state  type=string    Choices: present-in-group absent-in-group    Describes the state of CSG inside parent SG     child_storage_groups  type=list elements=string      This is a list of child storage groups     compression  type=boolean    Choices: no yes    compression on storage group. Compression parameter is ignored if service_level is not specified. Default is true.     new_sg_name  type=string      The new name of the storage group.     service_level  type=string      The Name of SLO.     sg_name  type=string required=true      The name of the storage group.     snapshot_policies  type=list elements=string      List of snapshot policy(s).     snapshot_policy_state  type=string    Choices: present-in-group absent-in-group    Describes the state of snapshot policy for an SG     srp  type=string      Name of the storage resource pool. This parameter is ignored if service_level is not specified. Default is to use whichever is the default SRP on the array.     state  type=string required=true    Choices: absent present    Define whether the storage group should exist or not.     vol_state  type=string    Choices: present-in-group absent-in-group    Describes the state of volumes inside the SG.     volumes  type=list elements=dictionary      This is a list of volumes. Each volume has four attributes- vol_name size cap_unit vol_id. Either the volume ID must be provided for existing volumes, or the name and size must be provided to add new volumes to SG. The unit is optional. vol_name - Represents the name of the volume size - Represents the volume size cap_unit - The unit in which size is represented. Default unit is GB. Choices are MB, GB, TB. vol_id - This is the volume ID    Examples - name: Get storage group details including volumes dellemc_powermax_storagegroup: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; universion: \u0026quot;{{universion}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; serial_no: \u0026quot;{{serial_no}}\u0026quot; sg_name: \u0026quot;ansible_sg\u0026quot; state: \u0026quot;present\u0026quot; - name: Create empty storage group dellemc_powermax_storagegroup: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; universion: \u0026quot;{{universion}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; serial_no: \u0026quot;{{serial_no}}\u0026quot; sg_name: \u0026quot;foo\u0026quot; service_level: \u0026quot;Diamond\u0026quot; srp: \u0026quot;SRP_1\u0026quot; compression: True state: \u0026quot;present\u0026quot; - name: Delete the storage Group dellemc_powermax_storagegroup: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; universion: \u0026quot;{{universion}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; serial_no: \u0026quot;{{serial_no}}\u0026quot; sg_name: \u0026quot;foo\u0026quot; state: \u0026quot;absent\u0026quot; - name: Adding existing volume(s) to existing SG dellemc_powermax_storagegroup: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; universion: \u0026quot;{{universion}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; serial_no: \u0026quot;{{serial_no}}\u0026quot; sg_name: \u0026quot;foo\u0026quot; state: \u0026quot;present\u0026quot; volumes: - vol_id: \u0026quot;00028\u0026quot; - vol_id: \u0026quot;00018\u0026quot; - vol_id: \u0026quot;00025\u0026quot; vol_state: \u0026quot;present-in-group\u0026quot; - name: Create new volumes for existing SG dellemc_powermax_storagegroup: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; universion: \u0026quot;{{universion}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; serial_no: \u0026quot;{{serial_no}}\u0026quot; sg_name: \u0026quot;foo\u0026quot; state: \u0026quot;present\u0026quot; volumes: - vol_name: \u0026quot;foo\u0026quot; size: 1 cap_unit: \u0026quot;GB\u0026quot; - vol_name: \u0026quot;bar\u0026quot; size: 1 cap_unit: \u0026quot;GB\u0026quot; vol_state: \u0026quot;present-in-group\u0026quot; - name: Remove volume(s) from existing SG dellemc_powermax_storagegroup: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; universion: \u0026quot;{{universion}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; serial_no: \u0026quot;{{serial_no}}\u0026quot; sg_name: \u0026quot;foo\u0026quot; state: \u0026quot;present\u0026quot; volumes: - vol_id: \u0026quot;00028\u0026quot; - vol_id: \u0026quot;00018\u0026quot; - vol_name: \u0026quot;ansible-vol\u0026quot; vol_state: \u0026quot;absent-in-group\u0026quot; - name: Adding child SG to parent SG dellemc_powermax_storagegroup: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; universion: \u0026quot;{{universion}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; serial_no: \u0026quot;{{serial_no}}\u0026quot; sg_name: \u0026quot;parent_sg\u0026quot; state: \u0026quot;present\u0026quot; child_storage_groups: - \u0026quot;pie\u0026quot; - \u0026quot;bar\u0026quot; child_sg_state: \u0026quot;present-in-group\u0026quot; - name: Removing child SG from parent SG dellemc_powermax_storagegroup: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; universion: \u0026quot;{{universion}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; serial_no: \u0026quot;{{serial_no}}\u0026quot; sg_name: \u0026quot;parent_sg\u0026quot; state: \u0026quot;present\u0026quot; child_storage_groups: - \u0026quot;pie\u0026quot; - \u0026quot;bar\u0026quot; child_sg_state: \u0026quot;absent-in-group\u0026quot; - name: Rename Storage Group dellemc_powermax_storagegroup: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; universion: \u0026quot;{{universion}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; serial_no: \u0026quot;{{serial_no}}\u0026quot; sg_name: \u0026quot;ansible_sg\u0026quot; new_sg_name: \u0026quot;ansible_sg_renamed\u0026quot; state: \u0026quot;present\u0026quot; - name: Create a storage group with snapshot policies dellemc_powermax_storagegroup: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; universion: \u0026quot;{{universion}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; serial_no: \u0026quot;{{serial_no}}\u0026quot; sg_name: \u0026quot;ansible_test_sg\u0026quot; service_level: \u0026quot;Diamond\u0026quot; srp: \u0026quot;SRP_1\u0026quot; compression: True snapshot_policies: - \u0026quot;10min_policy\u0026quot; - \u0026quot;30min_policy\u0026quot; snapshot_policy_state: \u0026quot;present-in-group\u0026quot; state: \u0026quot;present\u0026quot; - name: Add snapshot policy to a storage group dellemc_powermax_storagegroup: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; universion: \u0026quot;{{universion}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; serial_no: \u0026quot;{{serial_no}}\u0026quot; sg_name: \u0026quot;ansible_test_sg\u0026quot; snapshot_policies: - \u0026quot;15min_policy\u0026quot; snapshot_policy_state: \u0026quot;present-in-group\u0026quot; state: \u0026quot;present\u0026quot; - name: Remove snapshot policy from a storage group dellemc_powermax_storagegroup: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; universion: \u0026quot;{{universion}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; serial_no: \u0026quot;{{serial_no}}\u0026quot; sg_name: \u0026quot;ansible_test_sg\u0026quot; snapshot_policies: - \u0026quot;15min_policy\u0026quot; snapshot_policy_state: \u0026quot;absent-in-group\u0026quot; state: \u0026quot;present\u0026quot; Return Values The following are the fields unique to this module:\n Key Returned Description    add_child_sg  type=boolean   When value exists.  Sets to true when a child SG is added.      add_new_vols_to_sg  type=boolean   When value exists.  Sets to true when new volumes are added to the SG.      add_snapshot_policy_to_sg  type=boolean   When value exists.  Sets to true when snapshot policy(s) is added to SG.      add_vols_to_sg  type=boolean   When value exists.  Sets to true when existing volumes are added to the SG.      added_vols_details  type=list elements=string   When value exists.  Volume IDs of the volumes added.      changed  type=boolean   always  Whether or not the resource has changed.      create_sg  type=boolean   When value exists.  Sets to true when a new SG is created.      delete_sg  type=boolean   When value exists.  Sets to true when an SG is deleted.      modify_sg  type=boolean   When value exists.  Sets to true when an SG is modified.      remove_child_sg  type=boolean   When value exists.  Sets to true when a child SG is removed.      remove_snapshot_policy_to_sg  type=boolean   When value exists.  Sets to false when snapshot policy(s) is removed from SG.      remove_vols_from_sg  type=boolean   When value exists.  Sets to true when volumes are removed.      removed_vols_details  type=list elements=string   When value exists.  Volume IDs of the volumes removed.      rename_sg  type=boolean   When value exists.  Sets to true when an SG is renamed.      snapshot_policy_compliance_details  complex   When snapshot policy associated..  The compliance status of this storage group.     \u0026nbsp;  compliance  type=string   success  Compliance status     \u0026nbsp;  sl_compliance  complex   success  Compliance details     \u0026nbsp; \u0026nbsp;  compliance  type=string   success  Compliance status     \u0026nbsp; \u0026nbsp;  sl_name  type=string   success  Name of the snapshot policy     \u0026nbsp;  sl_count  type=integer   success  Number of snapshot policies associated with storage group     \u0026nbsp;  storage_group_name  type=string   success  Name of the storage group      storage_group_details  complex   When storage group exists.  Details of the storage group.     \u0026nbsp;  base_slo_name  type=string   success  Base Service Level Objective (SLO) of a storage group.     \u0026nbsp;  cap_gb  type=integer   success  Storage group capacity in GB.     \u0026nbsp;  compression  type=boolean   success  Compression flag.     \u0026nbsp;  device_emulation  type=string   success  Device emulation type.     \u0026nbsp;  num_of_child_sgs  type=integer   success  Number of child storage groups.     \u0026nbsp;  num_of_masking_views  type=integer   success  Number of masking views associated with the storage group.     \u0026nbsp;  num_of_parent_sgs  type=integer   success  Number of parent storage groups.     \u0026nbsp;  num_of_snapshots  type=integer   success  Number of snapshots for the storage group.     \u0026nbsp;  num_of_vols  type=integer   success  Number of volumes in the storage group.     \u0026nbsp;  service_level  type=string   success  Type of service level.     \u0026nbsp;  slo  type=string   success  Service level objective (SLO) type.     \u0026nbsp;  slo_compliance  type=string   success  Type of SLO compliance.     \u0026nbsp;  srp  type=string   success  Storage resource pool.     \u0026nbsp;  storageGroupId  type=string   success  Id for the storage group.     \u0026nbsp;  type  type=string   success  type of storage group.     \u0026nbsp;  unprotected  type=boolean   success  Flag for storage group protection.     \u0026nbsp;  vp_saved_percent  type=integer   success  Percentage saved for virtual pools.      storage_group_volumes  type=list elements=string   When value exists.  Volume IDs of storage group volumes.      storage_group_volumes_details  complex   When storage group volumes exists.  Details of the storage group volumes.     \u0026nbsp;  effective_wwn  type=string   success  Effective WWN of the volume.     \u0026nbsp;  type  type=string   success  Type of the volume.     \u0026nbsp;  volume_identifier  type=string   success  Name associated with the volume.     \u0026nbsp;  volumeId  type=string   success  Unique ID of the volume.     \u0026nbsp;  wwn  type=string   success  WWN of the volume.     Authors  Vasudevu Lakhinana (@unknown) \u0026lt;ansible.team@dell.com\u0026gt; Prashant Rakheja (@prashant-dell) \u0026lt;ansible.team@dell.com\u0026gt; Ambuj Dubey (@AmbujDube) \u0026lt;ansible.team@dell.com\u0026gt;  Storage Pool Module Synopsis Managing storage pools on PowerMax storage system includes getting details of storage pools.\nParameters  Parameter Choices/Defaults Comments    pool  type=string required=true      The name of the storage pool.     state  type=string required=true    Choices: absent present    State variable to determine whether storage pool will exist or not.    Examples - name: Get specific storage pool details dellemc_powermax_storagepool: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; universion: \u0026quot;{{universion}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; serial_no: \u0026quot;{{serial_no}}\u0026quot; pool: \u0026quot;SRP_1\u0026quot; state: \u0026quot;present\u0026quot; Return Values The following are the fields unique to this module:\n Key Returned Description    changed  type=boolean   always  Whether or not the resource has changed.      pool_details  complex   When storage pool exist.  Details of the storage pool.     \u0026nbsp;  serial_no  type=string   success  The PowerMax array on which storage pool resides     \u0026nbsp;  service_levels  type=list elements=string   success  The service levels supported by storage pool     \u0026nbsp;  srp_capacity  complex   success  SRP capacity details     \u0026nbsp; \u0026nbsp;  effective_used_capacity_percent  type=integer   success  The effective used capacity, expressed as a percentage     \u0026nbsp; \u0026nbsp;  usable_total_tb  type=float   success  Usable capacity of the storage pool in TB     \u0026nbsp; \u0026nbsp;  usable_used_tb  type=float   success  Used capacity of the storage pool in TB     \u0026nbsp;  srp_efficiency  complex   success  SRP efficiency details     \u0026nbsp; \u0026nbsp;  compression_state  type=string   success  Indicates whether compression is enabled or disabled for this storage resource pool.     \u0026nbsp;  srpId  type=string   success  The ID of the storage pool     \u0026nbsp;  total_free_tb  type=string   success  Free capacity of the storage pool in TB     Authors  Akash Shendge (@shenda1) \u0026lt;ansible.team@dell.com\u0026gt;  Volume Module Synopsis Managing volumes on PowerMax storage system includes creating a volume, renaming a volume, expanding a volume, and deleting a volume.\nParameters  Parameter Choices/Defaults Comments    cap_unit  type=string    Choices: MB GB TB    volume capacity units If not specified, default value is GB.     new_name  type=string      The new volume identifier for the volume.     new_sg_name  type=string      The name of the target storage group.     sg_name  type=string      The name of the storage group.     size  type=float      The new size of existing volume. Required for create and expand volume operations.     state  type=string required=true    Choices: absent present    Defines whether the volume should exist or not.     vol_id  type=string      The native id of the volume. Required for rename and delete volume operations.     vol_name  type=string      The name of the volume.     vol_wwn  type=string      The WWN of the volume.    Notes  To expand a volume, either provide vol_id or vol_name or vol_wwn and sg_name. size is required to create/expand a volume. vol_id is required to rename/delete a volume. vol_name, sg_name and new_sg_name is required to move volumes between storage groups. Deletion of volume will fail if the storage group is part of a masking view.  Examples - name: Create volume dellemc_powermax_volume: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; universion: \u0026quot;{{universion}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; serial_no: \u0026quot;{{serial_no}}\u0026quot; vol_name: \u0026quot;{{vol_name}}\u0026quot; sg_name: \u0026quot;{{sg_name}}\u0026quot; size: 1 cap_unit: \u0026quot;{{cap_unit}}\u0026quot; state: 'present' - name: Expanding volume size dellemc_powermax_volume: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; universion: \u0026quot;{{universion}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; serial_no: \u0026quot;{{serial_no}}\u0026quot; size: 3 cap_unit: \u0026quot;{{cap_unit}}\u0026quot; vol_id: \u0026quot;0059B\u0026quot; state: 'present' - name: Renaming volume dellemc_powermax_volume: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; universion: \u0026quot;{{universion}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; serial_no: \u0026quot;{{serial_no}}\u0026quot; new_name: \u0026quot;Test_GOLD_vol_Renamed\u0026quot; vol_id: \u0026quot;0059B\u0026quot; state: 'present' - name: Delete volume using volume ID dellemc_powermax_volume: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; universion: \u0026quot;{{universion}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; serial_no: \u0026quot;{{serial_no}}\u0026quot; vol_id: \u0026quot;0059B\u0026quot; state: 'absent' - name: Delete volume using volume WWN dellemc_powermax_volume: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; universion: \u0026quot;{{universion}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; serial_no: \u0026quot;{{serial_no}}\u0026quot; vol_wwn: \u0026quot;60000970000197900237533030303246\u0026quot; state: 'absent' - name: Move volume between storage group dellemc_powermax_volume: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; universion: \u0026quot;{{universion}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; serial_no: \u0026quot;{{serial_no}}\u0026quot; vol_name: \u0026quot;{{vol_name}}\u0026quot; sg_name: \u0026quot;{{sg_name}}\u0026quot; new_sg_name: \u0026quot;{{new_sg_name}}\u0026quot; state: 'present' Return Values The following are the fields unique to this module:\n Key Returned Description    changed  type=boolean   always  Whether or not the resource has changed.      volume_details  complex   When volume exists.  Details of the volume.     \u0026nbsp;  allocated_percent  type=integer   success  Allocated percentage the volume.     \u0026nbsp;  cap_cyl  type=integer   success  Number of cylinders.     \u0026nbsp;  cap_gb  type=integer   success  Volume capacity in GB.     \u0026nbsp;  cap_mb  type=integer   success  Volume capacity in MB.     \u0026nbsp;  effective_wwn  type=string   success  Effective WWN of the volume.     \u0026nbsp;  emulation  type=string   success  Volume emulation type.     \u0026nbsp;  encapsulated  type=boolean   success  Flag for encapsulation.     \u0026nbsp;  has_effective_wwn  type=string   success  Flag for effective WWN presence.     \u0026nbsp;  mobility_id_enabled  type=boolean   success  Flag for enabling mobility.     \u0026nbsp;  num_of_front_end_paths  type=integer   success  Number of front end paths in the volume.     \u0026nbsp;  num_of_storage_groups  type=integer   success  Number of storage groups in which volume is present.     \u0026nbsp;  pinned  type=boolean   success  Pinned flag.     \u0026nbsp;  rdfGroupId  type=integer   success  RDFG number for volume.     \u0026nbsp;  reserved  type=boolean   success  Reserved flag.     \u0026nbsp;  snapvx_source  type=boolean   success  Source SnapVX flag.     \u0026nbsp;  snapvx_target  type=boolean   success  Target SnapVX flag.     \u0026nbsp;  ssid  type=string   success  SSID of the volume.     \u0026nbsp;  status  type=string   success  Volume status.     \u0026nbsp;  storage_groups  type=list elements=string   success  List of storage groups for the volume.     \u0026nbsp;  storageGroupId  type=string   success  Storage group ID of the volume.     \u0026nbsp;  type  type=string   success  Type of the volume.     \u0026nbsp;  volume_identifier  type=string   success  Name identifier for the volume.     \u0026nbsp;  volumeId  type=string   success  Unique ID of the volume.     \u0026nbsp;  wwn  type=string   success  WWN of the volume.     Authors  Vasudevu Lakhinana (@unknown) \u0026lt;ansible.team@dell.com\u0026gt; Akash Shendge (@shenda1) \u0026lt;ansible.team@dell.com\u0026gt; Ambuj Dubey (@AmbujDube) \u0026lt;ansible.team@dell.com\u0026gt;  Process Storage Pool Dict Module Synopsis Process storage pools on PowerMax/VMAX storage system to find out the storage pool with maximum free storage\nParameters  Parameter Choices/Defaults Comments    pool_data  type=list elements=dictionary required=true      Storage pool details including service levels, usable total space, usable free space, total free space.     service_level  type=string      Service level of the storage group     sg_name  type=string      Name of the storage group     size  type=float required=true      Size of the storage group in GB    Examples - name: Get best suitable Pool using our python sorting module register: assigned_pool process_storage_pool_dict: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; pool_data: \u0026quot;{{ pools_list }}\u0026quot; size: 40 service_level: \u0026quot;Diamond\u0026quot; sg_name: \u0026quot;intellgent_provisioning\u0026quot; Return Values The following are the fields unique to this module:\n Key Returned Description    all_pools  type=list elements=string   when pool exists  List of all pools on unisphere     \u0026nbsp;  serial_no  type=string   when array satisfies the given criteria  The PowerMax array on which storage pool resides     \u0026nbsp;  storage_pool  type=string   when storage pool exists satisfying the given criteria  The ID of the storage pool      changed  type=boolean   always  Whether or not the resource has changed.      serial_no  type=string   when array satisfies the given criteria  The PowerMax array on which storage pool resides      storage_group  type=string   when storage group exists satisfying the given criteria  Name of the storage group      storage_pool  type=string   when storage pool exists satisfying the given criteria  The ID of the storage pool     Authors  Akash Shendge (@shenda1) \u0026lt;ansible.team@dell.com\u0026gt;  ","excerpt":"Ansible Modules for Dell EMC PowerMax Product Guide 1.5.0  © 2021 Dell Inc. or its subsidiaries. All …","ref":"/ansible-docs/docs/storage/platforms/powermax/product-guide/","title":"Product Guide"},{"body":"Ansible Modules for Dell EMC PowerScale Product Guide 1.2.0 © 2021 Dell Inc. or its subsidiaries. All rights reserved. Dell, EMC, and other trademarks are trademarks of Dell Inc. or its subsidiaries. Other trademarks may be trademarks of their respective owners.\n Contents  LDAP Module  Synopsis Parameters Notes Examples Return Values Authors   Smart Quota Module  Synopsis Parameters Notes Examples Return Values Authors   Active Directory Module  Synopsis Parameters Examples Return Values Authors   Snapshot Schedule Module  Synopsis Parameters Examples Return Values Authors   Users Module  Synopsis Parameters Examples Return Values Authors   SMB Module  Synopsis Parameters Examples Return Values Authors   Snapshot Module  Synopsis Parameters Examples Return Values Authors   Access Zone Module  Synopsis Parameters Notes Examples Return Values Authors   NFS Module  Synopsis Parameters Examples Return Values Authors   Groups Module  Synopsis Parameters Examples Return Values Authors   File System Module  Synopsis Parameters Examples Return Values Authors     LDAP Module Manage LDAP authentication provider on PowerScale\nSynopsis Managing LDAP authentication provider on PowerScale storage system includes creating, modifying, deleting and retrieving details of LDAP provider.\nParameters   Parameter Type Required Default Choices Description   ldap_name  str   True     Specifies the name of the LDAP provider.    server_uris  list elements: str      Specifies the server URIs. This parameter is mandatory during create. Server_uris should begin with ldap:// or ldaps:// if not validation error will be displayed.    server_uri_state  str      present-in-ldap absent-in-ldap   Specifies if the server_uris need to be added or removed from the provider. This parameter is mandatory if server_uris is specified. While creating LDAP provider, this parameter value should be specified as 'present-in-ldap'.    base_dn  str      Specifies the root of the tree in which to search identities. This parameter is mandatory during create.    ldap_parameters  dict      Specify additional parameters to configure LDAP domain.    \u0026nbsp; groupnet   str      Groupnet identifier. This is an optional parameter and defaults to groupnet0.    \u0026nbsp; bind_dn   str      Specifies the distinguished name for binding to the LDAP server.    \u0026nbsp; bind_password   str      Specifies the password for the distinguished name for binding to the LDAP server.    state  str   True     absent present   The state of the LDAP provider after the task is performed. present - indicates that the LDAP provider should exist on the system. absent - indicates that the LDAP provider should not exist on the system.    onefs_host  str   True     IP address or FQDN of the PowerScale cluster.    port_no  str    8080    Port number of the PowerScale cluster.It defaults to 8080 if not specified.    verify_ssl  bool   True     True False   boolean variable to specify whether to validate SSL certificate or not. True - indicates that the SSL certificate should be verified. False - indicates that the SSL certificate should not be verified.    api_user  str   True     username of the PowerScale cluster.    api_password  str   True     the password of the PowerScale cluster.    Notes  This module does not support modification of bind_password of LDAP provider. The value specified for bind_password will be ignored during modify.  Examples - name: Add an LDAP provider dellemc_powerscale_ldap: onefs_host: \u0026quot;{{onefs_host}}\u0026quot; api_user: \u0026quot;{{api_user}}\u0026quot; api_password: \u0026quot;{{api_password}}\u0026quot; verify_ssl: \u0026quot;{{verify_ssl}}\u0026quot; ldap_name: \u0026quot;ldap_test\u0026quot; server_uris: - \u0026quot;{{server_uri_1}}\u0026quot; - \u0026quot;{{server_uri_2}}\u0026quot; server_uri_state: 'present-in-ldap' base_dn: \u0026quot;DC=ansildap,DC=com\u0026quot; ldap_parameters: groupnet: \u0026quot;groupnet_ansildap\u0026quot; bind_dn: \u0026quot;cn=admin,dc=example,dc=com\u0026quot; bind_password: \u0026quot;{{bind_password}}\u0026quot; state: \u0026quot;present\u0026quot; - name: Add server_uris to an LDAP provider dellemc_powerscale_ldap: onefs_host: \u0026quot;{{onefs_host}}\u0026quot; api_user: \u0026quot;{{api_user}}\u0026quot; api_password: \u0026quot;{{api_password}}\u0026quot; verify_ssl: \u0026quot;{{verify_ssl}}\u0026quot; ldap_name: \u0026quot;ldap_test\u0026quot; server_uris: - \u0026quot;{{server_uri_1}}\u0026quot; server_uri_state: \u0026quot;present-in-ldap\u0026quot; state: \u0026quot;present\u0026quot; - name: Remove server_uris from an LDAP provider dellemc_powerscale_ldap: onefs_host: \u0026quot;{{onefs_host}}\u0026quot; api_user: \u0026quot;{{api_user}}\u0026quot; api_password: \u0026quot;{{api_password}}\u0026quot; verify_ssl: \u0026quot;{{verify_ssl}}\u0026quot; ldap_name: \u0026quot;ldap_test\u0026quot; server_uris: - \u0026quot;{{server_uri_1}}\u0026quot; server_uri_state: \u0026quot;absent-in-ldap\u0026quot; state: \u0026quot;present\u0026quot; - name: Modify LDAP provider dellemc_powerscale_ldap: onefs_host: \u0026quot;{{onefs_host}}\u0026quot; api_user: \u0026quot;{{api_user}}\u0026quot; api_password: \u0026quot;{{api_password}}\u0026quot; verify_ssl: \u0026quot;{{verify_ssl}}\u0026quot; ldap_name: \u0026quot;ldap_test\u0026quot; base_dn: \u0026quot;DC=ansi_ldap,DC=com\u0026quot; ldap_parameters: bind_dn: \u0026quot;cn=admin,dc=test,dc=com\u0026quot; state: \u0026quot;present\u0026quot; - name: Get LDAP provider details dellemc_powerscale_ldap: onefs_host: \u0026quot;{{onefs_host}}\u0026quot; api_user: \u0026quot;{{api_user}}\u0026quot; api_password: \u0026quot;{{api_password}}\u0026quot; verify_ssl: \u0026quot;{{verify_ssl}}\u0026quot; ldap_name: \u0026quot;ldap_test\u0026quot; state: \u0026quot;present\u0026quot; - name: Delete a LDAP provider dellemc_powerscale_ldap: onefs_host: \u0026quot;{{onefs_host}}\u0026quot; api_user: \u0026quot;{{api_user}}\u0026quot; api_password: \u0026quot;{{api_password}}\u0026quot; verify_ssl: \u0026quot;{{verify_ssl}}\u0026quot; ldap_name: \u0026quot;ldap_test\u0026quot; state: \u0026quot;absent\u0026quot; Return Values   Key Type Returned Description   changed   bool   always   Whether or not the resource has changed    ldap_provider_details   complex   When LDAP provider exists   The LDAP provider details    \u0026nbsp; base_dn   str  success  Specifies the root of the tree in which to search identities.    \u0026nbsp; bind_dn   str  success  Specifies the distinguished name for binding to the LDAP server.    \u0026nbsp; groupnet   str  success  Groupnet identifier.    \u0026nbsp; linked_access_zones   list  success  List of access zones linked to the authentication provider.    \u0026nbsp; name   str  success  Specifies the name of the LDAP provider.    \u0026nbsp; server_uris   str  success  Specifies the server URIs.    \u0026nbsp; status   str  success  Specifies the status of the provider.    Authors  Jennifer John (@johnj9) ansible.team@dell.com   Smart Quota Module Manage Smart Quotas on PowerScale\nSynopsis Manages Smart Quotas on a PowerScale storage system. This includes getting details, modifying, creating and deleting Smart Quotas.\nParameters   Parameter Type Required Default Choices Description   path  str   True     The path on which the quota will be imposed. For system access zone, the path is absolute. For all other access zones, the path is a relative path from the base of the access zone.    quota_type  str   True     user group directory default-user default-group   The type of quota which will be imposed on the path.    user_name  str      The name of the user account for which quota operations will be performed.    group_name  str      The name of the group for which quota operations will be performed.    access_zone  str    system    This option mentions the zone in which the user/group exists. For a non-system access zone, the path relative to the non-system Access Zone's base directory has to be given. For a system access zone, the absolute path has to be given.    provider_type  str    local    local file ldap ads   This option defines the type which is used to authenticate the user/group. If the provider_type is 'ads' then the domain name of the Active Directory Server has to be mentioned in the user_name. The format for the user_name should be 'DOMAIN_NAME\\user_name' or \"DOMAIN_NAME\\\\user_name\". This option acts as a filter for all operations except creation.    quota  dict   True     Specifies Smart Quota parameters.    \u0026nbsp; include_snapshots   bool    False    Whether to include the snapshots in the quota or not.    \u0026nbsp; include_overheads   bool      Whether to include the data protection overheads in the quota or not. If not passed during quota creation then quota will be created excluding the overheads. This parameter is supported for SDK 8.1.1    \u0026nbsp; thresholds_on   str      app_logical_size fs_logical_size physical_size   For SDK 9.0.0 the parameter include_overheads is deprecated and thresholds_on is used.    \u0026nbsp; advisory_limit_size   int      The threshold value after which the advisory notification will be sent.    \u0026nbsp; soft_limit_size   int      Threshold value after which the soft limit exceeded notification will be sent and the soft_grace period will start. Write access will be restricted after the grace period expires. Both soft_grace_period and soft_limit_size are required to modify soft threshold for the quota.    \u0026nbsp; soft_grace_period   int      Grace Period after the soft limit for quota is exceeded. After the grace period, the write access to the quota will be restricted. Both soft_grace_period and soft_limit_size are required to modify soft threshold for the quota.    \u0026nbsp; period_unit   str      days weeks months   Unit of the time period for soft_grace_period. For months the number of days is assumed to be 30 days. This parameter is required only if the soft_grace_period, is specified.    \u0026nbsp; hard_limit_size   int      Threshold value after which a hard limit exceeded notification will be sent. Write access will be restricted after the hard limit is exceeded.    \u0026nbsp; cap_unit   str      GB TB   Unit of storage for the hard, soft and advisory limits. This parameter is required if any of the hard, soft or advisory limits is specified.    state  str   True     absent present   Define whether the Smart Quota should exist or not. present - indicates that the Smart Quota should exist on the system. absent - indicates that the Smart Quota should not exist on the system.    onefs_host  str   True     IP address or FQDN of the PowerScale cluster.    port_no  str    8080    Port number of the PowerScale cluster.It defaults to 8080 if not specified.    verify_ssl  bool   True     True False   boolean variable to specify whether to validate SSL certificate or not. True - indicates that the SSL certificate should be verified. False - indicates that the SSL certificate should not be verified.    api_user  str   True     username of the PowerScale cluster.    api_password  str   True     the password of the PowerScale cluster.    Notes  To perform any operation, path, quota_type and state are mandatory parameters. There can be two quotas for each type per directory, one with snapshots included and one without snapshots included. Once the limits are assigned, then the quota can\u0026rsquo;t be converted to accounting. Only modification to the threshold limits is permitted.  Examples  - name: Create a Quota for a User excluding snapshot. dellemc_powerscale_smartquota: onefs_host: \u0026quot;{{onefs_host}}\u0026quot; verify_ssl: \u0026quot;{{verify_ssl}}\u0026quot; api_user: \u0026quot;{{api_user}}\u0026quot; api_password: \u0026quot;{{api_password}}\u0026quot; path: \u0026quot;/sample_az/sample_fs\u0026quot; quota_type: \u0026quot;user\u0026quot; user_name: \u0026quot;{{user_name}}\u0026quot; access_zone: \u0026quot;sample-zone\u0026quot; provider_type: \u0026quot;local\u0026quot; quota: include_overheads: False advisory_limit_size: \u0026quot;{{advisory_limit_size}}\u0026quot; soft_limit_size: \u0026quot;{{soft_limit_size}}\u0026quot; soft_grace_period: \u0026quot;{{soft_grace_period}}\u0026quot; period_unit: \u0026quot;{{period_unit}}\u0026quot; hard_limit_size: \u0026quot;{{hard_limit_size}}\u0026quot; cap_unit: \u0026quot;{{cap_unit}}\u0026quot; state: \u0026quot;present\u0026quot; - name: Create a Quota for a Directory for accounting includes snapshots and data protection overheads. dellemc_powerscale_smartquota: onefs_host: \u0026quot;{{onefs_host}}\u0026quot; verify_ssl: \u0026quot;{{verify_ssl}}\u0026quot; api_user: \u0026quot;{{api_user}}\u0026quot; api_password: \u0026quot;{{api_password}}\u0026quot; path: \u0026quot;/sample_az/sample_fs\u0026quot; quota_type: \u0026quot;directory\u0026quot; quota: include_snapshots: \u0026quot;True\u0026quot; include_overheads: True state: \u0026quot;present\u0026quot; - name: Create default-user Quota for a Directory with snaps and overheads dellemc_powerscale_smartquota: onefs_host: \u0026quot;{{onefs_host}}\u0026quot; verify_ssl: \u0026quot;{{verify_ssl}}\u0026quot; api_user: \u0026quot;{{api_user}}\u0026quot; api_password: \u0026quot;{{api_password}}\u0026quot; path: \u0026quot;/sample_az/sample_fs\u0026quot; quota_type: \u0026quot;default-user\u0026quot; quota: include_snapshots: \u0026quot;True\u0026quot; include_overheads: True state: \u0026quot;present\u0026quot; - name: Get a Quota Details for a Group dellemc_powerscale_smartquota: onefs_host: \u0026quot;{{onefs_host}}\u0026quot; verify_ssl: \u0026quot;{{verify_ssl}}\u0026quot; api_user: \u0026quot;{{api_user}}\u0026quot; api_password: \u0026quot;{{api_password}}\u0026quot; path: \u0026quot;/sample_az/sample_fs\u0026quot; quota_type: \u0026quot;group\u0026quot; group_name: \u0026quot;{{user_name}}\u0026quot; access_zone: \u0026quot;sample-zone\u0026quot; provider_type: \u0026quot;local\u0026quot; quota: include_snapshots: \u0026quot;True\u0026quot; state: \u0026quot;present\u0026quot; - name: Update Quota for a User dellemc_powerscale_smartquota: onefs_host: \u0026quot;{{onefs_host}}\u0026quot; verify_ssl: \u0026quot;{{verify_ssl}}\u0026quot; api_user: \u0026quot;{{api_user}}\u0026quot; api_password: \u0026quot;{{api_password}}\u0026quot; path: \u0026quot;/sample_az/sample_fs\u0026quot; quota_type: \u0026quot;user\u0026quot; user_name: \u0026quot;{{user_name}}\u0026quot; access_zone: \u0026quot;sample-zone\u0026quot; provider_type: \u0026quot;local\u0026quot; quota: include_snapshots: \u0026quot;True\u0026quot; include_overheads: True advisory_limit_size: \u0026quot;{{new_advisory_limit_size}}\u0026quot; hard_limit_size: \u0026quot;{{new_hard_limit_size}}\u0026quot; cap_unit: \u0026quot;{{cap_unit}}\u0026quot; state: \u0026quot;present\u0026quot; - name: Modify Soft Limit and Grace period of default-user Quota dellemc_powerscale_smartquota: onefs_host: \u0026quot;{{onefs_host}}\u0026quot; verify_ssl: \u0026quot;{{verify_ssl}}\u0026quot; api_user: \u0026quot;{{api_user}}\u0026quot; api_password: \u0026quot;{{api_password}}\u0026quot; path: \u0026quot;/sample_az/sample_fs\u0026quot; quota_type: \u0026quot;default-user\u0026quot; access_zone: \u0026quot;sample-zone\u0026quot; quota: include_snapshots: \u0026quot;True\u0026quot; include_overheads: True soft_limit_size: \u0026quot;{{soft_limit_size}}\u0026quot; cap_unit: \u0026quot;{{cap_unit}}\u0026quot; soft_grace_period: \u0026quot;{{soft_grace_period}}\u0026quot; period_unit: \u0026quot;{{period_unit}}\u0026quot; state: \u0026quot;present\u0026quot; - name: Delete a Quota for a Directory dellemc_powerscale_smartquota: onefs_host: \u0026quot;{{onefs_host}}\u0026quot; verify_ssl: \u0026quot;{{verify_ssl}}\u0026quot; api_user: \u0026quot;{{api_user}}\u0026quot; api_password: \u0026quot;{{api_password}}\u0026quot; path: \u0026quot;/sample_az/sample_fs\u0026quot; quota_type: \u0026quot;directory\u0026quot; quota: include_snapshots: \u0026quot;True\u0026quot; state: \u0026quot;absent\u0026quot; - name: Delete Quota for a default-group dellemc_powerscale_smartquota: onefs_host: \u0026quot;{{onefs_host}}\u0026quot; verify_ssl: \u0026quot;{{verify_ssl}}\u0026quot; api_user: \u0026quot;{{api_user}}\u0026quot; api_password: \u0026quot;{{api_password}}\u0026quot; path: \u0026quot;/sample_az/sample_fs\u0026quot; quota_type: \u0026quot;default-group\u0026quot; quota: include_snapshots: \u0026quot;True\u0026quot; state: \u0026quot;absent\u0026quot; Return Values   Key Type Returned Description   changed   bool   always   Whether or not the resource has changed    quota_details   complex   When Quota exists.   The quota details.    \u0026nbsp; enforced   bool  success  Whether the limits are enforced on Quota or not.    \u0026nbsp; id   str  success  The ID of the Quota.    \u0026nbsp; thresholds   dict  success  Includes information about all the limits imposed on quota. The limits are mentioned in bytes and soft_grace is in seconds.    \u0026nbsp; type   str  success  The type of Quota.    \u0026nbsp; usage   dict  success  The Quota usage.    Authors  P Srinivas Rao (@srinivas-rao5) ansible.team@dell.com   Active Directory Module Manages the ADS authentication provider on PowerScale\nSynopsis Manages the Active Directory authentication provider on the PowerScale storage system. This includes creating, modifying, deleting and retreiving the details of an ADS provider.\nParameters   Parameter Type Required Default Choices Description   domain_name  str      Specifies the domain name of an Active Directory provider. This parameter is mandatory during create.    instance_name  str      Specifies the instance name of Active Directory provider. This is an optional parameter during create, and defaults to the provider name if it is not specified during the create operation. get, modify and delete operations can also be performed through instance_name. It is mutually exclusive with domain_name for get, modify and delete operations.    ads_user  str      Specifies the user name that has permission to join a machine to the given domain. This parameter is mandatory during create.    ads_password  str      Specifies the password used during domain join. This parameter is mandatory during create.    ads_parameters  dict      Specify additional parameters to configure ADS domain.    \u0026nbsp; groupnet   str      Groupnet identifier. This is an optional parameter and defaults to groupnet0.    \u0026nbsp; home_directory_template   str      Specifies the path to the home directory template. This is an optional parameter and defaults to '/ifs/home/%D/%U'.    \u0026nbsp; login_shell   str      /bin/sh /bin/csh /bin/tcsh /bin/zsh /bin/bash /bin/rbash /sbin/nologin   Specifies the login shell path. This is an optional parameter and defaults to '/bin/zsh'.    state  str   True     absent present   The state of the ads provider after the task is performed. present - indicates that the ADS provider should exist on the system. absent - indicates that the ADS provider should not exist on the system.    onefs_host  str   True     IP address or FQDN of the PowerScale cluster.    port_no  str    8080    Port number of the PowerScale cluster.It defaults to 8080 if not specified.    verify_ssl  bool   True     True False   boolean variable to specify whether to validate SSL certificate or not. True - indicates that the SSL certificate should be verified. False - indicates that the SSL certificate should not be verified.    api_user  str   True     username of the PowerScale cluster.    api_password  str   True     the password of the PowerScale cluster.    Examples - name: Add an Active Directory provider dellemc_powerscale_ads: onefs_host: \u0026quot;{{onefs_host}}\u0026quot; api_user: \u0026quot;{{api_user}}\u0026quot; api_password: \u0026quot;{{api_password}}\u0026quot; verify_ssl: \u0026quot;{{verify_ssl}}\u0026quot; domain_name: \u0026quot;ansibleneo.com\u0026quot; instance_name: \u0026quot;ansibleneo.com\u0026quot; ads_user: \u0026quot;administrator\u0026quot; ads_password: \u0026quot;Password123!\u0026quot; ads_parameters: groupnet: \u0026quot;groupnet5\u0026quot; home_directory_template: \u0026quot;/ifs/home/%D/%U\u0026quot; login_shell: \u0026quot;/bin/zsh\u0026quot; state: \u0026quot;present\u0026quot; - name: Modify an Active Directory provider with domain name dellemc_powerscale_ads: onefs_host: \u0026quot;{{onefs_host}}\u0026quot; verify_ssl: \u0026quot;{{verify_ssl}}\u0026quot; api_user: \u0026quot;{{api_user}}\u0026quot; api_password: \u0026quot;{{api_password}}\u0026quot; domain_name: \u0026quot;ansibleneo.com\u0026quot; ads_parameters: home_directory_template: \u0026quot;/ifs/usr_home/%D/%U\u0026quot; login_shell: \u0026quot;/bin/rbash\u0026quot; state: \u0026quot;present\u0026quot; - name: Modify an Active Directory provider with instance name dellemc_powerscale_ads: onefs_host: \u0026quot;{{onefs_host}}\u0026quot; verify_ssl: \u0026quot;{{verify_ssl}}\u0026quot; api_user: \u0026quot;{{api_user}}\u0026quot; api_password: \u0026quot;{{api_password}}\u0026quot; instance_name: \u0026quot;ansibleneo.com\u0026quot; ads_parameters: home_directory_template: \u0026quot;/ifs/usr_home/%D/%U\u0026quot; login_shell: \u0026quot;/bin/rbash\u0026quot; state: \u0026quot;present\u0026quot; - name: Get Active Directory provider details with domain name dellemc_powerscale_ads: onefs_host: \u0026quot;{{onefs_host}}\u0026quot; api_user: \u0026quot;{{api_user}}\u0026quot; api_password: \u0026quot;{{api_password}}\u0026quot; verify_ssl: \u0026quot;{{verify_ssl}}\u0026quot; domain_name: \u0026quot;ansibleneo.com\u0026quot; state: \u0026quot;present\u0026quot; - name: Get Active Directory provider details with instance name dellemc_powerscale_ads: onefs_host: \u0026quot;{{onefs_host}}\u0026quot; api_user: \u0026quot;{{api_user}}\u0026quot; api_password: \u0026quot;{{api_password}}\u0026quot; verify_ssl: \u0026quot;{{verify_ssl}}\u0026quot; instance_name: \u0026quot;ansibleneo.com\u0026quot; state: \u0026quot;present\u0026quot; - name: Delete an Active Directory provider with domain name dellemc_powerscale_ads: onefs_host: \u0026quot;{{onefs_host}}\u0026quot; verify_ssl: \u0026quot;{{verify_ssl}}\u0026quot; api_user: \u0026quot;{{api_user}}\u0026quot; api_password: \u0026quot;{{api_password}}\u0026quot; domain_name: \u0026quot;ansibleneo.com\u0026quot; state: \u0026quot;absent\u0026quot; - name: Delete an Active Directory provider with instance name dellemc_powerscale_ads: onefs_host: \u0026quot;{{onefs_host}}\u0026quot; verify_ssl: \u0026quot;{{verify_ssl}}\u0026quot; api_user: \u0026quot;{{api_user}}\u0026quot; api_password: \u0026quot;{{api_password}}\u0026quot; instance_name: \u0026quot;ansibleneo.com\u0026quot; state: \u0026quot;absent\u0026quot; Return Values   Key Type Returned Description   ads_provider_details   complex   When Active Directory provider exists   The Active Directory provider details    \u0026nbsp; groupnet   str  success  Groupnet identifier.    \u0026nbsp; home_directory_template   str  success  Specifies the path to the home directory template.    \u0026nbsp; id   str  success  Specifies the ID of the Active Directory provider instance.    \u0026nbsp; linked_access_zones   list  success  List of access zones linked to the authentication provider.    \u0026nbsp; login_shell   str  success  Specifies the login shell path.    \u0026nbsp; name   str  success  Specifies the Active Directory provider name.    changed   bool   always   Whether or not the resource has changed    Authors  Jennifer John (@johnj9) ansible.team@dell.com   Snapshot Schedule Module Manage snapshot schedules on Dell EMC PowerScale.\nSynopsis You can perform the following operations Managing snapshot schedules on PowerScale. Create snapshot schedule. Modify snapshot schedule. Get details of snapshot schedule. Delete snapshot schedule.\nParameters   Parameter Type Required Default Choices Description   name  str   True     The name of the snapshot schedule.    path  str      The path on which the snapshot will be taken. This path is relative to the base path of the Access Zone. For 'System' access zone, the path is absolute. This parameter is required at the time of creation. Modification of the path is not allowed through the Ansible module.    access_zone  str    System    The effective path where the snapshot is created will be determined by the base path of the Access Zone and the path provided by the user in the playbook.    new_name  str      The new name of the snapshot schedule.    desired_retention  int      The number of hours/days for which snapshots created by this snapshot schedule should be retained. If retention is not specified at the time of creation, then the snapshots created by the snapshot schedule will be retained forever. Minimum retention duration is 2 hours. For large durations (beyond days/weeks), PowerScale may round off the retention to a somewhat larger value to match a whole number of days/weeks.    retention_unit  str    hours    hours days   The retention unit for the snapshot created by this schedule.    alias  str      The alias will point to the latest snapshot created by the snapshot schedule.    pattern  str      Pattern expanded with strftime to create snapshot names. This parameter is required at the time of creation.    schedule  str      The isidate compatible natural language description of the schedule. It specifies the frequency of the schedule. This parameter is required at the time of creation.    state  str   True     absent present   Defines whether the snapshot schedule should exist or not.    onefs_host  str   True     IP address or FQDN of the PowerScale cluster.    port_no  str    8080    Port number of the PowerScale cluster.It defaults to 8080 if not specified.    verify_ssl  bool   True     True False   boolean variable to specify whether to validate SSL certificate or not. True - indicates that the SSL certificate should be verified. False - indicates that the SSL certificate should not be verified.    api_user  str   True     username of the PowerScale cluster.    api_password  str   True     the password of the PowerScale cluster.    Examples - name: Create snapshot schedule dellemc_powerscale_snapshotschedule: onefs_host: \u0026quot;{{onefs_host}}\u0026quot; verify_ssl: \u0026quot;{{verify_ssl}}\u0026quot; api_user: \u0026quot;{{api_user}}\u0026quot; api_password: \u0026quot;{{api_password}}\u0026quot; name: \u0026quot;{{name}}\u0026quot; access_zone: '{{access_zone}}' path: '{{path1}}' alias: \u0026quot;{{alias1}}\u0026quot; desired_retention: \u0026quot;{{desired_retention1}}\u0026quot; pattern: \u0026quot;{{pattern1}}\u0026quot; schedule: \u0026quot;{{schedule1}}\u0026quot; state: \u0026quot;{{state_present}}\u0026quot; - name: Get details of snapshot schedule dellemc_powerscale_snapshotschedule: onefs_host: \u0026quot;{{onefs_host}}\u0026quot; verify_ssl: \u0026quot;{{verify_ssl}}\u0026quot; api_user: \u0026quot;{{api_user}}\u0026quot; api_password: \u0026quot;{{api_password}}\u0026quot; name: \u0026quot;{{name}}\u0026quot; state: \u0026quot;{{state_present}}\u0026quot; - name: Rename snapshot schedule dellemc_powerscale_snapshotschedule: onefs_host: \u0026quot;{{onefs_host}}\u0026quot; verify_ssl: \u0026quot;{{verify_ssl}}\u0026quot; api_user: \u0026quot;{{api_user}}\u0026quot; api_password: \u0026quot;{{api_password}}\u0026quot; name: \u0026quot;{{name}}\u0026quot; new_name: \u0026quot;{{new_name}}\u0026quot; state: \u0026quot;{{state_present}}\u0026quot; - name: Modify alias of snapshot schedule dellemc_powerscale_snapshotschedule: onefs_host: \u0026quot;{{onefs_host}}\u0026quot; verify_ssl: \u0026quot;{{verify_ssl}}\u0026quot; api_user: \u0026quot;{{api_user}}\u0026quot; api_password: \u0026quot;{{api_password}}\u0026quot; name: \u0026quot;{{new_name}}\u0026quot; alias: \u0026quot;{{alias2}}\u0026quot; state: \u0026quot;{{state_present}}\u0026quot; - name: Modify pattern of snapshot schedule dellemc_powerscale_snapshotschedule: onefs_host: \u0026quot;{{onefs_host}}\u0026quot; verify_ssl: \u0026quot;{{verify_ssl}}\u0026quot; api_user: \u0026quot;{{api_user}}\u0026quot; api_password: \u0026quot;{{api_password}}\u0026quot; name: \u0026quot;{{new_name}}\u0026quot; pattern: \u0026quot;{{pattern2}}\u0026quot; state: \u0026quot;{{state_present}}\u0026quot; - name: Modify schedule of snapshot schedule dellemc_powerscale_snapshotschedule: onefs_host: \u0026quot;{{onefs_host}}\u0026quot; verify_ssl: \u0026quot;{{verify_ssl}}\u0026quot; api_user: \u0026quot;{{api_user}}\u0026quot; api_password: \u0026quot;{{api_password}}\u0026quot; name: \u0026quot;{{new_name}}\u0026quot; schedule: \u0026quot;{{schedule2}}\u0026quot; state: \u0026quot;{{state_present}}\u0026quot; - name: Modify retention of snapshot schedule dellemc_powerscale_snapshotschedule: onefs_host: \u0026quot;{{onefs_host}}\u0026quot; verify_ssl: \u0026quot;{{verify_ssl}}\u0026quot; api_user: \u0026quot;{{api_user}}\u0026quot; api_password: \u0026quot;{{api_password}}\u0026quot; name: \u0026quot;{{new_name}}\u0026quot; desired_retention: 2 retention_unit: \u0026quot;{{retention_unit_days}}\u0026quot; state: \u0026quot;{{state_present}}\u0026quot; - name: Delete snapshot schedule dellemc_powerscale_snapshotschedule: onefs_host: \u0026quot;{{onefs_host}}\u0026quot; verify_ssl: \u0026quot;{{verify_ssl}}\u0026quot; api_user: \u0026quot;{{api_user}}\u0026quot; api_password: \u0026quot;{{api_password}}\u0026quot; name: \u0026quot;{{new_name}}\u0026quot; state: \u0026quot;{{state_absent}}\u0026quot; - name: Delete snapshot schedule - Idempotency dellemc_powerscale_snapshotschedule: onefs_host: \u0026quot;{{onefs_host}}\u0026quot; verify_ssl: \u0026quot;{{verify_ssl}}\u0026quot; api_user: \u0026quot;{{api_user}}\u0026quot; api_password: \u0026quot;{{api_password}}\u0026quot; name: \u0026quot;{{new_name}}\u0026quot; state: \u0026quot;{{state_absent}}\u0026quot; Return Values   Key Type Returned Description   changed   bool   always   Whether or not the resource has changed    snapshot_schedule_details   complex   When snapshot schedule exists   Details of the snapshot schedule including snapshot details    \u0026nbsp; schedules   complex  success  Details of snapshot schedule    \u0026nbsp; \u0026nbsp; duration   int  success  Time in seconds added to creation time to construction expiration time    \u0026nbsp; \u0026nbsp; id   int  success  The system ID given to the schedule    \u0026nbsp; \u0026nbsp; next_run   int  success  Unix Epoch time of next snapshot to be created    \u0026nbsp; \u0026nbsp; next_snapshot   str  success  Formatted name of next snapshot to be created    \u0026nbsp; snapshot_list   complex  success  List of snapshots taken by this schedule    \u0026nbsp; \u0026nbsp; snapshots   complex  success  Details of snapshot    \u0026nbsp; \u0026nbsp; \u0026nbsp; created   int  success  The Unix Epoch time the snapshot was created    \u0026nbsp; \u0026nbsp; \u0026nbsp; expires   int  success  The Unix Epoch time the snapshot will expire and be eligible for automatic deletion.    \u0026nbsp; \u0026nbsp; \u0026nbsp; id   int  success  The system ID given to the snapshot.This is useful for tracking the status of delete pending snapshots    \u0026nbsp; \u0026nbsp; \u0026nbsp; name   str  success  The user or system supplied snapshot name. This will be null for snapshots pending delete    \u0026nbsp; \u0026nbsp; \u0026nbsp; size   int  success  The amount of storage in bytes used to store this snapshot    \u0026nbsp; \u0026nbsp; total   int  success  Total number of items available    Authors  Akash Shendge (@shenda1) ansible.team@dell.com   Users Module Manage users on the PowerScale Storage System\nSynopsis Managing Users on the PowerScale Storage System includes create user, delete user, update user, get user, add role and remove role.\nParameters   Parameter Type Required Default Choices Description   user_name  str      The name of the user account. Required at the time of user creation, for rest of the operations either user_name or user_id is required.    user_id  str      The user_id is auto generated at the time of creation. For all other operations either user_name or user_id is needed.    password  str      The password for the user account. Required only in the creation of a user account. If given in other operations then the password will be ignored.    access_zone  str    system    This option mentions the zone in which a user is created. For creation, access_zone acts as an attribute for the user. For all other operations access_zone acts as a filter.    provider_type  str    local    local file ldap ads   This option defines the type which will be used to authenticate the user. Creation, Modification and Deletion is allowed for local users. Adding and removing roles is allowed for all users of the system access zone. Getting user details is allowed for all users. If the provider_type is 'ads' then domain name of the Active Directory Server has to be mentioned in the user_name. The format for the user_name should be 'DOMAIN_NAME\\user_name' or \"DOMAIN_NAME\\\\user_name\". This option acts as a filter for all operations except creation.    enabled  bool      Enabled is a bool variable which is used to enable or disable the user account.    primary_group  str      A user can be member of multiple groups of which one group has to be assigned as primary group. This group will be used for access checks and can also be used when creating files. A user can be added to the group using Group Name.    home_directory  str      The path specified in this option acts as a home directory for the user. The directory which is given should not be already in use. For a user in a system access zone, the absolute path has to be given. For users in a non-system access zone, the path relative to the non-system Access Zone's base directory has to be given.    shell  str      This option is for choosing the type of shell for the user account.    full_name  str      The additional information about the user can be provided using full_name option.    email  str      The email id of the user can be added using email option. The email id can be set at the time of creation and modified later.    state  str   True     absent present   The state option is used to mention the existence of the user account.    role_name  str      The name of the role which a user will be assigned. User can be added to multiple roles.    role_state  str      present-for-user absent-for-user   The role_state option is used to mention the existence of the role for a particular user. It is required when a role is added or removed from user.    onefs_host  str   True     IP address or FQDN of the PowerScale cluster.    port_no  str    8080    Port number of the PowerScale cluster.It defaults to 8080 if not specified.    verify_ssl  bool   True     True False   boolean variable to specify whether to validate SSL certificate or not. True - indicates that the SSL certificate should be verified. False - indicates that the SSL certificate should not be verified.    api_user  str   True     username of the PowerScale cluster.    api_password  str   True     the password of the PowerScale cluster.    Examples  - name: Get User Details using user name dellemc_powerscale_user: onefs_host: \u0026quot;{{onefs_host}}\u0026quot; port_no: \u0026quot;{{port_no}}\u0026quot; api_user: \u0026quot;{{api_user}}\u0026quot; api_password: \u0026quot;{{api_password}}\u0026quot; verify_ssl: \u0026quot;{{verify_ssl}}\u0026quot; access_zone: \u0026quot;{{access_zone}}\u0026quot; provider_type: \u0026quot;{{provider_type}}\u0026quot; user_name: \u0026quot;{{account_name}}\u0026quot; state: \u0026quot;present\u0026quot; - name: Create User dellemc_powerscale_user: onefs_host: \u0026quot;{{onefs_host}}\u0026quot; port_no: \u0026quot;{{port_no}}\u0026quot; api_user: \u0026quot;{{api_user}}\u0026quot; api_password: \u0026quot;{{api_password}}\u0026quot; verify_ssl: \u0026quot;{{verify_ssl}}\u0026quot; access_zone: \u0026quot;{{access_zone}}\u0026quot; provider_type: \u0026quot;{{provider_type}}\u0026quot; user_name: \u0026quot;{{account_name}}\u0026quot; password: \u0026quot;{{account_password}}\u0026quot; primary_group: \u0026quot;{{primary_group}}\u0026quot; enabled: \u0026quot;{{enabled}}\u0026quot; email: \u0026quot;{{email}}\u0026quot; full_name: \u0026quot;{{full_name}}\u0026quot; home_directory: \u0026quot;{{home_directory}}\u0026quot; shell: \u0026quot;{{shell}}\u0026quot; role_name: \u0026quot;{{role_name}}\u0026quot; role_state: \u0026quot;present-for-user\u0026quot; state: \u0026quot;present\u0026quot; - name: Update User's Full Name and email using user name dellemc_powerscale_user: onefs_host: \u0026quot;{{onefs_host}}\u0026quot; port_no: \u0026quot;{{port_no}}\u0026quot; api_user: \u0026quot;{{api_user}}\u0026quot; api_password: \u0026quot;{{api_password}}\u0026quot; verify_ssl: \u0026quot;{{verify_ssl}}\u0026quot; access_zone: \u0026quot;{{access_zone}}\u0026quot; provider_type: \u0026quot;{{provider_type}}\u0026quot; user_name: \u0026quot;{{account_name}}\u0026quot; email: \u0026quot;{{new_email}}\u0026quot; full_name: \u0026quot;{{full_name}}\u0026quot; state: \u0026quot;present\u0026quot; - name: Disable User Account using User Id dellemc_powerscale_user: onefs_host: \u0026quot;{{onefs_host}}\u0026quot; port_no: \u0026quot;{{port_no}}\u0026quot; api_user: \u0026quot;{{api_user}}\u0026quot; api_password: \u0026quot;{{api_password}}\u0026quot; verify_ssl: \u0026quot;{{verify_ssl}}\u0026quot; access_zone: \u0026quot;{{access_zone}}\u0026quot; provider_type: \u0026quot;{{provider_type}}\u0026quot; user_id: \u0026quot;{{id}}\u0026quot; enabled: \u0026quot;False\u0026quot; state: \u0026quot;present\u0026quot; - name: Add user to a role using Username dellemc_powerscale_user: onefs_host: \u0026quot;{{onefs_host}}\u0026quot; port_no: \u0026quot;{{port_no}}\u0026quot; api_user: \u0026quot;{{api_user}}\u0026quot; api_password: \u0026quot;{{api_password}}\u0026quot; verify_ssl: \u0026quot;{{verify_ssl}}\u0026quot; user_name: \u0026quot;{{account_name}}\u0026quot; provider_type: \u0026quot;{{provider_type}}\u0026quot; role_name: \u0026quot;{{role_name}}\u0026quot; role_state: \u0026quot;present-for-user\u0026quot; state: \u0026quot;present\u0026quot; - name: Remove user from a role using User id dellemc_powerscale_user: onefs_host: \u0026quot;{{onefs_host}}\u0026quot; port_no: \u0026quot;{{port_no}}\u0026quot; api_user: \u0026quot;{{api_user}}\u0026quot; api_password: \u0026quot;{{api_password}}\u0026quot; verify_ssl: \u0026quot;{{verify_ssl}}\u0026quot; user_id: \u0026quot;{{id}}\u0026quot; role_name: \u0026quot;{{role_name}}\u0026quot; role_state: \u0026quot;absent-for-user\u0026quot; state: \u0026quot;present\u0026quot; - name: Delete User using user name dellemc_powerscale_user: onefs_host: \u0026quot;{{onefs_host}}\u0026quot; port_no: \u0026quot;{{port_no}}\u0026quot; api_user: \u0026quot;{{api_user}}\u0026quot; api_password: \u0026quot;{{api_password}}\u0026quot; verify_ssl: \u0026quot;{{verify_ssl}}\u0026quot; access_zone: \u0026quot;{{access_zone}}\u0026quot; provider_type: \u0026quot;{{provider_type}}\u0026quot; user_name: \u0026quot;{{account_name}}\u0026quot; state: \u0026quot;absent\u0026quot; Return Values   Key Type Returned Description   changed   bool   always   Whether or not the resource has changed    user_details   complex   When user exists   Details of the user.    \u0026nbsp; email   str  success  The email of the user.    \u0026nbsp; enabled   bool  success  Enabled is a bool variable which is used to enable or disable the user account.    \u0026nbsp; gecos   str  success  The full description of the user.    \u0026nbsp; gid   complex  success  The details of the primary group for the user.    \u0026nbsp; \u0026nbsp; id   str  success  The id of the primary group.    \u0026nbsp; \u0026nbsp; name   str  success  The name of the primary group.    \u0026nbsp; \u0026nbsp; type   str  success  The resource's type is mentioned.    \u0026nbsp; home_directory   str  success  The directory path acts as the home directory for the user's account.    \u0026nbsp; name   str  success  The name of the user.    \u0026nbsp; provider   str  success  The provider contains the provider type and access zone.    \u0026nbsp; roles   list  success  The list of all the roles of which user is a member.    \u0026nbsp; shell   str  success  The type of shell for the user account.    \u0026nbsp; uid   complex  success  Details about the id and name of the user.    \u0026nbsp; \u0026nbsp; id   str  success  The id of the user.    \u0026nbsp; \u0026nbsp; name   str  success  The name of the user.    \u0026nbsp; \u0026nbsp; type   str  success  The resource's type is mentioned.    Authors  P Srinivas Rao (@srinivas-rao5) ansible.team@dell.com   SMB Module Manage SMB shares on Dell EMC PowerScale. You can perform the following operations\nSynopsis Managing SMB share on PowerScale. Create a new SMB share. Modify an existing SMB share. Get details of an existing SMB share. Delete an existing SMB share.\nParameters   Parameter Type Required Default Choices Description   share_name  str   True     The name of the SMB share.    path  str      The path of the SMB share. This parameter will be mandatory only for the create operation. This is the absolute path for System Access Zone and the relative path for non-System Access Zone.    access_zone  str    System    Access zone which contains this share. If not specified it will be considered as a System Access Zone. For a non-System Access Zone the effective path where the SMB is created will be determined by the base path of the Access Zone and the path provided by the user in the playbook. For a System Access Zone the effective path will be the absolute path provided by the user in the playbook.    new_share_name  str      The new name of the SMB share.    description  str      Description about the SMB share.    permissions  list elements: dict      Specifies permission for specific user, group, or trustee. Valid options read, write, and full. This is a list of dictionaries. Each dictionry entry has 3 mandatory values- a)'user_name'/'group_name'/'wellknown' can have actual name of the trustee like 'user'/'group'/'wellknown' b)'permission' can be 'read'/''write'/'full' c)'permission_type' can be 'allow'/'deny' The fourth entry 'provider_type' is optional (default is 'local') d)'provider_type' can be 'local'/'file'/'ads'/'ldap'    access_based_enumeration  bool      Only enumerates files and folders for the requesting user has access to.    access_based_enumeration_root_only  bool      Access-based enumeration on only the root directory of the share.    browsable  bool      Share is visible in net view and the browse list.    ntfs_acl_support  bool      Support NTFS ACLs on files and directories.    directory_create_mask  str      Directory creates mask bits. Octal value for owner, group, and others vs read, write, and execute    directory_create_mode  str      Directory creates mode bits. Octal value for owner, group, and others vs read, write, and execute    file_create_mask  str      File creates mask bits. Octal value for owner, group, and others vs read, write, and execute    file_create_mode  str      File creates mode bits. Octal value for owner, group, and others vs read, write, and execute    state  str   True     absent present   Defines whether the SMB share should exist or not.    onefs_host  str   True     IP address or FQDN of the PowerScale cluster.    port_no  str    8080    Port number of the PowerScale cluster.It defaults to 8080 if not specified.    verify_ssl  bool   True     True False   boolean variable to specify whether to validate SSL certificate or not. True - indicates that the SSL certificate should be verified. False - indicates that the SSL certificate should not be verified.    api_user  str   True     username of the PowerScale cluster.    api_password  str   True     the password of the PowerScale cluster.    Examples  - name: Create SMB share for non system access zone dellemc_powerscale_smb: onefs_host: \u0026quot;{{onefs_host}}\u0026quot; verify_ssl: \u0026quot;{{verify_ssl}}\u0026quot; api_user: \u0026quot;{{api_user}}\u0026quot; api_password: \u0026quot;{{api_password}}\u0026quot; share_name: \u0026quot;{{name}}\u0026quot; path: \u0026quot;{{path}}\u0026quot; access_zone: \u0026quot;{{non_system_access_zone}}\u0026quot; state: \u0026quot;{{state_present}}\u0026quot; - name: Create SMB share for system access zone dellemc_powerscale_smb: onefs_host: \u0026quot;{{onefs_host}}\u0026quot; verify_ssl: \u0026quot;{{verify_ssl}}\u0026quot; api_user: \u0026quot;{{api_user}}\u0026quot; api_password: \u0026quot;{{api_password}}\u0026quot; share_name: \u0026quot;{{name}}\u0026quot; path: \u0026quot;{{system_az_path}}\u0026quot; description: \u0026quot;{{description}}\u0026quot; permissions: - user_name: \u0026quot;{{system_az_user}}\u0026quot; permission: \u0026quot;full\u0026quot; permission_type: \u0026quot;allow\u0026quot; - group_name: \u0026quot;{{system_az_group}}\u0026quot; permission: \u0026quot;read\u0026quot; permission_type: \u0026quot;allow\u0026quot; - wellknown: \u0026quot;everyone\u0026quot; permission: \u0026quot;read\u0026quot; permission_type: \u0026quot;allow\u0026quot; state: \u0026quot;{{state_present}}\u0026quot; - name: Modify user permission for SMB share dellemc_powerscale_smb: onefs_host: \u0026quot;{{onefs_host}}\u0026quot; verify_ssl: \u0026quot;{{verify_ssl}}\u0026quot; api_user: \u0026quot;{{api_user}}\u0026quot; api_password: \u0026quot;{{api_password}}\u0026quot; share_name: \u0026quot;{{name}}\u0026quot; path: \u0026quot;{{system_az_path}}\u0026quot; description: \u0026quot;{{description}}\u0026quot; permissions: - user_name: \u0026quot;{{system_az_user}}\u0026quot; permission: \u0026quot;full\u0026quot; permission_type: \u0026quot;allow\u0026quot; - group_name: \u0026quot;{{system_az_group}}\u0026quot; permission: \u0026quot;write\u0026quot; permission_type: \u0026quot;allow\u0026quot; - wellknown: \u0026quot;everyone\u0026quot; permission: \u0026quot;write\u0026quot; permission_type: \u0026quot;deny\u0026quot; state: \u0026quot;{{state_present}}\u0026quot; - name: Delete system access zone SMB share dellemc_powerscale_smb: onefs_host: \u0026quot;{{onefs_host}}\u0026quot; verify_ssl: \u0026quot;{{verify_ssl}}\u0026quot; api_user: \u0026quot;{{api_user}}\u0026quot; api_password: \u0026quot;{{api_password}}\u0026quot; share_name: \u0026quot;{{name}}\u0026quot; state: \u0026quot;{{state_absent}}\u0026quot; - name: Get SMB share details dellemc_powerscale_smb: onefs_host: \u0026quot;{{onefs_host}}\u0026quot; verify_ssl: \u0026quot;{{verify_ssl}}\u0026quot; api_user: \u0026quot;{{api_user}}\u0026quot; api_password: \u0026quot;{{api_password}}\u0026quot; share_name: \u0026quot;{{name}}\u0026quot; state: \u0026quot;{{state_present}}\u0026quot; - name: Create SMB share for non system access zone dellemc_powerscale_smb: onefs_host: \u0026quot;{{onefs_host}}\u0026quot; verify_ssl: \u0026quot;{{verify_ssl}}\u0026quot; api_user: \u0026quot;{{api_user}}\u0026quot; api_password: \u0026quot;{{api_password}}\u0026quot; share_name: \u0026quot;{{name}}\u0026quot; path: \u0026quot;{{non_system_az_path}}\u0026quot; access_zone: \u0026quot;{{non_system_access_zone}}\u0026quot; description: \u0026quot;{{description}}\u0026quot; permissions: - user_name: \u0026quot;{{non_system_az_user}}\u0026quot; permission: \u0026quot;full\u0026quot; permission_type: \u0026quot;allow\u0026quot; - group_name: \u0026quot;{{non_system_az_group}}\u0026quot; permission: \u0026quot;read\u0026quot; permission_type: \u0026quot;allow\u0026quot; - wellknown: \u0026quot;everyone\u0026quot; permission: \u0026quot;read\u0026quot; permission_type: \u0026quot;allow\u0026quot; state: \u0026quot;{{state_present}}\u0026quot; - name: Modify description for an non system access zone SMB share dellemc_powerscale_smb: onefs_host: \u0026quot;{{onefs_host}}\u0026quot; verify_ssl: \u0026quot;{{verify_ssl}}\u0026quot; api_user: \u0026quot;{{api_user}}\u0026quot; api_password: \u0026quot;{{api_password}}\u0026quot; share_name: \u0026quot;{{name}}\u0026quot; access_zone: \u0026quot;{{non_system_access_zone}}\u0026quot; description: \u0026quot;new description\u0026quot; state: \u0026quot;{{state_present}}\u0026quot; - name: Modify name for an existing non system access zone SMB share dellemc_powerscale_smb: onefs_host: \u0026quot;{{onefs_host}}\u0026quot; verify_ssl: \u0026quot;{{verify_ssl}}\u0026quot; api_user: \u0026quot;{{api_user}}\u0026quot; api_password: \u0026quot;{{api_password}}\u0026quot; share_name: \u0026quot;{{name}}\u0026quot; new_share_name: \u0026quot;{{new_name}}\u0026quot; access_zone: \u0026quot;{{non_system_access_zone}}\u0026quot; description: \u0026quot;new description\u0026quot; state: \u0026quot;{{state_present}}\u0026quot; Return Values   Key Type Returned Description   changed   bool   always   A boolean indicating if the task had to make changes.    smb_details   complex   always   Details of the SMB Share.    \u0026nbsp; browsable   bool  success  Share is visible in net view and the browse list    \u0026nbsp; description   str  success  Description of the SMB Share    \u0026nbsp; directory_create_mask   int  success  Directory create mask bit for SMB Share    \u0026nbsp; directory_create_mask(octal)   str  success  Directory create mask bit for SMB Share in octal format    \u0026nbsp; directory_create_mode   int  success  Directory create mode bit for SMB Share    \u0026nbsp; directory_create_mode(octal)   str  success  Directory create mode bit for SMB Share in octal format    \u0026nbsp; file_create_mask   int  success  File create mask bit for SMB Share    \u0026nbsp; file_create_mask(octal)   str  success  File create mask bit for SMB Share in octal format    \u0026nbsp; file_create_mode   int  success  File create mode bit for SMB Share    \u0026nbsp; file_create_mode(octal)   str  success  File create mode bit for SMB Share in octal format    \u0026nbsp; id   str  success  Id of the SMB Share    \u0026nbsp; name   str  success  Name of the SMB Share    \u0026nbsp; path   str  success  Path of the SMB Share    \u0026nbsp; permission   list  success  permission on the of the SMB Share for user/group/wellknown    Authors  Arindam Datta (@dattaarindam) ansible.team@dell.com   Snapshot Module Manage snapshots on Dell EMC PowerScale.\nSynopsis You can perform the following operations Managing snapshots on PowerScale. Create a filesystem snapshot. Modify a filesystem snapshot. Get details of a filesystem snapshot. Delete a filesystem snapshot.\nParameters   Parameter Type Required Default Choices Description   snapshot_name  str   True     The name of the snapshot.    path  str      Specifies the filesystem path. It is the absolute path for System access zone and it is relative if using non-System access zone. For example, if your access zone is 'Ansible' and it has a base path '/ifs/ansible' and the path specified is '/user1', then the effective path would be '/ifs/ansible/user1'. If your access zone is System, and you have 'directory1' in the access zone, the path provided should be '/ifs/directory1'.    access_zone  str    System    The effective path where the Snapshot is created will be determined by the base path of the Access Zone and the path provided by the user in the playbook.    new_snapshot_name  str      The new name of the snapshot.    expiration_timestamp  str      The timestamp on which the snapshot will expire (UTC format). Either this or desired retention can be specified, but not both.    desired_retention  str      The number of days for which the snapshot can be retained. Either this or expiration timestamp can be specified, but not both.    retention_unit  str      hours days   The retention unit for the snapshot. The default value is hours.    alias  str      The alias for the snapshot.    state  str   True     absent present   Defines whether the snapshot should exist or not.    onefs_host  str   True     IP address or FQDN of the PowerScale cluster.    port_no  str    8080    Port number of the PowerScale cluster.It defaults to 8080 if not specified.    verify_ssl  bool   True     True False   boolean variable to specify whether to validate SSL certificate or not. True - indicates that the SSL certificate should be verified. False - indicates that the SSL certificate should not be verified.    api_user  str   True     username of the PowerScale cluster.    api_password  str   True     the password of the PowerScale cluster.    Examples  - name: Create a filesystem snapshot on PowerScale dellemc_powerscale_snapshot: onefs_host: \u0026quot;{{onefs_host}}\u0026quot; verify_ssl: \u0026quot;{{verify_ssl}}\u0026quot; api_user: \u0026quot;{{api_user}}\u0026quot; api_password: \u0026quot;{{api_password}}\u0026quot; path: \u0026quot;{{ansible_path}}\u0026quot; access_zone: \u0026quot;{{access_zone}}\u0026quot; snapshot_name: \u0026quot;{{snapshot_name}}\u0026quot; desired_retention: \u0026quot;{{desired_retention}}\u0026quot; retention_unit: \u0026quot;{{retention_unit_days}}\u0026quot; alias: \u0026quot;{{ansible_snap_alias}}\u0026quot; state: \u0026quot;{{present}}\u0026quot; - name: Get details of a filesystem snapshot dellemc_powerscale_snapshot: onefs_host: \u0026quot;{{onefs_host}}\u0026quot; verify_ssl: \u0026quot;{{verify_ssl}}\u0026quot; api_user: \u0026quot;{{api_user}}\u0026quot; api_password: \u0026quot;{{api_password}}\u0026quot; snapshot_name: \u0026quot;{{snapshot_name}}\u0026quot; state: \u0026quot;{{present}}\u0026quot; - name: Modify filesystem snapshot desired retention dellemc_powerscale_snapshot: onefs_host: \u0026quot;{{onefs_host}}\u0026quot; verify_ssl: \u0026quot;{{verify_ssl}}\u0026quot; api_user: \u0026quot;{{api_user}}\u0026quot; api_password: \u0026quot;{{api_password}}\u0026quot; snapshot_name: \u0026quot;{{snapshot_name}}\u0026quot; desired_retention: \u0026quot;{{desired_retention_new}}\u0026quot; retention_unit: \u0026quot;{{retention_unit_days}}\u0026quot; state: \u0026quot;{{present}}\u0026quot; - name: Modify filesystem snapshot expiration timestamp dellemc_powerscale_snapshot: onefs_host: \u0026quot;{{onefs_host}}\u0026quot; verify_ssl: \u0026quot;{{verify_ssl}}\u0026quot; api_user: \u0026quot;{{api_user}}\u0026quot; api_password: \u0026quot;{{api_password}}\u0026quot; snapshot_name: \u0026quot;{{snapshot_name}}\u0026quot; expiration_timestamp: \u0026quot;{{expiration_timestamp_new}}\u0026quot; state: \u0026quot;{{present}}\u0026quot; - name: Modify filesystem snapshot alias dellemc_powerscale_snapshot: onefs_host: \u0026quot;{{onefs_host}}\u0026quot; verify_ssl: \u0026quot;{{verify_ssl}}\u0026quot; api_user: \u0026quot;{{api_user}}\u0026quot; api_password: \u0026quot;{{api_password}}\u0026quot; snapshot_name: \u0026quot;{{snapshot_name}}\u0026quot; alias: \u0026quot;{{ansible_snap_alias_new}}\u0026quot; state: \u0026quot;{{present}}\u0026quot; - name: Delete snapshot alias dellemc_powerscale_snapshot: onefs_host: \u0026quot;{{onefs_host}}\u0026quot; verify_ssl: \u0026quot;{{verify_ssl}}\u0026quot; api_user: \u0026quot;{{api_user}}\u0026quot; api_password: \u0026quot;{{api_password}}\u0026quot; snapshot_name: \u0026quot;{{snapshot_name}}\u0026quot; alias: \u0026quot;\u0026quot; state: \u0026quot;{{present}}\u0026quot; - name: Rename filesystem snapshot dellemc_powerscale_snapshot: onefs_host: \u0026quot;{{onefs_host}}\u0026quot; verify_ssl: \u0026quot;{{verify_ssl}}\u0026quot; api_user: \u0026quot;{{api_user}}\u0026quot; api_password: \u0026quot;{{api_password}}\u0026quot; snapshot_name: \u0026quot;{{snapshot_name}}\u0026quot; new_snapshot_name: \u0026quot;{{new_snapshot_name}}\u0026quot; state: \u0026quot;{{present}}\u0026quot; - name: Delete filesystem snapshot dellemc_powerscale_snapshot: onefs_host: \u0026quot;{{onefs_host}}\u0026quot; verify_ssl: \u0026quot;{{verify_ssl}}\u0026quot; api_user: \u0026quot;{{api_user}}\u0026quot; api_password: \u0026quot;{{api_password}}\u0026quot; snapshot_name: \u0026quot;{{new_snapshot_name}}\u0026quot; state: \u0026quot;{{absent}}\u0026quot; Return Values   Key Type Returned Description   changed   bool   always   Whether or not the resource has changed    snapshot_details   complex   When snapshot exists.   The snapshot details.    \u0026nbsp; alias   str  success  Snapshot alias.    \u0026nbsp; created   int  success  The creation timestamp.    \u0026nbsp; expires   int  success  The expiration timestamp.    \u0026nbsp; has_locks   bool  success  Whether the snapshot has locks.    \u0026nbsp; id   int  success  The snapshot ID.    \u0026nbsp; name   str  success  The name of the snapshot.    \u0026nbsp; path   str  success  The directory path whose snapshot has been taken.    \u0026nbsp; pct_filesystem   float  success  The percentage of filesystem used.    \u0026nbsp; pct_reserve   float  success  The percentage of filesystem reserved.    \u0026nbsp; size   int  success  The snapshot size.    \u0026nbsp; state   str  success  The state of the snapshot.    \u0026nbsp; target_id   int  success  target ID of snapshot whose alias it is.    \u0026nbsp; target_name   str  success  target name of snapshot whose alias it is.    Authors  Prashant Rakheja (@prashant-dell) ansible.team@dell.com   Access Zone Module Manages access zones on PowerScale\nSynopsis Managing access zones on the PowerScale storage system includes getting details of the access zone and modifying the smb and nfs settings.\nParameters   Parameter Type Required Default Choices Description   az_name  str   True     The name of the access zone.    smb  dict      Specifies the default SMB setting parameters of access zone.    \u0026nbsp; create_permissions   str    default acl    default acl Inherit mode bits Use create mask and mode   Sets the default source permissions to apply when a file or directory is created.    \u0026nbsp; directory_create_mask   str      Specifies the UNIX mask bits(octal) that are removed when a directory is created, restricting permissions. Mask bits are applied before mode bits are applied.    \u0026nbsp; directory_create_mode   str      Specifies the UNIX mode bits(octal) that are added when a directory is created, enabling permissions.    \u0026nbsp; file_create_mask   str      Specifies the UNIX mask bits(octal) that are removed when a file is created, restricting permissions.    \u0026nbsp; file_create_mode   str      Specifies the UNIX mode bits(octal) that are added when a file is created, enabling permissions.    \u0026nbsp; access_based_enumeration   bool      Allows access based enumeration only on the files and folders that the requesting user can access.    \u0026nbsp; access_based_enumeration_root_only   bool      Access-based enumeration on only the root directory of the share.    \u0026nbsp; ntfs_acl_support   bool      Allows ACLs to be stored and edited from SMB clients.    \u0026nbsp; oplocks   bool      An oplock allows clients to provide performance improvements by using locally-cached information.    nfs  dict      Specifies the default NFS setting parameters of access zone.    \u0026nbsp; commit_asynchronous   bool      Set to True if NFS commit requests execute asynchronously.    \u0026nbsp; nfsv4_domain   str      Specifies the domain or realm through which users and groups are associated.    \u0026nbsp; nfsv4_allow_numeric_ids   bool      If true, sends owners and groups as UIDs and GIDs when look up fails or if the 'nfsv4_no_name' property is set to 1.    \u0026nbsp; nfsv4_no_domain   bool      If true, sends owners and groups without a domain name.    \u0026nbsp; nfsv4_no_domain_uids   bool      If true, sends UIDs and GIDs without a domain name.    \u0026nbsp; nfsv4_no_names   bool      If true, sends owners and groups as UIDs and GIDs.    provider_state  str      add remove   Defines whether the auth providers should be added or removed from access zone. If auth_providers are given, then provider_state should also be specified. add - indicates that the auth providers should be added to the access zone. remove - indicates that auth providers should be removed from the access zone.    auth_providers  list elements: dict      Specifies the auth providers which needs to be added or removed from access zone. If auth_providers are given, then provider_state should also be specified.    \u0026nbsp; provider_name   str   True     Specifies the auth provider name which needs to be added or removed from access zone.    \u0026nbsp; provider_type   str   True     local file ldap ads   Specifies the auth provider type which needs to be added or removed from access zone.    state  str   True     present absent   Defines whether the access zone should exist or not. present - indicates that the access zone should exist on the system. absent - indicates that the access zone should not exist on the system.    onefs_host  str   True     IP address or FQDN of the PowerScale cluster.    port_no  str    8080    Port number of the PowerScale cluster.It defaults to 8080 if not specified.    verify_ssl  bool   True     True False   boolean variable to specify whether to validate SSL certificate or not. True - indicates that the SSL certificate should be verified. False - indicates that the SSL certificate should not be verified.    api_user  str   True     username of the PowerScale cluster.    api_password  str   True     the password of the PowerScale cluster.    Notes  Creation/Deletion of access zone is not allowed through the Ansible module.  Examples - name: Get details of access zone including smb and nfs settings dellemc_powerscale_accesszone: onefs_host: \u0026quot;{{onefs_host}}\u0026quot; api_user: \u0026quot;{{api_user}}\u0026quot; api_password: \u0026quot;{{api_password}}\u0026quot; verify_ssl: \u0026quot;{{verify_ssl}}\u0026quot; az_name: \u0026quot;{{access zone}}\u0026quot; state: \u0026quot;present\u0026quot; - name: Modify smb settings of access zone dellemc_powerscale_accesszone: onefs_host: \u0026quot;{{onefs_host}}\u0026quot; api_user: \u0026quot;{{api_user}}\u0026quot; api_password: \u0026quot;{{api_password}}\u0026quot; verify_ssl: \u0026quot;{{verify_ssl}}\u0026quot; az_name: \u0026quot;{{access zone}}\u0026quot; state: \u0026quot;present\u0026quot; smb: create_permissions: 'default acl' directory_create_mask: '777' directory_create_mode: '700' file_create_mask: '700' file_create_mode: '100' access_based_enumeration: true access_based_enumeration_root_only: false ntfs_acl_support: true oplocks: true - name: Modify nfs settings of access zone dellemc_powerscale_accesszone: onefs_host: \u0026quot;{{onefs_host}}\u0026quot; api_user: \u0026quot;{{api_user}}\u0026quot; api_password: \u0026quot;{{api_password}}\u0026quot; verify_ssl: \u0026quot;{{verify_ssl}}\u0026quot; az_name: \u0026quot;{{access zone}}\u0026quot; state: \u0026quot;present\u0026quot; nfs: commit_asynchronous: false nfsv4_allow_numeric_ids: false nfsv4_domain: 'localhost' nfsv4_no_domain: false nfsv4_no_domain_uids: false nfsv4_no_names: false - name: Modify smb and nfs settings of access zone dellemc_powerscale_accesszone: onefs_host: \u0026quot;{{onefs_host}}\u0026quot; api_user: \u0026quot;{{api_user}}\u0026quot; api_password: \u0026quot;{{api_password}}\u0026quot; verify_ssl: \u0026quot;{{verify_ssl}}\u0026quot; az_name: \u0026quot;{{access zone}}\u0026quot; state: \u0026quot;present\u0026quot; smb: create_permissions: 'default acl' directory_create_mask: '777' directory_create_mode: '700' file_create_mask: '700' file_create_mode: '100' access_based_enumeration: true access_based_enumeration_root_only: false ntfs_acl_support: true oplocks: true nfs: commit_asynchronous: false nfsv4_allow_numeric_ids: false nfsv4_domain: 'localhost' nfsv4_no_domain: false nfsv4_no_domain_uids: false nfsv4_no_names: false - name: Add Auth Providers to the access zone dellemc_powerscale_accesszone: onefs_host: \u0026quot;{{onefs_host}}\u0026quot; api_user: \u0026quot;{{api_user}}\u0026quot; api_password: \u0026quot;{{api_password}}\u0026quot; verify_ssl: \u0026quot;{{verify_ssl}}\u0026quot; az_name: \u0026quot;{{access zone}}\u0026quot; provider_state: \u0026quot;add\u0026quot; auth_providers: - provider_name: \u0026quot;System\u0026quot; provider_type: \u0026quot;file\u0026quot; - provider_name: \u0026quot;ldap-prashant\u0026quot; provider_type: \u0026quot;ldap\u0026quot; state: \u0026quot;present\u0026quot; - name: Remove Auth Providers from the access zone dellemc_powerscale_accesszone: onefs_host: \u0026quot;{{onefs_host}}\u0026quot; api_user: \u0026quot;{{api_user}}\u0026quot; api_password: \u0026quot;{{api_password}}\u0026quot; verify_ssl: \u0026quot;{{verify_ssl}}\u0026quot; az_name: \u0026quot;{{access zone}}\u0026quot; provider_state: \u0026quot;remove\u0026quot; auth_providers: - provider_name: \u0026quot;System\u0026quot; provider_type: \u0026quot;file\u0026quot; state: \u0026quot;present\u0026quot; Return Values   Key Type Returned Description   access_zone_details   complex   When access zone exists   The access zone details    \u0026nbsp; nfs_settings   complex  success  NFS settings of access zone    \u0026nbsp; \u0026nbsp; export_settings   complex  success  Default values for NFS exports    \u0026nbsp; \u0026nbsp; \u0026nbsp; commit_asynchronous   bool  success  Set to True if NFS commit requests execute asynchronously    \u0026nbsp; \u0026nbsp; zone_settings   complex  success  NFS server settings for this zone    \u0026nbsp; \u0026nbsp; \u0026nbsp; nfsv4_allow_numeric_ids   bool  success  If true, sends owners and groups as UIDs and GIDs when look up fails or if the 'nfsv4_no_name' property is set to 1    \u0026nbsp; \u0026nbsp; \u0026nbsp; nfsv4_domain   str  success  Specifies the domain or realm through which users and groups are associated    \u0026nbsp; \u0026nbsp; \u0026nbsp; nfsv4_no_domain   bool  success  If true, sends owners and groups without a domain name    \u0026nbsp; \u0026nbsp; \u0026nbsp; nfsv4_no_domain_uids   bool  success  If true, sends UIDs and GIDs without a domain name    \u0026nbsp; \u0026nbsp; \u0026nbsp; nfsv4_no_names   bool  success  If true, sends owners and groups as UIDs and GIDs    \u0026nbsp; smb_settings   complex  success  SMB settings of access zone    \u0026nbsp; \u0026nbsp; directory_create_mask(octal)   str  success  UNIX mask bits for directory in octal format    \u0026nbsp; \u0026nbsp; directory_create_mode(octal)   str  success  UNIX mode bits for directory in octal format    \u0026nbsp; \u0026nbsp; file_create_mask(octal)   str  success  UNIX mask bits for file in octal format    \u0026nbsp; \u0026nbsp; file_create_mode(octal)   str  success  UNIX mode bits for file in octal format    access_zone_modify_flag   bool   on success   Whether auth providers linked to access zone has changed    changed   bool   always   Whether or not the resource has changed    nfs_modify_flag   bool   on success   Whether or not the default NFS settings of access zone has changed    smb_modify_flag   bool   on success   Whether or not the default SMB settings of access zone has changed    Authors  Akash Shendge (@shenda1) ansible.team@dell.com   NFS Module Manage NFS exports on a DellEMC PowerScale system\nSynopsis Managing NFS exports on an PowerScale system includes creating NFS export for a directory in an access zone, adding or removing clients, modifying different parameters of the export and deleting export.\nParameters   Parameter Type Required Default Choices Description   path  str   True     Specifies the filesystem path. It is the absolute path for System access zone and it is relative if using non-system access zone. For example, if your access zone is 'Ansible' and it has a base path '/ifs/ansible' and the path specified is '/user1', then the effective path would be '/ifs/ansible/user1'. If your access zone is System, and you have 'directory1' in the access zone, the path provided should be '/ifs/directory1'. The directory on the path must exist - the NFS module will not create the directory. Ansible module will only support exports with a unique path. If there are multiple exports present with the same path, fetching details, creation, modification or deletion of such exports will fail.    access_zone  str    System    Specifies the zone in which the export is valid. Access zone once set cannot be changed.    clients  list elements: str      Specifies the clients to the export. The type of access to clients in this list is determined by the 'read_only' parameter. This list can be changed anytime during the lifetime of the NFS export.    root_clients  list elements: str      Specifies the clients with root access to the export. This list can be changed anytime during the lifetime of the NFS export.    read_only_clients  list elements: str      Specifies the clients with read-only access to the export, even when the export is read/write. This list can be changed anytime during the lifetime of the NFS export.    read_write_clients  list elements: str      Specifies the clients with both read and write access to the export, even when the export is set to read-only. This list can be changed anytime during the lifetime of the NFS export.    read_only  bool      Specifies whether the export is read-only or read-write. This parameter only has effect on the 'clients' list and not the other three types of clients. This setting can be modified any time. If it is not set at the time of creation, the export will be of type read/write.    sub_directories_mountable  bool      True if all directories under the specified paths are mountable. If not set, sub-directories will not be mountable. This setting can be modified any time.    description  str      Optional description field for the NFS export. Can be modified by passing a new value.    state  str   True     absent present   Defines whether the NFS export should exist or not. present indicates that the NFS export should exist in system. absent indicates that the NFS export should not exist in system.    client_state  str      present-in-export absent-in-export   Defines whether the clients can access the NFS export. present-in-export indicates that the clients can access the NFS export. absent-in-export indicates that the client cannot access the NFS export. Required when adding or removing access of clients from the export. While removing clients, only the specified clients will be removed from the export, others will remain as is.    onefs_host  str   True     IP address or FQDN of the PowerScale cluster.    port_no  str    8080    Port number of the PowerScale cluster.It defaults to 8080 if not specified.    verify_ssl  bool   True     True False   boolean variable to specify whether to validate SSL certificate or not. True - indicates that the SSL certificate should be verified. False - indicates that the SSL certificate should not be verified.    api_user  str   True     username of the PowerScale cluster.    api_password  str   True     the password of the PowerScale cluster.    Examples  - name: Create NFS Export dellemc_powerscale_nfs: onefs_host: \u0026quot;{{onefs_host}}\u0026quot; api_user: \u0026quot;{{api_user}}\u0026quot; api_password: \u0026quot;{{api_password}}\u0026quot; verify_ssl: \u0026quot;{{verify_ssl}}\u0026quot; path: \u0026quot;{{path}}\u0026quot; access_zone: \u0026quot;{{access_zone}}\u0026quot; read_only_clients: - \u0026quot;{{client1}}\u0026quot; - \u0026quot;{{client2}}\u0026quot; read_only: True clients: [\u0026quot;{{client3}}\u0026quot;] client_state: 'present-in-export' state: 'present' - name: Get NFS Export dellemc_powerscale_nfs: onefs_host: \u0026quot;{{onefs_host}}\u0026quot; api_user: \u0026quot;{{api_user}}\u0026quot; api_password: \u0026quot;{{api_password}}\u0026quot; verify_ssl: \u0026quot;{{verify_ssl}}\u0026quot; path: \u0026quot;{{path}}\u0026quot; access_zone: \u0026quot;{{access_zone}}\u0026quot; state: 'present' - name: Add a root client dellemc_powerscale_nfs: onefs_host: \u0026quot;{{onefs_host}}\u0026quot; api_user: \u0026quot;{{api_user}}\u0026quot; api_password: \u0026quot;{{api_password}}\u0026quot; verify_ssl: \u0026quot;{{verify_ssl}}\u0026quot; path: \u0026quot;{{path}}\u0026quot; access_zone: \u0026quot;{{access_zone}}\u0026quot; root_clients: - \u0026quot;{{client4}}\u0026quot; client_state: 'present-in-export' state: 'present' - name: Set sub_directories_mountable flag to True dellemc_powerscale_nfs: onefs_host: \u0026quot;{{onefs_host}}\u0026quot; api_user: \u0026quot;{{api_user}}\u0026quot; api_password: \u0026quot;{{api_password}}\u0026quot; verify_ssl: \u0026quot;{{verify_ssl}}\u0026quot; path: \u0026quot;{{path}}\u0026quot; access_zone: \u0026quot;{{access_zone}}\u0026quot; sub_directories_mountable: True state: 'present' - name: Remove a root client dellemc_powerscale_nfs: onefs_host: \u0026quot;{{onefs_host}}\u0026quot; api_user: \u0026quot;{{api_user}}\u0026quot; api_password: \u0026quot;{{api_password}}\u0026quot; verify_ssl: \u0026quot;{{verify_ssl}}\u0026quot; path: \u0026quot;{{path}}\u0026quot; access_zone: \u0026quot;{{access_zone}}\u0026quot; root_clients: - \u0026quot;{{client4}}\u0026quot; client_state: 'absent-in-export' state: 'present' - name: Modify description dellemc_powerscale_nfs: onefs_host: \u0026quot;{{onefs_host}}\u0026quot; api_user: \u0026quot;{{api_user}}\u0026quot; api_password: \u0026quot;{{api_password}}\u0026quot; verify_ssl: \u0026quot;{{verify_ssl}}\u0026quot; path: \u0026quot;{{path}}\u0026quot; access_zone: \u0026quot;{{access_zone}}\u0026quot; description: \u0026quot;new description\u0026quot; state: 'present' - name: Set read_only flag to False dellemc_powerscale_nfs: onefs_host: \u0026quot;{{onefs_host}}\u0026quot; api_user: \u0026quot;{{api_user}}\u0026quot; api_password: \u0026quot;{{api_password}}\u0026quot; verify_ssl: \u0026quot;{{verify_ssl}}\u0026quot; path: \u0026quot;{{path}}\u0026quot; access_zone: \u0026quot;{{access_zone}}\u0026quot; read_only: False state: 'present' - name: Delete NFS Export dellemc_powerscale_nfs: onefs_host: \u0026quot;{{onefs_host}}\u0026quot; api_user: \u0026quot;{{api_user}}\u0026quot; api_password: \u0026quot;{{api_password}}\u0026quot; verify_ssl: \u0026quot;{{verify_ssl}}\u0026quot; path: \u0026quot;{{path}}\u0026quot; access_zone: \u0026quot;{{access_zone}}\u0026quot; state: 'absent' Return Values   Key Type Returned Description   NFS_export_details   complex   always   The updated NFS Export details.    \u0026nbsp; all_dirs   bool  success  sub_directories_mountable flag value.    \u0026nbsp; clients   list  success  The list of clients for the NFS Export.    \u0026nbsp; description   str  success  Description for the export.    \u0026nbsp; id   int  success  The ID of the NFS Export, generated by the array.    \u0026nbsp; paths   list  success  The filesystem path.    \u0026nbsp; read_only   bool  success  Specifies whether the export is read-only or read-write.    \u0026nbsp; read_only_clients   list  success  The list of read only clients for the NFS Export.    \u0026nbsp; read_write_clients   list  success  The list of read write clients for the NFS Export.    \u0026nbsp; root_clients   list  success  The list of root clients for the NFS Export.    \u0026nbsp; zone   str  success  Specifies the zone in which the export is valid.    changed   bool   always   A boolean indicating if the task had to make changes.    Authors  Manisha Agrawal(@agrawm3) ansible.team@dell.com   Groups Module Manage Groups on the PowerScale Storage System\nSynopsis Managing Groups on the PowerScale Storage System includes create group, delete group, get group, add users and remove users.\nParameters   Parameter Type Required Default Choices Description   group_name  str      The name of the group. Required at the time of group creation, for the rest of the operations either group_name or group_id is required.    group_id  str      The group_id is auto generated at the time of creation. For all other operations either group_name or group_id is needed.    access_zone  str    system    This option mentions the zone in which a group is created. For creation, access_zone acts as an attribute for the group. For all other operations access_zone acts as a filter.    provider_type  str    local    local file ldap ads   This option defines the type which will be used to authenticate the group members. Creation, Deletion and Modification is allowed only for local group. Details of groups of all provider types can be fetched. If the provider_type is 'ads' then the domain name of the Active Directory Server has to be mentioned in the group_name. The format for the group_name should be 'DOMAIN_NAME\\group_name' or \"DOMAIN_NAME\\\\group_name\". This option acts as a filter for all operations except creation.    state  str   True     absent present   The state option is used to determine whether the group will exist or not.    users  list elements: dict      Either user_name or user_id is needed to add or remove the user from the group. users can be part of multiple groups.    user_state  str      present-in-group absent-in-group   The user_state option is used to determine whether the users will exist for a particular group or not. It is required when users are added or removed from a group.    onefs_host  str   True     IP address or FQDN of the PowerScale cluster.    port_no  str    8080    Port number of the PowerScale cluster.It defaults to 8080 if not specified.    verify_ssl  bool   True     True False   boolean variable to specify whether to validate SSL certificate or not. True - indicates that the SSL certificate should be verified. False - indicates that the SSL certificate should not be verified.    api_user  str   True     username of the PowerScale cluster.    api_password  str   True     the password of the PowerScale cluster.    Examples  - name: Create a Group dellemc_powerscale_group: onefs_host: \u0026quot;{{onefs_host}}\u0026quot; api_user: \u0026quot;{{api_user}}\u0026quot; api_password: \u0026quot;{{api_password}}\u0026quot; verify_ssl: \u0026quot;{{verify_ssl}}\u0026quot; access_zone: \u0026quot;{{access_zone}}\u0026quot; provider_type: \u0026quot;{{provider_type}}\u0026quot; group_name: \u0026quot;{{group_name}}\u0026quot; state: \u0026quot;present\u0026quot; - name: Create Group with Users dellemc_powerscale_group: onefs_host: \u0026quot;{{onefs_host}}\u0026quot; api_user: \u0026quot;{{api_user}}\u0026quot; api_password: \u0026quot;{{api_password}}\u0026quot; verify_ssl: \u0026quot;{{verify_ssl}}\u0026quot; provider_type: \u0026quot;{{provider_type}}\u0026quot; access_zone: \u0026quot;{{access_zone}}\u0026quot; group_name: \u0026quot;{{group_name}}\u0026quot; users: - user_name: \u0026quot;{{user_name}}\u0026quot; - user_id: \u0026quot;{{user_id}}\u0026quot; - user_name: \u0026quot;{{user_name_2}}\u0026quot; user_state: \u0026quot;present-in-group\u0026quot; state: \u0026quot;present\u0026quot; - name: Get Details of the Group using Group Id dellemc_powerscale_group: onefs_host: \u0026quot;{{onefs_host}}\u0026quot; api_user: \u0026quot;{{api_user}}\u0026quot; api_password: \u0026quot;{{api_password}}\u0026quot; verify_ssl: \u0026quot;{{verify_ssl}}\u0026quot; provider_type: \u0026quot;{{provider_type}}\u0026quot; access_zone: \u0026quot;{{access_zone}}\u0026quot; group_id: \u0026quot;{{group_id}}\u0026quot; state: \u0026quot;present\u0026quot; - name: Delete the Group using Group Name dellemc_powerscale_group: onefs_host: \u0026quot;{{onefs_host}}\u0026quot; api_user: \u0026quot;{{api_user}}\u0026quot; api_password: \u0026quot;{{api_password}}\u0026quot; verify_ssl: \u0026quot;{{verify_ssl}}\u0026quot; provider_type: \u0026quot;{{provider_type}}\u0026quot; access_zone: \u0026quot;{{access_zone}}\u0026quot; group_name: \u0026quot;{{group_name}}\u0026quot; state: \u0026quot;absent\u0026quot; - name: Add Users to a Group dellemc_powerscale_group: onefs_host: \u0026quot;{{onefs_host}}\u0026quot; api_user: \u0026quot;{{api_user}}\u0026quot; api_password: \u0026quot;{{api_password}}\u0026quot; verify_ssl: \u0026quot;{{verify_ssl}}\u0026quot; provider_type: \u0026quot;{{provider_type}}\u0026quot; access_zone: \u0026quot;{{access_zone}}\u0026quot; group_id: \u0026quot;{{group_id}}\u0026quot; users: - user_name: \u0026quot;{{user_name}}\u0026quot; - user_id: \u0026quot;{{user_id}}\u0026quot; - user_name: \u0026quot;{{user_name_2}}\u0026quot; user_state: \u0026quot;present-in-group\u0026quot; state: \u0026quot;present\u0026quot; - name: Remove Users from a Group dellemc_powerscale_group: onefs_host: \u0026quot;{{onefs_host}}\u0026quot; api_user: \u0026quot;{{api_user}}\u0026quot; api_password: \u0026quot;{{api_password}}\u0026quot; verify_ssl: \u0026quot;{{verify_ssl}}\u0026quot; provider_type: \u0026quot;{{provider_type}}\u0026quot; access_zone: \u0026quot;{{access_zone}}\u0026quot; group_id: \u0026quot;{{group_id}}\u0026quot; users: - user_name: \u0026quot;{{user_name_1}}\u0026quot; - user_id: \u0026quot;{{user_id}}\u0026quot; - user_name: \u0026quot;{{user_name_2}}\u0026quot; user_state: \u0026quot;absent-in-group\u0026quot; state: \u0026quot;present\u0026quot; Return Values   Key Type Returned Description   changed   bool   always   Whether or not the resource has changed    group_details   complex   When group exists   Details of the group    \u0026nbsp; gid   complex  success  The details of the primary group for the user.    \u0026nbsp; \u0026nbsp; id   str  success  The id of the group.    \u0026nbsp; \u0026nbsp; name   str  success  The name of the group.    \u0026nbsp; \u0026nbsp; type_of_resource   str  success  The resource's type is mentioned.    \u0026nbsp; members   complex  success  The list of sid's the members of group.    \u0026nbsp; \u0026nbsp; sid   complex  success  The details of the associated resource.    \u0026nbsp; \u0026nbsp; \u0026nbsp; id   str  success  The unique security identifier of the resource.    \u0026nbsp; \u0026nbsp; \u0026nbsp; name   str  success  The name of the resource.    \u0026nbsp; \u0026nbsp; \u0026nbsp; type_of_resource   str  success  The resource's type is mentioned.    \u0026nbsp; name   str  success  The name of the group.    \u0026nbsp; provider   str  success  The provider contains the provider type and access zone.    Authors  P Srinivas Rao (@srinivas-rao5) ansible.team@dell.com   File System Module Manage Filesystems on PowerScale\nSynopsis Managing Filesystems on PowerScale Storage System includes Create a new Filesystem, Delete a Filesystem, Get details of a filesystem, Modify a Filesystem (Quota, ACLs).\nParameters   Parameter Type Required Default Choices Description   path  str   True     This is the directory path. It is the absolute path for System access zone and is relative if using a non-System access zone. For example, if your access zone is 'Ansible' and it has a base path '/ifs/ansible' and the path specified is '/user1', then the effective path would be '/ifs/ansible/user1'. If your access zone is System, and you have 'directory1' in the access zone, the path provided should be '/ifs/directory1'.    access_zone  str    System    The access zone. If no Access Zone is specified, the 'System' access zone would be taken by default.    owner  dict      The owner of the Filesystem. This parameter is required while creating a Filesystem. The following sub-options are supported for Owner. - name(str), - provider_type(str). If you specify owner, then the corresponding name is mandatory. The provider_type is optional and it defaults to 'local'. The supported values for provider_type are 'local', 'file', 'ldap' and 'ads'.    group  dict      The group of the Filesystem. The following sub-options are supported for Group. - name(str), - provider_type(str). If you specify a group, then the corresponding name is mandatory. The provider_type is optional, it defaults to 'local'. The supported values for provider_type are 'local', 'file', 'ldap' and 'ads'.    access_control  str      The ACL value for the directory. At the time of creation, users can either provide input such as 'private_read' , 'private' , 'public_read', 'public_read_write', 'public' or in POSIX format (eg 0700). Modification of ACL is only supported from POSIX to POSIX mode.    recursive  bool    True    Creates intermediate folders recursively when set to true.    quota  dict      The Smart Quota for the filesystem. Only directory Quotas are supported. The following sub-options are supported for Quota. - include_snap_data(boolean), - include_data_protection_overhead(boolean), - thresholds_on(app_logical_size, fs_logical_size, physical_size) - advisory_limit_size(int), - soft_limit_size(int), - hard_limit_size(int), - cap_unit (MB, GB or TB), - quota_state (present or absent). The default grace period is 7 days. Modification of grace period is not supported. The default capacity unit is GB. The parameter include_data_protection_overhead is supported for SDK 8.1.1 For SDK 9.0.0 the parameter include_data_protection_overhead is deprecated and thresholds_on is used.    state  str   True     absent present   Defines whether the Filesystem should exist or not. A filesystem with NFS exports or SMB shares cannot be deleted. Any Quotas on the Filesystem need to be removed before deleting the filesystem.    list_snapshots  bool    False    If set to true, the filesystem's snapshots are returned.    onefs_host  str   True     IP address or FQDN of the PowerScale cluster.    port_no  str    8080    Port number of the PowerScale cluster.It defaults to 8080 if not specified.    verify_ssl  bool   True     True False   boolean variable to specify whether to validate SSL certificate or not. True - indicates that the SSL certificate should be verified. False - indicates that the SSL certificate should not be verified.    api_user  str   True     username of the PowerScale cluster.    api_password  str   True     the password of the PowerScale cluster.    Examples  - name: Create Filesystem with Quota in given access zone dellemc_powerscale_filesystem: onefs_host: \u0026quot;{{powerscalehost}}\u0026quot; port: \u0026quot;{{powerscaleport}}\u0026quot; verify_ssl: \u0026quot;{{verify_ssl}}\u0026quot; username: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; path: \u0026quot;{{path}}\u0026quot; access_zone: \u0026quot;{{access_zone}}\u0026quot; owner: name: 'ansible_user' provider_type: 'ldap' group: name: 'ansible_group' provider_type: 'ldap' access_control: \u0026quot;{{access_control}}\u0026quot; quota: include_snap_data: False include_data_protection_overhead: False advisory_limit_size: 2 soft_limit_size: 5 hard_limit_size: 10 cap_unit: \u0026quot;GB\u0026quot; quota_state: \u0026quot;present\u0026quot; recursive: \u0026quot;{{recursive}}\u0026quot; state: \u0026quot;{{state_present}}\u0026quot; - name: Create Filesystem in default (system) access zone, without Quota dellemc_powerscale_filesystem: onefs_host: \u0026quot;{{powerscalehost}}\u0026quot; port: \u0026quot;{{powerscaleport}}\u0026quot; verify_ssl: \u0026quot;{{verify_ssl}}\u0026quot; username: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; path: \u0026quot;{{path}}\u0026quot; owner: name: 'ansible_user' provider_type: 'ldap' state: \u0026quot;{{state_present}}\u0026quot; - name: Get filesystem details dellemc_powerscale_filesystem: onefs_host: \u0026quot;{{powerscalehost}}\u0026quot; port: \u0026quot;{{powerscaleport}}\u0026quot; verify_ssl: \u0026quot;{{verify_ssl}}\u0026quot; username: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; access_zone: \u0026quot;{{access_zone}}\u0026quot; path: \u0026quot;{{path}}\u0026quot; state: \u0026quot;{{state_present}}\u0026quot; - name: Get filesystem details with snapshots dellemc_powerscale_filesystem: onefs_host: \u0026quot;{{powerscalehost}}\u0026quot; port: \u0026quot;{{powerscaleport}}\u0026quot; verify_ssl: \u0026quot;{{verify_ssl}}\u0026quot; username: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; access_zone: \u0026quot;{{access_zone}}\u0026quot; path: \u0026quot;{{path}}\u0026quot; list_snapshots: \u0026quot;{{list_snapshots_true}}\u0026quot; state: \u0026quot;{{state_present}}\u0026quot; - name: Modify Filesystem Hard Quota dellemc_powerscale_filesystem: onefs_host: \u0026quot;{{powerscalehost}}\u0026quot; port: \u0026quot;{{powerscaleport}}\u0026quot; verify_ssl: \u0026quot;{{verify_ssl}}\u0026quot; username: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; path: \u0026quot;{{path}}\u0026quot; access_zone: \u0026quot;{{access_zone}}\u0026quot; quota: hard_limit_size: 15 cap_unit: \u0026quot;GB\u0026quot; quota_state: \u0026quot;present\u0026quot; state: \u0026quot;{{state_present}}\u0026quot; - name: Modify Filesystem Owner, Group and ACL dellemc_powerscale_filesystem: onefs_host: \u0026quot;{{powerscalehost}}\u0026quot; port: \u0026quot;{{powerscaleport}}\u0026quot; verify_ssl: \u0026quot;{{verify_ssl}}\u0026quot; username: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; path: \u0026quot;{{path}}\u0026quot; access_zone: \u0026quot;{{access_zone}}\u0026quot; owner: name: 'ansible_user' provider_type: 'ldap' group: name: 'ansible_group' provider_type: 'ldap' access_control: \u0026quot;{{new_access_control}}\u0026quot; state: \u0026quot;{{state_present}}\u0026quot; - name: Remove Quota from FS dellemc_powerscale_filesystem: onefs_host: \u0026quot;{{onefs_host}}\u0026quot; verify_ssl: \u0026quot;{{verify_ssl}}\u0026quot; api_user: \u0026quot;{{api_user}}\u0026quot; api_password: \u0026quot;{{api_password}}\u0026quot; path: \u0026quot;{{path}}\u0026quot; access_zone: \u0026quot;{{access_zone}}\u0026quot; quota: quota_state: \u0026quot;absent\u0026quot; state: \u0026quot;{{state_present}}\u0026quot; - name: Delete filesystem dellemc_powerscale_filesystem: onefs_host: \u0026quot;{{powerscalehost}}\u0026quot; port: \u0026quot;{{powerscaleport}}\u0026quot; verify_ssl: \u0026quot;{{verify_ssl}}\u0026quot; username: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; access_zone: \u0026quot;{{access_zone}}\u0026quot; path: \u0026quot;{{path}}\u0026quot; state: \u0026quot;{{state_absent}}\u0026quot; Return Values   Key Type Returned Description   changed   bool   always   Whether or not the resource has changed    filesystem_details   complex   When Filesystem exists.   The filesystem details.    \u0026nbsp; attrs   dict  success  The attributes of the filesystem.    filesystem_snapshots   complex   When list_snapshots is True.   The filesystem snapshot details.    \u0026nbsp; created   int  success  The creation timestamp.    \u0026nbsp; expires   int  success  The expiration timestamp.    \u0026nbsp; id   int  success  The id of the snapshot.    \u0026nbsp; name   str  success  The name of the snapshot.    \u0026nbsp; path   str  success  The path of the snapshot.    quota_details   complex   When Quota exists.   The quota details.    \u0026nbsp; enforced   bool  success  Whether the Quota is enforced.    \u0026nbsp; id   str  success  The ID of the Quota.    \u0026nbsp; type   str  success  The type of Quota.    \u0026nbsp; usage   dict  success  The Quota usage.    Authors  Prashant Rakheja (@prashant-dell) ansible.team@dell.com   ","excerpt":"Ansible Modules for Dell EMC PowerScale Product Guide 1.2.0 © 2021 Dell Inc. or its subsidiaries. …","ref":"/ansible-docs/docs/storage/platforms/powerscale/product_guide/","title":"Product Guide"},{"body":"Ansible Modules for Dell EMC PowerStore Product Guide 1.2 © 2021 Dell Inc. or its subsidiaries. All rights reserved. Dell, EMC, and other trademarks are trademarks of Dell Inc. or its subsidiaries. Other trademarks may be trademarks of their respective owners.\n Contents  File System Module  Synopsis Parameters Notes Examples Return Values Authors   Volume Module  Synopsis Parameters Notes Examples Return Values Authors   Quota Module  Synopsis Parameters Notes Examples Return Values Authors   Host Module  Synopsis Parameters Examples Return Values Authors   Snapshot Rule Module  Synopsis Parameters Examples Return Values Authors   Gatherfacts Module  Synopsis Parameters Examples Return Values Authors   Replication Session Module  Synopsis Parameters Notes Examples Return Values Authors   Host Group Module  Synopsis Parameters Examples Return Values Authors   NFS Module  Synopsis Parameters Examples Return Values Authors   Volume Group Module  Synopsis Parameters Notes Examples Return Values Authors   NAS Server Module  Synopsis Parameters Examples Return Values Authors   SMB Share Module  Synopsis Parameters Notes Examples Return Values Authors   Snapshot Module  Synopsis Parameters Examples Return Values Authors   Replication Rule Module  Synopsis Parameters Examples Return Values Authors   Protection Policy Module  Synopsis Parameters Notes Examples Return Values Authors   Filesystem Snapshot Module  Synopsis Parameters Examples Return Values Authors     File System Module Filesystem operations on PowerStore Storage system\nSynopsis Supports the provisioning operations on a filesystem such as create, modify, delete and get the details of a filesystem.\nParameters   Parameter Type Required Default Choices Description   filesystem_name  str      Name of the file system. Mutually exclusive with filesystem_id. Mandatory only for create operation.    filesystem_id  str      Unique id of the file system. Mutually exclusive with filesystem_name.    description  str      Description of the file system.    nas_server  str      Name or ID of the NAS Server on which the file system is created. Mandatory parameter whenever filesystem_name is provided, since filesystem names are unique only within a NAS server    size  int      Size that the file system presents to the host or end user. Mandatory only for create operation.    cap_unit  str      GB TB   capacity unit for the size. It defaults to 'GB', if not specified.    access_policy  str      NATIVE UNIX WINDOWS   File system security access policies.    locking_policy  str      ADVISORY MANDATORY   File system locking policies. ADVISORY- No lock checking for NFS and honor SMB lock range only for SMB. MANDATORY- Honor SMB and NFS lock range.    folder_rename_policy  str      ALL_ALLOWED SMB_FORBIDDEN ALL_FORBIDDEN   File system folder rename policies for the file system with multi-protocol access enabled. ALL_ALLOWED - All protocols are allowed to rename directories without any restrictions. SMB_FORBIDDEN - A directory rename from the SMB protocol will be denied if at least one file is opened in the directory or in one of its child directories. All_FORBIDDEN - Any directory rename request will be denied regardless of the protocol used, if at least one file is opened in the directory or in one of its child directories.    smb_properties  dict      Advance settings for SMB. It contains below optional candidate variables    \u0026nbsp; is_smb_sync_writes_enabled   bool   False     Indicates whether the synchronous writes option is enabled on the file system.    \u0026nbsp; is_smb_no_notify_enabled   bool   False     Indicates whether notifications of changes to directory file structure are enabled.    \u0026nbsp; is_smb_op_locks_enabled   bool   False     Indicates whether opportunistic file locking is enabled on the file system.    \u0026nbsp; is_smb_notify_on_access_enabled   bool   False     Indicates whether file access notifications are enabled on the file system.    \u0026nbsp; is_smb_notify_on_write_enabled   bool   False     Indicates whether file write notifications are enabled on the file system    \u0026nbsp; smb_notify_on_change_dir_depth   int   False     Integer variable , determines the lowest directory level to which the enabled notifications apply. minimum value is 1.    protection_policy  str      Name or ID of the protection policy applied to the file system. Specifying \"\" (empty string) removes the existing protection policy from file system.    quota_defaults  dict      Contains the default attributes for a filesystem quota.It contains below optional candidate variables.    \u0026nbsp; grace_period   int   False     Grace period of soft limit.    \u0026nbsp; grace_period_unit   str   False     days weeks months   Unit of the grace period of soft limit.    \u0026nbsp; default_hard_limit   int   False     Default hard limit of user quotas and tree quotas.    \u0026nbsp; default_soft_limit   int   False     Default soft limit of user quotas and tree quotas.    \u0026nbsp; cap_unit   str   False     GB TB   Capacity unit for default hard \u0026 soft limit.    state  str   True     absent present   Define whether the filesystem should exist or not.    array_ip  str   True     IP or FQDN of the PowerStore management system.    verifycert  bool   True     True False   Boolean variable to specify whether to validate SSL certificate or not. True - indicates that the SSL certificate should be verified. Set the environment variable REQUESTS_CA_BUNDLE to the path of the SSL certificate. False - indicates that the SSL certificate should not be verified.    user  str   True     The username of the PowerStore host.    password  str   True     The password of the PowerStore host.    Notes  It is recommended to remove the protection policy before deleting the filesystem.  Examples  - name: Create FileSystem by Name register: result_fs dellemc_powerstore_filesystem: array_ip: \u0026quot;{{array_ip}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; filesystem_name: \u0026quot;{{filesystem_name}}\u0026quot; description: \u0026quot;{{description}}\u0026quot; nas_server: \u0026quot;{{nas_server_id}}\u0026quot; size: \u0026quot;5\u0026quot; cap_unit: \u0026quot;GB\u0026quot; access_policy: \u0026quot;UNIX\u0026quot; locking_policy: \u0026quot;MANDATORY\u0026quot; smb_properties: is_smb_no_notify_enabled: True is_smb_notify_on_access_enabled: True quota_defaults: grace_period: 1 grace_period_unit: 'days' default_hard_limit: 3 default_soft_limit: 2 protection_policy: \u0026quot;{{protection_policy_id}}\u0026quot; state: \u0026quot;present\u0026quot; - name: Modify File System by id dellemc_powerstore_filesystem: array_ip: \u0026quot;{{array_ip}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; filesystem_id: \u0026quot;{{fs_id}}\u0026quot; folder_rename_policy: \u0026quot;ALL_ALLOWED\u0026quot; smb_properties: is_smb_op_locks_enabled: True smb_notify_on_change_dir_depth: 3 quota_defaults: grace_period: 2 grace_period_unit: 'weeks' default_hard_limit: 2 default_soft_limit: 1 state: \u0026quot;present\u0026quot; - name: Get File System details by id dellemc_powerstore_filesystem: array_ip: \u0026quot;{{array_ip}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; filesystem_id: \u0026quot;{{result_fs.filesystem_details.id}}\u0026quot; state: \u0026quot;present\u0026quot; - name: Delete File System by id dellemc_powerstore_filesystem: array_ip: \u0026quot;{{array_ip}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; filesystem_id: \u0026quot;{{result_fs.filesystem_details.id}}\u0026quot; state: \u0026quot;absent\u0026quot; Return Values   Key Type Returned Description   changed   bool   always   Whether or not the resource has changed    filesystem_details   complex   When filesystem exists   Details of the filesystem    \u0026nbsp; access_policy   str  success  Access policy about the filesystem.    \u0026nbsp; default_hard_limit   int  success  Default hard limit period for a filesystem quota in byte.    \u0026nbsp; default_soft_limit   int  success  Default soft limit period for a filesystem quota in byte.    \u0026nbsp; description   str  success  The description about the filesystem.    \u0026nbsp; grace_period   int  success  Default grace period for a filesystem quota in second.    \u0026nbsp; id   str  success  The system generated ID given to the filesystem.    \u0026nbsp; is_smb_no_notify_enabled   bool  success  Whether smb notify policy is enabled for a filesystem.    \u0026nbsp; is_smb_notify_on_access_enabled   bool  success  Whether smb on access notify policy is enabled.    \u0026nbsp; is_smb_op_locks_enabled   bool  success  Whether smb op lock is enabled.    \u0026nbsp; locking_policy   str  success  Locking policy about the filesystem.    \u0026nbsp; name   str  success  Name of the filesystem.    \u0026nbsp; nas_server   dict  success  Id and name of the nas server to which the filesystem belongs.    \u0026nbsp; protection_policy   dict  success  Id and name of the protection policy associated with the filesystem.    \u0026nbsp; size_total   int  success  Total size of the filesystem in bytes.    \u0026nbsp; size_used   int  success  Used size of the filesystem in bytes.    \u0026nbsp; snapshots   list  success  Id and name of the snapshots of a filesystem.    \u0026nbsp; total_size_with_unit   str  success  Total size of the filesystem with appropriate unit.    \u0026nbsp; used_size_with_unit   str  success  Used size of the filesystem with appropriate unit.    Authors  Arindam Datta (@dattaarindam) ansible.team@dell.com   Volume Module Manage volumes on a PowerStore storage system.\nSynopsis Managing volume on PowerStore storage system includes create volume, get details of volume, modify name, size, description, protection policy, performance policy, map or unmap volume to host/host group, and delete volume.\nParameters   Parameter Type Required Default Choices Description   vol_name  str      Unique name of the volume. This value must contain 128 or fewer printable unicode characters. Required when creating a volume. All other functionalities on a volume are supported using volume name or ID.    vg_name  str      The name of the volume group. A volume can optionally be assigned to a volume group at the time of creation. Use the Volume Group Module for modification of the assignment.    vol_id  str      The 36 character long ID of the volume, automatically generated when a volume is created. Cannot be used while creating a volume. All other functionalities on a volume are supported using volume name or ID.    size  float      Size of the volume. Minimum volume size is 1MB. Maximum volume size is 256TB. Size must be a multiple of 8192. Required in case of create and expand volume.    cap_unit  str      MB GB TB   Volume size unit. Used to signify unit of the size provided for creation and expansion of volume. It defaults to 'GB', if not specified.    new_name  str      The new volume name for the volume, used in case of rename functionality.    description  str      Description for the volume. Optional parameter when creating a volume. To modify, pass the new value in description field.    protection_policy  str      The protection_policy of the volume. To represent policy, both name or ID can be used interchangably. The module will detect both. A volume can be assigned a protection policy at the time of creation of volume or later as well. The policy can also be changed for a given volume by simply passing the new value. The policy can be removed by passing an empty string. Check examples for more clarity.    performance_policy  str      high medium low   The performance_policy for the volume. A volume can be assigned a performance policy at the time of creation of the volume, or later as well. The policy can also be changed for a given volume, by simply passing the new value. Check examples for more clarity. If not given, performance policy will be 'medium'.    host  str      Host to be mapped/unmapped to a volume. If not specified, an unmapped volume is created. Only one of the host or host group can be supplied in one call. To represent host, both name or ID can be used interchangeably. The module will detect both.    hostgroup  str      Hostgroup to be mapped/unmapped to a volume. If not specified, an unmapped volume is created. Only one of the host or host group can be mapped in one call. To represent a hostgroup, both name or ID can be used interchangeably. The module will detect both.    mapping_state  str      mapped unmapped   Define whether the volume should be mapped to a host or hostgroup. mapped - indicates that the volume should be mapped to the host or host group. unmapped - indicates that the volume should not be mapped to the host or host group. Only one of a host or host group can be supplied in one call.    hlu  int      Logical unit number for the host/host group volume access. Optional parameter when mapping a volume to host/host group. HLU modification is not supported.    state  str   True     absent present   Define whether the volume should exist or not. present - indicates that the volume should exist on the system. absent - indicates that the volume should not exist on the system.    array_ip  str   True     IP or FQDN of the PowerStore management system.    verifycert  bool   True     True False   Boolean variable to specify whether to validate SSL certificate or not. True - indicates that the SSL certificate should be verified. Set the environment variable REQUESTS_CA_BUNDLE to the path of the SSL certificate. False - indicates that the SSL certificate should not be verified.    user  str   True     The username of the PowerStore host.    password  str   True     The password of the PowerStore host.    Notes  To create a new volume, vol_name and size is required. cap_unit, description, vg_name, performance_policy, and protection_policy are optional. new_name should not be provided when creating a new volume. size is a required parameter for expand volume. Clones or Snapshots of a deleted production volume or a clone are not deleted. A volume that is attached to a host/host group, or that is part of a volume group cannot be deleted.  Examples - name: Create stand-alone volume dellemc_powerstore_volume: array_ip: \u0026quot;{{array_ip}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; vol_name: \u0026quot;{{vol_name}}\u0026quot; size: 1 cap_unit: \u0026quot;{{cap_unit}}\u0026quot; state: 'present' - name: Create stand-alone volume with performance and protection policy dellemc_powerstore_volume: array_ip: \u0026quot;{{array_ip}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; vol_name: \u0026quot;{{vol_name}}\u0026quot; size: 5 cap_unit: \u0026quot;{{cap_unit}}\u0026quot; state: 'present' description: 'Description' performance_policy: 'low' protection_policy: 'protection_policy_name' - name: Create volume and assign to a volume group dellemc_powerstore_volume: array_ip: \u0026quot;{{array_ip}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; vol_name: \u0026quot;{{vol_name}}\u0026quot; vg_name: \u0026quot;{{vg_name}}\u0026quot; size: 1 cap_unit: \u0026quot;{{cap_unit}}\u0026quot; state: 'present' - name: Create volume and map it to a host dellemc_powerstore_volume: array_ip: \u0026quot;{{array_ip}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; vol_name: \u0026quot;{{vol_name}}\u0026quot; size: 1 cap_unit: \u0026quot;{{cap_unit}}\u0026quot; mapping_state: 'mapped' host: \u0026quot;{{host_name}}\u0026quot; state: 'present' - name: Get volume details using ID dellemc_powerstore_volume: array_ip: \u0026quot;{{array_ip}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; vol_id: \u0026quot;{{result.volume_details.id}}\u0026quot; state: \u0026quot;present\u0026quot; - name: Get volume details using name dellemc_powerstore_volume: array_ip: \u0026quot;{{array_ip}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; vol_name: \u0026quot;{{vol_name}}\u0026quot; state: \u0026quot;present\u0026quot; - name: Modify volume size, name, description and performance policy dellemc_powerstore_volume: array_ip: \u0026quot;{{array_ip}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; new_name: \u0026quot;{{new_name}}\u0026quot; vol_name: \u0026quot;{{vol_name}}\u0026quot; state: \u0026quot;present\u0026quot; size: 2 performance_policy: 'high' description: 'new description' - name: Remove protection policy from Volume dellemc_powerstore_volume: array_ip: \u0026quot;{{array_ip}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; new_name: \u0026quot;{{new_name}}\u0026quot; vol_name: \u0026quot;{{vol_name}}\u0026quot; state: \u0026quot;present\u0026quot; protection_policy: \u0026quot;\u0026quot; - name: Map volume to a host with HLU dellemc_powerstore_volume: array_ip: \u0026quot;{{array_ip}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; vol_name: \u0026quot;{{vol_name}}\u0026quot; state: 'present' mapping_state: 'mapped' host: 'host1' hlu: 12 - name: Map volume to a host without HLU dellemc_powerstore_volume: array_ip: \u0026quot;{{array_ip}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; vol_name: \u0026quot;{{vol_name}}\u0026quot; state: 'present' mapping_state: 'mapped' host: 'host2' - name: Delete volume dellemc_powerstore_volume: array_ip: \u0026quot;{{array_ip}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; vol_id: \u0026quot;{{result.volume_details.id}}\u0026quot; state: \u0026quot;absent\u0026quot; Return Values   Key Type Returned Description   add_vols_to_vg   bool   When value exists   A boolean flag to indicate whether volume/s got added to volume group    changed   bool   always   Whether or not the resource has changed    create_vg   bool   When value exists   A boolean flag to indicate whether volume group got created    delete_vg   bool   When value exists   A boolean flag to indicate whether volume group got deleted    modify_vg   bool   When value exists   A boolean flag to indicate whether volume group got modified    remove_vols_from_vg   bool   When value exists   A boolean flag to indicate whether volume/s got removed from volume group    volume_group_details   complex   When volume group exists   Details of the volume group    \u0026nbsp; description   str  success  ['description about the volume group']    \u0026nbsp; id   str  success  ['The system generated ID given to the volume group']    \u0026nbsp; is_write_order_consistent   bool  success  ['A boolean flag to indicate whether snapshot sets of the volume group will be write-order consistent']    \u0026nbsp; name   str  success  ['Name of the volume group']    \u0026nbsp; protection_policy_id   str  success  ['The protection policy of the volume group']    \u0026nbsp; type   str  success  ['The type of the volume group']    \u0026nbsp; volumes   complex  success  ['The volumes details of the volume group']    \u0026nbsp; \u0026nbsp; id   str  success  ['The system generated ID given to the volume associated with the volume group']    \u0026nbsp; \u0026nbsp; name   str  success  ['The name of the volume associated with the volume group.']    Authors  Ambuj Dubey (@AmbujDube) ansible.team@dell.com Manisha Agrawal (@agrawm3) ansible.team@dell.com   Quota Module Manage Tree Quotas and User Quotas on PowerStore.\nSynopsis Managing Quotas on Powerstore storage system includes getting details, modifying, creating and deleting Quotas.\nParameters   Parameter Type Required Default Choices Description   path  str      The path on which the quota will be imposed. Path is relative to the root of the filesystem. For user quota, if path is not specified, quota will be created at the root of the filesystem.    quota_type  str      user tree   The type of quota which will be imposed.    quota_id  str      Id of the user/tree quota. If quota_id is mentioned, then path/nas_server/file_system/quota_type is not required.    filesystem  str      The ID/Name of the filesystem for which the Tree/User Quota will be created. If filesystem name is specified, then nas_server is required to uniquely identify the filesystem.    nas_server  str      The NAS server. This could be the name or ID of the NAS server.    description  str      Additional information that can be mentioned for a Tree Quota. Description parameter can only be used when quota_type is 'tree'    unix_name  str      The name of the unix user account for which quota operations will be performed. Any one among uid/unix_name/windows_name/windows_sid is required when quota_type is 'user'.    windows_name  str      The name of the Windows User for which quota operations will be performed. The name should be mentioned along with Domain Name as 'DOMAIN_NAME\\user_name' or as \"DOMAIN_NAME\\\\user_name\". Any one among uid/unix_name/windows_name/windows_sid is required when quota_type is 'user'.    uid  int      The ID of the unix user account for which quota operations will be performed. Any one among uid/unix_name/windows_name/windows_sid is required when quota_type is 'user'.    windows_sid  str      The SID of the Windows User account for which quota operations will be performed. Any one among uid/unix_name/windows_name/windows_sid is required when quota_type is 'user'.    quota  dict      Specifies Quota parameters.    \u0026nbsp; soft_limit   int      Soft limit of the User/Tree quota. No Soft limit when set to 0.    \u0026nbsp; hard_limit   int      Hard limit of the user quota. No hard limit when set to 0.    \u0026nbsp; cap_unit   str    GB    GB TB   Unit of storage for the hard and soft limits. This parameter is required if limit is specified.    state  str   True     absent present   Define whether the Quota should exist or not. present indicates that the Quota should exist on the system. absent indicates that the Quota should not exist on the system.    array_ip  str   True     IP or FQDN of the PowerStore management system.    verifycert  bool   True     True False   Boolean variable to specify whether to validate SSL certificate or not. True - indicates that the SSL certificate should be verified. Set the environment variable REQUESTS_CA_BUNDLE to the path of the SSL certificate. False - indicates that the SSL certificate should not be verified.    user  str   True     The username of the PowerStore host.    password  str   True     The password of the PowerStore host.    Notes  Tree quota can not be created at the root of the filesystem. When the ID of the filesystem is passed then nas_server is not required. If passed, then filesystem should exist for the nas_server, else the task will fail. If a primary directory of the current directory or a subordinate directory of the path is having a Tree Quota configured, then the quota for that path can\u0026rsquo;t be created. Hierarchical tree quotas are not allowed. When the first quota is created for a directory/user in a filesystem then the quotas will be enabled for that filesystem automatically. If a user quota is to be created on a tree quota, then the user quotas will be enabled automatically in a tree quota. Delete User Quota operation is not supported.  Examples  - name: Create a Quota for a User using unix name dellemc_powerstore_quota: array_ip: \u0026quot;{{array_ip}}\u0026quot; verifycert: \u0026quot;{{verify_cert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; quota_type: \u0026quot;user\u0026quot; unix_name: \u0026quot;{{unix_name}}\u0026quot; filesystem: \u0026quot;sample_fs\u0026quot; nas_server: \u0026quot;{{nas_server_id}}\u0026quot; quota: soft_limit: 5 hard_limit: 10 cap_unit: \u0026quot;TB\u0026quot; state: \u0026quot;present\u0026quot; - name: Create a Tree Quota dellemc_powerstore_quota: array_ip: \u0026quot;{{array_ip}}\u0026quot; verifycert: \u0026quot;{{verify_cert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; quota_type: \u0026quot;tree\u0026quot; path: \u0026quot;/home\u0026quot; filesystem: \u0026quot;sample_fs\u0026quot; nas_server: \u0026quot;sample_nas_server\u0026quot; quota: soft_limit: 5 hard_limit: 10 cap_unit: \u0026quot;TB\u0026quot; state: \u0026quot;present\u0026quot; - name: Modify attributes for Tree Quota dellemc_powerstore_quota: array_ip: \u0026quot;{{array_ip}}\u0026quot; verifycert: \u0026quot;{{verify_cert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; quota_id: \u0026quot;{{quota_id}}\u0026quot; quota: soft_limit: 10 hard_limit: 15 cap_unit: \u0026quot;TB\u0026quot; state: \u0026quot;present\u0026quot; - name: Get details of User Quota dellemc_powerstore_quota: array_ip: \u0026quot;{{array_ip}}\u0026quot; verifycert: \u0026quot;{{verify_cert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; quota_type: \u0026quot;user\u0026quot; uid: 100 path: \u0026quot;/home\u0026quot; filesystem: \u0026quot;{{filesystem_id}}\u0026quot; state: \u0026quot;present\u0026quot; - name: Get details of Tree Quota dellemc_powerstore_quota: array_ip: \u0026quot;{{array_ip}}\u0026quot; verifycert: \u0026quot;{{verify_cert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; quota_id: \u0026quot;{{quota_id}}\u0026quot; state: \u0026quot;present\u0026quot; - name: Delete a Tree Quota dellemc_powerstore_quota: array_ip: \u0026quot;{{array_ip}}\u0026quot; verifycert: \u0026quot;{{verify_cert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; quota_type: \u0026quot;tree\u0026quot; path: \u0026quot;/home\u0026quot; filesystem: \u0026quot;sample_fs\u0026quot; nas_server: \u0026quot;sample_nas_server\u0026quot; state: \u0026quot;absent\u0026quot; Return Values   Key Type Returned Description   changed   bool   always   Whether or not the resource has changed    quota_details   complex   When Quota exists.   The quota details.    \u0026nbsp; description   str  success  ['Additional information about the tree quota.', 'Only applicable for Tree Quotas.']    \u0026nbsp; file_system   complex  success  Includes ID and Name of filesystem and nas server for which smb share exists.    \u0026nbsp; \u0026nbsp; filesystem_type   str  success  Type of filesystem.    \u0026nbsp; \u0026nbsp; id   str  success  ID of filesystem.    \u0026nbsp; \u0026nbsp; name   str  success  Name of filesystem.    \u0026nbsp; \u0026nbsp; nas_server   dict  success  nas_server of filesystem.    \u0026nbsp; hard_limit(cap_unit)   int  success  Value of the Hard Limit imposed on the quota.    \u0026nbsp; id   str  success  The ID of the Quota.    \u0026nbsp; remaining_grace_period   int  success  The time period remaining after which the grace period will expire.    \u0026nbsp; size_used   int  success  Size currently consumed by Tree/User on the filesystem.    \u0026nbsp; soft_limit(cap_unit)   int  success  Value of the Soft Limit imposed on the quota.    \u0026nbsp; state   str  success  ['State of the user quota or tree quota record period.', 'OK means No quota limits are exceeded.', 'Soft_Exceeded means Soft limit is exceeded, and grace period is not expired.', 'Soft_Exceeded_And_Expired means Soft limit is exceeded, and grace period is expired.', 'Hard_Reached means Hard limit is reached.']    \u0026nbsp; state_l10n   str  success  Localized message string corresponding to state.    \u0026nbsp; tree_quota_for_user_quota   complex  success  ['Additional Information of Tree Quota limits on which user quota exists.', 'Only applicable for User Quotas']    \u0026nbsp; \u0026nbsp; description   str  success  Description of Tree Quota for user quota.    \u0026nbsp; \u0026nbsp; hard_limit(cap_unit)   int  success  Value of the Hard Limit imposed on the quota.    \u0026nbsp; \u0026nbsp; path   str  success  The path on which the quota will be imposed.    \u0026nbsp; tree_quota_id   str  success  ['ID of the Tree Quota on which the specific User Quota exists.', 'Only applicable for user quotas.']    \u0026nbsp; uid   int  success  ['The ID of the unix host for which user quota exists.', 'Only applicable for user quotas.']    \u0026nbsp; unix_name   str  success  ['The Name of the unix host for which user quota exists.', 'Only applicable for user quotas.']    \u0026nbsp; windows_name   str  success  ['The Name of the Windows host for which user quota exists.', 'Only applicable for user quotas.']    \u0026nbsp; windows_sid   str  success  ['The SID of the windows host for which user quota exists.', 'Only applicable for user quotas.']    Authors  P Srinivas Rao (@srinivas-rao5) ansible.team@dell.com   Host Module Manage host on PowerStore storage system.\nSynopsis Managing host on PowerStore storage system includes create host with a set of initiators, add/remove initiators from host, rename host and delete host.\nParameters   Parameter Type Required Default Choices Description   host_name  str      The host name. This value must contain 128 or fewer printable Unicode characters. Creation of an empty host is not allowed. Required when creating a host. Use either host_id or host_name for modify and delete tasks.    host_id  str      The 36 character long host id automatically generated when a host is created. Use either host_id or host_name for modify and delete tasks. host_id cannot be used while creating host, as it is generated by the array after creation of host.    os_type  str      Windows Linux ESXi AIX HP-UX Solaris   Operating system of the host. Required when creating a host OS type cannot be modified for a given host.    initiators  list elements: str      List of Initiator WWN or IQN to be added or removed from the host. Subordinate initiators in a host can only be of one type, either FC or iSCSI. Required when creating a host.    state  str   True     absent present   Define whether the host should exist or not. present - indicates that the host should exist in system. absent - indicates that the host should not exist in system.    initiator_state  str      present-in-host absent-in-host   Define whether the initiators should be present or absent in host. present-in-host - indicates that the initiators should exist on host. absent-in-host - indicates that the initiators should not exist on host. Required when creating a host with initiators or adding/removing initiators to/from existing host.    new_name  str      The new name of host for renaming function. This value must contain 128 or fewer printable Unicode characters. Cannot be specified when creating a host.    array_ip  str   True     IP or FQDN of the PowerStore management system.    verifycert  bool   True     True False   Boolean variable to specify whether to validate SSL certificate or not. True - indicates that the SSL certificate should be verified. Set the environment variable REQUESTS_CA_BUNDLE to the path of the SSL certificate. False - indicates that the SSL certificate should not be verified.    user  str   True     The username of the PowerStore host.    password  str   True     The password of the PowerStore host.    Examples  - name: Create host dellemc_powerstore_host: array_ip: \u0026quot;{{array_ip}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; host_name: \u0026quot;{{host_name}}\u0026quot; os_type: 'Windows' initiators: -21:00:00:24:ff:31:e9:fc state: 'present' initiator_state: 'present-in-host' - name: Get host details by name dellemc_powerstore_host: array_ip: \u0026quot;{{array_ip}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; host_name: \u0026quot;{{host_name}}\u0026quot; state: 'present' - name: Get host details by id dellemc_powerstore_host: array_ip: \u0026quot;{{array_ip}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; host_id: \u0026quot;{{host_id}}\u0026quot; state: 'present' - name: Add initiators to host dellemc_powerstore_host: array_ip: \u0026quot;{{array_ip}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; host_name: \u0026quot;{{host_name}}\u0026quot; initiators: -21:00:00:24:ff:31:e9:ee initiator_state: 'present-in-host' state: 'present' - name: Remove initiators from host dellemc_powerstore_host: array_ip: \u0026quot;{{array_ip}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; host_name: \u0026quot;{{host_name}}\u0026quot; initiators: -21:00:00:24:ff:31:e9:ee initiator_state: 'absent-in-host' state: 'present' - name: Rename host dellemc_powerstore_host: array_ip: \u0026quot;{{array_ip}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; host_name: \u0026quot;{{host_name}}\u0026quot; new_name: \u0026quot;{{new_host_name}}\u0026quot; state: 'present' - name: Delete host dellemc_powerstore_host: array_ip: \u0026quot;{{array_ip}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; host_name: \u0026quot;{{new_host_name}}\u0026quot; state: 'absent' Return Values   Key Type Returned Description   changed   bool   always   Whether or not the resource has changed    hostgroup_details   complex   When host group exists   Details of the host group    \u0026nbsp; description   str  success  Description about the host group    \u0026nbsp; hosts   complex  success  The hosts details which are part of this host group    \u0026nbsp; \u0026nbsp; id   str  success  The ID of the host    \u0026nbsp; \u0026nbsp; name   str  success  The name of the host    \u0026nbsp; id   str  success  The system generated ID given to the host group    \u0026nbsp; name   str  success  Name of the host group    Authors  Manisha Agrawal (@agrawm3) ansible.team@dell.com   Snapshot Rule Module SnapshotRule operations on a PowerStore storage system.\nSynopsis Performs all snapshot rule operations on PowerStore Storage System. This modules supports get details of an existing snapshot rule, create new Snapshot Rule with Interval, create new Snapshot Rule with specific time and days_of_week with all supported. parameters. Modify Snapshot Rule with supported parameters. Delete a specific Snapshot Rule.\nParameters   Parameter Type Required Default Choices Description   name  str      String variable. Indicates the name of the Snapshot rule.    snapshotrule_id  str      String variable. Indicates the ID of the Snapshot rule.    new_name  str      String variable. Indicates the new name of the Snapshot rule. Used for renaming operation    days_of_week  list elements: str      Monday Tuesday Wednesday Thursday Friday Saturday Sunday   List of strings to specify days of the week on which the Snapshot rule. should be applied. Must be applied for Snapshot rules where the 'time_of_day' parameter is set. Optional for the Snapshot rule created with an interval. When 'days_of_week' is not specified for a new Snapshot rule, the rule is applied on every day of the week.    interval  str      Five_Minutes Fifteen_Minutes Thirty_Minutes One_Hour Two_Hours Three_Hours Four_Hours Six_Hours Eight_Hours Twelve_Hours One_Day   String variable. Indicates the interval between Snapshots. When creating a Snapshot rule, specify either \"interval\" or \"time_of_day\", but not both.    desired_retention  int      Integer variable. Indicates the desired Snapshot retention period. It is required when creating a new Snapshot rule.    time_of_day  str      String variable. Indicates the time of the day to take a daily Snapshot, with the format \"hh:mm\" in 24 hour time format When creating a Snapshot rule, specify either \"interval\"or \"time_of_day\" but not both.    delete_snaps  bool      Boolean variable to specify whether all Snapshots previously created by this rule should also be deleted when this rule is removed. True specifies to delete all previously created Snapshots by this rule while deleting this rule. False specifies to retain all previously created Snapshots while deleting this rule    state  str   True     present absent   String variable indicates the state of Snapshot rule. For \"Delete\" operation only, it should be set to \"absent\". For all Create, Modify or Get details operation it should be set to \"present\".    array_ip  str   True     IP or FQDN of the PowerStore management system.    verifycert  bool   True     True False   Boolean variable to specify whether to validate SSL certificate or not. True - indicates that the SSL certificate should be verified. Set the environment variable REQUESTS_CA_BUNDLE to the path of the SSL certificate. False - indicates that the SSL certificate should not be verified.    user  str   True     The username of the PowerStore host.    password  str   True     The password of the PowerStore host.    Examples - name: Get details of an existing snapshot rule by name dellemc_powerstore_snapshotrule: array_ip: \u0026quot;{{array_ip}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; name: \u0026quot;{{name}}\u0026quot; state: \u0026quot;present\u0026quot; - name: Get details of an existing snapshot rule by id dellemc_powerstore_snapshotrule: array_ip: \u0026quot;{{array_ip}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; snapshotrule_id: \u0026quot;{{snapshotrule_id}}\u0026quot; state: \u0026quot;present\u0026quot; - name: Create new snapshot rule by interval dellemc_powerstore_snapshotrule: array_ip: \u0026quot;{{array_ip}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; name: \u0026quot;{{name}}\u0026quot; interval: \u0026quot;{{interval}}\u0026quot; days_of_week: - Monday desired_retention: \u0026quot;{{desired_retention}}\u0026quot; state: \u0026quot;present\u0026quot; - name: Create new snapshot rule by time_of_day and days_of_week dellemc_powerstore_snapshotrule: array_ip: \u0026quot;{{array_ip}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; name: \u0026quot;{{name}}\u0026quot; desired_retention: \u0026quot;{{desired_retention}}\u0026quot; days_of_week: - Monday - Wednesday - Friday time_of_day: \u0026quot;{{time_of_day}}\u0026quot; state: \u0026quot;present\u0026quot; - name: Modify existing snapshot rule to time_of_day and days_of_week dellemc_powerstore_snapshotrule: array_ip: \u0026quot;{{array_ip}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; name: \u0026quot;{{name}}\u0026quot; days_of_week: - Monday - Wednesday - Friday - Sunday time_of_day: \u0026quot;{{time_of_day}}\u0026quot; state: \u0026quot;present\u0026quot; - name: Modify existing snapshot rule to interval dellemc_powerstore_snapshotrule: array_ip: \u0026quot;{{array_ip}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; name: \u0026quot;{{name}}\u0026quot; interval: \u0026quot;{{interval}}\u0026quot; state: \u0026quot;present\u0026quot; - name: Delete an existing snapshot rule by name dellemc_powerstore_snapshotrule: array_ip: \u0026quot;{{array_ip}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; name: \u0026quot;{{name}}\u0026quot; state: \u0026quot;absent\u0026quot; Return Values   Key Type Returned Description   changed   bool   always   Whether or not the resource has changed    snapshotrule_details   complex   When snapshot rule exists   Details of the snapshot rule    \u0026nbsp; days_of_week   list  success  List of string to specify days of the week on which the rule should be applied    \u0026nbsp; desired_retention   int  success  Desired snapshot retention period    \u0026nbsp; id   str  success  The system generated ID given to the snapshot rule    \u0026nbsp; interval   str  success  The interval between snapshots    \u0026nbsp; name   str  success  Name of the snapshot rule    \u0026nbsp; policies   complex  success  The protection policies details of the snapshot rule    \u0026nbsp; \u0026nbsp; id   str  success  The protection policy ID in which the snapshot rule is selected    \u0026nbsp; \u0026nbsp; name   str  success  Name of the protection policy in which the snapshot rule is selected    \u0026nbsp; time_of_day   str  success  The time of the day to take a daily snapshot    Authors  Arindam Datta (@dattaarindam) ansible.team@dell.com   Gatherfacts Module Gathers information about PowerStore Storage entities\nSynopsis Gathers the list of specified PowerStore Storage System entities, such as the list of cluster nodes, volumes, volume groups, hosts, host groups, snapshot rules, protection policies, NAS servers, NFS exports, SMB shares, tree quotas, user quotas, and file systems.\nParameters   Parameter Type Required Default Choices Description   gather_subset  list elements: str   True     vol vg host hg node protection_policy snapshot_rule nas_server nfs_export smb_share tree_quota user_quota file_system replication_rule replication_session remote_system   A list of string variables which specify the PowerStore system entities requiring information.information. vol - volumes node - all the nodes vg - volume groups protection_policy - protection policy host - hosts hg - host groups snapshot_rule - snapshot rule nas_server - NAS servers nfs_export - NFS exports smb_share - SMB shares tree_quota - tree quotas user_quota - user quotas file_system - file systems replication_rule - replication rules replication_session - replication sessions remote_system - remote systems    filters  list elements: dict      A list of filters to support filtered output for storage entities. Each filter is a list of filter_key, filter_operator, filter_value. Supports passing of multiple filters.    \u0026nbsp; filter_key   str   True     Name identifier of the filter.    \u0026nbsp; filter_operator   str   True     equal greater lesser like notequal   Operation to be performed on the filter key.    \u0026nbsp; filter_value   str   True     Value of the filter key.    all_pages  bool    False    Indicates whether to return all available entities on the storage system. If set to True, the Gather Facts module will implement pagination and return all entities. Otherwise, a maximum of the first 100 entities of any type will be returned.    array_ip  str   True     IP or FQDN of the PowerStore management system.    verifycert  bool   True     True False   Boolean variable to specify whether to validate SSL certificate or not. True - indicates that the SSL certificate should be verified. Set the environment variable REQUESTS_CA_BUNDLE to the path of the SSL certificate. False - indicates that the SSL certificate should not be verified.    user  str   True     The username of the PowerStore host.    password  str   True     The password of the PowerStore host.    Examples - name: Get list of volumes, volume groups, hosts, host groups and node dellemc_powerstore_gatherfacts: array_ip: \u0026quot;{{array_ip}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; gather_subset: - vol - vg - host - hg - node - name: Get list of replication related entities dellemc_powerstore_gatherfacts: array_ip: \u0026quot;{{array_ip}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; gather_subset: - replication_rule - replication_session - remote_system - name: Get list of volumes whose state notequal to ready dellemc_powerstore_gatherfacts: array_ip: \u0026quot;{{array_ip}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; gather_subset: - vol filters: - filter_key: \u0026quot;state\u0026quot; filter_operator: \u0026quot;notequal\u0026quot; filter_value: \u0026quot;ready\u0026quot; - name: Get list of protection policies and snapshot rules dellemc_powerstore_gatherfacts: array_ip: \u0026quot;{{array_ip}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; gather_subset: - protection_policy - snapshot_rule - name: Get list of snapshot rules whose desired_retention between 101-499 dellemc_powerstore_gatherfacts: array_ip: \u0026quot;{{array_ip}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; gather_subset: - snapshot_rule filters: - filter_key: \u0026quot;desired_retention\u0026quot; filter_operator: \u0026quot;greater\u0026quot; filter_value: \u0026quot;100\u0026quot; - filter_key: \u0026quot;desired_retention\u0026quot; filter_operator: \u0026quot;lesser\u0026quot; filter_value: \u0026quot;500\u0026quot; - name: Get list of nas server, nfs_export and smb share dellemc_powerstore_gatherfacts: array_ip: \u0026quot;{{array_ip}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; gather_subset: - nas_server - nfs_export - smb_share - name: Get list of tree quota, user quota and file system dellemc_powerstore_gatherfacts: array_ip: \u0026quot;{{array_ip}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; gather_subset: - tree_quota - user_quota - file_system - name: Get list of nas server whose name equal to 'nas_server' dellemc_powerstore_gatherfacts: array_ip: \u0026quot;{{array_ip}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; gather_subset: - nas_server filters: - filter_key: \u0026quot;name\u0026quot; filter_operator: \u0026quot;equal\u0026quot; filter_value: \u0026quot;nas_server\u0026quot; - name: Get list of smb share whose name contains 'share' dellemc_powerstore_gatherfacts: array_ip: \u0026quot;{{array_ip}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; gather_subset: - nas_server filters: - filter_key: \u0026quot;name\u0026quot; filter_operator: \u0026quot;like\u0026quot; filter_value: \u0026quot;*share*\u0026quot; Return Values   Key Type Returned Description   changed   bool   always   Shows whether or not the resource has changed    subset_result   complex   always   Provides details of all given subsets.    \u0026nbsp; Cluster   list  success  Provides details of all clusters.    \u0026nbsp; \u0026nbsp; id   str  success  cluster id    \u0026nbsp; \u0026nbsp; name   str  success  cluster name    \u0026nbsp; FileSystems   list  success  Provides details of all filesystems.    \u0026nbsp; \u0026nbsp; id   str  success  filesystem id    \u0026nbsp; \u0026nbsp; name   str  success  filesystem name    \u0026nbsp; HostGroups   list  success  Provides details of all hostgroups.    \u0026nbsp; \u0026nbsp; id   str  success  hostgroup id    \u0026nbsp; \u0026nbsp; name   str  success  hostgroup name    \u0026nbsp; Hosts   list  success  Provides details of all hosts.    \u0026nbsp; \u0026nbsp; id   str  success  host id    \u0026nbsp; \u0026nbsp; name   str  success  host name    \u0026nbsp; NASServers   list  success  Provides details of all nas servers.    \u0026nbsp; \u0026nbsp; id   str  success  nas server id    \u0026nbsp; \u0026nbsp; name   str  success  nas server name    \u0026nbsp; NFSExports   list  success  Provides details of all nfs exports.    \u0026nbsp; \u0026nbsp; id   str  success  nfs export id    \u0026nbsp; \u0026nbsp; name   str  success  nfs export name    \u0026nbsp; Nodes   list  success  Provides details of all nodes.    \u0026nbsp; \u0026nbsp; id   str  success  node id    \u0026nbsp; \u0026nbsp; name   str  success  node name    \u0026nbsp; ProtectionPolicies   list  success  Provides details of all protectionpolicies.    \u0026nbsp; \u0026nbsp; id   str  success  protectionpolicy id    \u0026nbsp; \u0026nbsp; name   str  success  protectionpolicy name    \u0026nbsp; RemoteSystems   list  success  Provides details of all remote systems.    \u0026nbsp; \u0026nbsp; id   str  success  remote system id    \u0026nbsp; \u0026nbsp; name   str  success  remote system name    \u0026nbsp; ReplicationRules   list  success  Provides details of all replication rules.    \u0026nbsp; \u0026nbsp; id   str  success  replication rule id    \u0026nbsp; \u0026nbsp; name   str  success  replication rule name    \u0026nbsp; ReplicationSession   list  success  details of all replication sessions    \u0026nbsp; \u0026nbsp; id   str  success  replication session id    \u0026nbsp; SMBShares   list  success  Provides details of all smb shares.    \u0026nbsp; \u0026nbsp; id   str  success  smb share id    \u0026nbsp; \u0026nbsp; name   str  success  smb share name    \u0026nbsp; SnapshotRules   list  success  Provides details of all snapshot rules.    \u0026nbsp; \u0026nbsp; id   str  success  snapshot rule id    \u0026nbsp; \u0026nbsp; name   str  success  snapshot rule name    \u0026nbsp; TreeQuotas   list  success  Provides details of all tree quotas.    \u0026nbsp; \u0026nbsp; id   str  success  tree quota id    \u0026nbsp; \u0026nbsp; path   str  success  tree quota path    \u0026nbsp; UserQuotas   list  success  Provides details of all user quotas    \u0026nbsp; \u0026nbsp; id   str  success  user quota id    \u0026nbsp; VolumeGroups   list  success  Provides details of all volumegroups.    \u0026nbsp; \u0026nbsp; id   str  success  volumegroup id    \u0026nbsp; \u0026nbsp; name   str  success  volumegroup name    \u0026nbsp; Volumes   list  success  Provides details of all volumes.    \u0026nbsp; \u0026nbsp; id   str  success  volume id    \u0026nbsp; \u0026nbsp; name   str  success  volume name    Authors  Arindam Datta (@dattaarindam) ansible.team@dell.com Vivek Soni (@v-soni11) ansible.team@dell.com   Replication Session Module Replication session operations on a PowerStore storage system.\nSynopsis Performs all replication session state change operations on a PowerStore Storage System. This module supports get details of an existing replication session. Updating the state of the replication session.\nParameters   Parameter Type Required Default Choices Description   volume_group  str      Name/ID of the volume group for which a replication session exists. volume_group, volume, and session_id are mutually exclusive.    volume  str      Name/ID of the volume for which replication session exists. volume_group, volume, and session_id are mutually exclusive.    session_id  str      ID of the replication session. volume_group, volume, and session_id are mutually exclusive.    session_state  str      failed_over paused synchronizing   State in which the replication session is present after performing the task.    array_ip  str   True     IP or FQDN of the PowerStore management system.    verifycert  bool   True     True False   Boolean variable to specify whether to validate SSL certificate or not. True - indicates that the SSL certificate should be verified. Set the environment variable REQUESTS_CA_BUNDLE to the path of the SSL certificate. False - indicates that the SSL certificate should not be verified.    user  str   True     The username of the PowerStore host.    password  str   True     The password of the PowerStore host.    Notes  Manual synchronization for a replication session is not supported through the Ansible module. When the current state of the replication session is \u0026lsquo;OK\u0026rsquo; and in the playbook task \u0026lsquo;synchronizing\u0026rsquo;, then it will return \u0026ldquo;changed\u0026rdquo; as False. This is because there is a scheduled synchronization in place with the associated replication rule\u0026rsquo;s RPO in the protection policy.  Examples - name: Pause a replication session dellemc_powerstore_replicationsession: array_ip: \u0026quot;{{array_ip}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; volume: \u0026quot;sample_volume_1\u0026quot; session_state: \u0026quot;paused\u0026quot; - name: Synchronize a replication session dellemc_powerstore_replicationsession: array_ip: \u0026quot;{{array_ip}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; volume: \u0026quot;sample_volume_1\u0026quot; session_state: \u0026quot;synchronizing\u0026quot; - name: Get details of a replication session dellemc_powerstore_replicationsession: array_ip: \u0026quot;{{array_ip}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; volume: \u0026quot;sample_volume_1\u0026quot; - name: Fail over a replication session dellemc_powerstore_replicationsession: array_ip: \u0026quot;{{array_ip}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; volume: \u0026quot;sample_volume_1\u0026quot; session_state: \u0026quot;failed_over\u0026quot; Return Values   Key Type Returned Description   changed   bool   always   Whether or not the resource has changed    replication_session_details   complex   When replication session exists   Details of the replication session    \u0026nbsp; estimated_completion_timestamp   str  success  Estimated completion time of the current replication operation.    \u0026nbsp; id   str  success  ['The system generated ID of the replication session.', 'Unique across source and destination roles.']    \u0026nbsp; last_sync_timestamp   str  success  Time of last successful synchronization.    \u0026nbsp; local_resource_id   str  success  Unique identifier of the local storage resource for the replication session.    \u0026nbsp; name   str  success  Name of the replication rule.    \u0026nbsp; progress_percentage   int  success  Progress of the current replication operation.    \u0026nbsp; remote_resource_id   str  success  Unique identifier of the remote storage resource for the replication session.    \u0026nbsp; remote_system_id   str  success  Unique identifier of the remote system instance.    \u0026nbsp; replication_rule_id   str  success  Associated replication rule instance if created by policy engine.    \u0026nbsp; resource_type   str  success  ['Storage resource type eligible for replication protection.', 'volume - Replication session created on a volume.', 'volume_group - Replication session created on a volume group.']    \u0026nbsp; role   str  success  ['Role of the replication session.', 'Source - The local resource is the source of the remote replication session.', 'Destination - The local resource is the destination of the remote replication session.']    \u0026nbsp; state   str  success  State of the replication session.    Authors  P Srinivas Rao (@srinivas-rao5) ansible.team@dell.com   Host Group Module Manage host group on PowerStore Storage System.\nSynopsis Managing host group on PowerStore storage system includes create host group with a set of hosts, add/remove hosts from host group, rename host group, and delete host group. Deletion of a host group results in deletion of the containing hosts as well. Remove hosts from the host group first to retain them.\nParameters   Parameter Type Required Default Choices Description   hostgroup_name  str      The host group name. This value must contain 128 or fewer printable Unicode characters. Creation of an empty host group is not allowed. Required when creating a host group. Use either hostgroup_id or hostgroup_name for modify and delete tasks.    hostgroup_id  str      The 36-character long host group id, automatically generated when a host group is created. Use either hostgroup_id or hostgroup_name for modify and delete tasks. hostgroup_id cannot be used while creating host group, as it is generated by the array after creation of host group.    hosts  list elements: str      List of hosts to be added or removed from the host group. Subordinate hosts in a host group can only be of one type, either FC or iSCSI. Required when creating a host group. To represent host, both name or ID can be used interchangeably. The module will detect both.    state  str   True     absent present   Define whether the host group should exist or not. present - indicates that the host group should exist on the system. absent - indicates that the host group should not exist on the system. Deletion of a host group results in deletion of the containing hosts as well. Remove hosts from the host group first to retain them.    host_state  str      present-in-group absent-in-group   Define whether the hosts should be present or absent in host group. present-in-group - indicates that the hosts should exist on the host group. absent-in-group - indicates that the hosts should not exist on the host group. Required when creating a host group with hosts or adding/removing hosts from existing host group.    new_name  str      The new name for host group renaming function. This value must contain 128 or fewer printable Unicode characters.    array_ip  str   True     IP or FQDN of the PowerStore management system.    verifycert  bool   True     True False   Boolean variable to specify whether to validate SSL certificate or not. True - indicates that the SSL certificate should be verified. Set the environment variable REQUESTS_CA_BUNDLE to the path of the SSL certificate. False - indicates that the SSL certificate should not be verified.    user  str   True     The username of the PowerStore host.    password  str   True     The password of the PowerStore host.    Examples  - name: Create host group with hosts using host name dellemc_powerstore_hostgroup: array_ip: \u0026quot;{{array_ip}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; hostgroup_name: \u0026quot;{{hostgroup_name}}\u0026quot; hosts: - host1 - host2 state: 'present' host_state: 'present-in-group' - name: Create host group with hosts using host ID dellemc_powerstore_hostgroup: array_ip: \u0026quot;{{array_ip}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; hostgroup_name: \u0026quot;{{hostgroup_name}}\u0026quot; hosts: - c17fc987-bf82-480c-af31-9307b89923c3 state: 'present' host_state: 'present-in-group' - name: Get host group details dellemc_powerstore_hostgroup: array_ip: \u0026quot;{{array_ip}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; hostgroup_name: \u0026quot;{{hostgroup_name}}\u0026quot; state: 'present' - name: Get host group details using ID dellemc_powerstore_hostgroup: array_ip: \u0026quot;{{array_ip}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; hostgroup_id: \u0026quot;{{host group_id}}\u0026quot; state: 'present' - name: Add hosts to host group dellemc_powerstore_hostgroup: array_ip: \u0026quot;{{array_ip}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; hostgroup_name: \u0026quot;{{hostgroup_name}}\u0026quot; hosts: - host3 host_state: 'present-in-group' state: 'present' - name: Remove hosts from host group dellemc_powerstore_hostgroup: array_ip: \u0026quot;{{array_ip}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; hostgroup_name: \u0026quot;{{hostgroup_name}}\u0026quot; hosts: - host3 host_state: 'absent-in-group' state: 'present' - name: Rename host group dellemc_powerstore_hostgroup: array_ip: \u0026quot;{{array_ip}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; hostgroup_name: \u0026quot;{{hostgroup_name}}\u0026quot; new_name: \u0026quot;{{new_hostgroup_name}}\u0026quot; state: 'present' - name: Delete host group dellemc_powerstore_hostgroup: array_ip: \u0026quot;{{array_ip}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; hostgroup_name: \u0026quot;{{hostgroup_name}}\u0026quot; state: 'absent' Return Values   Key Type Returned Description   changed   bool   always   Whether or not the resource has changed    hostgroup_details   complex   When host group exists   Details of the host group    \u0026nbsp; description   str  success  Description about the host group    \u0026nbsp; hosts   complex  success  The hosts details which are part of this host group    \u0026nbsp; \u0026nbsp; id   str  success  The ID of the host    \u0026nbsp; \u0026nbsp; name   str  success  The name of the host    \u0026nbsp; id   str  success  The system generated ID given to the host group    \u0026nbsp; name   str  success  Name of the host group    Authors  Manisha Agrawal (@agrawm3) ansible.team@dell.com   NFS Module Manage NFS exports on Dell EMC PowerStore.\nSynopsis Managing NFS exports on PowerStore Storage System includes creating new NFS Export, getting details of NFS export, modifying attributes of NFS export, and deleting NFS export.\nParameters   Parameter Type Required Default Choices Description   nfs_export_name  str      The name of the NFS export. Mandatory for create operation. Specify either nfs_export_name or nfs_export_id(but not both) for any operation.    nfs_export_id  str      The ID of the NFS export.    filesystem  str      The ID/Name of the filesystem for which the NFS export will be created. Either filesystem or snapshot is required for creation of the NFS Export. If filesystem name is specified, then nas_server is required to uniquely identify the filesystem. If filesystem parameter is provided, then snapshot cannot be specified.    snapshot  str      The ID/Name of the Snapshot for which NFS export will be created. Either filesystem or snapshot is required for creation of the NFS Export. If snapshot name is specified, then nas_server is required to uniquely identify the snapshot. If snapshot parameter is provided, then filesystem cannot be specified. NFS export can be created only if access type of snapshot is \"protocol\".    nas_server  str      The NAS server. This could be the name or ID of the NAS server.    path  str      Local path to export relative to the NAS server root. With NFS, each export of a file_system or file_snap must have a unique local path. Mandatory while creating NFS export.    description  str      The description for the NFS export.    default_access  str      NO_ACCESS READ_ONLY READ_WRITE ROOT READ_ONLY_ROOT   Default access level for all hosts that can access the Export. For hosts that need different access than the default, they can be configured by adding to the list. If default_access is not mentioned during creation, then NFS export will be created with No_Access.    no_access_hosts  list elements: str      Hosts with no access to the NFS export.    read_only_hosts  list elements: str      Hosts with read-only access to the NFS export.    read_only_root_hosts  list elements: str      Hosts with read-only access for root user to the NFS export.    read_write_hosts  list elements: str      Hosts with read and write access to the NFS export.    read_write_root_hosts  list elements: str      Hosts with read and write access for root user to the NFS export.    min_security  str      SYS KERBEROS KERBEROS_WITH_INTEGRITY KERBEROS_WITH_ENCRYPTION   NFS enforced security type for users accessing an NFS export. If not specified at the time of creation, it will be set to SYS.    anonymous_uid  int      Specifies the user ID of the anonymous account. If not specified at the time of creation, it will be set to -2.    anonymous_gid  int      Specifies the group ID of the anonymous account. If not specified at the time of creation, it will be set to -2.    is_no_suid  bool      If set, do not allow access to set SUID. Otherwise, allow access. If not specified at the time of creation, it will be set to False.    host_state  str      present-in-export absent-in-export   Define whether the hosts can access the NFS export. Required when adding or removing host access from the export.    state  str   True     absent present   Define whether the NFS export should exist or not.    array_ip  str   True     IP or FQDN of the PowerStore management system.    verifycert  bool   True     True False   Boolean variable to specify whether to validate SSL certificate or not. True - indicates that the SSL certificate should be verified. Set the environment variable REQUESTS_CA_BUNDLE to the path of the SSL certificate. False - indicates that the SSL certificate should not be verified.    user  str   True     The username of the PowerStore host.    password  str   True     The password of the PowerStore host.    Examples - name: Create NFS export (filesystem) dellemc_powerstore_nfs: array_ip: \u0026quot;{{array_ip}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; nfs_export_name: \u0026quot;{{export_name1}}\u0026quot; filesystem: \u0026quot;{{filesystem}}\u0026quot; nas_server: \u0026quot;{{nas_server}}\u0026quot; path: \u0026quot;{{path1}}\u0026quot; description: \u0026quot;sample description\u0026quot; default_access: \u0026quot;NO_ACCESS\u0026quot; no_access_hosts: - \u0026quot;{{host5}}\u0026quot; read_only_hosts: - \u0026quot;{{host1}}\u0026quot; read_only_root_hosts: - \u0026quot;{{host2}}\u0026quot; read_write_hosts: - \u0026quot;{{host3}}\u0026quot; read_write_root_hosts: - \u0026quot;{{host4}}\u0026quot; min_security: \u0026quot;SYS\u0026quot; anonymous_uid: 1000 anonymous_gid: 1000 is_no_suid: True host_state: \u0026quot;present-in-export\u0026quot; state: \u0026quot;present\u0026quot; - name: Create NFS export Create NFS export for filesystem snapshot with mandatory parameters dellemc_powerstore_nfs: array_ip: \u0026quot;{{array_ip}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; nfs_export_name: \u0026quot;{{export_name2}}\u0026quot; snapshot: \u0026quot;{{snapshot}}\u0026quot; nas_server: \u0026quot;{{nas_server}}\u0026quot; path: \u0026quot;{{path2}}\u0026quot; state: \u0026quot;present\u0026quot; - name: Get NFS export details using ID dellemc_powerstore_nfs: array_ip: \u0026quot;{{array_ip}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; nfs_export_id: \u0026quot;{{export_id}}\u0026quot; state: \u0026quot;present\u0026quot; - name: Add Read-Only and Read-Write hosts to NFS export dellemc_powerstore_nfs: array_ip: \u0026quot;{{array_ip}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; nfs_export_id: \u0026quot;{{export_id}}\u0026quot; read_only_hosts: - \u0026quot;{{host5}}\u0026quot; read_write_hosts: - \u0026quot;{{host6}}\u0026quot; host_state: \u0026quot;present-in-export\u0026quot; state: \u0026quot;present\u0026quot; - name: Remove Read-Only and Read-Write hosts from NFS export dellemc_powerstore_nfs: array_ip: \u0026quot;{{array_ip}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; nfs_export_id: \u0026quot;{{export_id}}\u0026quot; read_only_hosts: - \u0026quot;{{host1}}\u0026quot; read_write_hosts: - \u0026quot;{{host3}}\u0026quot; host_state: \u0026quot;absent-in-export\u0026quot; state: \u0026quot;present\u0026quot; - name: Modify the attributes of NFS export dellemc_powerstore_nfs: array_ip: \u0026quot;{{array_ip}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; nfs_export_id: \u0026quot;{{export_id}}\u0026quot; description: \u0026quot;modify description\u0026quot; default_access: \u0026quot;ROOT\u0026quot; state: \u0026quot;present\u0026quot; - name: Delete NFS export using name dellemc_powerstore_nfs: array_ip: \u0026quot;{{array_ip}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; nfs_export_name: \u0026quot;{{export_name}}\u0026quot; nas_server: \u0026quot;{{nas_server}}\u0026quot; state: \u0026quot;absent\u0026quot; Return Values   Key Type Returned Description   changed   bool   always   Whether or not the resource has changed    nfs_export_details   complex   When NFS export exists.   The NFS export details.    \u0026nbsp; anonymous_GID   int  success  The group ID of the anonymous account.    \u0026nbsp; anonymous_UID   int  success  The user ID of the anonymous account.    \u0026nbsp; default_access   str  success  Default access level for all hosts that can access the export.    \u0026nbsp; description   str  success  The description for the NFS export.    \u0026nbsp; file_system   complex  success  Details of filesystem and NAS server on which NFS export is present.    \u0026nbsp; \u0026nbsp; filesystem_type   str  success  The type of the filesystem.    \u0026nbsp; \u0026nbsp; id   str  success  The ID of the filesystem.    \u0026nbsp; \u0026nbsp; name   str  success  The name of the filesystem.    \u0026nbsp; \u0026nbsp; nas_server   complex  success  Details of NAS server.    \u0026nbsp; \u0026nbsp; \u0026nbsp; id   str  success  The ID of the NAS server.    \u0026nbsp; \u0026nbsp; \u0026nbsp; name   str  success  The name of the NAS server.    \u0026nbsp; id   str  success  The ID of the NFS export.    \u0026nbsp; is_no_SUID   bool  success  If set, do not allow access to set SUID. Otherwise, allow access.    \u0026nbsp; min_security   str  success  NFS enforced security type for users accessing an NFS export.    \u0026nbsp; name   str  success  The name of the NFS export.    \u0026nbsp; no_access_hosts   list  success  Hosts with no access to the NFS export.    \u0026nbsp; path   str  success  Local path to a location within the file system.    \u0026nbsp; read_only_hosts   list  success  Hosts with read-only access to the NFS export.    \u0026nbsp; read_only_root_hosts   list  success  Hosts with read-only for root user access to the NFS export.    \u0026nbsp; read_write_hosts   list  success  Hosts with read and write access to the NFS export.    \u0026nbsp; read_write_root_hosts   list  success  Hosts with read and write for root user access to the NFS export.    Authors  Akash Shendge (@shenda1) ansible.team@dell.com   Volume Group Module Manage volume groups on a PowerStore Storage System\nSynopsis Managing volume group on PowerStore Storage System includes creating new volume group, adding volumes to volume group, removing volumes from volume group, renaming volume group, modifying volume group, and deleting volume group.\nParameters   Parameter Type Required Default Choices Description   vg_name  str      The name of the volume group.    vg_id  str      The id of the volume group. It can be used only for Modify, Add/Remove, or Delete operation.    volumes  list elements: str      This is a list of volumes. Either the volume ID or name must be provided for adding/removing existing volumes from a volume group. If volumes are given, then vol_state should also be specified.    vol_state  str      present-in-group absent-in-group   String variable. Describes the state of volumes inside a volume group. If volume is given, then vol_state should also be specified.    new_vg_name  str      The new name of the volume group.    description  str      Description about the volume group.    protection_policy  str      String variable. Represents Protection policy id or name used for volume group. Specifying an empty string or \"\" removes the existing protection policy from volume group.    is_write_order_consistent  bool      A boolean flag to indicate whether Snapshot sets of the volume group will be write-order consistent. If this parameter is not specified, the array by default sets it to true.    state  str   True     absent present   Define whether the volume group should exist or not.    array_ip  str   True     IP or FQDN of the PowerStore management system.    verifycert  bool   True     True False   Boolean variable to specify whether to validate SSL certificate or not. True - indicates that the SSL certificate should be verified. Set the environment variable REQUESTS_CA_BUNDLE to the path of the SSL certificate. False - indicates that the SSL certificate should not be verified.    user  str   True     The username of the PowerStore host.    password  str   True     The password of the PowerStore host.    Notes  vol_state is mandatory if volumes are provided. A protection policy can be specified either for an volume group, or for the individual volumes inside the volume group. A volume can be a member of at most one volume group. Specifying \u0026ldquo;protection_policy\u0026rdquo; as empty string or \u0026quot;\u0026rdquo; removes the existing protection policy from a volume group.  Examples - name: Create volume group without protection policy dellemc_powerstore_volumegroup: array_ip: \u0026quot;{{array_ip}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; vg_name: \u0026quot;{{vg_name}}\u0026quot; description: \u0026quot;This volume group is for ansible\u0026quot; state: \u0026quot;present\u0026quot; - name: Get details of volume group dellemc_powerstore_volumegroup: array_ip: \u0026quot;{{array_ip}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; vg_name: \u0026quot;{{vg_name}}\u0026quot; state: \u0026quot;present\u0026quot; - name: Add volumes to volume group dellemc_powerstore_volumegroup: array_ip: \u0026quot;{{array_ip}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; vg_name: \u0026quot;{{vg_name}}\u0026quot; state: \u0026quot;present\u0026quot; volumes: - \u0026quot;7f879569-676c-4749-a06f-c2c30e09b295\u0026quot; - \u0026quot;68e4dad5-5de5-4644-a98f-6d4fb916e169\u0026quot; - \u0026quot;Ansible_Testing\u0026quot; vol_state: \u0026quot;present-in-group\u0026quot; - name: Remove volumes from volume group dellemc_powerstore_volumegroup: array_ip: \u0026quot;{{array_ip}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; vg_name: \u0026quot;{{vg_name}}\u0026quot; state: \u0026quot;present\u0026quot; volumes: - \u0026quot;7f879569-676c-4749-a06f-c2c30e09b295\u0026quot; - \u0026quot;Ansible_Testing\u0026quot; vol_state: \u0026quot;absent-in-group\u0026quot; - name: Rename volume group and change is_write_order_consistent flag dellemc_powerstore_volumegroup: array_ip: \u0026quot;{{array_ip}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; vg_name: \u0026quot;{{vg_name}}\u0026quot; new_vg_name: \u0026quot;{{new_vg_name}}\u0026quot; is_write_order_consistent: False state: \u0026quot;present\u0026quot; - name: Get details of volume group by ID dellemc_powerstore_volumegroup: array_ip: \u0026quot;{{array_ip}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; vg_id: \u0026quot;{{vg_id}}\u0026quot; state: \u0026quot;present\u0026quot; - name: Delete volume group dellemc_powerstore_volumegroup: array_ip: \u0026quot;{{array_ip}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; name: \u0026quot;{{new_vg_name}}\u0026quot; state: \u0026quot;absent\u0026quot; Return Values   Key Type Returned Description   add_vols_to_vg   bool   When value exists   A boolean flag to indicate whether volume/s got added to volume group    changed   bool   always   Whether or not the resource has changed    create_vg   bool   When value exists   A boolean flag to indicate whether volume group got created    delete_vg   bool   When value exists   A boolean flag to indicate whether volume group got deleted    modify_vg   bool   When value exists   A boolean flag to indicate whether volume group got modified    remove_vols_from_vg   bool   When value exists   A boolean flag to indicate whether volume/s got removed from volume group    volume_group_details   complex   When volume group exists   Details of the volume group    \u0026nbsp; description   str  success  ['description about the volume group']    \u0026nbsp; id   str  success  ['The system generated ID given to the volume group']    \u0026nbsp; is_write_order_consistent   bool  success  ['A boolean flag to indicate whether snapshot sets of the volume group will be write-order consistent']    \u0026nbsp; name   str  success  ['Name of the volume group']    \u0026nbsp; protection_policy_id   str  success  ['The protection policy of the volume group']    \u0026nbsp; type   str  success  ['The type of the volume group']    \u0026nbsp; volumes   complex  success  ['The volumes details of the volume group']    \u0026nbsp; \u0026nbsp; id   str  success  ['The system generated ID given to the volume associated with the volume group']    \u0026nbsp; \u0026nbsp; name   str  success  ['The name of the volume associated with the volume group.']    Authors  Akash Shendge (@shenda1) ansible.team@dell.com Arindam Datta (@dattaarindam) ansible.team@dell.com   NAS Server Module NAS Server operations on PowerStore Storage system.\nSynopsis Supports getting the details and modifying the attributes of a NAS server.\nParameters   Parameter Type Required Default Choices Description   nas_server_name  str      Name of the NAS server. Mutually exclusive with nas_server_id.    nas_server_id  str      Unique id of the NAS server. Mutually exclusive with nas_server_name.    description  str      Description of the NAS server.    nas_server_new_name  str      New name of the NAS server for a rename operation.    current_node  str      Unique identifier or name of the node on which the NAS server is running.    preferred_node  str      Unique identifier or name of the preferred node for the NAS server. The initial value (on NAS server create) is taken from the current node.    current_unix_directory_service  str      NIS LDAP LOCAL_FILES LOCAL_THEN_NIS LOCAL_THEN_LDAP   Define the Unix directory service used for looking up identity information for Unix such as UIDs, GIDs, net groups, and so on.    default_unix_user  str      Default Unix user name used for granting access in case of Windows to Unix user mapping failure. When empty, access in such case is denied.    default_windows_user  str      Default Windows user name used for granting access in case of Unix to Windows user mapping failure. When empty, access in such case is denied.    state  str   True     absent present   Define whether the nas server should exist or not.    array_ip  str   True     IP or FQDN of the PowerStore management system.    verifycert  bool   True     True False   Boolean variable to specify whether to validate SSL certificate or not. True - indicates that the SSL certificate should be verified. Set the environment variable REQUESTS_CA_BUNDLE to the path of the SSL certificate. False - indicates that the SSL certificate should not be verified.    user  str   True     The username of the PowerStore host.    password  str   True     The password of the PowerStore host.    Examples  - name: Get details of NAS Server by name dellemc_powerstore_nasserver: array_ip: \u0026quot;{{array_ip}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; nas_server_name: \u0026quot;{{nas_server_name}}\u0026quot; state: \u0026quot;present\u0026quot; - name: Get Details of NAS Server by ID dellemc_powerstore_nasserver: array_ip: \u0026quot;{{array_ip}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; nas_server_id: \u0026quot;{{nas_id}}\u0026quot; state: \u0026quot;present\u0026quot; - name: Rename NAS Server by Name dellemc_powerstore_nasserver: array_ip: \u0026quot;{{array_ip}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; nas_server_name: \u0026quot;{{nas_server_name}}\u0026quot; nas_server_new_name : \u0026quot;{{nas_server_new_name}}\u0026quot; state: \u0026quot;present\u0026quot; - name: Modify NAS Server attributes by ID dellemc_powerstore_nasserver: array_ip: \u0026quot;{{array_ip}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; nas_server_id: \u0026quot;{{nas_id}}\u0026quot; current_unix_directory_service: \u0026quot;LOCAL_FILES\u0026quot; current_node: \u0026quot;{{cur_node_n1}}\u0026quot; preferred_node: \u0026quot;{{prefered_node}}\u0026quot; state: \u0026quot;present\u0026quot; Return Values   Key Type Returned Description   changed   bool   always   Whether or not the resource has changed    nasserver_details   complex   When nas server exists   Details about the nas server    \u0026nbsp; backup_IPv4_interface_id   str  success  Unique identifier of the preferred IPv4 backup interface.    \u0026nbsp; backup_IPv6_interface_id   str  success  Unique identifier of the preferred IPv6 backup interface.    \u0026nbsp; current_node   dict  success  Unique identifier and name of the node on which the NAS server is running.    \u0026nbsp; current_unix_directory_service   str  success  Define the Unix directory service used for looking up identity information for Unix such as UIDs, GIDs, net groups, and so on.    \u0026nbsp; default_unix_user   str  success  Default Unix user name used for granting access in case of Windows to Unix user mapping failure.    \u0026nbsp; description   str  success  Additional information about the nas server.    \u0026nbsp; file_interfaces   dict  success  This is the inverse of the resource type file_interface association.Will return the id,name \u0026 ip_address of the associated file interface    \u0026nbsp; file_ldaps   str  success  This is the inverse of the resource type file_ldap association.    \u0026nbsp; file_systems   dict  success  This is the inverse of the resource type file_system association.    \u0026nbsp; id   str  success  The system generated ID given to the nas server    \u0026nbsp; is_username_translation_enabled   bool  success  Enable the possibility to match a windows account to a Unix account with different names.    \u0026nbsp; name   str  success  Name of the nas server    \u0026nbsp; nfs_servers   str  success  This is the inverse of the resource type nfs_server association.    \u0026nbsp; operational_status   str  success  NAS server operational status.    \u0026nbsp; preferred_node   dict  success  Unique identifier and name of the preferred node for the NAS server.    \u0026nbsp; production_IPv4_interface_id   str  success  Unique identifier of the preferred IPv4 production interface.    \u0026nbsp; production_IPv6_interface_id   str  success  Unique identifier of the preferred IPv6 production interface.    \u0026nbsp; smb_servers   str  success  This is the inverse of the resource type smb_server association.    Authors  Arindam Datta (@dattaarindam) ansible.team@dell.com   SMB Share Module Manage SMB shares on a PowerStore storage system.\nSynopsis Managing SMB Shares on PowerStore storage system includes create, get, modify, and delete the SMB shares.\nParameters   Parameter Type Required Default Choices Description   share_name  str      Name of the SMB share. Required during creation of the SMB share. For all other operations either share_name or share_id is required.    share_id  str      ID of the SMB share. Should not be specified during creation. ID is auto generated. For all other operations either share_name or share_id is required. If share_id is used then no need to pass nas_server/filesystem/snapshot/ path.    path  str      Local path to the file system/Snapshot or any existing sub-folder of the file system/Snapshot that is shared over the network. Path is relative to the base of the NAS server and must start with the name of the filesystem. Required for creation of the SMB share.    filesystem  str      The ID/Name of the File System. Either filesystem or snapshot is required for creation of the SMB share. If filesystem name is specified, then nas_server is required to uniquely identify the filesystem. If filesystem parameter is provided, then snapshot cannot be specified.    snapshot  str      The ID/Name of the Snapshot. Either filesystem or snapshot is required for creation of the SMB share. If snapshot name is specified, then nas_server is required to uniquely identify the snapshot. If snapshot parameter is provided, then filesystem cannot be specified. SMB share can be created only if access type of snapshot is \"protocol\".    nas_server  str      The ID/Name of the NAS Server. It is not required if share_id is used.    description  str      Description for the SMB share. Optional parameter when creating a share. To modify, pass the new value in description field.    is_abe_enabled  bool      Indicates whether Access-based Enumeration (ABE) for SMB share is enabled. During creation, if not mentioned, then the default is False.    is_branch_cache_enabled  bool      Indicates whether Branch Cache optimization for SMB share is enabled. During creation, if not mentioned then default is False.    is_continuous_availability_enabled  bool      Indicates whether continuous availability for SMB 3.0 is enabled. During creation, if not mentioned, then the default is False.    is_encryption_enabled  bool      Indicates whether encryption for SMB 3.0 is enabled at the shared folder level. During creation, if not mentioned then default is False.    offline_availability  str      MANUAL DOCUMENTS PROGRAMS NONE   Defines valid states of Offline Availability. MANUAL- Only specified files will be available offline. DOCUMENTS- All files that users open will be available offline. PROGRAMS- Program will preferably run from the offline cache even when connected to the network. All files that users open will be available offline. NONE- Prevents clients from storing documents and programs in offline cache.    umask  str      The default UNIX umask for new files created on the SMB Share. During creation, if not mentioned, then the default is \"022\". For all other operations, the default is None.    state  str   True     absent present   Define whether the SMB share should exist or not. present indicates that the share should exist on the system. absent indicates that the share should not exist on the system.    array_ip  str   True     IP or FQDN of the PowerStore management system.    verifycert  bool   True     True False   Boolean variable to specify whether to validate SSL certificate or not. True - indicates that the SSL certificate should be verified. Set the environment variable REQUESTS_CA_BUNDLE to the path of the SSL certificate. False - indicates that the SSL certificate should not be verified.    user  str   True     The username of the PowerStore host.    password  str   True     The password of the PowerStore host.    Notes  When the ID of the filesystem/snapshot is passed then nas_server is not required. If passed, then the filesystem/snapshot should exist for the nas_server, else the task will fail. Multiple SMB shares can be created for the same local path.  Examples - name: Create SMB share for a filesystem dellemc_powerstore_smbshare: array_ip: \u0026quot;{{array_ip}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; share_name: \u0026quot;sample_smb_share\u0026quot; filesystem: \u0026quot;sample_fs\u0026quot; nas_server: \u0026quot;{{nas_server_id}}\u0026quot; path: \u0026quot;{{path}}\u0026quot; description: \u0026quot;Sample SMB share created\u0026quot; is_abe_enabled: True is_branch_cache_enabled: True offline_availability: \u0026quot;DOCUMENTS\u0026quot; is_continuous_availability_enabled: True is_encryption_enabled: True state: \u0026quot;present\u0026quot; - name: Modify Attributes of SMB share for a filesystem dellemc_powerstore_smbshare: array_ip: \u0026quot;{{array_ip}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; share_name: \u0026quot;sample_smb_share\u0026quot; nas_server: \u0026quot;sample_nas_server\u0026quot; description: \u0026quot;Sample SMB share attributes updated\u0026quot; is_abe_enabled: False is_branch_cache_enabled: False offline_availability: \u0026quot;MANUAL\u0026quot; is_continuous_availability_enabled: False is_encryption_enabled: False umask: \u0026quot;022\u0026quot; state: \u0026quot;present\u0026quot; - name: Create SMB share for a snapshot dellemc_powerstore_smbshare: array_ip: \u0026quot;{{array_ip}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; share_name: \u0026quot;sample_snap_smb_share\u0026quot; snapshot: \u0026quot;sample_snapshot\u0026quot; nas_server: \u0026quot;{{nas_server_id}}\u0026quot; path: \u0026quot;{{path}}\u0026quot; description: \u0026quot;Sample SMB share created for snapshot\u0026quot; is_abe_enabled: True is_branch_cache_enabled: True is_continuous_availability_enabled: True state: \u0026quot;present\u0026quot; - name: Modify Attributes of SMB share for a snapshot dellemc_powerstore_smbshare: array_ip: \u0026quot;{{array_ip}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; share_name: \u0026quot;sample_snap_smb_share\u0026quot; nas_server: \u0026quot;sample_nas_server\u0026quot; description: \u0026quot;Sample SMB share attributes updated for snapshot\u0026quot; is_abe_enabled: False is_branch_cache_enabled: False offline_availability: \u0026quot;MANUAL\u0026quot; is_continuous_availability_enabled: False umask: \u0026quot;022\u0026quot; state: \u0026quot;present\u0026quot; - name: Get details of SMB share dellemc_powerstore_smbshare: array_ip: \u0026quot;{{array_ip}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; share_id: \u0026quot;{{smb_share_id}}\u0026quot; state: \u0026quot;present\u0026quot; - name: Delete SMB share dellemc_powerstore_smbshare: array_ip: \u0026quot;{{array_ip}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; share_id: \u0026quot;{{smb_share_id}}\u0026quot; state: \u0026quot;absent\u0026quot; Return Values   Key Type Returned Description   changed   bool   always   Whether or not the resource has changed    smb_share_details   complex   When share exists.   The SMB share details.    \u0026nbsp; description   str  success  Additional information about the share.    \u0026nbsp; file_system   complex  success  Includes ID and Name of filesystem and nas server for which smb share exists.    \u0026nbsp; \u0026nbsp; filesystem_type   str  success  Type of filesystem.    \u0026nbsp; \u0026nbsp; id   str  success  ID of filesystem.    \u0026nbsp; \u0026nbsp; name   str  success  Name of filesystem.    \u0026nbsp; \u0026nbsp; nas_server   dict  success  nas_server of filesystem.    \u0026nbsp; id   str  success  The ID of the SMB share.    \u0026nbsp; is_ABE_enabled   bool  success  Whether Access Based enumeration is enforced or not    \u0026nbsp; is_branch_cache_enabled   bool  success  Whether branch cache is enabled or not.    \u0026nbsp; is_continuous_availability_enabled   bool  success  Whether the share will be available continuously or not    \u0026nbsp; is_encryption_enabled   bool  success  Whether encryption is enabled or not    \u0026nbsp; name   str  success  Name of the SMB share.    Authors  P Srinivas Rao (@srinivas-rao5) ansible.team@dell.com   Snapshot Module Manage Snapshots on Dell EMC PowerStore.\nSynopsis Managing Snapshots on PowerStore. Create a new Volume Group Snapshot. Get details of Volume Group Snapshot. Modify Volume Group Snapshot Delete an existing Volume Group Snapshot. Create a new Volume Snapshot. Get details of Volume Snapshot. Modify Volume Snapshot. Delete an existing Volume Snapshot.\nParameters   Parameter Type Required Default Choices Description   snapshot_name  str      The name of the Snapshot. Either snapshot name or ID is required.    snapshot_id  str      The ID of the Snapshot. Either snapshot ID or Snapshot name is required.    volume  str      The volume. This could be the volume name or ID.    volume_group  str      The volume group. This could be the volume group name or ID.    new_snapshot_name  str      The new name of the Snapshot.    desired_retention  str      The retention value for the Snapshot. If the retention value is not specified, the Snapshot details would be returned. To create a Snapshot, either a retention or expiration timestamp must be given. If the Snapshot does not have any retention value - specify it as 'None'.    retention_unit  str      hours days   The unit for retention. If this unit is not specified, 'hours' is taken as default retention_unit. If desired_retention is specified, expiration_timestamp cannot be specified.    expiration_timestamp  str      The expiration timestamp of the Snapshot. This should be provided in UTC format, e.g 2019-07-24T10:54:54Z.    description  str      The description for the Snapshot.    state  str   True     absent present   Defines whether the Snapshot should exist or not.    array_ip  str   True     IP or FQDN of the PowerStore management system.    verifycert  bool   True     True False   Boolean variable to specify whether to validate SSL certificate or not. True - indicates that the SSL certificate should be verified. Set the environment variable REQUESTS_CA_BUNDLE to the path of the SSL certificate. False - indicates that the SSL certificate should not be verified.    user  str   True     The username of the PowerStore host.    password  str   True     The password of the PowerStore host.    Examples  - name: Create a volume snapshot on PowerStore dellemc_powerstore_snapshot: array_ip: \u0026quot;{{mgmt_ip}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; snapshot_name: \u0026quot;{{snapshot_name}}\u0026quot; volume: \u0026quot;{{volume}}\u0026quot; description: \u0026quot;{{description}}\u0026quot; desired_retention: \u0026quot;{{desired_retention}}\u0026quot; retention_unit: \u0026quot;{{retention_unit_days}}\u0026quot; state: \u0026quot;{{state_present}}\u0026quot; - name: Get details of a volume snapshot dellemc_powerstore_snapshot: array_ip: \u0026quot;{{mgmt_ip}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; snapshot_name: \u0026quot;{{snapshot_name}}\u0026quot; volume: \u0026quot;{{volume}}\u0026quot; state: \u0026quot;{{state_present}}\u0026quot; - name: Rename volume snapshot dellemc_powerstore_snapshot: array_ip: \u0026quot;{{mgmt_ip}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; snapshot_name: \u0026quot;{{snapshot_name}}\u0026quot; new_snapshot_name: \u0026quot;{{new_snapshot_name}}\u0026quot; volume: \u0026quot;{{volume}}\u0026quot; state: \u0026quot;{{state_present}}\u0026quot; - name: Delete volume snapshot dellemc_powerstore_snapshot: array_ip: \u0026quot;{{mgmt_ip}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; snapshot_name: \u0026quot;{{new_snapshot_name}}\u0026quot; volume: \u0026quot;{{volume}}\u0026quot; state: \u0026quot;{{state_absent}}\u0026quot; - name: Create a volume group snapshot on PowerStore dellemc_powerstore_snapshot: array_ip: \u0026quot;{{mgmt_ip}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; snapshot_name: \u0026quot;{{snapshot_name}}\u0026quot; volume_group: \u0026quot;{{volume_group}}\u0026quot; description: \u0026quot;{{description}}\u0026quot; expiration_timestamp: \u0026quot;{{expiration_timestamp}}\u0026quot; state: \u0026quot;{{state_present}}\u0026quot; - name: Get details of a volume group snapshot dellemc_powerstore_snapshot: array_ip: \u0026quot;{{mgmt_ip}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; snapshot_name: \u0026quot;{{snapshot_name}}\u0026quot; volume_group: \u0026quot;{{volume_group}}\u0026quot; state: \u0026quot;{{state_present}}\u0026quot; - name: Modify volume group snapshot expiration timestamp dellemc_powerstore_snapshot: array_ip: \u0026quot;{{mgmt_ip}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; snapshot_name: \u0026quot;{{snapshot_name}}\u0026quot; volume_group: \u0026quot;{{volume_group}}\u0026quot; description: \u0026quot;{{description}}\u0026quot; expiration_timestamp: \u0026quot;{{expiration_timestamp_new}}\u0026quot; state: \u0026quot;{{state_present}}\u0026quot; - name: Rename volume group snapshot dellemc_powerstore_snapshot: array_ip: \u0026quot;{{mgmt_ip}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; snapshot_name: \u0026quot;{{snapshot_name}}\u0026quot; new_snapshot_name: \u0026quot;{{new_snapshot_name}}\u0026quot; volume_group: \u0026quot;{{volume_group}}\u0026quot; state: \u0026quot;{{state_present}}\u0026quot; - name: Delete volume group snapshot dellemc_powerstore_snapshot: array_ip: \u0026quot;{{mgmt_ip}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; snapshot_name: \u0026quot;{{new_snapshot_name}}\u0026quot; volume_group: \u0026quot;{{volume_group}}\u0026quot; state: \u0026quot;{{state_absent}}\u0026quot; Return Values   Key Type Returned Description   changed   bool   always   Whether or not the resource has changed    create_vg_snap   bool   When value exists   A boolean flag to indicate whether volume group snapshot got created    create_vol_snap   bool   When value exists   A boolean flag to indicate whether volume snapshot got created    delete_vg_snap   bool   When value exists   A boolean flag to indicate whether volume group snapshot got deleted    delete_vol_snap   bool   When value exists   A boolean flag to indicate whether volume snapshot got deleted    modify_vg_snap   bool   When value exists   A boolean flag to indicate whether volume group snapshot got modified    modify_vol_snap   bool   When value exists   A boolean flag to indicate whether volume snapshot got modified    snap_details   complex   When snapshot exists   Details of the snapshot    \u0026nbsp; creation_timestamp   str  success  The creation timestamp of the snapshot    \u0026nbsp; description   str  success  Description about the snapshot    \u0026nbsp; id   str  success  The system generated ID given to the snapshot    \u0026nbsp; name   str  success  Name of the snapshot    \u0026nbsp; performance_policy_id   str  success  The performance policy for the snapshot    \u0026nbsp; protection_data   complex  success  The protection data of the snapshot    \u0026nbsp; \u0026nbsp; expiration_timestamp   str  success  The expiration timestamp of the snapshot    \u0026nbsp; protection_policy_id   str  success  The protection policy of the snapshot    \u0026nbsp; size   int  success  Size of the snapshot    \u0026nbsp; state   str  success  The state of the snapshot    \u0026nbsp; type   str  success  The type of the snapshot    \u0026nbsp; volumes   complex  success  The volumes details of the volume group snapshot    \u0026nbsp; \u0026nbsp; id   str  success  The system generated ID given to the volume associated with the volume group    Authors  Rajshree Khare (@khareRajshree) ansible.team@dell.com Prashant Rakheja (@prashant-dell) ansible.team@dell.com   Replication Rule Module Replication rule operations on a PowerStore storage system.\nSynopsis Performs all replication rule operations on a PowerStore Storage System. This module supports get details of an existing replication rule. Create new replication rule for all supported parameters. Modify replication rule with supported parameters. Delete a specific replication rule.\nParameters   Parameter Type Required Default Choices Description   replication_rule_name  str      Name of the replication rule. Required during creation of a replication rule. replication_rule_name and replication_rule_id are mutually exclusive.    replication_rule_id  str      ID of the replication rule. ID for the rule is autogenerated, cannot be passed during creation of a replication rule. replication_rule_name and replication_rule_id are mutually exclusive.    new_name  str      New name of the replication rule. Used for renaming a replication rule.    rpo  str      Five_Minutes Fifteen_Minutes Thirty_Minutes One_Hour Six_Hours Twelve_Hours One_Day   Recovery point objective (RPO), which is the acceptable amount of data, measured in units of time, that may be lost in case of a failure.    alert_threshold  int      Acceptable delay between the expected and actual replication sync intervals. The system generates an alert if the delay between the expected and actual sync exceeds this threshold. During creation, if not passed, then by default one RPO in minutes will be passed. The range of integers supported are in between 0 and 1440 (inclusive of both).    remote_system  str      ID or name of the remote system to which this rule will replicate the associated resources.    state  str   True     present absent   The state of the replication rule after the task is performed. For Delete operation only, it should be set to \"absent\". For all Create, Modify or Get details operations it should be set to \"present\".    array_ip  str   True     IP or FQDN of the PowerStore management system.    verifycert  bool   True     True False   Boolean variable to specify whether to validate SSL certificate or not. True - indicates that the SSL certificate should be verified. Set the environment variable REQUESTS_CA_BUNDLE to the path of the SSL certificate. False - indicates that the SSL certificate should not be verified.    user  str   True     The username of the PowerStore host.    password  str   True     The password of the PowerStore host.    Examples - name: Create new replication rule dellemc_powerstore_replicationrule: array_ip: \u0026quot;{{array_ip}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; replication_rule_name: \u0026quot;sample_replication_rule\u0026quot; rpo: \u0026quot;Five_Minutes\u0026quot; alert_threshold: \u0026quot;15\u0026quot; remote_system: \u0026quot;WN-D8877\u0026quot; state: \u0026quot;present\u0026quot; - name: Modify existing replication rule dellemc_powerstore_replicationrule: array_ip: \u0026quot;{{array_ip}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; replication_rule_name: \u0026quot;sample_replication_rule\u0026quot; new_name: \u0026quot;new_sample_replication_rule\u0026quot; rpo: \u0026quot;One_Hour\u0026quot; alert_threshold: \u0026quot;60\u0026quot; remote_system: \u0026quot;WN-D0517\u0026quot; state: \u0026quot;present\u0026quot; - name: Get details of replication rule dellemc_powerstore_replicationrule: array_ip: \u0026quot;{{array_ip}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; replication_rule_id: \u0026quot;{{id}}\u0026quot; state: \u0026quot;present\u0026quot; - name: Delete an existing replication rule dellemc_powerstore_replicationrule: array_ip: \u0026quot;{{array_ip}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; replication_rule_name: \u0026quot;new_sample_replication_rule\u0026quot; state: \u0026quot;absent\u0026quot; Return Values   Key Type Returned Description   changed   bool   always   Whether or not the resource has changed    replication_rule_details   complex   When replication rule exists   Details of the replication rule    \u0026nbsp; alert_threshold   int  success  Acceptable delay in minutes between the expected and actual replication sync intervals.    \u0026nbsp; id   str  success  The system generated ID of the replication rule    \u0026nbsp; name   str  success  Name of the replication rule    \u0026nbsp; remote_system_id   str  success  Unique identifier of the remote system to which this rule will replicate the associated resources.    \u0026nbsp; remote_system_name   str  success  Name of the remote system to which this rule will replicate the associated resources.    \u0026nbsp; rpo   str  success  Recovery point objective (RPO), which is the acceptable amount of data, measured in units of time, that may be lost in case of a failure.    Authors  P Srinivas Rao (@srinivas-rao5) ansible.team@dell.com   Protection Policy Module Perform Protection policy operations on PowerStore storage system\nSynopsis Performs all protection policy operations on PowerStore Storage System. This modules supports get details of an existing protection policy. Create new protection policy with existing Snapshot Rule or replication rule. Modify protection policy to change the name and description, and add or remove existing snapshot rules/ replication rule. Delete an existing protection policy.\nParameters   Parameter Type Required Default Choices Description   name  str      String variable. Indicates the name of the protection policy.    protectionpolicy_id  str      String variable. Indicates the id of the protection policy.    new_name  str      String variable. Indicates the new name of the protection policy. Used for renaming operation    snapshotrules  list elements: str      List of strings to specify the name or ids of snapshot rules which are to be added or removed, to or from, the protection policy.    replicationrule  str      The name or ids of the replcation rule which is to be added to the protection policy. To remove the replication rule, an empty string has to be passed.    description  str      String variable. Indicates the description of the protection policy.    state  str   True     present absent   String variable. Indicates the state of protection policy. For Delete operation only, it should be set to \"absent\" For all other operations like Create, Modify or Get details, it should be set to \"present\"    snapshotrule_state  str      present-in-policy absent-in-policy   String variable. Indicates the state of a snapshotrule in a protection policy. When snapshot rules are specified, this variable is required. present-in-policy indicates to add to protection policy. absent-in-policy indicates to remove from protection policy.    array_ip  str   True     IP or FQDN of the PowerStore management system.    verifycert  bool   True     True False   Boolean variable to specify whether to validate SSL certificate or not. True - indicates that the SSL certificate should be verified. Set the environment variable REQUESTS_CA_BUNDLE to the path of the SSL certificate. False - indicates that the SSL certificate should not be verified.    user  str   True     The username of the PowerStore host.    password  str   True     The password of the PowerStore host.    Notes  Before deleting a protection policy, the replication rule has to be removed from the protection policy.  Examples - name: Create a protection policy with snapshot rule and replication rule dellemc_powerstore_protectionpolicy: array_ip: \u0026quot;{{array_ip}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; name: \u0026quot;{{name}}\u0026quot; description: \u0026quot;{{description}}\u0026quot; snapshotrules: - \u0026quot;Ansible_test_snap_rule_1\u0026quot; replicationrule: \u0026quot;ansible_replication_rule_1\u0026quot; snapshotrule_state: \u0026quot;present-in-policy\u0026quot; state: \u0026quot;present\u0026quot; - name : Modify protection policy, change name dellemc_powerstore_protectionpolicy: array_ip: \u0026quot;{{array_ip}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; name: \u0026quot;{{name}}\u0026quot; new_name: \u0026quot;{{new_name}}\u0026quot; state: \u0026quot;present\u0026quot; - name : Modify protection policy, add snapshot rule dellemc_powerstore_protectionpolicy: array_ip: \u0026quot;{{array_ip}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; name: \u0026quot;{{name}}\u0026quot; snapshotrules: - \u0026quot;Ansible_test_snaprule_1\u0026quot; snapshotrule_state: \u0026quot;present-in-policy\u0026quot; state: \u0026quot;present\u0026quot; - name : Modify protection policy, remove snapshot rule, replication rule dellemc_powerstore_protectionpolicy: array_ip: \u0026quot;{{array_ip}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; name: \u0026quot;{{name}}\u0026quot; snapshotrules: - \u0026quot;Ansible_test_to_be_removed\u0026quot; replicationrule: \u0026quot;\u0026quot; snapshotrule_state: \u0026quot;absent-in-policy\u0026quot; state: \u0026quot;present\u0026quot; - name : Get details of protection policy by name dellemc_powerstore_protectionpolicy: array_ip: \u0026quot;{{array_ip}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; name: \u0026quot;{{name}}\u0026quot; state: \u0026quot;present\u0026quot; - name : Get details of protection policy by ID dellemc_powerstore_protectionpolicy: array_ip: \u0026quot;{{array_ip}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; protectionpolicy_id: \u0026quot;{{protectionpolicy_id}}\u0026quot; state: \u0026quot;present\u0026quot; - name : Delete protection policy dellemc_powerstore_protectionpolicy: array_ip: \u0026quot;{{array_ip}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; name: \u0026quot;{{name}}\u0026quot; state: \u0026quot;absent\u0026quot; Return Values   Key Type Returned Description   changed   bool   always   Whether or not the resource has changed    protectionpolicy_details   complex   When protection policy exists   Details of the protection policy    \u0026nbsp; description   str  success  description about the protection policy    \u0026nbsp; id   str  success  The system generated ID given to the protection policy    \u0026nbsp; name   str  success  Name of the protection policy    \u0026nbsp; replication_rules   complex  success  The replication rule details of the protection policy    \u0026nbsp; \u0026nbsp; id   str  success  The replication rule ID of the protection policy    \u0026nbsp; \u0026nbsp; name   str  success  The replication rule name of the protection policy    \u0026nbsp; snapshot_rules   complex  success  The snapshot rules details of the protection policy    \u0026nbsp; \u0026nbsp; id   str  success  The snapshot rule ID of the protection policy    \u0026nbsp; \u0026nbsp; name   str  success  The snapshot rule name of the protection policy    \u0026nbsp; type   str  success  The type for the protection policy    Authors  Arindam Datta (@dattaarindam) ansible.team@dell.com P Srinivas Rao (@srinivas-rao5) ansible.team@dell.com   Filesystem Snapshot Module Manage Filesystem Snapshots on Dell EMC PowerStore\nSynopsis Managing filesystem snapshots on PowerStore Storage System includes creating new filesystem snapshot, getting details of filesystem snapshot, modifying attributes of filesystem snapshot and deleting filesystem snapshot.\nParameters   Parameter Type Required Default Choices Description   snapshot_name  str      The name of the filesystem snapshot. Mandatory for create operation. Specify either snapshot name or ID (but not both) for any operation.    snapshot_id  str      The ID of the Snapshot.    filesystem  str      The ID/Name of the filesystem for which snapshot will be taken. If filesystem name is specified, then nas_server is required to uniquely identify the filesystem. Mandatory for create operation.    nas_server  str      The NAS server, this could be the name or ID of the NAS server.    description  str      The description for the filesystem snapshot.    desired_retention  int      The retention value for the Snapshot. If the desired_retention/expiration_timestamp is not mentioned during creation, snapshot will be created with unlimited retention. Maximum supported desired retention is 31 days.    retention_unit  str    hours    hours days   The unit for retention.    expiration_timestamp  str      The expiration timestamp of the snapshot. This should be provided in UTC format, e.g 2020-07-24T10:54:54Z. To remove the expiration timestamp, specify it as an empty string.    access_type  str      SNAPSHOT PROTOCOL   Specifies whether the snapshot directory or protocol access is granted to the filesystem snapshot. For create operation, if access_type is not specified, snapshot will be created with 'SNAPSHOT' access type.    state  str   True     absent present   Define whether the filesystem snapshot should exist or not.    array_ip  str   True     IP or FQDN of the PowerStore management system.    verifycert  bool   True     True False   Boolean variable to specify whether to validate SSL certificate or not. True - indicates that the SSL certificate should be verified. Set the environment variable REQUESTS_CA_BUNDLE to the path of the SSL certificate. False - indicates that the SSL certificate should not be verified.    user  str   True     The username of the PowerStore host.    password  str   True     The password of the PowerStore host.    Examples - name: Create filesystem snapshot dellemc_powerstore_filesystem_snapshot: array_ip: \u0026quot;{{array_ip}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; snapshot_name: \u0026quot;sample_filesystem_snapshot\u0026quot; nas_server: \u0026quot;ansible_nas_server\u0026quot; filesystem: \u0026quot;sample_filesystem\u0026quot; desired_retention: 20 retention_unit: \u0026quot;days\u0026quot; state: \u0026quot;present\u0026quot; - name: Get the details of filesystem snapshot dellemc_powerstore_filesystem_snapshot: array_ip: \u0026quot;{{array_ip}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; snapshot_id: \u0026quot;{{fs_snapshot_id}}\u0026quot; state: \u0026quot;present\u0026quot; - name: Modify the filesystem snapshot dellemc_powerstore_filesystem_snapshot: array_ip: \u0026quot;{{array_ip}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; snapshot_name: \u0026quot;sample_filesystem_snapshot\u0026quot; nas_server: \u0026quot;ansible_nas_server\u0026quot; description: \u0026quot;modify description\u0026quot; expiration_timestamp: \u0026quot;\u0026quot; state: \u0026quot;present\u0026quot; - name: Delete filesystem snapshot dellemc_powerstore_filesystem_snapshot: array_ip: \u0026quot;{{array_ip}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user: \u0026quot;{{user}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; snapshot_id: \u0026quot;{{fs_snapshot_id}}\u0026quot; state: \u0026quot;absent\u0026quot; Return Values   Key Type Returned Description   changed   bool   always   Whether or not the resource has changed    create_fs_snap   bool   always   Whether or not the resource has created    delete_fs_snap   bool   always   Whether or not the resource has deleted    filesystem_snap_details   dict   When snapshot exists.   Details of the snapshot.    \u0026nbsp; access_type   str  success  Displays the type of access allowed to the snapshot.    \u0026nbsp; creation_timestamp   str  success  The date and time the snapshot was created.    \u0026nbsp; description   str  success  Description of the filesystem snapshot.    \u0026nbsp; expiration_timestamp   str  success  The date and time the snapshot is due to be automatically deleted by the system.    \u0026nbsp; id   str  success  Unique identifier of the filesystem snapshot instance.    \u0026nbsp; name   str  success  The name of the snapshot.    \u0026nbsp; nas_server   dict  success  Details of NAS server on which snapshot is present.    \u0026nbsp; \u0026nbsp; id   str  success  ID of the NAS server.    \u0026nbsp; \u0026nbsp; name   str  success  Name of the NAS server    \u0026nbsp; parent_id   str  success  ID of the filesystem on which snapshot is taken.    \u0026nbsp; parent_name   str  success  Name of the filesystem on which snapshot is taken.    modify_fs_snap   bool   always   Whether or not the resource has modified    Authors  Akash Shendge (@shenda1) ansible.team@dell.com   ","excerpt":"Ansible Modules for Dell EMC PowerStore Product Guide 1.2 © 2021 Dell Inc. or its subsidiaries. All …","ref":"/ansible-docs/docs/storage/platforms/powerstore/product-guide/","title":"Product Guide"},{"body":"Ansible Modules for Dell EMC Unity Product Guide 1.2.0 © 2021 Dell Inc. or its subsidiaries. All rights reserved. Dell, EMC, and other trademarks are trademarks of Dell Inc. or its subsidiaries. Other trademarks may be trademarks of their respective owners.\n Contents  NFS Module  Synopsis Parameters Examples Return Values Authors   Volume Module  Synopsis Parameters Examples Return Values Authors   NAS Server Module  Synopsis Parameters Examples Return Values Authors   Quota Tree Module  Synopsis Parameters Examples Return Values Authors   File System Module  Synopsis Parameters Notes Examples Return Values Authors   Storage Pool Module  Synopsis Parameters Notes Examples Return Values Authors   Gatherfacts Module  Synopsis Parameters Examples Return Values Authors   User Quota Module  Synopsis Parameters Examples Return Values Authors   Filesystem Snapshot Module  Synopsis Parameters Notes Examples Return Values Authors   Snapshot Module  Synopsis Parameters Examples Return Values Authors   SMB Share Module  Synopsis Parameters Notes Examples Return Values Authors   Host Module  Synopsis Parameters Examples Return Values Authors   Consistency Group Module  Synopsis Parameters Examples Return Values Authors   Snapshot Schedule Module  Synopsis Parameters Notes Examples Return Values Authors     NFS Module Manage NFS export on Unity storage system\nSynopsis Managing NFS export on Unity storage system includes- Create new NFS export, Modify NFS export attributes, Display NFS export details, Delete NFS export\nParameters   Parameter Type Required Default Choices Description   nfs_export_name  str      Name of the nfs export. Mandatory for create operation. Specify either nfs_export_name or nfs_export_id(but not both) for any operation.    nfs_export_id  str      ID of the nfs export. This is a unique ID generated by Unity storage system.    filesystem_name  str      Name of the filesystem for which NFS export will be created. Either filesystem or snapshot is required for creation of the NFS. If filesystem name is specified, then nas_server is required to uniquely identify the filesystem If filesystem parameter is provided, then snapshot cannot be specified.    filesystem_id  str      ID of the filesystem This is a unique ID generated by Unity storage system.    snapshot_name  str      Name of the snapshot for which NFS export will be created. Either filesystem or snapshot is required for creation of the NFS export. If snapshot parameter is provided, then filesystem cannot be specified.    snapshot_id  str      ID of the snapshot. This is a unique ID generated by Unity storage system.    nas_server_name  str      Name of the NAS server on which filesystem will be hosted.    nas_server_id  str      ID of the NAS server on which filesystem will be hosted.    path  str      Local path to export relative to the NAS server root. With NFS, each export of a file_system or file_snap must have a unique local path. Mandatory while creating NFS export.    description  str      Description of the NFS export. Optional parameter when creating a NFS export. To modify description, pass the new value in description field. To remove description, pass the empty value in description field.    host_state  str      present-in-export absent-in-export   Define whether the hosts can access the NFS export. Required when adding or removing access of hosts from the export.    anonymous_uid  int      Specifies the user ID of the anonymous account. If not specified at the time of creation, it will be set to 4294967294.    anonymous_gid  int      Specifies the group ID of the anonymous account. If not specified at the time of creation, it will be set to 4294967294.    state  str   True     absent present   State variable to determine whether NFS export will exist or not.    default_access  str      NO_ACCESS READ_ONLY READ_WRITE ROOT READ_ONLY_ROOT   Default access level for all hosts that can access the NFS export. For hosts that need different access than the default, they can be configured by adding to the list. If default_access is not mentioned during creation, then NFS export will be created with NO_ACCESS.    min_security  str      SYS KERBEROS KERBEROS_WITH_INTEGRITY KERBEROS_WITH_ENCRYPTION   NFS enforced security type for users accessing a NFS export. If not specified at the time of creation, it will be set to SYS.    no_access_hosts  list elements: dict      Hosts with no access to the NFS export. List of dictionaries. Each dictionary will have any of the keys from host_name, host_id, and ip_address.    \u0026nbsp; host_name   str   False     Name of the host.    \u0026nbsp; host_id   str   False     ID of the host.    \u0026nbsp; ip_address   str   False     IP address of the host.    read_only_hosts  list elements: dict      Hosts with read-only access to the NFS export. List of dictionaries. Each dictionary will have any of the keys from host_name, host_id, and ip_address    \u0026nbsp; host_name   str   False     Name of the host.    \u0026nbsp; host_id   str   False     ID of the host.    \u0026nbsp; ip_address   str   False     IP address of the host.    read_only_root_hosts  list elements: dict      Hosts with read-only for root user access to the NFS export. List of dictionaries. Each dictionary will have any of the keys from host_name, host_id, and ip_address    \u0026nbsp; host_name   str   False     Name of the host.    \u0026nbsp; host_id   str   False     ID of the host.    \u0026nbsp; ip_address   str   False     IP address of the host.    read_write_hosts  list elements: dict      Hosts with read and write access to the NFS export. List of dictionaries. Each dictionary will have any of the keys from host_name, host_id, and ip_address.    \u0026nbsp; host_name   str   False     Name of the host.    \u0026nbsp; host_id   str   False     ID of the host.    \u0026nbsp; ip_address   str   False     IP address of the host.    read_write_root_hosts  list elements: dict      Hosts with read and write for root user access to the NFS export. List of dictionaries. Each dictionary will have any of the keys from host_name, host_id, and ip_address.    \u0026nbsp; host_name   str   False     Name of the host.    \u0026nbsp; host_id   str   False     ID of the host.    \u0026nbsp; ip_address   str   False     IP address of the host.    unispherehost  str   True     IP or FQDN of the Unity management server.    username  str   True     The username of the Unity management server.    password  str   True     The password of the Unity management server.    verifycert  bool    True    True False   Boolean variable to specify whether or not to validate SSL certificate. True - Indicates that the SSL certificate should be verified. False - Indicates that the SSL certificate should not be verified.    port  int    443    Port number through which communication happens with Unity management server.    Examples - name: Create nfs export from filesystem dellemc_unity_nfs: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; username: \u0026quot;{{username}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; nfs_export_name: \u0026quot;ansible_nfs_from_fs\u0026quot; path: '/' filesystem_id: \u0026quot;fs_377\u0026quot; state: \u0026quot;present\u0026quot; - name: Create nfs export from snapshot dellemc_unity_nfs: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; username: \u0026quot;{{username}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; nfs_export_name: \u0026quot;ansible_nfs_from_snap\u0026quot; path: '/' snapshot_name: \u0026quot;ansible_fs_snap\u0026quot; state: \u0026quot;present\u0026quot; - name: Modify nfs export dellemc_unity_nfs: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; username: \u0026quot;{{username}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; nfs_export_name: \u0026quot;ansible_nfs_from_fs\u0026quot; nas_server_id: \u0026quot;nas_3\u0026quot; description: \u0026quot;\u0026quot; default_access: \u0026quot;READ_ONLY_ROOT\u0026quot; anonymous_gid: 4294967290 anonymous_uid: 4294967290 state: \u0026quot;present\u0026quot; - name: Add host in nfs export dellemc_unity_nfs: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; username: \u0026quot;{{username}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; nfs_export_name: \u0026quot;ansible_nfs_from_fs\u0026quot; filesystem_id: \u0026quot;fs_377\u0026quot; no_access_hosts: - host_id: \u0026quot;Host_1\u0026quot; read_only_hosts: - host_id: \u0026quot;Host_2\u0026quot; read_only_root_hosts: - host_name: \u0026quot;host_name1\u0026quot; read_write_hosts: - host_name: \u0026quot;host_name2\u0026quot; read_write_root_hosts: - ip_address: \u0026quot;1.1.1.1\u0026quot; host_state: \u0026quot;present-in-export\u0026quot; state: \u0026quot;present\u0026quot; - name: Remove host in nfs export dellemc_unity_nfs: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; username: \u0026quot;{{username}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; nfs_export_name: \u0026quot;ansible_nfs_from_fs\u0026quot; filesystem_id: \u0026quot;fs_377\u0026quot; no_access_hosts: - host_id: \u0026quot;Host_1\u0026quot; read_only_hosts: - host_id: \u0026quot;Host_2\u0026quot; read_only_root_hosts: - host_name: \u0026quot;host_name1\u0026quot; read_write_hosts: - host_name: \u0026quot;host_name2\u0026quot; read_write_root_hosts: - ip_address: \u0026quot;1.1.1.1\u0026quot; host_state: \u0026quot;absent-in-export\u0026quot; state: \u0026quot;present\u0026quot; - name: Get nfs details dellemc_unity_nfs: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; username: \u0026quot;{{username}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; nfs_export_id: \u0026quot;NFSShare_291\u0026quot; state: \u0026quot;present\u0026quot; - name: Delete nfs export by nfs name dellemc_unity_nfs: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; username: \u0026quot;{{username}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; nfs_export_name: \u0026quot;ansible_nfs_name\u0026quot; nas_server_name: \u0026quot;ansible_nas_name\u0026quot; state: \u0026quot;absent\u0026quot; Return Values   Key Type Returned Description   changed   bool   always   Whether or not the resource has changed.    nfs_share_details   complex   When nfs export exists.   Details of the nfs export.    \u0026nbsp; anonymous_gid   int  success  Group ID of the anonymous account    \u0026nbsp; anonymous_uid   int  success  User ID of the anonymous account    \u0026nbsp; default_access   str  success  Default access level for all hosts that can access export    \u0026nbsp; description   str  success  Description about the nfs export    \u0026nbsp; export_paths   list  success  Export paths that can be used to mount and access export    \u0026nbsp; filesystem   complex  success  Details of the filesystem on which nfs export is present    \u0026nbsp; \u0026nbsp; UnityFileSystem   complex  success  filesystem details    \u0026nbsp; \u0026nbsp; \u0026nbsp; id   str  success  ID of the filesystem    \u0026nbsp; \u0026nbsp; \u0026nbsp; name   str  success  Name of the filesystem    \u0026nbsp; id   str  success  ID of the nfs export    \u0026nbsp; min_security   str  success  NFS enforced security type for users accessing an export    \u0026nbsp; name   str  success  Name of the nfs export    \u0026nbsp; nas_server   complex  success  Details of the nas server    \u0026nbsp; \u0026nbsp; UnityNasServer   complex  success  NAS server details    \u0026nbsp; \u0026nbsp; \u0026nbsp; id   str  success  ID of the nas server    \u0026nbsp; \u0026nbsp; \u0026nbsp; name   str  success  Name of the nas server    \u0026nbsp; no_access_hosts_string   str  success  Hosts with no access to the nfs export    \u0026nbsp; read_only_hosts_string   str  success  Hosts with read-only access to the nfs export    \u0026nbsp; read_only_root_hosts_string   str  success  Hosts with read-only for root user access to the nfs export    \u0026nbsp; read_write_hosts_string   str  success  Hosts with read and write access to the nfs export    \u0026nbsp; read_write_root_hosts_string   str  success  Hosts with read and write for root user access to export    \u0026nbsp; type   str  success  NFS export type. i.e. filesystem or snapshot    Authors  Vivek Soni (@v-soni11) ansible.team@dell.com   Volume Module Manage volume on Unity storage system\nSynopsis Managing volume on Unity storage system includes- Create new volume, Modify volume attributes, Map Volume to host, Unmap volume to host, Display volume details, Delete volume\nParameters   Parameter Type Required Default Choices Description   vol_name  str      The name of the volume. Mandatory only for create operation.    vol_id  str      The id of the volume. It can be used only for get, modify, map/unmap host, or delete operation.    pool_name  str      This is the name of the pool where the volume will be created. Either the pool_name or pool_id must be provided to create a new volume.    pool_id  str      This is the id of the pool where the volume will be created. Either the pool_name or pool_id must be provided to create a new volume.    size  int      The size of the volume.    cap_unit  str      GB TB   The unit of the volume size. It defaults to 'GB', if not specified.    description  str      Description about the volume. Description can be removed by passing empty string (\"\").    snap_schedule  str      Snapshot schedule assigned to the volume. Add/Remove/Modify the snapshot schedule for the volume.    compression  bool      Boolean variable , specifies whether or not to enable compression. Compression is supported only for thin volumes    is_thin  bool    True    Boolean variable , specifies whether or not it's a thin volume.    sp  str      SPA SPB   Storage Processor for this volume.    io_limit_policy  str      IO limit policy associated with this volume. Once it's set, it cannot be removed through ansible module but it can be changed.    host_name  str      Name of the host to be mapped/unmapped with this volume. Either host_name or host_id can be specified in one task along with mapping_state.    host_id  str      ID of the host to be mapped/unmapped with this volume. Either host_name or host_id can be specified in one task along with mapping_state.    hlu  int      Host Lun Unit to be mapped/unmapped with this volume. It's an optional parameter, hlu can be specified along with host_name or host_id and mapping_state. If hlu is not specified, unity will choose it automatically. The maximum value supported is 255.    mapping_state  str      mapped unmapped   State of host access for volume.    new_vol_name  str      New name of the volume for rename operation.    tiering_policy  str      AUTOTIER_HIGH AUTOTIER HIGHEST LOWEST   Tiering policy choices for how the storage resource data will be distributed among the tiers available in the pool.    state  str   True     absent present   State variable to determine whether volume will exist or not.    hosts  list elements: dict      Name of hosts for mapping to a volume    \u0026nbsp; host_name   str      Name of the host.    \u0026nbsp; host_id   str      ID of the host.    \u0026nbsp; hlu   str      Host Lun Unit to be mapped/unmapped with this volume. It's an optional parameter, hlu can be specified along with host_name or host_id and mapping_state. If hlu is not specified, unity will choose it automatically. The maximum value supported is 255.    unispherehost  str   True     IP or FQDN of the Unity management server.    username  str   True     The username of the Unity management server.    password  str   True     The password of the Unity management server.    verifycert  bool    True    True False   Boolean variable to specify whether or not to validate SSL certificate. True - Indicates that the SSL certificate should be verified. False - Indicates that the SSL certificate should not be verified.    port  int    443    Port number through which communication happens with Unity management server.    Examples - name: Create Volume dellemc_unity_volume: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; username: \u0026quot;{{username}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; vol_name: \u0026quot;{{vol_name}}\u0026quot; description: \u0026quot;{{description}}\u0026quot; pool_name: \u0026quot;{{pool}}\u0026quot; size: 2 cap_unit: \u0026quot;{{cap_GB}}\u0026quot; state: \u0026quot;{{state_present}}\u0026quot; - name: Expand Volume by volume id dellemc_unity_volume: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; username: \u0026quot;{{username}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; vol_id: \u0026quot;{{vol_id}}\u0026quot; size: 5 cap_unit: \u0026quot;{{cap_GB}}\u0026quot; state: \u0026quot;{{state_present}}\u0026quot; - name: Modify Volume, map host by host_name dellemc_unity_volume: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; username: \u0026quot;{{username}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; vol_name: \u0026quot;{{vol_name}}\u0026quot; host_name: \u0026quot;{{host_name}}\u0026quot; hlu: 5 mapping_state: \u0026quot;{{state_mapped}}\u0026quot; state: \u0026quot;{{state_present}}\u0026quot; - name: Modify Volume, unmap host mapping by host_name dellemc_unity_volume: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; username: \u0026quot;{{username}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; vol_name: \u0026quot;{{vol_name}}\u0026quot; host_name: \u0026quot;{{host_name}}\u0026quot; mapping_state: \u0026quot;{{state_unmapped}}\u0026quot; state: \u0026quot;{{state_present}}\u0026quot; - name: Map multiple hosts to a Volume dellemc_unity_volume: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; username: \u0026quot;{{username}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; vol_id: \u0026quot;{{vol_id}}\u0026quot; hosts: - host_name: \u0026quot;10.226.198.248\u0026quot; hlu: 1 - host_id: \u0026quot;Host_929\u0026quot; hlu: 2 mapping_state: \u0026quot;mapped\u0026quot; state: \u0026quot;present\u0026quot; - name: Modify Volume attributes dellemc_unity_volume: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; username: \u0026quot;{{username}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; vol_name: \u0026quot;{{vol_name}}\u0026quot; new_vol_name: \u0026quot;{{new_vol_name}}\u0026quot; tiering_policy: \u0026quot;AUTOTIER\u0026quot; compression: True state: \u0026quot;{{state_present}}\u0026quot; - name: Delete Volume by vol name dellemc_unity_volume: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; username: \u0026quot;{{username}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; vol_name: \u0026quot;{{vol_name}}\u0026quot; state: \u0026quot;{{state_absent}}\u0026quot; - name: Delete Volume by vol id dellemc_unity_volume: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; username: \u0026quot;{{username}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; vol_id: \u0026quot;{{vol_id}}\u0026quot; state: \u0026quot;{{state_absent}}\u0026quot; Return Values   Key Type Returned Description   changed   bool   always   Whether or not the resource has changed    volume_details   complex   When volume exists   Details of the volume    \u0026nbsp; current_sp   str  success  Current storage processor for this volume    \u0026nbsp; description   str  success  Description about the volume    \u0026nbsp; host_access   list  success  Host mapped to this volume    \u0026nbsp; id   str  success  The system generated ID given to the volume    \u0026nbsp; io_limit_policy   dict  success  IO limit policy associated with this volume    \u0026nbsp; is_data_reduction_enabled   bool  success  Whether or not compression enabled on this volume    \u0026nbsp; is_thin_enabled   bool  success  Indicates whether thin provisioning is enabled for this volume    \u0026nbsp; name   str  success  Name of the volume    \u0026nbsp; pool   dict  success  The pool in which this volume is allocated.    \u0026nbsp; size_total_with_unit   str  success  Size of the volume with actual unit.    \u0026nbsp; snap_schedule   dict  success  Snapshot schedule applied to this volume    \u0026nbsp; tiering_policy   str  success  Tiering policy applied to this volume    \u0026nbsp; wwn   str  success  The world wide name of this volume    Authors  Arindam Datta (@arindam-emc) ansible.team@dell.com   NAS Server Module Manage NAS servers on Unity storage system\nSynopsis Managing NAS servers on Unity storage system includes get, modification to the NAS servers.\nParameters   Parameter Type Required Default Choices Description   nas_server_id  str      The ID of the NAS server. nas_server_name and nas_server_id are mutually exclusive parameters. Either one is required to perform the task.    nas_server_name  str      The Name of the NAS server. nas_server_name and nas_server_id are mutually exclusive parameters. Either one is required to perform the task.    nas_server_new_name  str      The new name of the NAS server. It can be mentioned during modification of the NAS server.    is_replication_destination  bool      It specifies whether the NAS server is a replication destination. It can be mentioned during modification of the NAS server.    is_backup_only  bool      It specifies whether the NAS server is used as backup only. It can be mentioned during modification of the NAS server.    is_multiprotocol_enabled  bool      This parameter indicates whether multiprotocol sharing mode is enabled. It can be mentioned during modification of the NAS server.    allow_unmapped_user  bool      This flag is used to mandatorily disable access in case of any user mapping failure. If true, then enable access in case of any user mapping failure. If false, then disable access in case of any user mapping failure. It can be mentioned during modification of the NAS server.    default_windows_user  str      Default windows user name used for granting access in the case of Unix to Windows user mapping failure. It can be mentioned during modification of the NAS server.    default_unix_user  str      Default Unix user name used for granting access in the case of Windows to Unix user mapping failure. It can be mentioned during modification of the NAS server.    enable_windows_to_unix_username_mapping  bool      This parameter indicates whether a Unix to/from Windows user name mapping is enabled. It can be mentioned during modification of the NAS server.    is_packet_reflect_enabled  bool      If the packet has to be reflected, then this parameter has to be set to True. It can be mentioned during modification of the NAS server.    current_unix_directory_service  str      NONE NIS LOCAL LDAP LOCAL_THEN_NIS LOCAL_THEN_LDAP   This is the directory service used for querying identity information for UNIX (such as UIDs, GIDs, net groups). It can be mentioned during modification of the NAS server.    state  str   True     present absent   Define the state of NAS server on the array. present indicates that NAS server should exist on the system after the task is executed. Right now deletion of NAS server is not supported. Hence, if state is set to absent for any existing NAS server then error will be thrown. For any non-existing NAS server, if state is set to absent then it will return None.    unispherehost  str   True     IP or FQDN of the Unity management server.    username  str   True     The username of the Unity management server.    password  str   True     The password of the Unity management server.    verifycert  bool    True    True False   Boolean variable to specify whether or not to validate SSL certificate. True - Indicates that the SSL certificate should be verified. False - Indicates that the SSL certificate should not be verified.    port  int    443    Port number through which communication happens with Unity management server.    Examples  - name: Get Details of NAS Server dellemc_unity_nasserver: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; username: \u0026quot;{{username}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; nas_server_name: \u0026quot;{{nas_server_name}}\u0026quot; state: \u0026quot;present\u0026quot; - name: Modify Details of NAS Server dellemc_unity_nasserver: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; username: \u0026quot;{{username}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; nas_server_name: \u0026quot;{{nas_server_name}}\u0026quot; nas_server_new_name: \u0026quot;updated_sample_nas_server\u0026quot; is_replication_destination: False is_backup_only: False is_multiprotocol_enabled: True allow_unmapped_user: True default_unix_user: \u0026quot;default_unix_sample_user\u0026quot; default_windows_user: \u0026quot;default_windows_sample_user\u0026quot; enable_windows_to_unix_username_mapping: True current_unix_directory_service: \u0026quot;LDAP\u0026quot; is_packet_reflect_enabled: True state: \u0026quot;present\u0026quot; Return Values   Key Type Returned Description   changed   bool   always   Whether or not the resource has changed    nas_server_details   complex   When NAS server exists.   The NAS server details.    \u0026nbsp; allow_unmapped_user   bool  success  enable/disable access status in case of any user mapping failure    \u0026nbsp; current_unix_directory_service   str  success  Directory service used for querying identity information for UNIX (such as UIDs, GIDs, net groups).    \u0026nbsp; default_unix_user   str  success  Default Unix user name used for granting access in the case of Windows to Unix user mapping failure.    \u0026nbsp; default_windows_user   str  success  Default windows user name used for granting access in the case of Unix to Windows user mapping failure    \u0026nbsp; id   str  success  ID of the NAS server    \u0026nbsp; is_backup_only   bool  success  Whether the NAS server is used as backup only.    \u0026nbsp; is_multi_protocol_enabled   bool  success  Indicates whether multiprotocol sharing mode is enabled    \u0026nbsp; is_packet_reflect_enabled   bool  success  If the packet reflect has to be enabled    \u0026nbsp; is_replication_destination   bool  success  If the NAS server is a replication destination then True.    \u0026nbsp; is_windows_to_unix_username_mapping_enabled   bool  success  Indicates whether a Unix to/from Windows user name mapping is enabled.    \u0026nbsp; name   str  success  Name of the NAS server    Authors  P Srinivas Rao (@srinivas-rao5) ansible.team@dell.com   Quota Tree Module Manage quota tree on the Unity storage system\nSynopsis Managing Quota tree on the Unity storage system includes Create quota tree, Get quota tree, Modify quota tree and Delete quota tree\nParameters   Parameter Type Required Default Choices Description   filesystem_name  str      The name of the filesystem for which quota tree is created. For creation or modification of a quota tree either filesystem_name or filesystem_id is required.    filesystem_id  str      The ID of the filesystem for which the quota tree is created. For creation of a quota tree either filesystem_id or filesystem_name is required.    nas_server_name  str      The name of the NAS server in which the filesystem is created. For creation of a quota tree either nas_server_name or nas_server_id is required.    nas_server_id  str      The ID of the NAS server in which the filesystem is created. For creation of a quota tree either filesystem_id or filesystem_name is required.    tree_quota_id  str      The ID of the quota tree. Either tree_quota_id or path to quota tree is required to view/modify/delete quota tree.    path  str      The path to the quota tree. Either tree_quota_id or path to quota tree is required to create/view/modify/delete a quota tree. Path must start with a forward slash '/'.    hard_limit  int      Hard limitation for a quota tree on the total space available. If exceeded, users in quota tree cannot write data. Value 0 implies no limit. One of the values of soft_limit and hard_limit can be 0, however, both cannot be both 0 during creation of a quota tree.    soft_limit  int      Soft limitation for a quota tree on the total space available. If exceeded, notification will be sent to users in the quota tree for the grace period mentioned, beyond which users cannot use space. Value 0 implies no limit. Both soft_limit and hard_limit cannot be 0 during creation of quota tree.    cap_unit  str      MB GB TB   Unit of soft_limit and hard_limit size. It defaults to 'GB' if not specified.    description  str      Description of a quota tree.    state  str   True     absent present   The state option is used to mention the existence of the filesystem quota tree.    unispherehost  str   True     IP or FQDN of the Unity management server.    username  str   True     The username of the Unity management server.    password  str   True     The password of the Unity management server.    verifycert  bool    True    True False   Boolean variable to specify whether or not to validate SSL certificate. True - Indicates that the SSL certificate should be verified. False - Indicates that the SSL certificate should not be verified.    port  int    443    Port number through which communication happens with Unity management server.    Examples  - name: Get quota tree details by quota tree id dellemc_unity_tree_quota: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; username: \u0026quot;{{username}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; quota_tree_id: \u0026quot;treequota_171798700679_10\u0026quot; state: \u0026quot;present\u0026quot; - name: Get quota tree details by quota tree path dellemc_unity_tree_quota: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; username: \u0026quot;{{username}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; filesystem_name: \u0026quot;fs_2171\u0026quot; nas_server_id: \u0026quot;nas_21\u0026quot; path: \u0026quot;/test\u0026quot; state: \u0026quot;present\u0026quot; - name: Create quota tree for a filesystem with filesystem id dellemc_unity_tree_quota: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; username: \u0026quot;{{username}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; filesystem_id: \u0026quot;fs_2171\u0026quot; hard_limit: 6 cap_unit: \u0026quot;TB\u0026quot; soft_limit: 5 path: \u0026quot;/test_new\u0026quot; state: \u0026quot;present\u0026quot; - name: Create quota tree for a filesystem with filesystem name dellemc_unity_tree_quota: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; username: \u0026quot;{{username}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; filesystem_name: \u0026quot;Test_filesystem\u0026quot; nas_server_name: \u0026quot;lglad068\u0026quot; hard_limit: 6 cap_unit: \u0026quot;TB\u0026quot; soft_limit: 5 path: \u0026quot;/test_new\u0026quot; state: \u0026quot;present\u0026quot; - name: Modify quota tree limit usage by quota tree path dellemc_unity_tree_quota: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; username: \u0026quot;{{username}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; path: \u0026quot;/test_new\u0026quot; hard_limit: 10 cap_unit: \u0026quot;TB\u0026quot; soft_limit: 8 state: \u0026quot;present\u0026quot; - name: Modify quota tree by quota tree id dellemc_unity_tree_quota: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; username: \u0026quot;{{username}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; filesystem_id: \u0026quot;fs_2171\u0026quot; quota_tree_id: \u0026quot;treequota_171798700679_10\u0026quot; hard_limit: 12 cap_unit: \u0026quot;TB\u0026quot; soft_limit: 10 state: \u0026quot;present\u0026quot; - name: Delete quota tree by quota tree id dellemc_unity_tree_quota: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; username: \u0026quot;{{username}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; filesystem_id: \u0026quot;fs_2171\u0026quot; quota_tree_id: \u0026quot;treequota_171798700679_10\u0026quot; state: \u0026quot;absent\u0026quot; - name: Delete quota tree by path dellemc_unity_tree_quota: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; username: \u0026quot;{{username}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; filesystem_id: \u0026quot;fs_2171\u0026quot; path: \u0026quot;/test_new\u0026quot; state: \u0026quot;absent\u0026quot; Return Values   Key Type Returned Description   changed   bool   always   Whether or not the resource has changed    get_quota_tree_details   complex   When quota tree exists   Details of the quota tree.    \u0026nbsp; description   str  success  Description of the quota tree.    \u0026nbsp; filesystem   complex  success  Filesystem details for which the quota tree is created.    \u0026nbsp; \u0026nbsp; UnityFileSystem   complex  success  Filesystem details for which the quota tree is created.    \u0026nbsp; \u0026nbsp; \u0026nbsp; id   str  success  ID of the filesystem for which the quota tree is create.    \u0026nbsp; gp_left   int  success  The grace period left after the soft limit for the user quota is exceeded.    \u0026nbsp; hard_limit   int  success  Hard limit of quota tree. If the quota tree's space usage exceeds the hard limit, users in quota tree cannot write data.    \u0026nbsp; id   str  success  Quota tree ID.    \u0026nbsp; path   str  success  Path to quota tree. A valid path must start with a forward slash '/'. It is mandatory while creating a quota tree.    \u0026nbsp; size_used   int  success  Size of used space in the filesystem by the user files.    \u0026nbsp; soft_limit   int  success  Soft limit of the quota tree. If the quota tree's space usage exceeds the soft limit, the storage system starts to count down based on the specified grace period.    \u0026nbsp; state   int  success  State of the quota tree.    Authors  Spandita Panigrahi (@panigs7) ansible.team@dell.com   File System Module Manage filesystem on Unity storage system\nSynopsis Managing filesystem on Unity storage system includes- Create new filesystem, Modify snapschedule attribute of filesystem Modify filesystem attributes, Display filesystem details, Display filesystem snapshots, Display filesystem snapschedule, Delete snapschedule associated with the filesystem, Delete filesystem, Create new filesystem with quota configuration\nParameters   Parameter Type Required Default Choices Description   filesystem_name  str      The name of the filesystem. Mandatory only for the create operation. All the operations are supported through 'filesystem_name' It's mutually exclusive with 'filesystem_id'.    filesystem_id  str      The id of the filesystem.It's mutually exclusive with 'filesystem_name' It can be used only for get, modify, or delete operations.    pool_name  str      This is the name of the pool where the filesystem will be created. Either the pool_name or pool_id must be provided to create a new filesystem.    pool_id  str      This is the ID of the pool where the filesystem will be created. Either the pool_name or pool_id must be provided to create a new filesystem.    size  int      The size of the filesystem.    cap_unit  str      GB TB   The unit of the filesystem size. It defaults to 'GB', if not specified.    nas_server_name  str      Name of the NAS server on which filesystem will be hosted.    nas_server_id  str      ID of the NAS server on which filesystem will be hosted.    supported_protocols  str      NFS CIFS MULTIPROTOCOL   Protocols supported by the file system. It will be overridden by NAS server configuration if NAS Server is Multiprotocol    description  str      Description about the filesystem. Description can be removed by passing empty string (\"\").    smb_properties  dict      Advance settings for SMB. It contains optional candidate variables    \u0026nbsp; is_smb_sync_writes_enabled   bool      Indicates whether the synchronous writes option is enabled on the file system.    \u0026nbsp; is_smb_notify_on_access_enabled   bool      Indicates whether notifications of changes to directory file structure are enabled.    \u0026nbsp; is_smb_op_locks_enabled   bool      Indicates whether opportunistic file locking is enabled on the file system.    \u0026nbsp; is_smb_notify_on_write_enabled   bool      Indicates whether file write notifications are enabled on the file system.    \u0026nbsp; smb_notify_on_change_dir_depth   int      Integer variable, determines the lowest directory level to which the enabled notifications apply. Minimum value is 1.    data_reduction  bool      Boolean variable, specifies whether or not to enable compression. Compression is supported only for thin filesystem    is_thin  bool      Boolean variable, specifies whether or not it's a thin filesystem.    access_policy  str      NATIVE UNIX WINDOWS   Access policy of a filesystem.    locking_policy  str      ADVISORY MANDATORY   File system locking policies. These policy choices control whether the NFSv4 range locks must be honored.    tiering_policy  str      AUTOTIER_HIGH AUTOTIER HIGHEST LOWEST   Tiering policy choices for how the storage resource data will be distributed among the tiers available in the pool.    quota_config  dict      Configuration for quota management. It contains optional parameters.    \u0026nbsp; grace_period   int      Grace period set in quota configuration after soft limit is reached. If grace_period is not set during creation of filesystem, it will be set to '7 days' by default.    \u0026nbsp; grace_period_unit   str      minutes hours days   Unit of grace period. Default unit is 'days'.    \u0026nbsp; default_hard_limit   int      Default hard limit for user quotas and tree quotas. If default_hard_limit is not set while creation of filesystem, it will be set to 0B by default.    \u0026nbsp; default_soft_limit   int      Default soft limit for user quotas and tree quotas. If default_soft_limit is not set while creation of filesystem, it will be set to 0B by default.    \u0026nbsp; is_user_quota_enabled   bool      Indicates whether the user quota is enabled. Parameters 'is_user_quota_enabled' and 'quota_policy' are mutually exclusive. If is_user_quota_enabled is not set while creation of filesystem, it will be set to false by default.    \u0026nbsp; quota_policy   str      FILE_SIZE BLOCKS   Quota policy set in quota configuration. Parameters 'is_user_quota_enabled' and 'quota_policy' are mutually exclusive. If quota_policy is not set while creation of filesystem, it will be set to \"FILE_SIZE\" by default.    \u0026nbsp; cap_unit   str      MB GB TB   Unit of default_soft_limit and default_hard_limit size. Default unit is 'GB'.    state  str   True     absent present   State variable to determine whether filesystem will exist or not.    snap_schedule_name  str      This is the name of an existing snapshot schedule which is to be associated with the filesystem. This is mutually exclusive with snapshot schedule id.    snap_schedule_id  str      This is the id of an existing snapshot schedule which is to be associated with the filesystem. This is mutually exclusive with snapshot schedule name. filesystem.    unispherehost  str   True     IP or FQDN of the Unity management server.    username  str   True     The username of the Unity management server.    password  str   True     The password of the Unity management server.    verifycert  bool    True    True False   Boolean variable to specify whether or not to validate SSL certificate. True - Indicates that the SSL certificate should be verified. False - Indicates that the SSL certificate should not be verified.    port  int    443    Port number through which communication happens with Unity management server.    Notes  SMB shares, NFS exports, and snapshots associated with filesystem need to be deleted prior to deleting a filesystem. quota_config parameter can be used to update default hard limit and soft limit values to limit the maximum space that can be used. By default they both are set to 0 during filesystem creation which means unlimited.  Examples - name: Create FileSystem dellemc_unity_filesystem: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; username: \u0026quot;{{username}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; filesystem_name: \u0026quot;ansible_test_fs\u0026quot; nas_server_name: \u0026quot;lglap761\u0026quot; pool_name: \u0026quot;pool_1\u0026quot; size: 5 state: \u0026quot;present\u0026quot; - name: Create FileSystem with quota configuration dellemc_unity_filesystem: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; username: \u0026quot;{{username}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; filesystem_name: \u0026quot;ansible_test_fs\u0026quot; nas_server_name: \u0026quot;lglap761\u0026quot; pool_name: \u0026quot;pool_1\u0026quot; size: 5 quota_config: grace_period: 8 grace_period_unit: \u0026quot;days\u0026quot; default_soft_limit: 10 is_user_quota_enabled: False state: \u0026quot;present\u0026quot; - name: Expand FileSystem size dellemc_unity_filesystem: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; username: \u0026quot;{{username}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; filesystem_name: \u0026quot;ansible_test_fs\u0026quot; nas_server_name: \u0026quot;lglap761\u0026quot; size: 10 state: \u0026quot;present\u0026quot; - name: Expand FileSystem size dellemc_unity_filesystem: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; username: \u0026quot;{{username}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; filesystem_name: \u0026quot;ansible_test_fs\u0026quot; nas_server_name: \u0026quot;lglap761\u0026quot; size: 10 state: \u0026quot;present\u0026quot; - name: Modify FileSystem smb_properties dellemc_unity_filesystem: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; username: \u0026quot;{{username}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; filesystem_name: \u0026quot;ansible_test_fs\u0026quot; nas_server_name: \u0026quot;lglap761\u0026quot; smb_properties: is_smb_op_locks_enabled: True smb_notify_on_change_dir_depth: 5 is_smb_notify_on_access_enabled: True state: \u0026quot;present\u0026quot; - name: Modify FileSystem Snap Schedule dellemc_unity_filesystem: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; username: \u0026quot;{{username}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; filesystem_id: \u0026quot;fs_141\u0026quot; snap_schedule_id: \u0026quot;{{snap_schedule_id}}\u0026quot; state: \u0026quot;{{state_present}}\u0026quot; - name: Get details of FileSystem using id dellemc_unity_filesystem: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; username: \u0026quot;{{username}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; filesystem_id: \u0026quot;rs_405\u0026quot; state: \u0026quot;present\u0026quot; - name: Delete a FileSystem using id dellemc_unity_filesystem: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; username: \u0026quot;{{username}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; filesystem_id: \u0026quot;rs_405\u0026quot; state: \u0026quot;absent\u0026quot; Return Values   Key Type Returned Description   changed   bool   always   Whether or not the resource has changed    filesystem_snapshot_details   complex   When filesystem snapshot exists   Details of the filesystem snapshot.    \u0026nbsp; access_type   str  success  Access type of filesystem snapshot.    \u0026nbsp; attached_wwn   str  success  Attached WWN details.    \u0026nbsp; creation_time   str  success  Creation time of filesystem snapshot.    \u0026nbsp; creator_schedule   str  success  Creator schedule of filesystem snapshot.    \u0026nbsp; creator_type   str  success  Creator type for filesystem snapshot.    \u0026nbsp; creator_user   str  success  Creator user for filesystem snapshot.    \u0026nbsp; description   str  success  Description of the filesystem snapshot.    \u0026nbsp; expiration_time   str  success  Date and time after which the filesystem snapshot will expire.    \u0026nbsp; filesystem_id   str  success  Id of the filesystem for which the snapshot exists.    \u0026nbsp; filesystem_name   str  success  Name of the filesystem for which the snapshot exists.    \u0026nbsp; id   str  success  Unique identifier of the filesystem snapshot instance.    \u0026nbsp; is_auto_delete   bool  success  Is the filesystem snapshot is auto deleted or not.    \u0026nbsp; name   str  success  The name of the filesystem snapshot.    \u0026nbsp; nas_server_id   str  success  Id of the NAS server on which filesystem exists.    \u0026nbsp; nas_server_name   str  success  Name of the NAS server on which filesystem exists.    \u0026nbsp; size   int  success  Size of the filesystem snapshot.    Authors  Arindam Datta (@dattaarindam) ansible.team@dell.com Meenakshi Dembi (@dembim) ansible.team@dell.com Spandita Panigrahi (@panigs7) ansible.team@dell.com   Storage Pool Module Manage storage pool on Unity\nSynopsis Managing storage pool on Unity storage system contains the following operations Get details of storage pool Modify storage pool\nParameters   Parameter Type Required Default Choices Description   pool_name  str      Name of the storage pool, unique in the storage system.    pool_id  str      Unique identifier of the pool instance.    new_pool_name  str      New name of the storage pool, unique in the storage system.    pool_description  str      The description of the storage pool.    fast_cache  str      enabled disabled   Indicates whether the fast cache is enabled for the storage pool. enabled - FAST Cache is enabled for the pool. disabled - FAST Cache is disabled for the pool.    fast_vp  str      enabled disabled   Indicates whether to enable scheduled data relocations for the pool. enabled - Enabled scheduled data relocations for the pool. disabled - Disabled scheduled data relocations for the pool.    state  str   True     absent present   Define whether the storage pool should exist or not. present - indicates that the storage pool should exist on the system. absent - indicates that the storage pool should not exist on the system.    unispherehost  str   True     IP or FQDN of the Unity management server.    username  str   True     The username of the Unity management server.    password  str   True     The password of the Unity management server.    verifycert  bool    True    True False   Boolean variable to specify whether or not to validate SSL certificate. True - Indicates that the SSL certificate should be verified. False - Indicates that the SSL certificate should not be verified.    port  int    443    Port number through which communication happens with Unity management server.    Notes  Creation/Deletion of storage pool is not allowed through Ansible module.  Examples - name: Get Storage pool details using pool_name dellemc_unity_storagepool: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; username: \u0026quot;{{username}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; pool_name: \u0026quot;{{pool_name}}\u0026quot; state: \u0026quot;present\u0026quot; - name: Get Storage pool details using pool_id dellemc_unity_storagepool: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; username: \u0026quot;{{username}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; pool_id: \u0026quot;{{pool_id}}\u0026quot; state: \u0026quot;present\u0026quot; - name: Modify Storage pool attributes using pool_name dellemc_unity_storagepool: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; username: \u0026quot;{{username}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; pool_name: \u0026quot;{{pool_name}}\u0026quot; new_pool_name: \u0026quot;{{new_pool_name}}\u0026quot; pool_description: \u0026quot;{{pool_description}}\u0026quot; fast_cache: \u0026quot;{{fast_cache_enabled}}\u0026quot; fast_vp: \u0026quot;{{fast_vp_enabled}}\u0026quot; state: \u0026quot;present\u0026quot; - name: Modify Storage pool attributes using pool_id dellemc_unity_storagepool: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; username: \u0026quot;{{username}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; pool_id: \u0026quot;{{pool_id}}\u0026quot; new_pool_name: \u0026quot;{{new_pool_name}}\u0026quot; pool_description: \u0026quot;{{pool_description}}\u0026quot; fast_cache: \u0026quot;{{fast_cache_enabled}}\u0026quot; fast_vp: \u0026quot;{{fast_vp_enabled}}\u0026quot; state: \u0026quot;present\u0026quot; Return Values   Key Type Returned Description   changed   bool   always   Whether or not the storage pool has changed.    storage_pool_details   complex   When storage pool exists.   The storage pool details.    \u0026nbsp; id   str  success  Pool id, unique identifier of the pool.    \u0026nbsp; is_fast_cache_enabled   bool  success  Indicates whether the fast cache is enabled for the storage pool. true - FAST Cache is enabled for the pool. false - FAST Cache is disabled for the pool.    \u0026nbsp; is_fast_vp_enabled   bool  success  Indicates whether to enable scheduled data relocations for the storage pool. true - Enabled scheduled data relocations for the pool. false - Disabled scheduled data relocations for the pool.    \u0026nbsp; name   str  success  Pool name, unique in the storage system.    \u0026nbsp; size_free_with_unit   str  success  Indicates size_free with its appropriate unit in human readable form.    \u0026nbsp; size_subscribed_with_unit   str  success  Indicates size_subscribed with its appropriate unit in human readable form.    \u0026nbsp; size_total_with_unit   str  success  Indicates size_total with its appropriate unit in human readable form.    \u0026nbsp; size_used_with_unit   str  success  Indicates size_used with its appropriate unit in human readable form.    \u0026nbsp; snap_size_subscribed_with_unit   str  success  Indicates snap_size_subscribed with its appropriate unit in human readable form.    \u0026nbsp; snap_size_used_with_unit   str  success  Indicates snap_size_used with its appropriate unit in human readable form.    Authors  Ambuj Dubey (@AmbujDube) ansible.team@dell.com   Gatherfacts Module Gathering information about DellEMC Unity\nSynopsis Gathering information about DellEMC Unity storage system includes Get the details of Unity array, Get list of Hosts in Unity array, Get list of FC initiators in Unity array, Get list of iSCSI initiators in Unity array, Get list of Consistency groups in Unity array, Get list of Storage pools in Unity array, Get list of Volumes in Unity array, Get list of Snapshot schedules in Unity array, Get list of NAS servers in Unity array, Get list of File systems in Unity array, Get list of Snapshots in Unity array, Get list of SMB shares in Unity array, Get list of NFS exports in Unity array, Get list of User quotas in Unity array, Get list of Quota tree in Unity array\nParameters   Parameter Type Required Default Choices Description   gather_subset  list elements: str      host fc_initiator iscsi_initiator cg storage_pool vol snapshot_schedule nas_server file_system snapshot nfs_export smb_share user_quota tree_quota   List of string variables to specify the Unity storage system entities for which information is required. host fc_initiator iscsi_initiator cg storage_pool vol snapshot_schedule nas_server file_system snapshot nfs_export smb_share user_quota tree_quota    unispherehost  str   True     IP or FQDN of the Unity management server.    username  str   True     The username of the Unity management server.    password  str   True     The password of the Unity management server.    verifycert  bool    True    True False   Boolean variable to specify whether or not to validate SSL certificate. True - Indicates that the SSL certificate should be verified. False - Indicates that the SSL certificate should not be verified.    port  int    443    Port number through which communication happens with Unity management server.    Examples  - name: Get detailed list of Unity entities. dellemc_unity_gatherfacts: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; username: \u0026quot;{{username}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; gather_subset: - host - fc_initiator - iscsi_initiator - cg - storage_pool - vol - snapshot_schedule - nas_server - file_system - snapshot - nfs_export - smb_share - user_quota - tree_quota - name: Get information of Unity array. dellemc_unity_gatherfacts: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; username: \u0026quot;{{username}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; - name: Get list of hosts on Unity array. dellemc_unity_gatherfacts: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; username: \u0026quot;{{username}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; gather_subset: - host - name: Get list of FC initiators on Unity array. dellemc_unity_gatherfacts: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; username: \u0026quot;{{username}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; gather_subset: - fc_initiator - name: Get list of ISCSI initiators on Unity array. dellemc_unity_gatherfacts: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; username: \u0026quot;{{username}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; gather_subset: - iscsi_initiator - name: Get list of consistency groups on Unity array. dellemc_unity_gatherfacts: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; username: \u0026quot;{{username}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; gather_subset: - cg - name: Get list of storage pools on Unity array. dellemc_unity_gatherfacts: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; username: \u0026quot;{{username}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; gather_subset: - storage_pool - name: Get list of volumes on Unity array. dellemc_unity_gatherfacts: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; username: \u0026quot;{{username}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; gather_subset: - vol - name: Get list of snapshot schedules on Unity array. dellemc_unity_gatherfacts: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; username: \u0026quot;{{username}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; gather_subset: - snapshot_schedule - name: Get list of NAS Servers on Unity array. dellemc_unity_gatherfacts: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; username: \u0026quot;{{username}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; gather_subset: - nas_server - name: Get list of File Systems on Unity array. dellemc_unity_gatherfacts: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; username: \u0026quot;{{username}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; gather_subset: - file_system - name: Get list of Snapshots on Unity array. dellemc_unity_gatherfacts: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; username: \u0026quot;{{username}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; gather_subset: - snapshot - name: Get list of NFS exports on Unity array. dellemc_unity_gatherfacts: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; username: \u0026quot;{{username}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; gather_subset: - nfs_export - name: Get list of SMB shares on Unity array. dellemc_unity_gatherfacts: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; username: \u0026quot;{{username}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; gather_subset: - smb_share - name: Get list of user quotas on Unity array. dellemc_unity_gatherfacts: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; username: \u0026quot;{{username}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; gather_subset: - user_quota - name: Get list of quota trees on Unity array. dellemc_unity_gatherfacts: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; username: \u0026quot;{{username}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; gather_subset: - tree_quota Return Values   Key Type Returned Description   Array_Details   complex   always   Details of the Unity Array.    \u0026nbsp; api_version   str  success  The current api version of the Unity Array.    \u0026nbsp; earliest_api_version   str  success  The earliest api version of the Unity Array.    \u0026nbsp; model   str  success  The model of the Unity Array.    \u0026nbsp; name   str  success  The name of the Unity Array.    \u0026nbsp; software_version   str  success  The software version of the Unity Array.    Consistency_Groups   complex   When Consistency Groups exist.   Details of the Consistency Groups.    \u0026nbsp; id   str  success  The ID of the Consistency Group.    \u0026nbsp; name   str  success  The name of the Consistency Group.    FC_initiators   complex   When FC initiator exist.   Details of the FC initiators.    \u0026nbsp; WWN   str  success  The WWN of the FC initiator.    \u0026nbsp; id   str  success  The id of the FC initiator.    File_Systems   complex   When File Systems exist.   Details of the File Systems.    \u0026nbsp; id   str  success  The ID of the File System.    \u0026nbsp; name   str  success  The name of the File System.    Hosts   complex   When hosts exist.   Details of the hosts.    \u0026nbsp; id   str  success  The ID of the host.    \u0026nbsp; name   str  success  The name of the host.    ISCSI_initiators   complex   When ISCSI initiators exist.   Details of the ISCSI initiators.    \u0026nbsp; IQN   str  success  The IQN of the ISCSI initiator.    \u0026nbsp; id   str  success  The id of the ISCSI initiator.    NAS_Servers   complex   When NAS Servers exist.   Details of the NAS Servers.    \u0026nbsp; id   str  success  The ID of the NAS Server.    \u0026nbsp; name   str  success  The name of the NAS Server.    NFS_Exports   complex   When NFS Exports exist.   Details of the NFS Exports.    \u0026nbsp; id   str  success  The ID of the NFS Export.    \u0026nbsp; name   str  success  The name of the NFS Export.    SMB_Shares   complex   When SMB Shares exist.   Details of the SMB Shares.    \u0026nbsp; id   str  success  The ID of the SMB Share.    \u0026nbsp; name   str  success  The name of the SMB Share.    Snapshot_Schedules   complex   When Snapshot Schedules exist.   Details of the Snapshot Schedules.    \u0026nbsp; id   str  success  The ID of the Snapshot Schedule.    \u0026nbsp; name   str  success  The name of the Snapshot Schedule.    Snapshots   complex   When Snapshots exist.   Details of the Snapshots.    \u0026nbsp; id   str  success  The ID of the Snapshot.    \u0026nbsp; name   str  success  The name of the Snapshot.    Storage_Pools   complex   When Storage Pools exist.   Details of the Storage Pools.    \u0026nbsp; id   str  success  The ID of the Storage Pool.    \u0026nbsp; name   str  success  The name of the Storage Pool.    Tree_Quotas   complex   When quota trees exist.   Details of the quota trees.    \u0026nbsp; id   str  success  The ID of the quota tree.    \u0026nbsp; path   str  success  The path of the quota tree.    User_Quotas   complex   When user quotas exist.   Details of the user quotas.    \u0026nbsp; id   str  success  The ID of the user quota.    \u0026nbsp; uid   str  success  The UID of the user quota.    Volumes   complex   When Volumes exist.   Details of the Volumes.    \u0026nbsp; id   str  success  The ID of the Volume.    \u0026nbsp; name   str  success  The name of the Volume.    Authors  Rajshree Khare (@kharer5) ansible.team@dell.com Akash Shendge (@shenda1) ansible.team@dell.com   User Quota Module Manage user quota on the Unity storage system\nSynopsis Managing User Quota on the Unity storage system includes Create user quota, Get user quota, Modify user quota, Delete user quota, Create user quota for quota tree, Modify user quota for quota tree and Delete user quota for quota tree.\nParameters   Parameter Type Required Default Choices Description   filesystem_name  str      The name of the filesystem for which the user quota is created. For creation of a user quota either filesystem_name or filesystem_id is required.    filesystem_id  str      The ID of the filesystem for which the user quota is created. For creation of a user quota either filesystem_id or filesystem_name is required.    nas_server_name  str      The name of the NAS server in which the filesystem is created. For creation of a user quota either nas_server_name or nas_server_id is required.    nas_server_id  str      The ID of the NAS server in which the filesystem is created. For creation of a user quota either filesystem_id or filesystem_name is required.    hard_limit  int      Hard limitation for a user on the total space available. If exceeded, user cannot write data. Value 0 implies no limit. One of the values of soft_limit and hard_limit can be 0, however, both cannot be 0 during creation or modification of user quota.    soft_limit  int      Soft limitation for a user on the total space available. If exceeded, notification will be sent to the user for the grace period mentioned, beyond which the user cannot use space. Value 0 implies no limit. Both soft_limit and hard_limit cannot be 0 during creation or modification of user quota.    cap_unit  str      MB GB TB   Unit of soft_limit and hard_limit size. It defaults to 'GB' if not specified.    user_type  str      Unix Windows   Type of user creating a user quota. Mandatory while creating or modifying user quota.    win_domain  str      Fully qualified or short domain name for Windows user type. Mandatory when user_type is 'Windows'.    user_name  str      User name of the user quota when user_type is 'Windows' or 'Unix'. user_name must be specified along with win_domain when user_type is 'Windows'.    uid  str      User ID of the user quota.    user_quota_id  str      User quota ID generated after creation of a user quota.    tree_quota_id  str      The ID of the quota tree. Either tree_quota_id or path to quota tree is required to create/modify/delete user quota for a quota tree.    path  str      The path to the quota tree. Either tree_quota_id or path to quota tree is required to create/modify/delete user quota for a quota tree. Path must start with a forward slash '/'.    state  str   True     absent present   The state option is used to mention the existence of the user quota.    unispherehost  str   True     IP or FQDN of the Unity management server.    username  str   True     The username of the Unity management server.    password  str   True     The password of the Unity management server.    verifycert  bool    True    True False   Boolean variable to specify whether or not to validate SSL certificate. True - Indicates that the SSL certificate should be verified. False - Indicates that the SSL certificate should not be verified.    port  int    443    Port number through which communication happens with Unity management server.    Examples  - name: Get user quota details by user quota id dellemc_unity_user_quota: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; username: \u0026quot;{{username}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user_quota_id: \u0026quot;userquota_171798700679_0_123\u0026quot; state: \u0026quot;present\u0026quot; - name: Get user quota details by user quota uid/user name dellemc_unity_user_quota: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; username: \u0026quot;{{username}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; filesystem_name: \u0026quot;fs_2171\u0026quot; nas_server_id: \u0026quot;nas_21\u0026quot; user_name: \u0026quot;test\u0026quot; state: \u0026quot;present\u0026quot; - name: Create user quota for a filesystem with filesystem id dellemc_unity_user_quota: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; username: \u0026quot;{{username}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; filesystem_id: \u0026quot;fs_2171\u0026quot; hard_limit: 6 cap_unit: \u0026quot;TB\u0026quot; soft_limit: 5 user_type: \u0026quot;UID\u0026quot; uid: \u0026quot;111\u0026quot; state: \u0026quot;present\u0026quot; - name: Create user quota for a filesystem with filesystem name dellemc_unity_user_quota: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; username: \u0026quot;{{username}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; filesystem_name: \u0026quot;Test_filesystem\u0026quot; nas_server_name: \u0026quot;lglad068\u0026quot; hard_limit: 6 cap_unit: \u0026quot;TB\u0026quot; soft_limit: 5 user_type: \u0026quot;UID\u0026quot; uid: \u0026quot;111\u0026quot; state: \u0026quot;present\u0026quot; - name: Modify user quota limit usage by user quota id dellemc_unity_user_quota: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; username: \u0026quot;{{username}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user_quota_id: \u0026quot;userquota_171798700679_0_123\u0026quot; hard_limit: 10 cap_unit: \u0026quot;TB\u0026quot; soft_limit: 8 state: \u0026quot;present\u0026quot; - name: Modify user quota by filesystem id and user quota uid/user_name dellemc_unity_user_quota: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; username: \u0026quot;{{username}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; filesystem_id: \u0026quot;fs_2171\u0026quot; user_type: \u0026quot;Windows\u0026quot; win_domain: \u0026quot;prod\u0026quot; user_name: \u0026quot;sample\u0026quot; hard_limit: 12 cap_unit: \u0026quot;TB\u0026quot; soft_limit: 10 state: \u0026quot;present\u0026quot; - name: Delete user quota dellemc_unity_user_quota: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; username: \u0026quot;{{username}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; filesystem_id: \u0026quot;fs_2171\u0026quot; win_domain: \u0026quot;prod\u0026quot; user_name: \u0026quot;sample\u0026quot; state: \u0026quot;absent\u0026quot; - name: Create user quota of a quota tree dellemc_unity_user_quota: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; username: \u0026quot;{{username}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; tree_quota_id: \u0026quot;treequota_171798700679_4\u0026quot; user_type: \u0026quot;Windows\u0026quot; win_domain: \u0026quot;prod\u0026quot; user_name: \u0026quot;sample\u0026quot; soft_limit: 9 cap_unit: \u0026quot;TB\u0026quot; state: \u0026quot;present\u0026quot; - name: Create user quota of a quota tree by quota tree path dellemc_unity_user_quota: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; username: \u0026quot;{{username}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; filesystem_id: \u0026quot;fs_2171\u0026quot; path: \u0026quot;/sample\u0026quot; user_type: \u0026quot;Unix\u0026quot; user_name: \u0026quot;test\u0026quot; hard_limit: 2 cap_unit: \u0026quot;TB\u0026quot; state: \u0026quot;present\u0026quot; - name: Modify user quota of a quota tree dellemc_unity_user_quota: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; username: \u0026quot;{{username}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; tree_quota_id: \u0026quot;treequota_171798700679_4\u0026quot; user_type: \u0026quot;Windows\u0026quot; win_domain: \u0026quot;prod\u0026quot; user_name: \u0026quot;sample\u0026quot; soft_limit: 10 cap_unit: \u0026quot;TB\u0026quot; state: \u0026quot;present\u0026quot; - name: Modify user quota of a quota tree by quota tree path dellemc_unity_user_quota: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; username: \u0026quot;{{username}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; filesystem_id: \u0026quot;fs_2171\u0026quot; path: \u0026quot;/sample\u0026quot; user_type: \u0026quot;Windows\u0026quot; win_domain: \u0026quot;prod\u0026quot; user_name: \u0026quot;sample\u0026quot; hard_limit: 12 cap_unit: \u0026quot;TB\u0026quot; state: \u0026quot;present\u0026quot; - name: Delete user quota of a quota tree by quota tree path dellemc_unity_user_quota: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; username: \u0026quot;{{username}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; filesystem_id: \u0026quot;fs_2171\u0026quot; path: \u0026quot;/sample\u0026quot; win_domain: \u0026quot;prod\u0026quot; user_name: \u0026quot;sample\u0026quot; state: \u0026quot;absent\u0026quot; - name: Delete user quota of a quota tree by quota tree id dellemc_unity_user_quota: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; username: \u0026quot;{{username}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; tree_quota_id: \u0026quot;treequota_171798700679_4\u0026quot; win_domain: \u0026quot;prod\u0026quot; user_name: \u0026quot;sample\u0026quot; state: \u0026quot;absent\u0026quot; Return Values   Key Type Returned Description   changed   bool   always   Whether or not the resource has changed    get_user_quota_details   complex   When user quota exists   Details of the user quota.    \u0026nbsp; filesystem   complex  success  Filesystem details for which the user quota is created.    \u0026nbsp; \u0026nbsp; UnityFileSystem   complex  success  Filesystem details for which the user quota is created.    \u0026nbsp; \u0026nbsp; \u0026nbsp; id   str  success  ID of the filesystem for which the user quota is created.    \u0026nbsp; \u0026nbsp; \u0026nbsp; name   str  success  Name of filesystem.    \u0026nbsp; \u0026nbsp; \u0026nbsp; nas_server   complex  success  Nasserver details where filesystem is created.    \u0026nbsp; gp_left   int  success  The grace period left after the soft limit for the user quota is exceeded.    \u0026nbsp; hard_limit   int  success  Hard limitation for a user on the total space available. If exceeded, user cannot write data.    \u0026nbsp; hard_ratio   str  success  The hard ratio is the ratio between the hard limit size of the user quota and the amount of storage actually consumed.    \u0026nbsp; id   str  success  User quota ID.    \u0026nbsp; size_used   int  success  Size of used space in the filesystem by the user files.    \u0026nbsp; soft_limit   int  success  Soft limitation for a user on the total space available. If exceeded, notification will be sent to user for the grace period mentioned, beyond which user cannot use space.    \u0026nbsp; soft_ratio   str  success  The soft ratio is the ratio between the soft limit size of the user quota and the amount of storage actually consumed.    \u0026nbsp; state   int  success  State of the user quota.    \u0026nbsp; tree_quota   complex  success  Quota tree details for which the user quota is created.    \u0026nbsp; \u0026nbsp; UnityTreeQuota   complex  success  Quota tree details for which the user quota is created.    \u0026nbsp; \u0026nbsp; \u0026nbsp; id   str  success  ID of the quota tree.    \u0026nbsp; \u0026nbsp; \u0026nbsp; path   str  success  Path to quota tree    \u0026nbsp; uid   int  success  User ID of the user.    \u0026nbsp; unix_name   str  success  Unix user name for this user quota's uid.    \u0026nbsp; windows_names   str  success  Windows user name that maps to this quota's uid.    \u0026nbsp; windows_sids   str  success  Windows SIDs that maps to this quota's uid    Authors  Spandita Panigrahi (@panigs7) ansible.team@dell.com   Filesystem Snapshot Module Manage filesystem snapshot on the Unity storage system\nSynopsis Managing Filesystem Snapshot on the Unity storage system includes create filesystem snapshot, get filesystem snapshot, modify filesystem snapshot and delete filesystem snapshot.\nParameters   Parameter Type Required Default Choices Description   snapshot_name  str      The name of the filesystem snapshot. Mandatory parameter for creating a filesystem snapshot. For all other operations either snapshot_name or snapshot_id is required.    snapshot_id  str      During creation snapshot_id is auto generated. For all other operations either snapshot_id or snapshot_name is required.    filesystem_name  str      The name of the Filesystem for which snapshot is created. For creation of filesystem snapshot either filesystem_name or filesystem_id is required. Not required for other operations.    filesystem_id  str      The ID of the Filesystem for which snapshot is created. For creation of filesystem snapshot either filesystem_id or filesystem_name is required. Not required for other operations.    nas_server_name  str      The name of the NAS server in which the Filesystem is created. For creation of filesystem snapshot either nas_server_name or nas_server_id is required. Not required for other operations.    nas_server_id  str      The ID of the NAS server in which the Filesystem is created. For creation of filesystem snapshot either filesystem_id or filesystem_name is required. Not required for other operations.    auto_delete  bool      This option specifies whether or not the filesystem snapshot will be automatically deleted. If set to true, the filesystem snapshot will expire based on the pool auto deletion policy. If set to false, the filesystem snapshot will not be auto deleted based on the pool auto deletion policy. auto_delete can not be set to True, if expiry_time is specified. If during creation neither auto_delete nor expiry_time is mentioned then the filesystem snapshot will be created keeping auto_delete as True. Once the expiry_time is set, then the filesystem snapshot cannot be assigned to the auto delete policy.    expiry_time  str      This option is for specifying the date and time after which the filesystem snapshot will expire. The time is to be mentioned in UTC timezone. The format is \"MM/DD/YYYY HH:MM\". Year must be in 4 digits.    description  str      The additional information about the filesystem snapshot can be provided using this option. The description can be removed by passing an empty string.    fs_access_type  str      Checkpoint Protocol   Access type of the filesystem snapshot. Required only during creation of filesystem snapshot. If not given, snapshot's access type will be 'Checkpoint'.    state  str   True     absent present   The state option is used to mention the existence of the filesystem snapshot.    unispherehost  str   True     IP or FQDN of the Unity management server.    username  str   True     The username of the Unity management server.    password  str   True     The password of the Unity management server.    verifycert  bool    True    True False   Boolean variable to specify whether or not to validate SSL certificate. True - Indicates that the SSL certificate should be verified. False - Indicates that the SSL certificate should not be verified.    port  int    443    Port number through which communication happens with Unity management server.    Notes  Filesystem snapshot cannot be deleted, if it has nfs or smb share.  Examples  - name: Create Filesystem Snapshot dellemc_unity_filesystem_snapshot: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; username: \u0026quot;{{username}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; snapshot_name: \u0026quot;ansible_test_FS_snap\u0026quot; filesystem_name: \u0026quot;ansible_test_FS\u0026quot; nas_server_name: \u0026quot;lglad069\u0026quot; description: \u0026quot;Created using playbook\u0026quot; auto_delete: True fs_access_type: \u0026quot;Protocol\u0026quot; state: \u0026quot;present\u0026quot; - name: Create Filesystem Snapshot with expiry time. dellemc_unity_filesystem_snapshot: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; username: \u0026quot;{{username}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; snapshot_name: \u0026quot;ansible_test_FS_snap_1\u0026quot; filesystem_name: \u0026quot;ansible_test_FS_1\u0026quot; nas_server_name: \u0026quot;lglad069\u0026quot; description: \u0026quot;Created using playbook\u0026quot; expiry_time: \u0026quot;04/15/2021 2:30\u0026quot; fs_access_type: \u0026quot;Protocol\u0026quot; state: \u0026quot;present\u0026quot; - name: Get Filesystem Snapshot Details using Name dellemc_unity_filesystem_snapshot: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; username: \u0026quot;{{username}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; snapshot_name: \u0026quot;ansible_test_FS_snap\u0026quot; state: \u0026quot;present\u0026quot; - name: Get Filesystem Snapshot Details using ID dellemc_unity_filesystem_snapshot: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; username: \u0026quot;{{username}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; snapshot_id: \u0026quot;10008000403\u0026quot; state: \u0026quot;present\u0026quot; - name: Update Filesystem Snapshot attributes dellemc_unity_filesystem_snapshot: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; username: \u0026quot;{{username}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; snapshot_name: \u0026quot;ansible_test_FS_snap\u0026quot; description: \u0026quot;Description updated\u0026quot; auto_delete: False expiry_time: \u0026quot;04/15/2021 5:30\u0026quot; state: \u0026quot;present\u0026quot; - name: Update Filesystem Snapshot attributes using ID dellemc_unity_filesystem_snapshot: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; username: \u0026quot;{{username}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; snapshot_id: \u0026quot;10008000403\u0026quot; expiry_time: \u0026quot;04/18/2021 8:30\u0026quot; state: \u0026quot;present\u0026quot; - name: Delete Filesystem Snapshot using Name dellemc_unity_filesystem_snapshot: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; username: \u0026quot;{{username}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; snapshot_name: \u0026quot;ansible_test_FS_snap\u0026quot; state: \u0026quot;absent\u0026quot; - name: Delete Filesystem Snapshot using ID dellemc_unity_filesystem_snapshot: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; username: \u0026quot;{{username}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; snapshot_id: \u0026quot;10008000403\u0026quot; state: \u0026quot;absent\u0026quot; Return Values   Key Type Returned Description   changed   bool   always   Whether or not the resource has changed    filesystem_snapshot_details   complex   When filesystem snapshot exists   Details of the filesystem snapshot.    \u0026nbsp; access_type   str  success  Access type of filesystem snapshot.    \u0026nbsp; attached_wwn   str  success  Attached WWN details.    \u0026nbsp; creation_time   str  success  Creation time of filesystem snapshot.    \u0026nbsp; creator_schedule   str  success  Creator schedule of filesystem snapshot.    \u0026nbsp; creator_type   str  success  Creator type for filesystem snapshot.    \u0026nbsp; creator_user   str  success  Creator user for filesystem snapshot.    \u0026nbsp; description   str  success  Description of the filesystem snapshot.    \u0026nbsp; expiration_time   str  success  Date and time after which the filesystem snapshot will expire.    \u0026nbsp; filesystem_id   str  success  Id of the filesystem for which the snapshot exists.    \u0026nbsp; filesystem_name   str  success  Name of the filesystem for which the snapshot exists.    \u0026nbsp; id   str  success  Unique identifier of the filesystem snapshot instance.    \u0026nbsp; is_auto_delete   bool  success  Is the filesystem snapshot is auto deleted or not.    \u0026nbsp; name   str  success  The name of the filesystem snapshot.    \u0026nbsp; nas_server_id   str  success  Id of the NAS server on which filesystem exists.    \u0026nbsp; nas_server_name   str  success  Name of the NAS server on which filesystem exists.    \u0026nbsp; size   int  success  Size of the filesystem snapshot.    Authors  Rajshree Khare (@kharer5) ansible.team@dell.com   Snapshot Module Manage snapshots on the Unity storage system.\nSynopsis Managing snapshots on the Unity storage system includes create snapshot, delete snapshot, update snapshot, get snapshot, map host and unmap host.\nParameters   Parameter Type Required Default Choices Description   snapshot_name  str      The name of the snapshot. Mandatory parameter for creating a snapshot. For all other operations either snapshot name or snapshot id is required.    vol_name  str      The name of the volume for which snapshot is created. For creation of a snapshot either vol_name or cg_name is required. Not required for other operations.    cg_name  str      The name of the Consistency Group for which snapshot is created. For creation of a snapshot either vol_name or cg_name is required. Not required for other operations.    snapshot_id  str      The id of the snapshot. For all operations other than creation either snapshot name or snapshot id is required.    auto_delete  bool      This option specifies whether the snapshot is auto deleted or not. If set to true, snapshot will expire based on the pool auto deletion policy. If set to false, snapshot will not be auto deleted based on the pool auto deletion policy. auto_delete can not be set to True, if expiry_time is specified. If during creation neither auto_delete nor expiry_time is mentioned then snapshot will be created keeping auto_delete as True. Once the expiry_time is set then snapshot cannot be assigned to the auto delete policy.    expiry_time  str      This option is for specifying the date and time after which the snapshot will expire. The time is to be mentioned in UTC timezone. The format is \"MM/DD/YYYY HH:MM\". Year must be in 4 digits.    description  str      The additional information about the snapshot can be provided using this option.    new_snapshot_name  str      New name for the snapshot.    state  str   True     absent present   The state option is used to mention the existence of the snapshot.    host_name  str      The name of the host. Either host_name or host_id is required to map or unmap a snapshot from a host. Snapshot can be attached to multiple hosts.    host_id  str      The id of the host. Either host_name or host_id is required to map or unmap a snapshot from a host Snapshot can be attached to multiple hosts.    host_state  str      mapped unmapped   The host_state option is used to mention the existence of the host for snapshot. It is required when a snapshot is mapped or unmapped from host.    unispherehost  str   True     IP or FQDN of the Unity management server.    username  str   True     The username of the Unity management server.    password  str   True     The password of the Unity management server.    verifycert  bool    True    True False   Boolean variable to specify whether or not to validate SSL certificate. True - Indicates that the SSL certificate should be verified. False - Indicates that the SSL certificate should not be verified.    port  int    443    Port number through which communication happens with Unity management server.    Examples  - name: Create a Snapshot for a CG dellemc_unity_snapshot: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; username: \u0026quot;{{username}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; port: \u0026quot;{{port}}\u0026quot; cg_name: \u0026quot;{{cg_name}}\u0026quot; snapshot_name: \u0026quot;{{cg_snapshot_name}}\u0026quot; description: \u0026quot;{{description}}\u0026quot; auto_delete: False state: \u0026quot;present\u0026quot; - name: Create a Snapshot for a volume with Host attached. dellemc_unity_snapshot: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; username: \u0026quot;{{username}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; port: \u0026quot;{{port}}\u0026quot; vol_name: \u0026quot;{{vol_name}}\u0026quot; snapshot_name: \u0026quot;{{vol_snapshot_name}}\u0026quot; description: \u0026quot;{{description}}\u0026quot; expiry_time: \u0026quot;04/15/2025 16:30\u0026quot; host_name: \u0026quot;{{host_name}}\u0026quot; host_state: \u0026quot;mapped\u0026quot; state: \u0026quot;present\u0026quot; - name: Unmap a host for a Snapshot dellemc_unity_snapshot: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; username: \u0026quot;{{username}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; port: \u0026quot;{{port}}\u0026quot; snapshot_name: \u0026quot;{{vol_snapshot_name}}\u0026quot; host_name: \u0026quot;{{host_name}}\u0026quot; host_state: \u0026quot;unmapped\u0026quot; state: \u0026quot;present\u0026quot; - name: Map snapshot to a host dellemc_unity_snapshot: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; username: \u0026quot;{{username}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; port: \u0026quot;{{port}}\u0026quot; snapshot_name: \u0026quot;{{vol_snapshot_name}}\u0026quot; host_name: \u0026quot;{{host_name}}\u0026quot; host_state: \u0026quot;mapped\u0026quot; state: \u0026quot;present\u0026quot; - name: Update attributes of a Snapshot for a volume dellemc_unity_snapshot: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; username: \u0026quot;{{username}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; snapshot_name: \u0026quot;{{vol_snapshot_name}}\u0026quot; new_snapshot_name: \u0026quot;{{new_snapshot_name}}\u0026quot; description: \u0026quot;{{new_description}}\u0026quot; host_name: \u0026quot;{{host_name}}\u0026quot; host_state: \u0026quot;unmapped\u0026quot; state: \u0026quot;present\u0026quot; - name: Delete Snapshot of CG. dellemc_unity_snapshot: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; username: \u0026quot;{{username}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; snapshot_name: \u0026quot;{{cg_snapshot_name}}\u0026quot; state: \u0026quot;absent\u0026quot; Return Values   Key Type Returned Description   changed   bool   always   Whether or not the resource has changed    snapshot_details   complex   When snapshot exists   Details of the snapshot.    \u0026nbsp; expiration_time   str  success  Date and time after which the snapshot will expire.    \u0026nbsp; hosts_list   dict  success  Contains the name and id of the associated hosts.    \u0026nbsp; id   str  success  Unique identifier of the snapshot instance.    \u0026nbsp; is_auto_delete   str  success  Additional information mentioned for snapshot.    \u0026nbsp; name   str  success  The name of the snapshot.    \u0026nbsp; storage_resource_id   str  success  Id of the storage resource for which the snapshot exists.    \u0026nbsp; storage_resource_name   str  success  Name of the storage resource for which the snapshot exists.    Authors  P Srinivas Rao (@srinivas-rao5) ansible.team@dell.com   SMB Share Module Manage SMB shares on Unity storage system.\nSynopsis Managing SMB Shares on Unity storage system includes create, get, modify, and delete the smb shares.\nParameters   Parameter Type Required Default Choices Description   share_name  str      Name of the SMB share. Required during creation of the SMB share. For all other operations either share_name or share_id is required.    share_id  str      ID of the SMB share. Should not be specified during creation. Id is auto generated. For all other operations either share_name or share_id is required. If share_id is used then no need to pass nas_server/filesystem/snapshot/path.    path  str      Local path to the file system/Snapshot or any existing sub-folder of the file system/Snapshot that is shared over the network. Path is relative to the root of the filesystem. Required for creation of the SMB share.    filesystem_id  str      The ID of the File System. Either filesystem_name or filesystem_id is required for creation of the SMB share for filesystem. If filesystem name is specified, then nas_server_name/nas_server_id is required to uniquely identify the filesystem. filesystem_name and filesystem_id are mutually exclusive parameters.    snapshot_id  str      The ID of the Filesystem Snapshot. Either snapshot_name or snapshot_id is required for creation of the SMB share for a snapshot. If snapshot name is specified, then nas_server_name/nas_server_id is required to uniquely identify the snapshot. snapshot_name and snapshot_id are mutually exclusive parameters.    nas_server_id  str      The ID of the NAS Server. It is not required if share_id is used.    filesystem_name  str      The Name of the File System. Either filesystem_name or filesystem_id is required for creation of the SMB share for filesystem. If filesystem name is specified, then nas_server_name/nas_server_id is required to uniquely identify the filesystem. filesystem_name and filesytem_id are mutually exclusive parameters.    snapshot_name  str      The Name of the Filesystem Snapshot. Either snapshot_name or snapshot_id is required for creation of the SMB share for a snapshot. If snapshot name is specified, then nas_server_name/nas_server_id is required to uniquely identify the snapshot. snapshot_name and snapshot_id are mutually exclusive parameters.    nas_server_name  str      The Name of the NAS Server. It is not required if share_id is used. nas_server_name and nas_server_id are mutually exclusive parameters.    description  str      Description for the SMB share. Optional parameter when creating a share. To modify, pass the new value in description field.    is_abe_enabled  bool      Indicates whether Access-based Enumeration (ABE) for SMB share is enabled. During creation, if not mentioned then default is False.    is_branch_cache_enabled  bool      Indicates whether Branch Cache optimization for SMB share is enabled. During creation, if not mentioned then default is False.    is_continuous_availability_enabled  bool      Indicates whether continuous availability for SMB 3.0 is enabled. During creation, if not mentioned then default is False.    is_encryption_enabled  bool      Indicates whether encryption for SMB 3.0 is enabled at the shared folder level. During creation, if not mentioned then default is False.    offline_availability  str      MANUAL DOCUMENTS PROGRAMS NONE   Defines valid states of Offline Availability. MANUAL- Only specified files will be available offline. DOCUMENTS- All files that users open will be available offline. PROGRAMS- Program will preferably run from the offline cache even when connected to the network. All files that users open will be available offline. NONE- Prevents clients from storing documents and programs in offline cache.    umask  str      The default UNIX umask for new files created on the SMB Share.    state  str   True     absent present   Define whether the SMB share should exist or not. present indicates that the share should exist on the system. absent indicates that the share should not exist on the system.    unispherehost  str   True     IP or FQDN of the Unity management server.    username  str   True     The username of the Unity management server.    password  str   True     The password of the Unity management server.    verifycert  bool    True    True False   Boolean variable to specify whether or not to validate SSL certificate. True - Indicates that the SSL certificate should be verified. False - Indicates that the SSL certificate should not be verified.    port  int    443    Port number through which communication happens with Unity management server.    Notes  When ID/Name of the filesystem/snapshot is passed then nas_server is not required. If passed, then filesystem/snapshot should exist for the mentioned nas_server, else the task will fail.  Examples - name: Create SMB share for a filesystem dellemc_unity_smbshare: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; username: \u0026quot;{{username}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; share_name: \u0026quot;sample_smb_share\u0026quot; filesystem_name: \u0026quot;sample_fs\u0026quot; nas_server_id: \u0026quot;NAS_11\u0026quot; path: \u0026quot;/sample_fs\u0026quot; description: \u0026quot;Sample SMB share created\u0026quot; is_abe_enabled: True is_branch_cache_enabled: True offline_availability: \u0026quot;DOCUMENTS\u0026quot; is_continuous_availability_enabled: True is_encryption_enabled: True umask: \u0026quot;777\u0026quot; state: \u0026quot;present\u0026quot; - name: Modify Attributes of SMB share for a filesystem dellemc_unity_smbshare: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; username: \u0026quot;{{username}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; share_name: \u0026quot;sample_smb_share\u0026quot; nas_server_name: \u0026quot;sample_nas_server\u0026quot; description: \u0026quot;Sample SMB share attributes updated\u0026quot; is_abe_enabled: False is_branch_cache_enabled: False offline_availability: \u0026quot;MANUAL\u0026quot; is_continuous_availability_enabled: \u0026quot;False\u0026quot; is_encryption_enabled: \u0026quot;False\u0026quot; umask: \u0026quot;022\u0026quot; state: \u0026quot;present\u0026quot; - name: Create SMB share for a snapshot dellemc_unity_smbshare: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; username: \u0026quot;{{username}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; share_name: \u0026quot;sample_snap_smb_share\u0026quot; snapshot_name: \u0026quot;sample_snapshot\u0026quot; nas_server_id: \u0026quot;NAS_11\u0026quot; path: \u0026quot;/sample_snapshot\u0026quot; description: \u0026quot;Sample SMB share created for snapshot\u0026quot; is_abe_enabled: True is_branch_cache_enabled: True is_continuous_availability_enabled: True is_encryption_enabled: True umask: \u0026quot;777\u0026quot; state: \u0026quot;present\u0026quot; - name: Modify Attributes of SMB share for a snapshot dellemc_unity_smbshare: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; username: \u0026quot;{{username}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; share_name: \u0026quot;sample_snap_smb_share\u0026quot; snapshot_name: \u0026quot;sample_snapshot\u0026quot; description: \u0026quot;Sample SMB share attributes updated for snapshot\u0026quot; is_abe_enabled: False is_branch_cache_enabled: False offline_availability: \u0026quot;MANUAL\u0026quot; is_continuous_availability_enabled: \u0026quot;False\u0026quot; is_encryption_enabled: \u0026quot;False\u0026quot; umask: \u0026quot;022\u0026quot; state: \u0026quot;present\u0026quot; - name: Get details of SMB share dellemc_unity_smbshare: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; username: \u0026quot;{{username}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; share_id: \u0026quot;{{smb_share_id}}\u0026quot; state: \u0026quot;present\u0026quot; - name: Delete SMB share dellemc_unity_smbshare: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; username: \u0026quot;{{username}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; share_id: \u0026quot;{{smb_share_id}}\u0026quot; state: \u0026quot;absent\u0026quot; Return Values   Key Type Returned Description   changed   bool   always   Whether or not the resource has changed    smb_share_details   complex   When share exists.   The SMB share details.    \u0026nbsp; description   str  success  Additional information about the share.    \u0026nbsp; filesystem_id   str  success  The ID of the Filesystem.    \u0026nbsp; filesystem_name   str  success  The Name of the filesystem    \u0026nbsp; id   str  success  The ID of the SMB share.    \u0026nbsp; is_abe_enabled   bool  success  Whether Access Based enumeration is enforced or not    \u0026nbsp; is_branch_cache_enabled   bool  success  Whether branch cache is enabled or not.    \u0026nbsp; is_continuous_availability_enabled   bool  success  Whether the share will be available continuously or not    \u0026nbsp; is_encryption_enabled   bool  success  Whether encryption is enabled or not.    \u0026nbsp; name   str  success  Name of the SMB share.    \u0026nbsp; nas_server_id   str  success  The ID of the nas_server.    \u0026nbsp; nas_server_name   str  success  The Name of the nas_server.    \u0026nbsp; snapshot_id   str  success  The ID of the Snapshot.    \u0026nbsp; snapshot_name   str  success  The Name of the Snapshot.    \u0026nbsp; umask   str  success  Unix mask for the SMB share    Authors  P Srinivas Rao (@srinivas-rao5) ansible.team@dell.com   Host Module Manage Host operations on Unity.\nSynopsis The Host module contains the following operations Creation of a Host. Addition of initiators to Host. Removal of initiators from Host. Modification of host attributes. Get details of a Host. Deletion of a Host.\nParameters   Parameter Type Required Default Choices Description   host_name  str      Name of the host. Mandatory for host creation.    host_id  str      Unique identifier of the host. host_id is auto generated during creation. Except create, all other operations require either host_id or host_name.    description  str      Host description.    host_os  str      AIX Citrix XenServer HP-UX IBM VIOS Linux Mac OS Solaris VMware ESXi Windows Client Windows Server   Operating system running on the host.    new_host_name  str      New name for the host. Only required in rename host operation.    initiators  list elements: str      List of initiators to be added/removed to/from host.    initiator_state  str      present-in-host absent-in-host   State of the initiator.    state  str   True     present absent   State of the host.    unispherehost  str   True     IP or FQDN of the Unity management server.    username  str   True     The username of the Unity management server.    password  str   True     The password of the Unity management server.    verifycert  bool    True    True False   Boolean variable to specify whether or not to validate SSL certificate. True - Indicates that the SSL certificate should be verified. False - Indicates that the SSL certificate should not be verified.    port  int    443    Port number through which communication happens with Unity management server.    Examples - name: Create empty Host. dellemc_unity_host: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; username: \u0026quot;{{username}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; host_name: \u0026quot;ansible-test-host\u0026quot; host_os: \u0026quot;Linux\u0026quot; description: \u0026quot;ansible-test-host\u0026quot; state: \u0026quot;present\u0026quot; - name: Create Host with Initiators. dellemc_unity_host: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; username: \u0026quot;{{username}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; host_name: \u0026quot;ansible-test-host-1\u0026quot; host_os: \u0026quot;Linux\u0026quot; description: \u0026quot;ansible-test-host-1\u0026quot; initiators: - \u0026quot;iqn.1994-05.com.redhat:c38e6e8cfd81\u0026quot; - \u0026quot;20:00:00:90:FA:13:81:8D:10:00:00:90:FA:13:81:8D\u0026quot; initiator_state: \u0026quot;present-in-host\u0026quot; state: \u0026quot;present\u0026quot; - name: Modify Host using host_id. dellemc_unity_host: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; username: \u0026quot;{{username}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; host_id: \u0026quot;Host_253\u0026quot; new_host_name: \u0026quot;ansible-test-host-2\u0026quot; host_os: \u0026quot;Mac OS\u0026quot; description: \u0026quot;Ansible tesing purpose\u0026quot; state: \u0026quot;present\u0026quot; - name: Add Initiators to Host. dellemc_unity_host: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; username: \u0026quot;{{username}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; host_name: \u0026quot;ansible-test-host-2\u0026quot; initiators: - \u0026quot;20:00:00:90:FA:13:81:8C:10:00:00:90:FA:13:81:8C\u0026quot; initiator_state: \u0026quot;present-in-host\u0026quot; state: \u0026quot;present\u0026quot; - name: Get Host details using host_name. dellemc_unity_host: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; username: \u0026quot;{{username}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; host_name: \u0026quot;ansible-test-host-2\u0026quot; state: \u0026quot;present\u0026quot; - name: Get Host details using host_id. dellemc_unity_host: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; username: \u0026quot;{{username}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; host_id: \u0026quot;Host_253\u0026quot; state: \u0026quot;present\u0026quot; - name: Delete Host. dellemc_unity_host: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; username: \u0026quot;{{username}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; host_name: \u0026quot;ansible-test-host-2\u0026quot; state: \u0026quot;absent\u0026quot; Return Values   Key Type Returned Description   changed   bool   always   Whether or not the resource has changed.    host_details   complex   When host exists.   Details of the host.    \u0026nbsp; description   str  success  Description about the host.    \u0026nbsp; fc_host_initiators   complex  success  Details of the FC initiators associated with the host.    \u0026nbsp; \u0026nbsp; UnityHostInitiatorList   complex  success  FC initiators with system generated unique hash value.    \u0026nbsp; id   str  success  The system ID given to the host.    \u0026nbsp; iscsi_host_initiators   complex  success  Details of the ISCSI initiators associated with the host.    \u0026nbsp; \u0026nbsp; UnityHostInitiatorList   complex  success  ISCSI initiators with sytem genrated unique hash value.    \u0026nbsp; name   str  success  The name of the host.    \u0026nbsp; os_type   str  success  Operating system running on the host.    \u0026nbsp; type   str  success  HostTypeEnum of the host.    Authors  Rajshree Khare (@kharer5) ansible.team@dell.com   Consistency Group Module Manage consistency groups on Unity storage system\nSynopsis Managing the consistency group on the Unity storage system includes creating new consistency group, adding volumes to consistency group, removing volumes from consistency group, mapping hosts to consistency group, unmapping hosts from consistency group, renaming consistency group, modifying attributes of consistency group and deleting consistency group.\nParameters   Parameter Type Required Default Choices Description   cg_name  str      The name of the consistency group. It is mandatory for the create operation. Specify either cg_name or cg_id (but not both) for any operation.    cg_id  str      The ID of the consistency group. It can be used only for get, modify, add/remove volumes, or delete operations.    volumes  list elements: dict      This is a list of volumes. Either the volume ID or name must be provided for adding/removing existing volumes from consistency group. If volumes are given, then vol_state should also be specified. Volumes cannot be added/removed from consistency group, if the consistency group or the volume has snapshots.    \u0026nbsp; vol_id   str      The ID of the volume.    \u0026nbsp; vol_name   str      The name of the volume.    vol_state  str      present-in-group absent-in-group   String variable, describes the state of volumes inside consistency group. If volumes are given, then vol_state should also be specified.    new_cg_name  str      The new name of the consistency group, used in rename operation.    description  str      Description of the consistency group.    snap_schedule  str      Snapshot schedule assigned to the consistency group. Specifying an empty string \"\" removes the existing snapshot schedule from consistency group.    tiering_policy  str      AUTOTIER_HIGH AUTOTIER HIGHEST LOWEST   Tiering policy choices for how the storage resource data will be distributed among the tiers available in the pool.    hosts  list elements: dict      This is a list of hosts. Either the host ID or name must be provided for mapping/unmapping hosts for a consistency group. If hosts are given, then mapping_state should also be specified. Hosts cannot be mapped to a consistency group, if the consistency group has no volumes. When a consistency group is being mapped to the host, users should not use the volume module to map the volumes in the consistency group to hosts.    \u0026nbsp; host_id   str      The ID of the host.    \u0026nbsp; host_name   str      The name of the host.    mapping_state  str      mapped unmapped   String variable, describes the state of hosts inside the consistency group. If hosts are given, then mapping_state should also be specified.    state  str   True     absent present   Define whether the consistency group should exist or not.    unispherehost  str   True     IP or FQDN of the Unity management server.    username  str   True     The username of the Unity management server.    password  str   True     The password of the Unity management server.    verifycert  bool    True    True False   Boolean variable to specify whether or not to validate SSL certificate. True - Indicates that the SSL certificate should be verified. False - Indicates that the SSL certificate should not be verified.    port  int    443    Port number through which communication happens with Unity management server.    Examples - name: Create consistency group dellemc_unity_consistencygroup: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; username: \u0026quot;{{username}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; cg_name: \u0026quot;{{cg_name}}\u0026quot; description: \u0026quot;{{description}}\u0026quot; snap_schedule: \u0026quot;{{snap_schedule1}}\u0026quot; state: \u0026quot;present\u0026quot; - name: Get details of consistency group using id dellemc_unity_consistencygroup: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; username: \u0026quot;{{username}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; cg_id: \u0026quot;{{cg_id}}\u0026quot; state: \u0026quot;present\u0026quot; - name: Add volumes to consistency group dellemc_unity_consistencygroup: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; username: \u0026quot;{{username}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; cg_id: \u0026quot;{{cg_id}}\u0026quot; volumes: - vol_name: \u0026quot;Ansible_Test-3\u0026quot; - vol_id: \u0026quot;sv_1744\u0026quot; vol_state: \u0026quot;{{vol_state_present}}\u0026quot; state: \u0026quot;present\u0026quot; - name: Rename consistency group dellemc_unity_consistencygroup: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; username: \u0026quot;{{username}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; cg_name: \u0026quot;{{cg_name}}\u0026quot; new_cg_name: \u0026quot;{{new_cg_name}}\u0026quot; state: \u0026quot;present\u0026quot; - name: Modify consistency group details dellemc_unity_consistencygroup: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; username: \u0026quot;{{username}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; cg_name: \u0026quot;{{new_cg_name}}\u0026quot; snap_schedule: \u0026quot;{{snap_schedule2}}\u0026quot; tiering_policy: \u0026quot;{{tiering_policy1}}\u0026quot; state: \u0026quot;present\u0026quot; - name: Map hosts to a consistency group dellemc_unity_consistencygroup: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; username: \u0026quot;{{username}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; cg_id: \u0026quot;{{cg_id}}\u0026quot; hosts: - host_name: \u0026quot;10.226.198.248\u0026quot; - host_id: \u0026quot;Host_511\u0026quot; mapping_state: \u0026quot;mapped\u0026quot; state: \u0026quot;present\u0026quot; - name: Unmap hosts from a consistency group dellemc_unity_consistencygroup: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; username: \u0026quot;{{username}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; cg_id: \u0026quot;{{cg_id}}\u0026quot; hosts: - host_id: \u0026quot;Host_511\u0026quot; - host_name: \u0026quot;10.226.198.248\u0026quot; mapping_state: \u0026quot;unmapped\u0026quot; state: \u0026quot;present\u0026quot; - name: Remove volumes from consistency group dellemc_unity_consistencygroup: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; username: \u0026quot;{{username}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; cg_name: \u0026quot;{{new_cg_name}}\u0026quot; volumes: - vol_name: \u0026quot;Ansible_Test-3\u0026quot; - vol_id: \u0026quot;sv_1744\u0026quot; vol_state: \u0026quot;{{vol_state_absent}}\u0026quot; state: \u0026quot;present\u0026quot; - name: Delete consistency group dellemc_unity_consistencygroup: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; username: \u0026quot;{{username}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; cg_name: \u0026quot;{{new_cg_name}}\u0026quot; state: \u0026quot;absent\u0026quot; Return Values   Key Type Returned Description   changed   bool   always   Whether or not the resource has changed    consistency_group_details   complex   When consistency group exists   Details of the consistency group    \u0026nbsp; block_host_access   complex  success  Details of hosts mapped to the consistency group    \u0026nbsp; \u0026nbsp; UnityBlockHostAccessList   complex  success  List of hosts mapped to consistency group    \u0026nbsp; \u0026nbsp; \u0026nbsp; UnityBlockHostAccess   complex  success  Details of host    \u0026nbsp; id   str  success  The system ID given to the consistency group    \u0026nbsp; luns   complex  success  Details of volumes part of consistency group    \u0026nbsp; \u0026nbsp; UnityLunList   complex  success  List of volumes part of consistency group    \u0026nbsp; \u0026nbsp; \u0026nbsp; UnityLun   complex  success  Detail of volume    \u0026nbsp; relocation_policy   str  success  FAST VP tiering policy for the consistency group    \u0026nbsp; snap_schedule   complex  success  Snapshot schedule applied to consistency group    \u0026nbsp; \u0026nbsp; UnitySnapSchedule   complex  success  Snapshot schedule applied to consistency group    \u0026nbsp; \u0026nbsp; \u0026nbsp; id   str  success  The system ID given to the snapshot schedule    \u0026nbsp; \u0026nbsp; \u0026nbsp; name   str  success  The name of the snapshot schedule    \u0026nbsp; snapshots   complex  success  List of snapshots of consistency group    \u0026nbsp; \u0026nbsp; creation_time   str  success  Date and time on which the snapshot was taken    \u0026nbsp; \u0026nbsp; expirationTime   str  success  Date and time after which the snapshot will expire    \u0026nbsp; \u0026nbsp; name   str  success  Name of the snapshot    \u0026nbsp; \u0026nbsp; storageResource   complex  success  Storage resource for which the snapshot was taken    \u0026nbsp; \u0026nbsp; \u0026nbsp; UnityStorageResource   complex  success  Details of the storage resource    Authors  Akash Shendge (@shenda1) ansible.team@dell.com   Snapshot Schedule Module Manage snapshot schedules on Unity storage system\nSynopsis Managing snapshot schedules on Unity storage system includes creating new snapshot schedule, getting details of snapshot schedule, modifying attributes of snapshot schedule, and deleting snapshot schedule.\nParameters   Parameter Type Required Default Choices Description   name  str      The name of the snapshot schedule. Name is mandatory for a create operation. Specify either name or id (but not both) for any operation.    id  str      The ID of the snapshot schedule.    type  str      every_n_hours every_day every_n_days every_week every_month   Type of the rule to be included in snapshot schedule. Type is mandatory for any create or modify operation. Once the snapshot schedule is created with one type it can be modified.    interval  int      Number of hours between snapshots. Applicable only when rule type is 'every_n_hours'.    hours_of_day  list elements: int      Hours of the day when the snapshot will be taken. Applicable only when rule type is 'every_day'.    day_interval  int      Number of days between snapshots. Applicable only when rule type is 'every_n_days'.    days_of_week  list elements: str      SUNDAY MONDAY TUESDAY WEDNESDAY THURSDAY FRIDAY SATURDAY   Days of the week for which the snapshot schedule rule applies. Applicable only when rule type is 'every_week'.    day_of_month  int      Day of the month for which the snapshot schedule rule applies. Applicable only when rule type is 'every_month'. Value should be [1, 31].    hour  int      the hour when the snapshot will be taken. Applicable for 'every_n_days', 'every_week', 'every_month' rule types. For create operation, if 'hour' parameter is not specified, value will be taken as 0. Value should be [0, 23].    minute  int      Minute offset from the hour when the snapshot will be taken. Applicable for all rule types. For a create operation, if 'minute' parameter is not specified, value will be taken as 0. Value should be [0, 59].    desired_retention  int      The number of days/hours for which snapshot will be retained. When auto_delete is True, desired_retention cannot be specified. Maximum desired retention supported is 31 days or 744 hours.    retention_unit  str    hours    hours days   The retention unit for the snapshot.    auto_delete  bool      Indicates whether the system can automatically delete the snapshot.    state  str   True     absent present   Define whether the snapshot schedule should exist or not.    unispherehost  str   True     IP or FQDN of the Unity management server.    username  str   True     The username of the Unity management server.    password  str   True     The password of the Unity management server.    verifycert  bool    True    True False   Boolean variable to specify whether or not to validate SSL certificate. True - Indicates that the SSL certificate should be verified. False - Indicates that the SSL certificate should not be verified.    port  int    443    Port number through which communication happens with Unity management server.    Notes  Snapshot schedule created via Ansible will have only one rule. Modification of rule type is not allowed. Within the same type, other parameters can be modified. If an existing snapshot schedule has more than 1 rule in it, only get and delete operation is allowed.  Examples - name: Create snapshot schedule (Rule Type - every_n_hours) dellemc_unity_snapshotschedule: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; username: \u0026quot;{{username}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; name: \u0026quot;Ansible_Every_N_Hours_Testing\u0026quot; type: \u0026quot;every_n_hours\u0026quot; interval: 6 desired_retention: 24 state: \u0026quot;{{state_present}}\u0026quot; - name: Create snapshot schedule (Rule Type - every_day) dellemc_unity_snapshotschedule: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; username: \u0026quot;{{username}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; name: \u0026quot;Ansible_Every_Day_Testing\u0026quot; type: \u0026quot;every_day\u0026quot; hours_of_day: - 8 - 14 auto_delete: True state: \u0026quot;{{state_present}}\u0026quot; - name: Create snapshot schedule (Rule Type - every_n_days) dellemc_unity_snapshotschedule: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; username: \u0026quot;{{username}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; name: \u0026quot;Ansible_Every_N_Day_Testing\u0026quot; type: \u0026quot;every_n_days\u0026quot; day_interval: 2 desired_retention: 16 retention_unit: \u0026quot;days\u0026quot; state: \u0026quot;{{state_present}}\u0026quot; - name: Create snapshot schedule (Rule Type - every_week) dellemc_unity_snapshotschedule: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; username: \u0026quot;{{username}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; name: \u0026quot;Ansible_Every_Week_Testing\u0026quot; type: \u0026quot;every_week\u0026quot; days_of_week: - MONDAY - FRIDAY hour: 12 minute: 30 desired_retention: 200 state: \u0026quot;{{state_present}}\u0026quot; - name: Create snapshot schedule (Rule Type - every_month) dellemc_unity_snapshotschedule: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; username: \u0026quot;{{username}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; name: \u0026quot;Ansible_Every_Month_Testing\u0026quot; type: \u0026quot;every_month\u0026quot; day_of_month: 17 auto_delete: True state: \u0026quot;{{state_present}}\u0026quot; - name: Get snapshot schedule details using name dellemc_unity_snapshotschedule: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; username: \u0026quot;{{username}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; name: \u0026quot;Ansible_Every_N_Hours_Testing\u0026quot; state: \u0026quot;{{state_present}}\u0026quot; - name: Get snapshot schedule details using id dellemc_unity_snapshotschedule: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; username: \u0026quot;{{username}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; id: \u0026quot;{{id}}\u0026quot; state: \u0026quot;{{state_present}}\u0026quot; - name: Modify snapshot schedule details id dellemc_unity_snapshotschedule: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; username: \u0026quot;{{username}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; id: \u0026quot;{{id}}\u0026quot; type: \u0026quot;every_n_hours\u0026quot; interval: 8 state: \u0026quot;{{state_present}}\u0026quot; - name: Modify snapshot schedule using name dellemc_unity_snapshotschedule: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; username: \u0026quot;{{username}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; name: \u0026quot;Ansible_Every_Day_Testing\u0026quot; type: \u0026quot;every_day\u0026quot; desired_retention: 200 auto_delete: False state: \u0026quot;{{state_present}}\u0026quot; - name: Delete snapshot schedule using id dellemc_unity_snapshotschedule: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; username: \u0026quot;{{username}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; id: \u0026quot;{{id}}\u0026quot; state: \u0026quot;{{state_absent}}\u0026quot; - name: Delete snapshot schedule using name dellemc_unity_snapshotschedule: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; username: \u0026quot;{{username}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; name: \u0026quot;Ansible_Every_Day_Testing\u0026quot; state: \u0026quot;{{state_absent}}\u0026quot; Return Values   Key Type Returned Description   changed   bool   always   Whether or not the resource has changed.    snapshot_schedule_details   complex   When snapshot schedule exists   Details of the snapshot schedule.    \u0026nbsp; id   str  success  The system ID given to the snapshot schedule.    \u0026nbsp; luns   complex  success  Details of volumes for which snapshot schedule applied.    \u0026nbsp; \u0026nbsp; UnityLunList   complex  success  List of volumes for which snapshot schedule applied.    \u0026nbsp; \u0026nbsp; \u0026nbsp; UnityLun   complex  success  Detail of volume.    \u0026nbsp; name   str  success  The name of the snapshot schedule.    \u0026nbsp; rules   complex  success  Details of rules that apply to snapshot schedule.    \u0026nbsp; \u0026nbsp; days_of_month   list  success  Days of the month for which the snapshot schedule rule applies.    \u0026nbsp; \u0026nbsp; days_of_week   complex  success  Days of the week for which the snapshot schedule rule applies.    \u0026nbsp; \u0026nbsp; \u0026nbsp; DayOfWeekEnumList   list  success  Enumeration of days of the week.    \u0026nbsp; \u0026nbsp; hours   list  success  Hourly frequency for the snapshot schedule rule.    \u0026nbsp; \u0026nbsp; id   str  success  The system ID of the rule.    \u0026nbsp; \u0026nbsp; interval   int  success  Number of days or hours between snaps, depending on the rule type.    \u0026nbsp; \u0026nbsp; is_auto_delete   bool  success  Indicates whether the system can automatically delete the snapshot based on pool automatic-deletion thresholds.    \u0026nbsp; \u0026nbsp; minute   int  success  Minute frequency for the snapshot schedule rule.    \u0026nbsp; \u0026nbsp; retention_time   int  success  Period of time in seconds for which to keep the snapshot.    \u0026nbsp; \u0026nbsp; retention_time_in_hours   int  success  Period of time in hours for which to keep the snapshot.    \u0026nbsp; \u0026nbsp; rule_type   str  success  Type of the rule applied to snapshot schedule.    \u0026nbsp; storage_resources   complex  success  Details of storage resources for which snapshot schedule applied.    \u0026nbsp; \u0026nbsp; UnityStorageResourceList   complex  success  List of storage resources for which snapshot schedule applied.    \u0026nbsp; \u0026nbsp; \u0026nbsp; UnityStorageResource   complex  success  Detail of storage resource.    Authors  Akash Shendge (@shenda1) ansible.team@dell.com   ","excerpt":"Ansible Modules for Dell EMC Unity Product Guide 1.2.0 © 2021 Dell Inc. or its subsidiaries. All …","ref":"/ansible-docs/docs/storage/platforms/unity/product-guide/","title":"Product Guide"},{"body":"Ansible Modules for Dell EMC PowerFlex The Ansible Modules for Dell EMC PowerFlex allow Data Center and IT administrators to use RedHat Ansible to automate and orchestrate the provisioning and management of Dell EMC PowerFlex storage systems.\nThe capabilities of the Ansible modules are managing SDCs, volumes, snapshots and storage pools; and to gather high level facts from the storage system. The options available for each are list, show, create, modify and delete. These tasks can be executed by running simple playbooks written in yaml syntax. The modules are written so that all the operations are idempotent, so making multiple identical requests has the same effect as making a single request.\nSupport Ansible modules for PowerFlex are supported by Dell EMC and are provided under the terms of the license attached to the source code. Dell EMC does not provide support for any source code modifications. For any Ansible module issues, questions or feedback, join the Dell EMC Automation community.\nSupported Platforms  Dell EMC PowerFlex (VxFlex OS) version 3.5  Prerequisites  Ansible 2.9 or later Python 3.5 or later Red Hat Enterprise Linux 7.6, 7.7, 7.8, 8.2 PyPowerFlex python library for PowerFlex 1.1.0  Idempotency The modules are written in such a way that all requests are idempotent and hence fault-tolerant. It essentially means that the result of a successfully performed request is independent of the number of times it is executed.\nList of Ansible Modules for Dell EMC PowerFlex  Gather facts module Snapshot module SDC module Storage pool module Volume module  Installation of SDK Install python sdk named \u0026lsquo;PyPowerFlex\u0026rsquo;. It can be installed using pip, based on appropriate python version.\n  Clone the repo \u0026ldquo;https://github.com/dell/python-powerflex\u0026quot; using command:\ngit clone https://github.com/dell/python-powerflex.git    Go to the root directory of setup.\n  Execute the following command:\npip install .    Installing Collections   Download the tar build and execute the following command to install the collection anywhere in your system:\nansible-galaxy collection install dellemc-powerflex-1.0.0.tar.gz -p \u0026lt;install_path\u0026gt;    Set the environment variable:\nexport ANSIBLE_COLLECTIONS_PATHS=$ANSIBLE_COLLECTIONS_PATHS:\u0026lt;install_path\u0026gt;    Using Collections   In order to use any Ansible module, ensure that the importing of proper FQCN(Fully Qualified Collection Name) must be embedded in the playbook. Below example can be referred\ncollections: - dellemc.powerflex    For generating Ansible documentation for a specific module, embed the FQCN before the module name. Refer to the following example:\nansible-doc dellemc.powerflex.dellemc_powerflex_gatherfacts    Running Ansible Modules The Ansible server must be configured with Python library for PowerFlex to run the Ansible playbooks. The Documents provide information on different Ansible modules along with their functions and syntax. The parameters table in the Product Guide provides information on various parameters which needs to be configured before running the modules.\nSSL Certificate Validation   Copy the CA certificate to the \u0026ldquo;/etc/pki/ca-trust/source/anchors\u0026rdquo; path of the host by any external means.\n  Set the \u0026ldquo;REQUESTS_CA_BUNDLE\u0026rdquo; environment variable to the path of the SSL certificate using the command:\n export REQUESTS_CA_BUNDLE=/etc/pki/ca-trust/source/anchors/\u0026lt;\u0026lt;Certificate_Name\u0026gt;\u0026gt;    Import the SSL certificate to host using the command:\n update-ca-trust extract    If \u0026ldquo;TLS CA certificate bundle error\u0026rdquo; occurs, then follow below steps:\n cd /etc/pki/tls/certs/ openssl x509 -in ca-bundle.crt -text -noout    Results Each module returns the updated state and details of the entity, For example, if you are using the Volume module, all calls will return the updated details of the volume. Sample result is shown in each module\u0026rsquo;s documentation.\n","excerpt":"Ansible Modules for Dell EMC PowerFlex The Ansible Modules for Dell EMC PowerFlex allow Data Center …","ref":"/ansible-docs/docs/storage/platforms/powerflex/readme/","title":"ReadMe"},{"body":"Ansible Modules for Dell EMC PowerMax The Ansible Modules for Dell EMC PowerMax allow data center and IT administrators to use RedHat Ansible to automate and orchestrate the configuration and management of Dell EMC PowerMax arrays.\nThe capabilities of Ansible modules are managing volumes, storage groups, ports, port groups, hosts, host groups, masking views, snapshots, SRDF links, RDF groups, metro DR environments, jobs, snapshot policies, storage pools and gathering high-level facts about the arrays. The options available for each capability are list, show, create, delete, and modify. These tasks can be executed by running simple playbooks written in yaml syntax. The modules are written so that all the operations are idempotent, therefore making multiple identical requests has the same effect as making a single request.\nSupport Ansible modules for PowerMax are supported by Dell EMC and are provided under the terms of the license attached to the source code. Dell EMC does not provide support for any source code modifications. For any Ansible module issues, questions or feedback, join the Dell EMC Automation community.\nSupported Platforms  Dell EMC PowerMax and VMAX All Flash arrays with Unisphere version 9.1 and later.  Prerequisites This table provides information about the software prerequisites for the Ansible Modules for Dell EMC PowerMax.\n   Ansible Modules Unisphere Version PowerMaxOS Red Hat Enterprise Linux Python version Python library version Ansible     v1.5.0 9.1 9.2 5978.221.221 5978.444.444 5978.669.669 5978.711.711 7.5 7.6, 7.7, 7.8, and 8.2 2.7.12 3.5.2 3.6.x 3.7.x 9.1.x.x 9.2.x.x 2.9 and 2.10     Please follow PyU4V installation instructions on PyU4V Documentation  Idempotency The modules are written in such a way that all requests are idempotent and hence fault-tolerant. This means that the result of a successfully performed request is independent of the number of times it is executed.\nList of Ansible Modules for Dell EMC PowerMax  Volume module Host module Host Group module Snapshot module Masking View module Port module Port Group module Storage Group module Gatherfacts module SRDF module RDF Group module Metro DR module Job module Snapshot Policy module Storage Pool module Process Storage Pool module  Installation of SDK Install the python sdk named \u0026lsquo;PyU4V\u0026rsquo;. It can be installed using pip, based on the appropriate python version.\nInstalling Collections   Download the tar build and run the following command to install the collection anywhere in your system:\nansible-galaxy collection install dellemc-powermax-1.5.0.tar.gz -p \u0026lt;install_path\u0026gt;    Set the environment variable:\nexport ANSIBLE_COLLECTIONS_PATHS=$ANSIBLE_COLLECTIONS_PATHS:\u0026lt;install_path\u0026gt;    Using Collections  In order to use any Ansible module, ensure that the importation of the proper FQCN (Fully Qualified Collection Name) must be embedded in the playbook. For example, collections: - dellemc.powermax To generate Ansible documentation for a specific module, embed the FQCN before the module name. For example, ansible-doc dellemc.powermax.dellemc_powermax_gatherfacts  Running Ansible Modules The Ansible server must be configured with Python library for Unisphere to run the Ansible playbooks. The Documents provide information on different Ansible modules along with their functions and syntax. The parameters table in the Product Guide provides information on various parameters which needs to be configured before running the modules.\nSSL Certificate Validation  Copy the CA certificate to the \u0026ldquo;/etc/pki/ca-trust/source/anchors\u0026rdquo; path of the host by any external means. Set the \u0026ldquo;REQUESTS_CA_BUNDLE\u0026rdquo; environment variable to the path of the SSL certificate using the command: export REQUESTS_CA_BUNDLE=/etc/pki/ca-trust/source/anchors/\u0026laquo;Certificate_Name\u0026raquo; Import the SSL certificate to host using the command: update-ca-trust extract If \u0026ldquo;TLS CA certificate bundle error\u0026rdquo; occurs, then follow below steps:  cd /etc/pki/tls/certs/ openssl x509 -in ca-bundle.crt -text -noout    Results Each module returns the updated state and details of the entity, for example, if you are using the Volume module, all calls will return the updated details of the volume. A sample result is shown in each module\u0026rsquo;s documentation.\n","excerpt":"Ansible Modules for Dell EMC PowerMax The Ansible Modules for Dell EMC PowerMax allow data center …","ref":"/ansible-docs/docs/storage/platforms/powermax/readme/","title":"ReadMe"},{"body":"Ansible Modules for Dell EMC PowerScale The Ansible Modules for Dell EMC PowerScale allow Data Center and IT administrators to use RedHat Ansible to automate and orchestrate the configuration and management of Dell EMC PowerScale arrays.\nThe capabilities of the Ansible modules are managing users, groups, node, active directory, ldap, access zones, file system, nfs exports, smb shares, snapshots, snapshot schedules and smart quotas, and to gather facts from the array. The tasks can be executed by running simple playbooks written in yaml syntax.\nSupport  Ansible modules for PowerScale are supported by Dell EMC and are provided under the terms of the license attached to the source code. For any setup, configuration issues, questions or feedback, join the Dell EMC Automation community. For any Dell EMC storage issues, please contact Dell support at: https://www.dell.com/support. Dell EMC does not provide support for any source code modifications.  Supported Platforms  Dell EMC PowerScale Arrays version 8.0 and above.  Prerequisites  Ansible 2.9, 2.10 Python 3.5, 3.6, 3.7 Red Hat Enterprise Linux 7.6, 7.7, 7.8, 8.2 Python SDK for PowerScale (version 8.1.1 and 9.0.0)  Idempotency The modules are written in such a way that all requests are idempotent and hence fault-tolerant. It essentially means that the result of a successfully performed request is independent of the number of times it is executed.\nList of Ansible Modules for Dell EMC PowerScale  File System Module Access Zone Module Users Module Groups Module Snapshot Module Snapshot Schedule Module NFS Module SMB Module Smart Quota Module Gather Facts Module Active Directory Module LDAP Module Node Module  Installation of SDK Based on which PowerScale version is being used, install sdk as follows:\n  For PowerScale version \u0026lt; 9.0.0, install python sdk named \u0026lsquo;isi-sdk-8-1-1\u0026rsquo; as below:\n pip install isi_sdk_8_1_1    For PowerScale version 9.0.0 and above, install python sdk named \u0026lsquo;isi-sdk-9-0-0\u0026rsquo; as below:\n pip install isi_sdk_9_0_0    Installing Collections   Download the tar build and run the following command to install the collection anywhere in your system:\nansible-galaxy collection install dellemc-powerscale-1.2.0.tar.gz -p \u0026lt;install_path\u0026gt;    Set the environment variable:\nexport ANSIBLE_COLLECTIONS_PATHS=$ANSIBLE_COLLECTIONS_PATHS:\u0026lt;install_path\u0026gt;    Using Collections  In order to use any Ansible module, ensure that the importation of the proper FQCN (Fully Qualified Collection Name) is embedded in the playbook. For example, collections: - dellemc.powerscale To generate Ansible documentation for a specific module, embed the FQCN before the module name. For example, ansible-doc dellemc.powerscale.dellemc_powerscale_gatherfacts  Running Ansible Modules The Ansible server must be configured with Python library for OneFS to run the Ansible playbooks. The Documents provide information on different Ansible modules along with their functions and syntax. The parameters table in the Product Guide provides information on various parameters which need to be configured before running the modules.\nSSL Certificate Validation  Export the SSL certificate using KeyStore Explorer tool or from the browser in .crt format. Append the SSL certificate to the Certifi package file cacert.pem.  For Python 3.5 : cat \u0026lt;\u0026gt; \u0026raquo; /usr/local/lib/python3.5/dist-packages/certifi/cacert.pem For Python 2.7 : cat \u0026lt;\u0026gt; \u0026raquo; /usr/local/lib/python2.7/dist-packages/certifi/cacert.pem    Results Each module returns the updated state and details of the entity. For example, if you are using the group module, all calls will return the updated details of the group. Sample result is shown in each module\u0026rsquo;s documentation.\n","excerpt":"Ansible Modules for Dell EMC PowerScale The Ansible Modules for Dell EMC PowerScale allow Data …","ref":"/ansible-docs/docs/storage/platforms/powerscale/readme/","title":"ReadMe"},{"body":"Ansible Modules for Dell EMC PowerStore The Ansible Modules for Dell EMC PowerStore allow Data Center and IT administrators to use RedHat Ansible to automate and orchestrate the configuration and management of Dell EMC PowerStore arrays.\nThe capabilities of the Ansible modules are managing volumes, volume groups, hosts, host groups, snapshots, snapshot rules, replication rules, replication sessions, protection policies, file systems, NAS servers, SMB shares, user and tree quotas, file system snapshots and NFS exports. It also allows the capability to gather facts from the array. The options available for each are list, show, create, modify and delete. These tasks can be executed by running simple playbooks written in yaml syntax. The modules are written so that all the operations are idempotent, so making multiple identical requests has the same effect as making a single request.\nSupport Ansible modules for PowerStore are supported by Dell EMC and are provided under the terms of the license attached to the source code. Dell EMC does not provide support for any source code modifications. For any Ansible module issues, questions or feedback, join the Dell EMC Automation community.\nSupported Platforms  Dell EMC PowerStore Arrays version 1.0, 2.0  Prerequisites  Ansible 2.9, 2.10 Python 3.5, 3.6, 3.7, 3.8 Red Hat Enterprise Linux 7.6, 7.7, 7.8, 8.2 Python Library for PowerStore version 1.3.0 or higher  Idempotency The modules are written in such a way that all requests are idempotent and hence fault-tolerant. It essentially means that the result of a successfully performed request is independent of the number of times it is executed.\nList of Ansible Modules for Dell EMC PowerStore  Volume module Volume group module Host module Host group module Snapshot module Snapshot rule module Replication rule module Replication session module Protection policy module Gather facts module File system module NAS server module SMB share module Quota module File system snapshot module NFS export module  Installation of SDK  Clone the repo using the command:  git clone https://github.com/dell/python-powerstore/tree/1.3.0  Go to the root directory of setup. Execute the following command:  pip install .  Installing Collections   Download the tar build and use the following command to install the collection anywhere in your system:\nansible-galaxy collection install dellemc-powerstore-1.2.1.tar.gz -p \u0026lt;install_path\u0026gt;\n  Set the environemnt variable:\nexport ANSIBLE_COLLECTIONS_PATHS=$ANSIBLE_COLLECTIONS_PATHS:\u0026lt;install_path\u0026gt;\n  Using Collections   In order to use any Ansible module, ensure that the importing of a proper FQCN(Fully Qualified Collection Name) must be embedded in the playbook. Refer to the followig example: collections: - dellemc.powerstore\n  For generating Ansible documentaion for a specific module, embed the FQCN before the module name. Refer to the following example:\nansible-doc dellemc.powerstore.dellemc_powerstore_gatherfacts\n  Running Ansible Modules The Ansible server must be configured with Python library for PowerStore to run the Ansible playbooks. The Documents provide information on different Ansible modules along with their functions and syntax. The parameters table in the Product Guide provides information on various parameters which needs to be configured before running the modules.\nSSL Certificate Validation   Copy the CA certificate to this \u0026ldquo;/etc/pki/ca-trust/source/anchors\u0026rdquo; path of the host by any external means. 2.Set the \u0026ldquo;REQUESTS_CA_BUNDLE\u0026rdquo; environment variable to the path of the SSL certificate using the following command:\nexport REQUESTS_CA_BUNDLE=/etc/pki/ca-trust/source/anchors/\u0026lt;\u0026lt;Certificate_Name\u0026gt;\u0026gt;    Import the SSL certificate to host using the following command:\nupdate-ca-trust\n  Results Each module returns the updated state and details of the entity. For example, if you are using the group module, all calls will return the updated details of the group. Sample result is shown in each module\u0026rsquo;s documentation.\n","excerpt":"Ansible Modules for Dell EMC PowerStore The Ansible Modules for Dell EMC PowerStore allow Data …","ref":"/ansible-docs/docs/storage/platforms/powerstore/readme/","title":"ReadMe"},{"body":"Ansible Modules for Dell EMC Unity The Ansible Modules for Dell EMC Unity allow Data Center and IT administrators to use RedHat Ansible to automate and orchestrate the configuration and management of Dell EMC Unity arrays.\nThe capabilities of the Ansible modules are managing consistency groups, filesystem, filesystem snapshots, NAS server, NFS export, SMB share, hosts, snapshots, snapshot schedules, storage pools, user quotas, quota trees and volumes; and to gather facts from the array. The options available for each are list, show, create, modify and delete. These tasks can be executed by running simple playbooks written in yaml syntax. The modules are written so that all the operations are idempotent, so making multiple identical requests has the same effect as making a single request.\nSupport Ansible modules for Unity are supported by Dell EMC and are provided under the terms of the license attached to the source code. Dell EMC does not provide support for any source code modifications. For any Ansible module issues, questions or feedback, join the Dell EMC Automation community.\nSupported Platforms  Dell EMC Unity Arrays version 5.0, 5.1.0  Prerequisites  Ansible 2.9, 2.10 Python 2.8, 3.5, 3.6, 3.7, 3.8 Red Hat Enterprise Linux 7.6, 7.7, 7.8, 8.2 Python Library for Unity storops version 1.2.10 or higher  Idempotency The modules are written in such a way that all requests are idempotent and hence fault-tolerant. It essentially means that the result of a successfully performed request is independent of the number of times it is executed.\nList of Ansible Modules for Dell EMC Unity  Consistency group module Filesystem module Filesystem snapshot module Gather facts module Host module NAS server module NFS export module SMB share module Snapshot module Snapshot schedule module Storage pool module User quota module Quota tree module Volume module  Installation of SDK Install python sdk named \u0026lsquo;storops\u0026rsquo;. It can be installed using pip, based on appropriate python version.\npip install storops  Installing Collections   Download the tar build and follow the below command to install the collection anywhere in your system:\nansible-galaxy collection install dellemc-unity-1.2.0.tar.gz -p \u0026lt;install_path\u0026gt;    Set the environment variable:\nexport ANSIBLE_COLLECTIONS_PATHS=$ANSIBLE_COLLECTIONS_PATHS:\u0026lt;install_path\u0026gt;    Using Collections   In order to use any Ansible module, ensure that the importing of proper FQCN(Fully Qualified Collection Name) must be embedded in the playbook. Below example can be referred\ncollections: - dellemc.unity    For generating Ansible documentaion for a specific module, embed the FQCN before the module name. The below example can be referred.\nansible-doc dellemc.unity.dellemc_unity_gatherfacts    Running Ansible Modules The Ansible server must be configured with Python library for Unity to run the Ansible playbooks. The Documents provide information on different Ansible modules along with their functions and syntax. The parameters table in the Product Guide provides information on various parameters which need to be configured before running the modules.\nSSL Certificate Validation NOTE: These modules are supported through CA certified certificate only, however a self-signed certificate is not supported.\n Copy the CA certificate to this \u0026ldquo;/etc/pki/ca-trust/source/anchors\u0026rdquo; path of the host by any external means. Set the \u0026ldquo;REQUESTS_CA_BUNDLE\u0026rdquo; environment variable to the path of the SSL certificate using the command \u0026ldquo;export REQUESTS_CA_BUNDLE=/etc/pki/ca-trust/source/anchors/\u0026laquo;Certificate_Name\u0026raquo;\u0026rdquo; Import the SSL certificate to host using the command \u0026ldquo;update-ca-trust\u0026rdquo;.  Results Each module returns the updated state and details of the entity, For example, if you are using the Volume module, all calls will return the updated details of the volume. Sample result is shown in each module\u0026rsquo;s documentation.\n","excerpt":"Ansible Modules for Dell EMC Unity The Ansible Modules for Dell EMC Unity allow Data Center and IT …","ref":"/ansible-docs/docs/storage/platforms/unity/readme/","title":"ReadMe"},{"body":"Ansible Modules for Dell EMC PowerFlex Release Notes 1.0  © 2021 Dell Inc. or its subsidiaries. All rights reserved. Dell EMC, and other trademarks are trademarks of Dell Inc. or its subsidiaries. Other trademarks may be trademarks of their respective owners.\n Contents These release notes contain supplemental information about Ansible Modules for Dell EMC PowerFlex.\n Product Description New Features Known issues Limitations Distribution Documentation  Product Description The Ansible Modules for Dell EMC PowerFlex are used for managing volumes, storage pools, SDCs, and snapshots for PowerFlex storage devices. The modules use playbooks to list, show, create, delete, and modify each of the entities.\nThe Ansible Modules for Dell EMC PowerFlex supports the following features:\n Create volumes, storage pools and snapshots. Modify volumes, storage pools, SDCs and snapshots. Delete volumes and snapshots. Get details of a volumes, snapshots, SDCs and storage pool. Get entities of the PowerFlex storage device.  New Features The Ansible Modules for Dell EMC PowerFlex release 1.0 supports the following features:\n  The following are the features of the gatherfacts module:\n Get the API details of a PowerFlex storage device. Get the list of SDCs. Get the list of SDSs. Get the list of volumes. Get the list of snapshots. Get the list of storage pools. Get list of protection domains. Get list of snapshot policies.    The following are the features of the volume module:\n Get the details of a volume. Create a volume. Modify details of a volume. Delete a volume.    The following are the features of the snapshot module:\n Get the details of a snapshot. Create a snapshot. Modify details of a snapshot. Delete a snapshot.    The following are the features of the storage pools module:\n Get the details of a storage pool. Create a storage pool. Modify details of a storage pool.    The following are the features of the SDCs module:\n Get the details of the SDC. Rename a SDC.    Known issues There are no known issues.\nLimitations There are no known limitations.\nDistribution The software package is available for download from the Ansible Modules for PowerFlex GitHub page.\nDocumentation The documentation is available on Ansible Modules for PowerFlex GitHub page. It includes the following:\n README Release Notes (this document) Product Guide  ","excerpt":"Ansible Modules for Dell EMC PowerFlex Release Notes 1.0  © 2021 Dell Inc. or its subsidiaries. All …","ref":"/ansible-docs/docs/storage/platforms/powerflex/release-notes/","title":"Release Notes"},{"body":"Ansible Modules for Dell EMC PowerMax Release Notes 1.5.0  © 2021 Dell Inc. or its subsidiaries. All rights reserved. Dell EMC, and other trademarks are trademarks of Dell Inc. or its subsidiaries. Other trademarks may be trademarks of their respective owners.\n Contents These release notes contain supplemental information about Ansible Modules for Dell EMC PowerMax.\n Revision History Product Description New Features \u0026amp; Enhancements Known issues Limitations Distribution Documentation  Revision History    Date Document revision Description of changes     May 2021 01 Ansible Modules for Dell EMC PowerMax release 1.5.0    Product Description The Ansible Modules for Dell EMC PowerMax are used for managing volumes, storage groups, ports, port groups, host, host groups, masking views, SRDF links, RDF groups, snapshots, job, snapshot policies, storage pools, role for automatic volume provisioning and Metro DR environments for PowerMax arrays. The modules use playbooks to list, show, create, delete, and modify each of the entities.\nThe Ansible Modules for Dell EMC PowerMax supports the following features:\n Create volumes, storage groups, hosts, host groups, port groups, masking views, Metro DR environments, snapshot policies, and snapshots of a storage group. Modify volumes, storage groups, hosts, host groups, Metro DR environments, snapshot policies, and port groups in the array. Delete volumes, storage groups, hosts, host groups, port groups, masking views, Metro DR environments, snapshot policies, and snapshots of a storage group. Get details of volumes, storage groups, hosts, host groups, port, port groups, masking views, Metro DR environments, Job, RDF groups, snapshot policies, storage pools, and snapshots of a storage group.  New Features \u0026amp; Enhancements The Ansible Modules for Dell EMC PowerMax release 1.5.0 supports the following features:\n The Snapshot policy module supports the following functionalities:  Create a snapshot policy. Get details of any specific snapshot policy. Modify the snapshot policy attributes. Delete a snapshot policy.  NOTE: Supports PyU4V 9.2.1.3 and above.\n    The storage pool module supports the following functionality:  Get storage pool details for a given storage pool.   The following enhancements have been made to the gatherfacts module:  Get list of snapshot policies present on the PowerMax array.  NOTE: Supports PyU4V 9.2.1.3 and above for getting snapshot policy details and PyU4V 9.2.0.8 and above for getting snapshot details.\n    The following enhancements have been made to the storage group module:  Snapshot policy can be associated/disassociated to/from a storage group.  NOTE: Supports PyU4V 9.2.1.3 and above.\n    The following enhancements have been made to the snapshot module:  New parameter \u0026lsquo;snapshot_id\u0026rsquo; has been added which indicates unique ID of snapshot. snapshot_id is required for link, unlink, rename and delete operations.It is optional for getting details of snapshot.  NOTE: Supports PyU4V 9.2.0.8 and above.\n    Following functionalities are available for ansible role for automatic volume placement:  Finding if there is enough capacity of the given service level in any array. If multiple arrays available, return which is least used as \u0026lsquo;assigned_pool\u0026rsquo;. assigned_pool includes:  serial_no srp_id sg_name (if passed)     The following enhancements have been made to the host module:  Check mode feature of ansible is enabled for host module.   The following enhancements have been made to the host group module:  Check mode feature of ansible is enabled for host group module.   The following enhancements have been made to the volume module:  Check mode feature of ansible is enabled for volume module.   Support for Unisphere 9.1 and above Support for Python version 2.8 and above Support for PyU4V python library version 9.1.2.0 and above   NOTE: Unisphere Version 9.1 is compatible with PowerMax Python library version 9.1.x.x and similarly Unisphere versions later than 9.1 will only work with Python library versions later than 9.1.x.x.\n Known issues   Modify state operation from Establish to Suspend in Adaptive Copy mode in presence of force flag is not implemented. The REST API does not support this hence Python SDK (PyU4V) has no support for this operation.\n  Task to link a snapshot to a target storage group which is already linked is not implemented. The REST API does not support this hence Python SDK (PyU4V) has no support for this operation.\n  Limitations There are no known limitations.\nDistribution The software package is available for download from the Ansible Modules for PowerMax GitHub page.\nDocumentation The documentation is available on Ansible Modules for PowerMax GitHub page. It includes the following:\n README Release Notes (this document) Product Guide  ","excerpt":"Ansible Modules for Dell EMC PowerMax Release Notes 1.5.0  © 2021 Dell Inc. or its subsidiaries. All …","ref":"/ansible-docs/docs/storage/platforms/powermax/release-notes/","title":"Release Notes"},{"body":"Ansible Modules for Dell EMC PowerScale Release Notes 1.2.0  © 2021 Dell Inc. or its subsidiaries. All rights reserved. Dell, EMC, and other trademarks are trademarks of Dell Inc. or its subsidiaries. Other trademarks may be trademarks of their respective owners.\n Content These release notes contain supplemental information about Ansible Modules for Dell EMC PowerScale.\n Revision History Product Description Features Known Problem and limitations Software media, organization, and files Additional resources  Revision history The table in this section lists the revision history of this document.\nTable 1. Revision history\n   Revision Date Description     01 Jun 2021 Ansible Modules for Dell EMC PowerScale 1.2.0    Product Description This section describes the Ansible Modules for Dell EMC PowerScale. The Ansible Modules for Dell EMC PowerScale allow Data Center and IT administrators to use RedHat Ansible to automate and orchestrate the configuration and management of Dell EMC PowerScale arrays.\nThe Ansible Modules for Dell EMC PowerScale support the following features:\n Create user, groups, filesystem, NFS export, smart quotas, SMB share, snapshot and snapshot schedule of a filesystem. Modify user, groups, filesystem, access zone, NFS export, smart quotas, SMB share, snapshot and snapshot schedule of a filesystem. Delete user, groups, filesystem, NFS export, smart quotas, SMB share, snapshot and snapshot schedule of a filesystem. Get details of user, groups, node, filesystem, access zone, NFS export, smart quotas, SMB share, snapshot and snapshot schedule of a filesystem. Add, modify and remove Active Directory and LDAP to Authentication providers list. Map or unmap Active Directory and LDAP Authentication providers to Access zone. Get attributes and entities of the array.  The Ansible modules use playbooks, written in yaml syntax, to list, show, create, delete, and modify each of these entities.\nFeatures This section describes the features of the Ansible Modules for Dell EMC PowerScale for this release.\nThe Ansible Modules for Dell EMC PowerScale release 1.2.0 supports the following features:\n  Idempotency\n Has been handled in all modules. Allows the playbook to be run multiple times . Avoids the need for complex rollbacks.    Access Zones\n PowerScale has a concept of access zones. These are to partition the cluster into multiple isolated sections. Ansible modules support access zone operations that can also operate on the default (system) access zone. Users and Groups can be specific to a particular access zone. For non-system access zones, the path provided by the playbook is a relative path. Absolute path = Access zone base path + relative path provided by the user.    MODULES\n  The Access Zone module has the following enhancements:\n Map or unmap authentication providers to/from an access zone.    The File System module is enhanced to support the following functionality:\n Create a filesystem is updated to support both isi_sdk_8_1_1 and isi_sdk_9_0_0. Update a filesystem is updated to support both isi_sdk_8_1_1 and isi_sdk_9_0_0.    The ADS module supports the following functionality:\n Add Active Directory provider to authentication providers. Modify Active Directory provider parameters. Remove Active Directory provider from authentication providers. Retrieve details of Active Directory provider.    The LDAP module supports the following functionality:\n Add LDAP provider to authentication providers. Modify LDAP provider parameters. Remove LDAP provider from authentication providers. Retrieve details of LDAP provider.    The Node module supports the following functionality:\n Get Node details of Dell EMC PowerScale storage    The Gather Facts module is enhanced to support the following functionality:\n  Get details of the any entity listed below:\n Nodes Nfs exports Smb shares Active clients      The Smart Quotas module is enhanced to support the following functionality:\n Create a default-user/default-group quota. Modify the attributes of quota like include_overheads(8_1_1)/thresholds_on(9_0_0), soft_grace_period, hard_limit_size. Updated code to support both isi_sdk_8_1_1 and isi_sdk_9_0_0. Get details of the default-user/default-group quota. Delete the default-user/default-group quota.    Known issues Known problems in this release are listed.\n  Snapshot schedule\n If the playbook has a desired_retention field, running same the playbook again returns the changed as True (Idempotency does not work).    Filesystem Creation\n  Creation of a filesystem can fail when api_user: \u0026ldquo;admin\u0026rdquo; because it is possible that the admin user may not have privileges to set an ACLs.\n  In that case, create a filesystem with api_user: \u0026ldquo;root\u0026rdquo;.\n    Snapshot creation with alias name\n Alias name attribute remains null in spite of creating snapshot with alias name. This is an issue with PowerScale rest API. Alias name is not getting appended to the attribute in response.    Limitations This section lists the limitations in this release of Ansible Modules for Dell EMC PowerScale.\n  Gatherfacts\n Getting the list of users and groups with very long names may fail.    Users and Groups\n Only local users and groups can be created. Operations on users and groups with very long names may fail.    Access Zone\n Creation and deletion of access zones is not supported.    Filesystems\n ACLs can only be modified from POSIX to POSIX mode. Only directory quotas are supported but not user or group quotas. Modification of include_snap_data flag is not supported.    NFS Export\n If there multiple exports present with the same path in an access zone, operations on such exports fail.    Smart Quota\n Once the limits are assigned to the quota, then the quota can\u0026rsquo;t be converted to accounting. Only modification to the threshold limits is permitted. Its mandatory to pass \u0026lsquo;quota\u0026rsquo; parameter for create and modify operations for any quota type.    No support for advanced PowerScale features\n Advanced PowerScale features include SyncIQ, tiering, replication, and so on.     Software media, organization, and files The software package is available for download from the Ansible Modules for PowerScale GitHub page.\nAdditional resources This section provides more information about the product, how to get support, and provide feedback.\nDocumentation This section lists the related documentation for Ansible Modules for Dell EMC PowerScale. The documentation is available on the Ansible Modules for PowerScale GitHub page. The documentation includes the following:\n Ansible Modules for Dell EMC PowerScale Release Notes (this document). Ansible Modules for Dell EMC PowerScale Product Guide  Troubleshooting and support The Dell Container Community provides your primary source of support services.\nFor any setup, configuration issues, questions or feedback, join the Dell EMC Container community at https://www.dell.com/community/ Containers/bd-p/Containers.\n  Technical support\n  Dell EMC Online Support also provides technical support services. To open a service request, you must have a valid support agreement.\n  To get a valid support agreement or for other questions about your account, contact your Dell EMC sales representative.\n  For documentation, release notes, software updates, and other information about Dell EMC products, go to Dell EMC Online Support.\n    Support   Use the resources in this topic to get help and support.\n  The source code available on Github is unsupported and provided solely under the terms of the license attached to the source code.\n  For clarity, Dell EMC does not provide support for any source code modifications.\n  For any Ansible module setup, configuration issues, questions or feedback, join the Dell EMC Automation community at https:// www.dell.com/community/Automation/bd-p/Automation?ref=lithium_menu\n  For any Dell EMC storage issues, please contact Dell support at: https://www.dell.com/support.\n  ","excerpt":"Ansible Modules for Dell EMC PowerScale Release Notes 1.2.0  © 2021 Dell Inc. or its subsidiaries. …","ref":"/ansible-docs/docs/storage/platforms/powerscale/release-notes/","title":"Release Notes"},{"body":"Ansible Modules for Dell EMC PowerStore Release Notes 1.2.0  © 2021 Dell Inc. or its subsidiaries. All rights reserved. Dell, EMC, and other trademarks are trademarks of Dell Inc. or its subsidiaries. Other trademarks may be trademarks of their respective owners.\n Content These release notes contain supplemental information about Ansible Modules for Dell EMC PowerStore.\n Revision History Product Description New Features \u0026amp; Enhancements Known Issues Limitations Distribution Documentation  Revision history The table in this section lists the revision history of this document.\nTable 1. Revision history\n   Revision Date Description     01 June 2021 Current release of Ansible Modules for Dell EMC PowerStore 1.2.0    Product Description The Ansible modules for Dell EMC PowerStore are used to automate and orchestrate the deployment, configuration, and management of Dell EMC PowerStore storage systems. The capabilities of Ansible modules are managing Volumes, Volume groups, Hosts, Host groups, Protection policies, Replication rules, Replication sessions, NFS exports, SMB shares, NAS server, File systems, File system snapshots, Quota tree, Quotas for filesystem and obtaining PowerStore system information. The options available for each capability are list, show, create, delete, and modify. The only exception is for NAS server for which the options available are list \u0026amp; modify.\nNew features \u0026amp; enhancements Along with the previous release deliverables, this release supports the following features -\n  Replication rule module supports the following functionalities:\n Create a replication rule Get replication rule details Modify attributes of replication rule Delete replication rule    Replication session module supports the following functionalities:\n Get replication session details Modify the state of the replication session    Protection policy module has the following enhancements:\n Add a replication rule to protection policy Remove a replication rule from protection policy    Gather Facts Module has the following enhancements:\n List of remote systems List of replication sessions List of replication rules    Known issues There are no known issues.\nLimitations There are no known limitations.\nDistribution The software package is available for download from the Ansible Modules for PowerStore GitHub page.\nDocumentation The documentation is available on Ansible Modules for PowerStore GitHub page. It includes the following:\n README Release Notes (this document) Product Guide  ","excerpt":"Ansible Modules for Dell EMC PowerStore Release Notes 1.2.0  © 2021 Dell Inc. or its subsidiaries. …","ref":"/ansible-docs/docs/storage/platforms/powerstore/release-notes/","title":"Release Notes"},{"body":"Ansible Modules for Dell EMC Unity Release Notes 1.2.0  © 2021 Dell Inc. or its subsidiaries. All rights reserved. Dell, EMC, and other trademarks are trademarks of Dell Inc. or its subsidiaries. Other trademarks may be trademarks of their respective owners.\n Content These release notes contain supplemental information about Ansible Modules for Dell EMC Unity.\n Revision History Product Description New Features \u0026amp; Enhancements Known Issues Limitations Distribution Documentation  Revision history The table in this section lists the revision history of this document.\nTable 1. Revision history\n   Revision Date Description     01 June 2021 Current release of Ansible Modules for Dell EMC Unity 1.2.0    Product Description The Ansible modules for Dell EMC Unity are used to automate and orchestrate the deployment, configuration, and management of Dell EMC Unity Family systems, including Unity, Unity XT, and the UnityVSA. The capabilities of Ansible modules are managing NFS exports, SMB shares, NAS server, File Systems, File System Snapshots, Quota tree, User quotas for filesystem and quota tree and obtaining Unity system information. The options available for each capability are list, show, create, delete, and modify; except for NAS server for which options available are list \u0026amp; modify.\nNew features \u0026amp; enhancements This release supports the following features -\n  Application Tagging:\n A new HTTP header (Application-Type) is added in Unity REST API in Goshawk release which is used to set REST client name and its version and this information is recorded in Unity logs Ansible modules support application tagging which is used to identify the REST application that makes the request to Unisphere The value of application type parameter from Ansible module is set to Ansible/1.2.0    User quota module supports the following functionalities:\n Create User quota for a Filesystem/Quota tree Get User quota details Modify attributes of User quota Delete User quota    Quota tree module supports the following functionalities:\n Create Quota tree for a Filesystem Get Quota tree details Modify attributes of Quota tree Delete Quota tree    Consistency group module has the following enhancements:\n Map hosts to a new or an existing Consistency group Unmap hosts from a Consistency group    Filesystem module has the following enhancements:\n Set the attributes of Quota config while Filesystem creation Associate an existing snapshot schedule to existing or new Filesystem Remove snapshot schedule from a Filesystem    Volume module has the following enhancements:\n Map multiple hosts to a new or existing volume Unmap multiple hosts from a volume    Gather Facts Module has the following enhancements:\n List of User quota List of Quota tree    Known issues Known issues in this release are listed below:\n  Filesystem creation with quota config\n Setting quota configuration while creating a filesystem may sometime observe delay in fetching the details about the quota config of the new filesystem. The module will throw an error to rerun the task to see expected result.    Mapping and unmapping of hosts for a Consistency group\n Interoperability between Ansible Unity playbooks and Unisphere REST API is not supported for mapping and unmapping of hosts for a consistency group.  WORKAROUND: It is recommended to use Ansible Unity modules consistently for all mapping and unmapping of hosts for a consistency group instead of partially/mutually doing it through Unisphere and Ansible modules.\n     Limitations There are no known limitations.\nDistribution The software package is available for download from the Ansible Modules for Unity GitHub page.\nDocumentation The documentation is available on Ansible Modules for Unity GitHub page. It includes the following:\n README Release Notes (this document) Product Guide  ","excerpt":"Ansible Modules for Dell EMC Unity Release Notes 1.2.0  © 2021 Dell Inc. or its subsidiaries. All …","ref":"/ansible-docs/docs/storage/platforms/unity/release-notes/","title":"Release notes"},{"body":"","excerpt":"","ref":"/ansible-docs/docs/network/support/","title":"Support"},{"body":"","excerpt":"","ref":"/ansible-docs/docs/server/support/","title":"Support"},{"body":"Support Ansible modules for PowerMax are supported by Dell EMC and are provided under the terms of the license attached to the source code. Dell EMC does not provide support for any source code modifications. For any Ansible module issues, questions or feedback, join the Dell EMC Automation community.\n","excerpt":"Support Ansible modules for PowerMax are supported by Dell EMC and are provided under the terms of …","ref":"/ansible-docs/docs/storage/support/","title":"Support"},{"body":"","excerpt":"","ref":"/ansible-docs/docs/server/platforms/unity/","title":"Unity"},{"body":"","excerpt":"","ref":"/ansible-docs/docs/storage/platforms/unity/","title":"Unity"},{"body":"You can upgrade the CSI Driver for Dell EMC Unity using Helm or Dell CSI Operator.\nUsing Helm Note: While upgrading the driver via helm, controllerCount variable in myvalues.yaml can be at most one less than the number of worker nodes.\nPreparing myvalues.yaml is the same as explained in install section.\nTo upgrade the driver from csi-unity v1.4 to csi-unity 1.5 (across K8S 1.18, K8S 1.19, K8S 1.20).\n  Get the latest csi-unity 1.5 code from Github.\n  Create myvalues.yaml according to csi-unity 1.5 .\n  Delete the existing default storage classes of csi-unity 1.4 .\n  Clone the repository https://github.com/dell/csi-unity , copy the helm/csi-unity/values.yaml to the new location csi-unity/dell-csi-helm-installer with name say myvalues.yaml, to customize settings for installation edit myvalues.yaml as per the requirements.\n  Navigate to common-helm-installer folder and execute the following command: ./csi-install.sh --namespace unity --values ./myvalues.yaml --upgrade\n  If the value of \u0026lsquo;createStorageClassesWithTopology\u0026rsquo; is set to \u0026ldquo;true\u0026rdquo; in myvalues.yaml , then\n Check the default storage classes, VolumeBindingMode should be \u0026lsquo;WaitForFirstConsumer\u0026rsquo;.    Note: User has to re-create existing custom-storage classes (if any) according to latest (v1.5) format.\nUsing Operator Note: While upgrading the driver via operator, replicas count in sample CR yaml can be at most one less than the number of worker nodes.\nTo upgrade the driver from csi-unity v1.4 to csi-unity v1.5 (OpenShift 4.6) :\n  Clone operator version 1.3.0\n  Execute bash scripts/install.sh --upgrade This command will install latest version of operator.\n  Furnish the sample CR yaml according to your environment.\n  For upgrading the csi-unity driver execute the following command:\nkubectl apply -f \u0026lt;furnished-cr.yaml\u0026gt;\n  ","excerpt":"You can upgrade the CSI Driver for Dell EMC Unity using Helm or Dell CSI Operator.\nUsing Helm Note: …","ref":"/ansible-docs/v1/upgradation/drivers/unity/","title":"Unity"},{"body":"You can upgrade the CSI Driver for Dell EMC Unity using Helm or Dell CSI Operator.\nUpdate Driver from v1.3 to v1.4 using Helm Steps\n Run git clone https://github.com/dell/csi-unity.git to clone the git repository and get the v1.3 driver. Update values file as needed. Run the csi-install script with the option --upgrade by running: cd ../dell-csi-helm-installer \u0026amp;\u0026amp; ./csi-install.sh --namespace unity --values ./my-values.yaml --upgrade.  Upgrade using Dell CSI Operator: Follow the instructions for upgrade on the Dell CSI Operator GitHub page.\n","excerpt":"You can upgrade the CSI Driver for Dell EMC Unity using Helm or Dell CSI Operator.\nUpdate Driver …","ref":"/ansible-docs/v2/upgradation/drivers/unity/","title":"Unity"},{"body":"Ansible Modules for Dell EMC Unity Product Guide 1.2.0 © 2021 Dell Inc. or its subsidiaries. All rights reserved. Dell, EMC, and other trademarks are trademarks of Dell Inc. or its subsidiaries. Other trademarks may be trademarks of their respective owners.\n Contents  NFS Module  Synopsis Parameters Examples Return Values Authors   Volume Module  Synopsis Parameters Examples Return Values Authors   NAS Server Module  Synopsis Parameters Examples Return Values Authors   Quota Tree Module  Synopsis Parameters Examples Return Values Authors   File System Module  Synopsis Parameters Notes Examples Return Values Authors   Storage Pool Module  Synopsis Parameters Notes Examples Return Values Authors   Gatherfacts Module  Synopsis Parameters Examples Return Values Authors   User Quota Module  Synopsis Parameters Examples Return Values Authors   Filesystem Snapshot Module  Synopsis Parameters Notes Examples Return Values Authors   Snapshot Module  Synopsis Parameters Examples Return Values Authors   SMB Share Module  Synopsis Parameters Notes Examples Return Values Authors   Host Module  Synopsis Parameters Examples Return Values Authors   Consistency Group Module  Synopsis Parameters Examples Return Values Authors   Snapshot Schedule Module  Synopsis Parameters Notes Examples Return Values Authors     NFS Module Manage NFS export on Unity storage system\nSynopsis Managing NFS export on Unity storage system includes- Create new NFS export, Modify NFS export attributes, Display NFS export details, Delete NFS export\nParameters   Parameter Type Required Default Choices Description   nfs_export_name  str      Name of the nfs export. Mandatory for create operation. Specify either nfs_export_name or nfs_export_id(but not both) for any operation.    nfs_export_id  str      ID of the nfs export. This is a unique ID generated by Unity storage system.    filesystem_name  str      Name of the filesystem for which NFS export will be created. Either filesystem or snapshot is required for creation of the NFS. If filesystem name is specified, then nas_server is required to uniquely identify the filesystem If filesystem parameter is provided, then snapshot cannot be specified.    filesystem_id  str      ID of the filesystem This is a unique ID generated by Unity storage system.    snapshot_name  str      Name of the snapshot for which NFS export will be created. Either filesystem or snapshot is required for creation of the NFS export. If snapshot parameter is provided, then filesystem cannot be specified.    snapshot_id  str      ID of the snapshot. This is a unique ID generated by Unity storage system.    nas_server_name  str      Name of the NAS server on which filesystem will be hosted.    nas_server_id  str      ID of the NAS server on which filesystem will be hosted.    path  str      Local path to export relative to the NAS server root. With NFS, each export of a file_system or file_snap must have a unique local path. Mandatory while creating NFS export.    description  str      Description of the NFS export. Optional parameter when creating a NFS export. To modify description, pass the new value in description field. To remove description, pass the empty value in description field.    host_state  str      present-in-export absent-in-export   Define whether the hosts can access the NFS export. Required when adding or removing access of hosts from the export.    anonymous_uid  int      Specifies the user ID of the anonymous account. If not specified at the time of creation, it will be set to 4294967294.    anonymous_gid  int      Specifies the group ID of the anonymous account. If not specified at the time of creation, it will be set to 4294967294.    state  str   True     absent present   State variable to determine whether NFS export will exist or not.    default_access  str      NO_ACCESS READ_ONLY READ_WRITE ROOT READ_ONLY_ROOT   Default access level for all hosts that can access the NFS export. For hosts that need different access than the default, they can be configured by adding to the list. If default_access is not mentioned during creation, then NFS export will be created with NO_ACCESS.    min_security  str      SYS KERBEROS KERBEROS_WITH_INTEGRITY KERBEROS_WITH_ENCRYPTION   NFS enforced security type for users accessing a NFS export. If not specified at the time of creation, it will be set to SYS.    no_access_hosts  list elements: dict      Hosts with no access to the NFS export. List of dictionaries. Each dictionary will have any of the keys from host_name, host_id, and ip_address.    \u0026nbsp; host_name   str   False     Name of the host.    \u0026nbsp; host_id   str   False     ID of the host.    \u0026nbsp; ip_address   str   False     IP address of the host.    read_only_hosts  list elements: dict      Hosts with read-only access to the NFS export. List of dictionaries. Each dictionary will have any of the keys from host_name, host_id, and ip_address    \u0026nbsp; host_name   str   False     Name of the host.    \u0026nbsp; host_id   str   False     ID of the host.    \u0026nbsp; ip_address   str   False     IP address of the host.    read_only_root_hosts  list elements: dict      Hosts with read-only for root user access to the NFS export. List of dictionaries. Each dictionary will have any of the keys from host_name, host_id, and ip_address    \u0026nbsp; host_name   str   False     Name of the host.    \u0026nbsp; host_id   str   False     ID of the host.    \u0026nbsp; ip_address   str   False     IP address of the host.    read_write_hosts  list elements: dict      Hosts with read and write access to the NFS export. List of dictionaries. Each dictionary will have any of the keys from host_name, host_id, and ip_address.    \u0026nbsp; host_name   str   False     Name of the host.    \u0026nbsp; host_id   str   False     ID of the host.    \u0026nbsp; ip_address   str   False     IP address of the host.    read_write_root_hosts  list elements: dict      Hosts with read and write for root user access to the NFS export. List of dictionaries. Each dictionary will have any of the keys from host_name, host_id, and ip_address.    \u0026nbsp; host_name   str   False     Name of the host.    \u0026nbsp; host_id   str   False     ID of the host.    \u0026nbsp; ip_address   str   False     IP address of the host.    unispherehost  str   True     IP or FQDN of the Unity management server.    username  str   True     The username of the Unity management server.    password  str   True     The password of the Unity management server.    verifycert  bool    True    True False   Boolean variable to specify whether or not to validate SSL certificate. True - Indicates that the SSL certificate should be verified. False - Indicates that the SSL certificate should not be verified.    port  int    443    Port number through which communication happens with Unity management server.    Examples - name: Create nfs export from filesystem dellemc_unity_nfs: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; username: \u0026quot;{{username}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; nfs_export_name: \u0026quot;ansible_nfs_from_fs\u0026quot; path: '/' filesystem_id: \u0026quot;fs_377\u0026quot; state: \u0026quot;present\u0026quot; - name: Create nfs export from snapshot dellemc_unity_nfs: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; username: \u0026quot;{{username}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; nfs_export_name: \u0026quot;ansible_nfs_from_snap\u0026quot; path: '/' snapshot_name: \u0026quot;ansible_fs_snap\u0026quot; state: \u0026quot;present\u0026quot; - name: Modify nfs export dellemc_unity_nfs: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; username: \u0026quot;{{username}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; nfs_export_name: \u0026quot;ansible_nfs_from_fs\u0026quot; nas_server_id: \u0026quot;nas_3\u0026quot; description: \u0026quot;\u0026quot; default_access: \u0026quot;READ_ONLY_ROOT\u0026quot; anonymous_gid: 4294967290 anonymous_uid: 4294967290 state: \u0026quot;present\u0026quot; - name: Add host in nfs export dellemc_unity_nfs: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; username: \u0026quot;{{username}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; nfs_export_name: \u0026quot;ansible_nfs_from_fs\u0026quot; filesystem_id: \u0026quot;fs_377\u0026quot; no_access_hosts: - host_id: \u0026quot;Host_1\u0026quot; read_only_hosts: - host_id: \u0026quot;Host_2\u0026quot; read_only_root_hosts: - host_name: \u0026quot;host_name1\u0026quot; read_write_hosts: - host_name: \u0026quot;host_name2\u0026quot; read_write_root_hosts: - ip_address: \u0026quot;1.1.1.1\u0026quot; host_state: \u0026quot;present-in-export\u0026quot; state: \u0026quot;present\u0026quot; - name: Remove host in nfs export dellemc_unity_nfs: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; username: \u0026quot;{{username}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; nfs_export_name: \u0026quot;ansible_nfs_from_fs\u0026quot; filesystem_id: \u0026quot;fs_377\u0026quot; no_access_hosts: - host_id: \u0026quot;Host_1\u0026quot; read_only_hosts: - host_id: \u0026quot;Host_2\u0026quot; read_only_root_hosts: - host_name: \u0026quot;host_name1\u0026quot; read_write_hosts: - host_name: \u0026quot;host_name2\u0026quot; read_write_root_hosts: - ip_address: \u0026quot;1.1.1.1\u0026quot; host_state: \u0026quot;absent-in-export\u0026quot; state: \u0026quot;present\u0026quot; - name: Get nfs details dellemc_unity_nfs: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; username: \u0026quot;{{username}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; nfs_export_id: \u0026quot;NFSShare_291\u0026quot; state: \u0026quot;present\u0026quot; - name: Delete nfs export by nfs name dellemc_unity_nfs: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; username: \u0026quot;{{username}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; nfs_export_name: \u0026quot;ansible_nfs_name\u0026quot; nas_server_name: \u0026quot;ansible_nas_name\u0026quot; state: \u0026quot;absent\u0026quot; Return Values   Key Type Returned Description   changed   bool   always   Whether or not the resource has changed.    nfs_share_details   complex   When nfs export exists.   Details of the nfs export.    \u0026nbsp; anonymous_gid   int  success  Group ID of the anonymous account    \u0026nbsp; anonymous_uid   int  success  User ID of the anonymous account    \u0026nbsp; default_access   str  success  Default access level for all hosts that can access export    \u0026nbsp; description   str  success  Description about the nfs export    \u0026nbsp; export_paths   list  success  Export paths that can be used to mount and access export    \u0026nbsp; filesystem   complex  success  Details of the filesystem on which nfs export is present    \u0026nbsp; \u0026nbsp; UnityFileSystem   complex  success  filesystem details    \u0026nbsp; \u0026nbsp; \u0026nbsp; id   str  success  ID of the filesystem    \u0026nbsp; \u0026nbsp; \u0026nbsp; name   str  success  Name of the filesystem    \u0026nbsp; id   str  success  ID of the nfs export    \u0026nbsp; min_security   str  success  NFS enforced security type for users accessing an export    \u0026nbsp; name   str  success  Name of the nfs export    \u0026nbsp; nas_server   complex  success  Details of the nas server    \u0026nbsp; \u0026nbsp; UnityNasServer   complex  success  NAS server details    \u0026nbsp; \u0026nbsp; \u0026nbsp; id   str  success  ID of the nas server    \u0026nbsp; \u0026nbsp; \u0026nbsp; name   str  success  Name of the nas server    \u0026nbsp; no_access_hosts_string   str  success  Hosts with no access to the nfs export    \u0026nbsp; read_only_hosts_string   str  success  Hosts with read-only access to the nfs export    \u0026nbsp; read_only_root_hosts_string   str  success  Hosts with read-only for root user access to the nfs export    \u0026nbsp; read_write_hosts_string   str  success  Hosts with read and write access to the nfs export    \u0026nbsp; read_write_root_hosts_string   str  success  Hosts with read and write for root user access to export    \u0026nbsp; type   str  success  NFS export type. i.e. filesystem or snapshot    Authors  Vivek Soni (@v-soni11) ansible.team@dell.com   Volume Module Manage volume on Unity storage system\nSynopsis Managing volume on Unity storage system includes- Create new volume, Modify volume attributes, Map Volume to host, Unmap volume to host, Display volume details, Delete volume\nParameters   Parameter Type Required Default Choices Description   vol_name  str      The name of the volume. Mandatory only for create operation.    vol_id  str      The id of the volume. It can be used only for get, modify, map/unmap host, or delete operation.    pool_name  str      This is the name of the pool where the volume will be created. Either the pool_name or pool_id must be provided to create a new volume.    pool_id  str      This is the id of the pool where the volume will be created. Either the pool_name or pool_id must be provided to create a new volume.    size  int      The size of the volume.    cap_unit  str      GB TB   The unit of the volume size. It defaults to 'GB', if not specified.    description  str      Description about the volume. Description can be removed by passing empty string (\"\").    snap_schedule  str      Snapshot schedule assigned to the volume. Add/Remove/Modify the snapshot schedule for the volume.    compression  bool      Boolean variable , specifies whether or not to enable compression. Compression is supported only for thin volumes    is_thin  bool    True    Boolean variable , specifies whether or not it's a thin volume.    sp  str      SPA SPB   Storage Processor for this volume.    io_limit_policy  str      IO limit policy associated with this volume. Once it's set, it cannot be removed through ansible module but it can be changed.    host_name  str      Name of the host to be mapped/unmapped with this volume. Either host_name or host_id can be specified in one task along with mapping_state.    host_id  str      ID of the host to be mapped/unmapped with this volume. Either host_name or host_id can be specified in one task along with mapping_state.    hlu  int      Host Lun Unit to be mapped/unmapped with this volume. It's an optional parameter, hlu can be specified along with host_name or host_id and mapping_state. If hlu is not specified, unity will choose it automatically. The maximum value supported is 255.    mapping_state  str      mapped unmapped   State of host access for volume.    new_vol_name  str      New name of the volume for rename operation.    tiering_policy  str      AUTOTIER_HIGH AUTOTIER HIGHEST LOWEST   Tiering policy choices for how the storage resource data will be distributed among the tiers available in the pool.    state  str   True     absent present   State variable to determine whether volume will exist or not.    hosts  list elements: dict      Name of hosts for mapping to a volume    \u0026nbsp; host_name   str      Name of the host.    \u0026nbsp; host_id   str      ID of the host.    \u0026nbsp; hlu   str      Host Lun Unit to be mapped/unmapped with this volume. It's an optional parameter, hlu can be specified along with host_name or host_id and mapping_state. If hlu is not specified, unity will choose it automatically. The maximum value supported is 255.    unispherehost  str   True     IP or FQDN of the Unity management server.    username  str   True     The username of the Unity management server.    password  str   True     The password of the Unity management server.    verifycert  bool    True    True False   Boolean variable to specify whether or not to validate SSL certificate. True - Indicates that the SSL certificate should be verified. False - Indicates that the SSL certificate should not be verified.    port  int    443    Port number through which communication happens with Unity management server.    Examples - name: Create Volume dellemc_unity_volume: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; username: \u0026quot;{{username}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; vol_name: \u0026quot;{{vol_name}}\u0026quot; description: \u0026quot;{{description}}\u0026quot; pool_name: \u0026quot;{{pool}}\u0026quot; size: 2 cap_unit: \u0026quot;{{cap_GB}}\u0026quot; state: \u0026quot;{{state_present}}\u0026quot; - name: Expand Volume by volume id dellemc_unity_volume: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; username: \u0026quot;{{username}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; vol_id: \u0026quot;{{vol_id}}\u0026quot; size: 5 cap_unit: \u0026quot;{{cap_GB}}\u0026quot; state: \u0026quot;{{state_present}}\u0026quot; - name: Modify Volume, map host by host_name dellemc_unity_volume: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; username: \u0026quot;{{username}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; vol_name: \u0026quot;{{vol_name}}\u0026quot; host_name: \u0026quot;{{host_name}}\u0026quot; hlu: 5 mapping_state: \u0026quot;{{state_mapped}}\u0026quot; state: \u0026quot;{{state_present}}\u0026quot; - name: Modify Volume, unmap host mapping by host_name dellemc_unity_volume: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; username: \u0026quot;{{username}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; vol_name: \u0026quot;{{vol_name}}\u0026quot; host_name: \u0026quot;{{host_name}}\u0026quot; mapping_state: \u0026quot;{{state_unmapped}}\u0026quot; state: \u0026quot;{{state_present}}\u0026quot; - name: Map multiple hosts to a Volume dellemc_unity_volume: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; username: \u0026quot;{{username}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; vol_id: \u0026quot;{{vol_id}}\u0026quot; hosts: - host_name: \u0026quot;10.226.198.248\u0026quot; hlu: 1 - host_id: \u0026quot;Host_929\u0026quot; hlu: 2 mapping_state: \u0026quot;mapped\u0026quot; state: \u0026quot;present\u0026quot; - name: Modify Volume attributes dellemc_unity_volume: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; username: \u0026quot;{{username}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; vol_name: \u0026quot;{{vol_name}}\u0026quot; new_vol_name: \u0026quot;{{new_vol_name}}\u0026quot; tiering_policy: \u0026quot;AUTOTIER\u0026quot; compression: True state: \u0026quot;{{state_present}}\u0026quot; - name: Delete Volume by vol name dellemc_unity_volume: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; username: \u0026quot;{{username}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; vol_name: \u0026quot;{{vol_name}}\u0026quot; state: \u0026quot;{{state_absent}}\u0026quot; - name: Delete Volume by vol id dellemc_unity_volume: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; username: \u0026quot;{{username}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; vol_id: \u0026quot;{{vol_id}}\u0026quot; state: \u0026quot;{{state_absent}}\u0026quot; Return Values   Key Type Returned Description   changed   bool   always   Whether or not the resource has changed    volume_details   complex   When volume exists   Details of the volume    \u0026nbsp; current_sp   str  success  Current storage processor for this volume    \u0026nbsp; description   str  success  Description about the volume    \u0026nbsp; host_access   list  success  Host mapped to this volume    \u0026nbsp; id   str  success  The system generated ID given to the volume    \u0026nbsp; io_limit_policy   dict  success  IO limit policy associated with this volume    \u0026nbsp; is_data_reduction_enabled   bool  success  Whether or not compression enabled on this volume    \u0026nbsp; is_thin_enabled   bool  success  Indicates whether thin provisioning is enabled for this volume    \u0026nbsp; name   str  success  Name of the volume    \u0026nbsp; pool   dict  success  The pool in which this volume is allocated.    \u0026nbsp; size_total_with_unit   str  success  Size of the volume with actual unit.    \u0026nbsp; snap_schedule   dict  success  Snapshot schedule applied to this volume    \u0026nbsp; tiering_policy   str  success  Tiering policy applied to this volume    \u0026nbsp; wwn   str  success  The world wide name of this volume    Authors  Arindam Datta (@arindam-emc) ansible.team@dell.com   NAS Server Module Manage NAS servers on Unity storage system\nSynopsis Managing NAS servers on Unity storage system includes get, modification to the NAS servers.\nParameters   Parameter Type Required Default Choices Description   nas_server_id  str      The ID of the NAS server. nas_server_name and nas_server_id are mutually exclusive parameters. Either one is required to perform the task.    nas_server_name  str      The Name of the NAS server. nas_server_name and nas_server_id are mutually exclusive parameters. Either one is required to perform the task.    nas_server_new_name  str      The new name of the NAS server. It can be mentioned during modification of the NAS server.    is_replication_destination  bool      It specifies whether the NAS server is a replication destination. It can be mentioned during modification of the NAS server.    is_backup_only  bool      It specifies whether the NAS server is used as backup only. It can be mentioned during modification of the NAS server.    is_multiprotocol_enabled  bool      This parameter indicates whether multiprotocol sharing mode is enabled. It can be mentioned during modification of the NAS server.    allow_unmapped_user  bool      This flag is used to mandatorily disable access in case of any user mapping failure. If true, then enable access in case of any user mapping failure. If false, then disable access in case of any user mapping failure. It can be mentioned during modification of the NAS server.    default_windows_user  str      Default windows user name used for granting access in the case of Unix to Windows user mapping failure. It can be mentioned during modification of the NAS server.    default_unix_user  str      Default Unix user name used for granting access in the case of Windows to Unix user mapping failure. It can be mentioned during modification of the NAS server.    enable_windows_to_unix_username_mapping  bool      This parameter indicates whether a Unix to/from Windows user name mapping is enabled. It can be mentioned during modification of the NAS server.    is_packet_reflect_enabled  bool      If the packet has to be reflected, then this parameter has to be set to True. It can be mentioned during modification of the NAS server.    current_unix_directory_service  str      NONE NIS LOCAL LDAP LOCAL_THEN_NIS LOCAL_THEN_LDAP   This is the directory service used for querying identity information for UNIX (such as UIDs, GIDs, net groups). It can be mentioned during modification of the NAS server.    state  str   True     present absent   Define the state of NAS server on the array. present indicates that NAS server should exist on the system after the task is executed. Right now deletion of NAS server is not supported. Hence, if state is set to absent for any existing NAS server then error will be thrown. For any non-existing NAS server, if state is set to absent then it will return None.    unispherehost  str   True     IP or FQDN of the Unity management server.    username  str   True     The username of the Unity management server.    password  str   True     The password of the Unity management server.    verifycert  bool    True    True False   Boolean variable to specify whether or not to validate SSL certificate. True - Indicates that the SSL certificate should be verified. False - Indicates that the SSL certificate should not be verified.    port  int    443    Port number through which communication happens with Unity management server.    Examples  - name: Get Details of NAS Server dellemc_unity_nasserver: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; username: \u0026quot;{{username}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; nas_server_name: \u0026quot;{{nas_server_name}}\u0026quot; state: \u0026quot;present\u0026quot; - name: Modify Details of NAS Server dellemc_unity_nasserver: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; username: \u0026quot;{{username}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; nas_server_name: \u0026quot;{{nas_server_name}}\u0026quot; nas_server_new_name: \u0026quot;updated_sample_nas_server\u0026quot; is_replication_destination: False is_backup_only: False is_multiprotocol_enabled: True allow_unmapped_user: True default_unix_user: \u0026quot;default_unix_sample_user\u0026quot; default_windows_user: \u0026quot;default_windows_sample_user\u0026quot; enable_windows_to_unix_username_mapping: True current_unix_directory_service: \u0026quot;LDAP\u0026quot; is_packet_reflect_enabled: True state: \u0026quot;present\u0026quot; Return Values   Key Type Returned Description   changed   bool   always   Whether or not the resource has changed    nas_server_details   complex   When NAS server exists.   The NAS server details.    \u0026nbsp; allow_unmapped_user   bool  success  enable/disable access status in case of any user mapping failure    \u0026nbsp; current_unix_directory_service   str  success  Directory service used for querying identity information for UNIX (such as UIDs, GIDs, net groups).    \u0026nbsp; default_unix_user   str  success  Default Unix user name used for granting access in the case of Windows to Unix user mapping failure.    \u0026nbsp; default_windows_user   str  success  Default windows user name used for granting access in the case of Unix to Windows user mapping failure    \u0026nbsp; id   str  success  ID of the NAS server    \u0026nbsp; is_backup_only   bool  success  Whether the NAS server is used as backup only.    \u0026nbsp; is_multi_protocol_enabled   bool  success  Indicates whether multiprotocol sharing mode is enabled    \u0026nbsp; is_packet_reflect_enabled   bool  success  If the packet reflect has to be enabled    \u0026nbsp; is_replication_destination   bool  success  If the NAS server is a replication destination then True.    \u0026nbsp; is_windows_to_unix_username_mapping_enabled   bool  success  Indicates whether a Unix to/from Windows user name mapping is enabled.    \u0026nbsp; name   str  success  Name of the NAS server    Authors  P Srinivas Rao (@srinivas-rao5) ansible.team@dell.com   Quota Tree Module Manage quota tree on the Unity storage system\nSynopsis Managing Quota tree on the Unity storage system includes Create quota tree, Get quota tree, Modify quota tree and Delete quota tree\nParameters   Parameter Type Required Default Choices Description   filesystem_name  str      The name of the filesystem for which quota tree is created. For creation or modification of a quota tree either filesystem_name or filesystem_id is required.    filesystem_id  str      The ID of the filesystem for which the quota tree is created. For creation of a quota tree either filesystem_id or filesystem_name is required.    nas_server_name  str      The name of the NAS server in which the filesystem is created. For creation of a quota tree either nas_server_name or nas_server_id is required.    nas_server_id  str      The ID of the NAS server in which the filesystem is created. For creation of a quota tree either filesystem_id or filesystem_name is required.    tree_quota_id  str      The ID of the quota tree. Either tree_quota_id or path to quota tree is required to view/modify/delete quota tree.    path  str      The path to the quota tree. Either tree_quota_id or path to quota tree is required to create/view/modify/delete a quota tree. Path must start with a forward slash '/'.    hard_limit  int      Hard limitation for a quota tree on the total space available. If exceeded, users in quota tree cannot write data. Value 0 implies no limit. One of the values of soft_limit and hard_limit can be 0, however, both cannot be both 0 during creation of a quota tree.    soft_limit  int      Soft limitation for a quota tree on the total space available. If exceeded, notification will be sent to users in the quota tree for the grace period mentioned, beyond which users cannot use space. Value 0 implies no limit. Both soft_limit and hard_limit cannot be 0 during creation of quota tree.    cap_unit  str      MB GB TB   Unit of soft_limit and hard_limit size. It defaults to 'GB' if not specified.    description  str      Description of a quota tree.    state  str   True     absent present   The state option is used to mention the existence of the filesystem quota tree.    unispherehost  str   True     IP or FQDN of the Unity management server.    username  str   True     The username of the Unity management server.    password  str   True     The password of the Unity management server.    verifycert  bool    True    True False   Boolean variable to specify whether or not to validate SSL certificate. True - Indicates that the SSL certificate should be verified. False - Indicates that the SSL certificate should not be verified.    port  int    443    Port number through which communication happens with Unity management server.    Examples  - name: Get quota tree details by quota tree id dellemc_unity_tree_quota: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; username: \u0026quot;{{username}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; quota_tree_id: \u0026quot;treequota_171798700679_10\u0026quot; state: \u0026quot;present\u0026quot; - name: Get quota tree details by quota tree path dellemc_unity_tree_quota: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; username: \u0026quot;{{username}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; filesystem_name: \u0026quot;fs_2171\u0026quot; nas_server_id: \u0026quot;nas_21\u0026quot; path: \u0026quot;/test\u0026quot; state: \u0026quot;present\u0026quot; - name: Create quota tree for a filesystem with filesystem id dellemc_unity_tree_quota: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; username: \u0026quot;{{username}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; filesystem_id: \u0026quot;fs_2171\u0026quot; hard_limit: 6 cap_unit: \u0026quot;TB\u0026quot; soft_limit: 5 path: \u0026quot;/test_new\u0026quot; state: \u0026quot;present\u0026quot; - name: Create quota tree for a filesystem with filesystem name dellemc_unity_tree_quota: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; username: \u0026quot;{{username}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; filesystem_name: \u0026quot;Test_filesystem\u0026quot; nas_server_name: \u0026quot;lglad068\u0026quot; hard_limit: 6 cap_unit: \u0026quot;TB\u0026quot; soft_limit: 5 path: \u0026quot;/test_new\u0026quot; state: \u0026quot;present\u0026quot; - name: Modify quota tree limit usage by quota tree path dellemc_unity_tree_quota: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; username: \u0026quot;{{username}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; path: \u0026quot;/test_new\u0026quot; hard_limit: 10 cap_unit: \u0026quot;TB\u0026quot; soft_limit: 8 state: \u0026quot;present\u0026quot; - name: Modify quota tree by quota tree id dellemc_unity_tree_quota: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; username: \u0026quot;{{username}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; filesystem_id: \u0026quot;fs_2171\u0026quot; quota_tree_id: \u0026quot;treequota_171798700679_10\u0026quot; hard_limit: 12 cap_unit: \u0026quot;TB\u0026quot; soft_limit: 10 state: \u0026quot;present\u0026quot; - name: Delete quota tree by quota tree id dellemc_unity_tree_quota: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; username: \u0026quot;{{username}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; filesystem_id: \u0026quot;fs_2171\u0026quot; quota_tree_id: \u0026quot;treequota_171798700679_10\u0026quot; state: \u0026quot;absent\u0026quot; - name: Delete quota tree by path dellemc_unity_tree_quota: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; username: \u0026quot;{{username}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; filesystem_id: \u0026quot;fs_2171\u0026quot; path: \u0026quot;/test_new\u0026quot; state: \u0026quot;absent\u0026quot; Return Values   Key Type Returned Description   changed   bool   always   Whether or not the resource has changed    get_quota_tree_details   complex   When quota tree exists   Details of the quota tree.    \u0026nbsp; description   str  success  Description of the quota tree.    \u0026nbsp; filesystem   complex  success  Filesystem details for which the quota tree is created.    \u0026nbsp; \u0026nbsp; UnityFileSystem   complex  success  Filesystem details for which the quota tree is created.    \u0026nbsp; \u0026nbsp; \u0026nbsp; id   str  success  ID of the filesystem for which the quota tree is create.    \u0026nbsp; gp_left   int  success  The grace period left after the soft limit for the user quota is exceeded.    \u0026nbsp; hard_limit   int  success  Hard limit of quota tree. If the quota tree's space usage exceeds the hard limit, users in quota tree cannot write data.    \u0026nbsp; id   str  success  Quota tree ID.    \u0026nbsp; path   str  success  Path to quota tree. A valid path must start with a forward slash '/'. It is mandatory while creating a quota tree.    \u0026nbsp; size_used   int  success  Size of used space in the filesystem by the user files.    \u0026nbsp; soft_limit   int  success  Soft limit of the quota tree. If the quota tree's space usage exceeds the soft limit, the storage system starts to count down based on the specified grace period.    \u0026nbsp; state   int  success  State of the quota tree.    Authors  Spandita Panigrahi (@panigs7) ansible.team@dell.com   File System Module Manage filesystem on Unity storage system\nSynopsis Managing filesystem on Unity storage system includes- Create new filesystem, Modify snapschedule attribute of filesystem Modify filesystem attributes, Display filesystem details, Display filesystem snapshots, Display filesystem snapschedule, Delete snapschedule associated with the filesystem, Delete filesystem, Create new filesystem with quota configuration\nParameters   Parameter Type Required Default Choices Description   filesystem_name  str      The name of the filesystem. Mandatory only for the create operation. All the operations are supported through 'filesystem_name' It's mutually exclusive with 'filesystem_id'.    filesystem_id  str      The id of the filesystem.It's mutually exclusive with 'filesystem_name' It can be used only for get, modify, or delete operations.    pool_name  str      This is the name of the pool where the filesystem will be created. Either the pool_name or pool_id must be provided to create a new filesystem.    pool_id  str      This is the ID of the pool where the filesystem will be created. Either the pool_name or pool_id must be provided to create a new filesystem.    size  int      The size of the filesystem.    cap_unit  str      GB TB   The unit of the filesystem size. It defaults to 'GB', if not specified.    nas_server_name  str      Name of the NAS server on which filesystem will be hosted.    nas_server_id  str      ID of the NAS server on which filesystem will be hosted.    supported_protocols  str      NFS CIFS MULTIPROTOCOL   Protocols supported by the file system. It will be overridden by NAS server configuration if NAS Server is Multiprotocol    description  str      Description about the filesystem. Description can be removed by passing empty string (\"\").    smb_properties  dict      Advance settings for SMB. It contains optional candidate variables    \u0026nbsp; is_smb_sync_writes_enabled   bool      Indicates whether the synchronous writes option is enabled on the file system.    \u0026nbsp; is_smb_notify_on_access_enabled   bool      Indicates whether notifications of changes to directory file structure are enabled.    \u0026nbsp; is_smb_op_locks_enabled   bool      Indicates whether opportunistic file locking is enabled on the file system.    \u0026nbsp; is_smb_notify_on_write_enabled   bool      Indicates whether file write notifications are enabled on the file system.    \u0026nbsp; smb_notify_on_change_dir_depth   int      Integer variable, determines the lowest directory level to which the enabled notifications apply. Minimum value is 1.    data_reduction  bool      Boolean variable, specifies whether or not to enable compression. Compression is supported only for thin filesystem    is_thin  bool      Boolean variable, specifies whether or not it's a thin filesystem.    access_policy  str      NATIVE UNIX WINDOWS   Access policy of a filesystem.    locking_policy  str      ADVISORY MANDATORY   File system locking policies. These policy choices control whether the NFSv4 range locks must be honored.    tiering_policy  str      AUTOTIER_HIGH AUTOTIER HIGHEST LOWEST   Tiering policy choices for how the storage resource data will be distributed among the tiers available in the pool.    quota_config  dict      Configuration for quota management. It contains optional parameters.    \u0026nbsp; grace_period   int      Grace period set in quota configuration after soft limit is reached. If grace_period is not set during creation of filesystem, it will be set to '7 days' by default.    \u0026nbsp; grace_period_unit   str      minutes hours days   Unit of grace period. Default unit is 'days'.    \u0026nbsp; default_hard_limit   int      Default hard limit for user quotas and tree quotas. If default_hard_limit is not set while creation of filesystem, it will be set to 0B by default.    \u0026nbsp; default_soft_limit   int      Default soft limit for user quotas and tree quotas. If default_soft_limit is not set while creation of filesystem, it will be set to 0B by default.    \u0026nbsp; is_user_quota_enabled   bool      Indicates whether the user quota is enabled. Parameters 'is_user_quota_enabled' and 'quota_policy' are mutually exclusive. If is_user_quota_enabled is not set while creation of filesystem, it will be set to false by default.    \u0026nbsp; quota_policy   str      FILE_SIZE BLOCKS   Quota policy set in quota configuration. Parameters 'is_user_quota_enabled' and 'quota_policy' are mutually exclusive. If quota_policy is not set while creation of filesystem, it will be set to \"FILE_SIZE\" by default.    \u0026nbsp; cap_unit   str      MB GB TB   Unit of default_soft_limit and default_hard_limit size. Default unit is 'GB'.    state  str   True     absent present   State variable to determine whether filesystem will exist or not.    snap_schedule_name  str      This is the name of an existing snapshot schedule which is to be associated with the filesystem. This is mutually exclusive with snapshot schedule id.    snap_schedule_id  str      This is the id of an existing snapshot schedule which is to be associated with the filesystem. This is mutually exclusive with snapshot schedule name. filesystem.    unispherehost  str   True     IP or FQDN of the Unity management server.    username  str   True     The username of the Unity management server.    password  str   True     The password of the Unity management server.    verifycert  bool    True    True False   Boolean variable to specify whether or not to validate SSL certificate. True - Indicates that the SSL certificate should be verified. False - Indicates that the SSL certificate should not be verified.    port  int    443    Port number through which communication happens with Unity management server.    Notes  SMB shares, NFS exports, and snapshots associated with filesystem need to be deleted prior to deleting a filesystem. quota_config parameter can be used to update default hard limit and soft limit values to limit the maximum space that can be used. By default they both are set to 0 during filesystem creation which means unlimited.  Examples - name: Create FileSystem dellemc_unity_filesystem: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; username: \u0026quot;{{username}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; filesystem_name: \u0026quot;ansible_test_fs\u0026quot; nas_server_name: \u0026quot;lglap761\u0026quot; pool_name: \u0026quot;pool_1\u0026quot; size: 5 state: \u0026quot;present\u0026quot; - name: Create FileSystem with quota configuration dellemc_unity_filesystem: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; username: \u0026quot;{{username}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; filesystem_name: \u0026quot;ansible_test_fs\u0026quot; nas_server_name: \u0026quot;lglap761\u0026quot; pool_name: \u0026quot;pool_1\u0026quot; size: 5 quota_config: grace_period: 8 grace_period_unit: \u0026quot;days\u0026quot; default_soft_limit: 10 is_user_quota_enabled: False state: \u0026quot;present\u0026quot; - name: Expand FileSystem size dellemc_unity_filesystem: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; username: \u0026quot;{{username}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; filesystem_name: \u0026quot;ansible_test_fs\u0026quot; nas_server_name: \u0026quot;lglap761\u0026quot; size: 10 state: \u0026quot;present\u0026quot; - name: Expand FileSystem size dellemc_unity_filesystem: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; username: \u0026quot;{{username}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; filesystem_name: \u0026quot;ansible_test_fs\u0026quot; nas_server_name: \u0026quot;lglap761\u0026quot; size: 10 state: \u0026quot;present\u0026quot; - name: Modify FileSystem smb_properties dellemc_unity_filesystem: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; username: \u0026quot;{{username}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; filesystem_name: \u0026quot;ansible_test_fs\u0026quot; nas_server_name: \u0026quot;lglap761\u0026quot; smb_properties: is_smb_op_locks_enabled: True smb_notify_on_change_dir_depth: 5 is_smb_notify_on_access_enabled: True state: \u0026quot;present\u0026quot; - name: Modify FileSystem Snap Schedule dellemc_unity_filesystem: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; username: \u0026quot;{{username}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; filesystem_id: \u0026quot;fs_141\u0026quot; snap_schedule_id: \u0026quot;{{snap_schedule_id}}\u0026quot; state: \u0026quot;{{state_present}}\u0026quot; - name: Get details of FileSystem using id dellemc_unity_filesystem: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; username: \u0026quot;{{username}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; filesystem_id: \u0026quot;rs_405\u0026quot; state: \u0026quot;present\u0026quot; - name: Delete a FileSystem using id dellemc_unity_filesystem: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; username: \u0026quot;{{username}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; filesystem_id: \u0026quot;rs_405\u0026quot; state: \u0026quot;absent\u0026quot; Return Values   Key Type Returned Description   changed   bool   always   Whether or not the resource has changed    filesystem_snapshot_details   complex   When filesystem snapshot exists   Details of the filesystem snapshot.    \u0026nbsp; access_type   str  success  Access type of filesystem snapshot.    \u0026nbsp; attached_wwn   str  success  Attached WWN details.    \u0026nbsp; creation_time   str  success  Creation time of filesystem snapshot.    \u0026nbsp; creator_schedule   str  success  Creator schedule of filesystem snapshot.    \u0026nbsp; creator_type   str  success  Creator type for filesystem snapshot.    \u0026nbsp; creator_user   str  success  Creator user for filesystem snapshot.    \u0026nbsp; description   str  success  Description of the filesystem snapshot.    \u0026nbsp; expiration_time   str  success  Date and time after which the filesystem snapshot will expire.    \u0026nbsp; filesystem_id   str  success  Id of the filesystem for which the snapshot exists.    \u0026nbsp; filesystem_name   str  success  Name of the filesystem for which the snapshot exists.    \u0026nbsp; id   str  success  Unique identifier of the filesystem snapshot instance.    \u0026nbsp; is_auto_delete   bool  success  Is the filesystem snapshot is auto deleted or not.    \u0026nbsp; name   str  success  The name of the filesystem snapshot.    \u0026nbsp; nas_server_id   str  success  Id of the NAS server on which filesystem exists.    \u0026nbsp; nas_server_name   str  success  Name of the NAS server on which filesystem exists.    \u0026nbsp; size   int  success  Size of the filesystem snapshot.    Authors  Arindam Datta (@dattaarindam) ansible.team@dell.com Meenakshi Dembi (@dembim) ansible.team@dell.com Spandita Panigrahi (@panigs7) ansible.team@dell.com   Storage Pool Module Manage storage pool on Unity\nSynopsis Managing storage pool on Unity storage system contains the following operations Get details of storage pool Modify storage pool\nParameters   Parameter Type Required Default Choices Description   pool_name  str      Name of the storage pool, unique in the storage system.    pool_id  str      Unique identifier of the pool instance.    new_pool_name  str      New name of the storage pool, unique in the storage system.    pool_description  str      The description of the storage pool.    fast_cache  str      enabled disabled   Indicates whether the fast cache is enabled for the storage pool. enabled - FAST Cache is enabled for the pool. disabled - FAST Cache is disabled for the pool.    fast_vp  str      enabled disabled   Indicates whether to enable scheduled data relocations for the pool. enabled - Enabled scheduled data relocations for the pool. disabled - Disabled scheduled data relocations for the pool.    state  str   True     absent present   Define whether the storage pool should exist or not. present - indicates that the storage pool should exist on the system. absent - indicates that the storage pool should not exist on the system.    unispherehost  str   True     IP or FQDN of the Unity management server.    username  str   True     The username of the Unity management server.    password  str   True     The password of the Unity management server.    verifycert  bool    True    True False   Boolean variable to specify whether or not to validate SSL certificate. True - Indicates that the SSL certificate should be verified. False - Indicates that the SSL certificate should not be verified.    port  int    443    Port number through which communication happens with Unity management server.    Notes  Creation/Deletion of storage pool is not allowed through Ansible module.  Examples - name: Get Storage pool details using pool_name dellemc_unity_storagepool: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; username: \u0026quot;{{username}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; pool_name: \u0026quot;{{pool_name}}\u0026quot; state: \u0026quot;present\u0026quot; - name: Get Storage pool details using pool_id dellemc_unity_storagepool: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; username: \u0026quot;{{username}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; pool_id: \u0026quot;{{pool_id}}\u0026quot; state: \u0026quot;present\u0026quot; - name: Modify Storage pool attributes using pool_name dellemc_unity_storagepool: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; username: \u0026quot;{{username}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; pool_name: \u0026quot;{{pool_name}}\u0026quot; new_pool_name: \u0026quot;{{new_pool_name}}\u0026quot; pool_description: \u0026quot;{{pool_description}}\u0026quot; fast_cache: \u0026quot;{{fast_cache_enabled}}\u0026quot; fast_vp: \u0026quot;{{fast_vp_enabled}}\u0026quot; state: \u0026quot;present\u0026quot; - name: Modify Storage pool attributes using pool_id dellemc_unity_storagepool: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; username: \u0026quot;{{username}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; pool_id: \u0026quot;{{pool_id}}\u0026quot; new_pool_name: \u0026quot;{{new_pool_name}}\u0026quot; pool_description: \u0026quot;{{pool_description}}\u0026quot; fast_cache: \u0026quot;{{fast_cache_enabled}}\u0026quot; fast_vp: \u0026quot;{{fast_vp_enabled}}\u0026quot; state: \u0026quot;present\u0026quot; Return Values   Key Type Returned Description   changed   bool   always   Whether or not the storage pool has changed.    storage_pool_details   complex   When storage pool exists.   The storage pool details.    \u0026nbsp; id   str  success  Pool id, unique identifier of the pool.    \u0026nbsp; is_fast_cache_enabled   bool  success  Indicates whether the fast cache is enabled for the storage pool. true - FAST Cache is enabled for the pool. false - FAST Cache is disabled for the pool.    \u0026nbsp; is_fast_vp_enabled   bool  success  Indicates whether to enable scheduled data relocations for the storage pool. true - Enabled scheduled data relocations for the pool. false - Disabled scheduled data relocations for the pool.    \u0026nbsp; name   str  success  Pool name, unique in the storage system.    \u0026nbsp; size_free_with_unit   str  success  Indicates size_free with its appropriate unit in human readable form.    \u0026nbsp; size_subscribed_with_unit   str  success  Indicates size_subscribed with its appropriate unit in human readable form.    \u0026nbsp; size_total_with_unit   str  success  Indicates size_total with its appropriate unit in human readable form.    \u0026nbsp; size_used_with_unit   str  success  Indicates size_used with its appropriate unit in human readable form.    \u0026nbsp; snap_size_subscribed_with_unit   str  success  Indicates snap_size_subscribed with its appropriate unit in human readable form.    \u0026nbsp; snap_size_used_with_unit   str  success  Indicates snap_size_used with its appropriate unit in human readable form.    Authors  Ambuj Dubey (@AmbujDube) ansible.team@dell.com   Gatherfacts Module Gathering information about DellEMC Unity\nSynopsis Gathering information about DellEMC Unity storage system includes Get the details of Unity array, Get list of Hosts in Unity array, Get list of FC initiators in Unity array, Get list of iSCSI initiators in Unity array, Get list of Consistency groups in Unity array, Get list of Storage pools in Unity array, Get list of Volumes in Unity array, Get list of Snapshot schedules in Unity array, Get list of NAS servers in Unity array, Get list of File systems in Unity array, Get list of Snapshots in Unity array, Get list of SMB shares in Unity array, Get list of NFS exports in Unity array, Get list of User quotas in Unity array, Get list of Quota tree in Unity array\nParameters   Parameter Type Required Default Choices Description   gather_subset  list elements: str      host fc_initiator iscsi_initiator cg storage_pool vol snapshot_schedule nas_server file_system snapshot nfs_export smb_share user_quota tree_quota   List of string variables to specify the Unity storage system entities for which information is required. host fc_initiator iscsi_initiator cg storage_pool vol snapshot_schedule nas_server file_system snapshot nfs_export smb_share user_quota tree_quota    unispherehost  str   True     IP or FQDN of the Unity management server.    username  str   True     The username of the Unity management server.    password  str   True     The password of the Unity management server.    verifycert  bool    True    True False   Boolean variable to specify whether or not to validate SSL certificate. True - Indicates that the SSL certificate should be verified. False - Indicates that the SSL certificate should not be verified.    port  int    443    Port number through which communication happens with Unity management server.    Examples  - name: Get detailed list of Unity entities. dellemc_unity_gatherfacts: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; username: \u0026quot;{{username}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; gather_subset: - host - fc_initiator - iscsi_initiator - cg - storage_pool - vol - snapshot_schedule - nas_server - file_system - snapshot - nfs_export - smb_share - user_quota - tree_quota - name: Get information of Unity array. dellemc_unity_gatherfacts: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; username: \u0026quot;{{username}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; - name: Get list of hosts on Unity array. dellemc_unity_gatherfacts: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; username: \u0026quot;{{username}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; gather_subset: - host - name: Get list of FC initiators on Unity array. dellemc_unity_gatherfacts: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; username: \u0026quot;{{username}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; gather_subset: - fc_initiator - name: Get list of ISCSI initiators on Unity array. dellemc_unity_gatherfacts: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; username: \u0026quot;{{username}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; gather_subset: - iscsi_initiator - name: Get list of consistency groups on Unity array. dellemc_unity_gatherfacts: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; username: \u0026quot;{{username}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; gather_subset: - cg - name: Get list of storage pools on Unity array. dellemc_unity_gatherfacts: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; username: \u0026quot;{{username}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; gather_subset: - storage_pool - name: Get list of volumes on Unity array. dellemc_unity_gatherfacts: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; username: \u0026quot;{{username}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; gather_subset: - vol - name: Get list of snapshot schedules on Unity array. dellemc_unity_gatherfacts: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; username: \u0026quot;{{username}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; gather_subset: - snapshot_schedule - name: Get list of NAS Servers on Unity array. dellemc_unity_gatherfacts: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; username: \u0026quot;{{username}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; gather_subset: - nas_server - name: Get list of File Systems on Unity array. dellemc_unity_gatherfacts: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; username: \u0026quot;{{username}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; gather_subset: - file_system - name: Get list of Snapshots on Unity array. dellemc_unity_gatherfacts: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; username: \u0026quot;{{username}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; gather_subset: - snapshot - name: Get list of NFS exports on Unity array. dellemc_unity_gatherfacts: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; username: \u0026quot;{{username}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; gather_subset: - nfs_export - name: Get list of SMB shares on Unity array. dellemc_unity_gatherfacts: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; username: \u0026quot;{{username}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; gather_subset: - smb_share - name: Get list of user quotas on Unity array. dellemc_unity_gatherfacts: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; username: \u0026quot;{{username}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; gather_subset: - user_quota - name: Get list of quota trees on Unity array. dellemc_unity_gatherfacts: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; username: \u0026quot;{{username}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; gather_subset: - tree_quota Return Values   Key Type Returned Description   Array_Details   complex   always   Details of the Unity Array.    \u0026nbsp; api_version   str  success  The current api version of the Unity Array.    \u0026nbsp; earliest_api_version   str  success  The earliest api version of the Unity Array.    \u0026nbsp; model   str  success  The model of the Unity Array.    \u0026nbsp; name   str  success  The name of the Unity Array.    \u0026nbsp; software_version   str  success  The software version of the Unity Array.    Consistency_Groups   complex   When Consistency Groups exist.   Details of the Consistency Groups.    \u0026nbsp; id   str  success  The ID of the Consistency Group.    \u0026nbsp; name   str  success  The name of the Consistency Group.    FC_initiators   complex   When FC initiator exist.   Details of the FC initiators.    \u0026nbsp; WWN   str  success  The WWN of the FC initiator.    \u0026nbsp; id   str  success  The id of the FC initiator.    File_Systems   complex   When File Systems exist.   Details of the File Systems.    \u0026nbsp; id   str  success  The ID of the File System.    \u0026nbsp; name   str  success  The name of the File System.    Hosts   complex   When hosts exist.   Details of the hosts.    \u0026nbsp; id   str  success  The ID of the host.    \u0026nbsp; name   str  success  The name of the host.    ISCSI_initiators   complex   When ISCSI initiators exist.   Details of the ISCSI initiators.    \u0026nbsp; IQN   str  success  The IQN of the ISCSI initiator.    \u0026nbsp; id   str  success  The id of the ISCSI initiator.    NAS_Servers   complex   When NAS Servers exist.   Details of the NAS Servers.    \u0026nbsp; id   str  success  The ID of the NAS Server.    \u0026nbsp; name   str  success  The name of the NAS Server.    NFS_Exports   complex   When NFS Exports exist.   Details of the NFS Exports.    \u0026nbsp; id   str  success  The ID of the NFS Export.    \u0026nbsp; name   str  success  The name of the NFS Export.    SMB_Shares   complex   When SMB Shares exist.   Details of the SMB Shares.    \u0026nbsp; id   str  success  The ID of the SMB Share.    \u0026nbsp; name   str  success  The name of the SMB Share.    Snapshot_Schedules   complex   When Snapshot Schedules exist.   Details of the Snapshot Schedules.    \u0026nbsp; id   str  success  The ID of the Snapshot Schedule.    \u0026nbsp; name   str  success  The name of the Snapshot Schedule.    Snapshots   complex   When Snapshots exist.   Details of the Snapshots.    \u0026nbsp; id   str  success  The ID of the Snapshot.    \u0026nbsp; name   str  success  The name of the Snapshot.    Storage_Pools   complex   When Storage Pools exist.   Details of the Storage Pools.    \u0026nbsp; id   str  success  The ID of the Storage Pool.    \u0026nbsp; name   str  success  The name of the Storage Pool.    Tree_Quotas   complex   When quota trees exist.   Details of the quota trees.    \u0026nbsp; id   str  success  The ID of the quota tree.    \u0026nbsp; path   str  success  The path of the quota tree.    User_Quotas   complex   When user quotas exist.   Details of the user quotas.    \u0026nbsp; id   str  success  The ID of the user quota.    \u0026nbsp; uid   str  success  The UID of the user quota.    Volumes   complex   When Volumes exist.   Details of the Volumes.    \u0026nbsp; id   str  success  The ID of the Volume.    \u0026nbsp; name   str  success  The name of the Volume.    Authors  Rajshree Khare (@kharer5) ansible.team@dell.com Akash Shendge (@shenda1) ansible.team@dell.com   User Quota Module Manage user quota on the Unity storage system\nSynopsis Managing User Quota on the Unity storage system includes Create user quota, Get user quota, Modify user quota, Delete user quota, Create user quota for quota tree, Modify user quota for quota tree and Delete user quota for quota tree.\nParameters   Parameter Type Required Default Choices Description   filesystem_name  str      The name of the filesystem for which the user quota is created. For creation of a user quota either filesystem_name or filesystem_id is required.    filesystem_id  str      The ID of the filesystem for which the user quota is created. For creation of a user quota either filesystem_id or filesystem_name is required.    nas_server_name  str      The name of the NAS server in which the filesystem is created. For creation of a user quota either nas_server_name or nas_server_id is required.    nas_server_id  str      The ID of the NAS server in which the filesystem is created. For creation of a user quota either filesystem_id or filesystem_name is required.    hard_limit  int      Hard limitation for a user on the total space available. If exceeded, user cannot write data. Value 0 implies no limit. One of the values of soft_limit and hard_limit can be 0, however, both cannot be 0 during creation or modification of user quota.    soft_limit  int      Soft limitation for a user on the total space available. If exceeded, notification will be sent to the user for the grace period mentioned, beyond which the user cannot use space. Value 0 implies no limit. Both soft_limit and hard_limit cannot be 0 during creation or modification of user quota.    cap_unit  str      MB GB TB   Unit of soft_limit and hard_limit size. It defaults to 'GB' if not specified.    user_type  str      Unix Windows   Type of user creating a user quota. Mandatory while creating or modifying user quota.    win_domain  str      Fully qualified or short domain name for Windows user type. Mandatory when user_type is 'Windows'.    user_name  str      User name of the user quota when user_type is 'Windows' or 'Unix'. user_name must be specified along with win_domain when user_type is 'Windows'.    uid  str      User ID of the user quota.    user_quota_id  str      User quota ID generated after creation of a user quota.    tree_quota_id  str      The ID of the quota tree. Either tree_quota_id or path to quota tree is required to create/modify/delete user quota for a quota tree.    path  str      The path to the quota tree. Either tree_quota_id or path to quota tree is required to create/modify/delete user quota for a quota tree. Path must start with a forward slash '/'.    state  str   True     absent present   The state option is used to mention the existence of the user quota.    unispherehost  str   True     IP or FQDN of the Unity management server.    username  str   True     The username of the Unity management server.    password  str   True     The password of the Unity management server.    verifycert  bool    True    True False   Boolean variable to specify whether or not to validate SSL certificate. True - Indicates that the SSL certificate should be verified. False - Indicates that the SSL certificate should not be verified.    port  int    443    Port number through which communication happens with Unity management server.    Examples  - name: Get user quota details by user quota id dellemc_unity_user_quota: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; username: \u0026quot;{{username}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user_quota_id: \u0026quot;userquota_171798700679_0_123\u0026quot; state: \u0026quot;present\u0026quot; - name: Get user quota details by user quota uid/user name dellemc_unity_user_quota: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; username: \u0026quot;{{username}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; filesystem_name: \u0026quot;fs_2171\u0026quot; nas_server_id: \u0026quot;nas_21\u0026quot; user_name: \u0026quot;test\u0026quot; state: \u0026quot;present\u0026quot; - name: Create user quota for a filesystem with filesystem id dellemc_unity_user_quota: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; username: \u0026quot;{{username}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; filesystem_id: \u0026quot;fs_2171\u0026quot; hard_limit: 6 cap_unit: \u0026quot;TB\u0026quot; soft_limit: 5 user_type: \u0026quot;UID\u0026quot; uid: \u0026quot;111\u0026quot; state: \u0026quot;present\u0026quot; - name: Create user quota for a filesystem with filesystem name dellemc_unity_user_quota: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; username: \u0026quot;{{username}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; filesystem_name: \u0026quot;Test_filesystem\u0026quot; nas_server_name: \u0026quot;lglad068\u0026quot; hard_limit: 6 cap_unit: \u0026quot;TB\u0026quot; soft_limit: 5 user_type: \u0026quot;UID\u0026quot; uid: \u0026quot;111\u0026quot; state: \u0026quot;present\u0026quot; - name: Modify user quota limit usage by user quota id dellemc_unity_user_quota: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; username: \u0026quot;{{username}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; user_quota_id: \u0026quot;userquota_171798700679_0_123\u0026quot; hard_limit: 10 cap_unit: \u0026quot;TB\u0026quot; soft_limit: 8 state: \u0026quot;present\u0026quot; - name: Modify user quota by filesystem id and user quota uid/user_name dellemc_unity_user_quota: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; username: \u0026quot;{{username}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; filesystem_id: \u0026quot;fs_2171\u0026quot; user_type: \u0026quot;Windows\u0026quot; win_domain: \u0026quot;prod\u0026quot; user_name: \u0026quot;sample\u0026quot; hard_limit: 12 cap_unit: \u0026quot;TB\u0026quot; soft_limit: 10 state: \u0026quot;present\u0026quot; - name: Delete user quota dellemc_unity_user_quota: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; username: \u0026quot;{{username}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; filesystem_id: \u0026quot;fs_2171\u0026quot; win_domain: \u0026quot;prod\u0026quot; user_name: \u0026quot;sample\u0026quot; state: \u0026quot;absent\u0026quot; - name: Create user quota of a quota tree dellemc_unity_user_quota: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; username: \u0026quot;{{username}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; tree_quota_id: \u0026quot;treequota_171798700679_4\u0026quot; user_type: \u0026quot;Windows\u0026quot; win_domain: \u0026quot;prod\u0026quot; user_name: \u0026quot;sample\u0026quot; soft_limit: 9 cap_unit: \u0026quot;TB\u0026quot; state: \u0026quot;present\u0026quot; - name: Create user quota of a quota tree by quota tree path dellemc_unity_user_quota: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; username: \u0026quot;{{username}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; filesystem_id: \u0026quot;fs_2171\u0026quot; path: \u0026quot;/sample\u0026quot; user_type: \u0026quot;Unix\u0026quot; user_name: \u0026quot;test\u0026quot; hard_limit: 2 cap_unit: \u0026quot;TB\u0026quot; state: \u0026quot;present\u0026quot; - name: Modify user quota of a quota tree dellemc_unity_user_quota: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; username: \u0026quot;{{username}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; tree_quota_id: \u0026quot;treequota_171798700679_4\u0026quot; user_type: \u0026quot;Windows\u0026quot; win_domain: \u0026quot;prod\u0026quot; user_name: \u0026quot;sample\u0026quot; soft_limit: 10 cap_unit: \u0026quot;TB\u0026quot; state: \u0026quot;present\u0026quot; - name: Modify user quota of a quota tree by quota tree path dellemc_unity_user_quota: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; username: \u0026quot;{{username}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; filesystem_id: \u0026quot;fs_2171\u0026quot; path: \u0026quot;/sample\u0026quot; user_type: \u0026quot;Windows\u0026quot; win_domain: \u0026quot;prod\u0026quot; user_name: \u0026quot;sample\u0026quot; hard_limit: 12 cap_unit: \u0026quot;TB\u0026quot; state: \u0026quot;present\u0026quot; - name: Delete user quota of a quota tree by quota tree path dellemc_unity_user_quota: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; username: \u0026quot;{{username}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; filesystem_id: \u0026quot;fs_2171\u0026quot; path: \u0026quot;/sample\u0026quot; win_domain: \u0026quot;prod\u0026quot; user_name: \u0026quot;sample\u0026quot; state: \u0026quot;absent\u0026quot; - name: Delete user quota of a quota tree by quota tree id dellemc_unity_user_quota: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; username: \u0026quot;{{username}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; tree_quota_id: \u0026quot;treequota_171798700679_4\u0026quot; win_domain: \u0026quot;prod\u0026quot; user_name: \u0026quot;sample\u0026quot; state: \u0026quot;absent\u0026quot; Return Values   Key Type Returned Description   changed   bool   always   Whether or not the resource has changed    get_user_quota_details   complex   When user quota exists   Details of the user quota.    \u0026nbsp; filesystem   complex  success  Filesystem details for which the user quota is created.    \u0026nbsp; \u0026nbsp; UnityFileSystem   complex  success  Filesystem details for which the user quota is created.    \u0026nbsp; \u0026nbsp; \u0026nbsp; id   str  success  ID of the filesystem for which the user quota is created.    \u0026nbsp; \u0026nbsp; \u0026nbsp; name   str  success  Name of filesystem.    \u0026nbsp; \u0026nbsp; \u0026nbsp; nas_server   complex  success  Nasserver details where filesystem is created.    \u0026nbsp; gp_left   int  success  The grace period left after the soft limit for the user quota is exceeded.    \u0026nbsp; hard_limit   int  success  Hard limitation for a user on the total space available. If exceeded, user cannot write data.    \u0026nbsp; hard_ratio   str  success  The hard ratio is the ratio between the hard limit size of the user quota and the amount of storage actually consumed.    \u0026nbsp; id   str  success  User quota ID.    \u0026nbsp; size_used   int  success  Size of used space in the filesystem by the user files.    \u0026nbsp; soft_limit   int  success  Soft limitation for a user on the total space available. If exceeded, notification will be sent to user for the grace period mentioned, beyond which user cannot use space.    \u0026nbsp; soft_ratio   str  success  The soft ratio is the ratio between the soft limit size of the user quota and the amount of storage actually consumed.    \u0026nbsp; state   int  success  State of the user quota.    \u0026nbsp; tree_quota   complex  success  Quota tree details for which the user quota is created.    \u0026nbsp; \u0026nbsp; UnityTreeQuota   complex  success  Quota tree details for which the user quota is created.    \u0026nbsp; \u0026nbsp; \u0026nbsp; id   str  success  ID of the quota tree.    \u0026nbsp; \u0026nbsp; \u0026nbsp; path   str  success  Path to quota tree    \u0026nbsp; uid   int  success  User ID of the user.    \u0026nbsp; unix_name   str  success  Unix user name for this user quota's uid.    \u0026nbsp; windows_names   str  success  Windows user name that maps to this quota's uid.    \u0026nbsp; windows_sids   str  success  Windows SIDs that maps to this quota's uid    Authors  Spandita Panigrahi (@panigs7) ansible.team@dell.com   Filesystem Snapshot Module Manage filesystem snapshot on the Unity storage system\nSynopsis Managing Filesystem Snapshot on the Unity storage system includes create filesystem snapshot, get filesystem snapshot, modify filesystem snapshot and delete filesystem snapshot.\nParameters   Parameter Type Required Default Choices Description   snapshot_name  str      The name of the filesystem snapshot. Mandatory parameter for creating a filesystem snapshot. For all other operations either snapshot_name or snapshot_id is required.    snapshot_id  str      During creation snapshot_id is auto generated. For all other operations either snapshot_id or snapshot_name is required.    filesystem_name  str      The name of the Filesystem for which snapshot is created. For creation of filesystem snapshot either filesystem_name or filesystem_id is required. Not required for other operations.    filesystem_id  str      The ID of the Filesystem for which snapshot is created. For creation of filesystem snapshot either filesystem_id or filesystem_name is required. Not required for other operations.    nas_server_name  str      The name of the NAS server in which the Filesystem is created. For creation of filesystem snapshot either nas_server_name or nas_server_id is required. Not required for other operations.    nas_server_id  str      The ID of the NAS server in which the Filesystem is created. For creation of filesystem snapshot either filesystem_id or filesystem_name is required. Not required for other operations.    auto_delete  bool      This option specifies whether or not the filesystem snapshot will be automatically deleted. If set to true, the filesystem snapshot will expire based on the pool auto deletion policy. If set to false, the filesystem snapshot will not be auto deleted based on the pool auto deletion policy. auto_delete can not be set to True, if expiry_time is specified. If during creation neither auto_delete nor expiry_time is mentioned then the filesystem snapshot will be created keeping auto_delete as True. Once the expiry_time is set, then the filesystem snapshot cannot be assigned to the auto delete policy.    expiry_time  str      This option is for specifying the date and time after which the filesystem snapshot will expire. The time is to be mentioned in UTC timezone. The format is \"MM/DD/YYYY HH:MM\". Year must be in 4 digits.    description  str      The additional information about the filesystem snapshot can be provided using this option. The description can be removed by passing an empty string.    fs_access_type  str      Checkpoint Protocol   Access type of the filesystem snapshot. Required only during creation of filesystem snapshot. If not given, snapshot's access type will be 'Checkpoint'.    state  str   True     absent present   The state option is used to mention the existence of the filesystem snapshot.    unispherehost  str   True     IP or FQDN of the Unity management server.    username  str   True     The username of the Unity management server.    password  str   True     The password of the Unity management server.    verifycert  bool    True    True False   Boolean variable to specify whether or not to validate SSL certificate. True - Indicates that the SSL certificate should be verified. False - Indicates that the SSL certificate should not be verified.    port  int    443    Port number through which communication happens with Unity management server.    Notes  Filesystem snapshot cannot be deleted, if it has nfs or smb share.  Examples  - name: Create Filesystem Snapshot dellemc_unity_filesystem_snapshot: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; username: \u0026quot;{{username}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; snapshot_name: \u0026quot;ansible_test_FS_snap\u0026quot; filesystem_name: \u0026quot;ansible_test_FS\u0026quot; nas_server_name: \u0026quot;lglad069\u0026quot; description: \u0026quot;Created using playbook\u0026quot; auto_delete: True fs_access_type: \u0026quot;Protocol\u0026quot; state: \u0026quot;present\u0026quot; - name: Create Filesystem Snapshot with expiry time. dellemc_unity_filesystem_snapshot: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; username: \u0026quot;{{username}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; snapshot_name: \u0026quot;ansible_test_FS_snap_1\u0026quot; filesystem_name: \u0026quot;ansible_test_FS_1\u0026quot; nas_server_name: \u0026quot;lglad069\u0026quot; description: \u0026quot;Created using playbook\u0026quot; expiry_time: \u0026quot;04/15/2021 2:30\u0026quot; fs_access_type: \u0026quot;Protocol\u0026quot; state: \u0026quot;present\u0026quot; - name: Get Filesystem Snapshot Details using Name dellemc_unity_filesystem_snapshot: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; username: \u0026quot;{{username}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; snapshot_name: \u0026quot;ansible_test_FS_snap\u0026quot; state: \u0026quot;present\u0026quot; - name: Get Filesystem Snapshot Details using ID dellemc_unity_filesystem_snapshot: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; username: \u0026quot;{{username}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; snapshot_id: \u0026quot;10008000403\u0026quot; state: \u0026quot;present\u0026quot; - name: Update Filesystem Snapshot attributes dellemc_unity_filesystem_snapshot: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; username: \u0026quot;{{username}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; snapshot_name: \u0026quot;ansible_test_FS_snap\u0026quot; description: \u0026quot;Description updated\u0026quot; auto_delete: False expiry_time: \u0026quot;04/15/2021 5:30\u0026quot; state: \u0026quot;present\u0026quot; - name: Update Filesystem Snapshot attributes using ID dellemc_unity_filesystem_snapshot: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; username: \u0026quot;{{username}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; snapshot_id: \u0026quot;10008000403\u0026quot; expiry_time: \u0026quot;04/18/2021 8:30\u0026quot; state: \u0026quot;present\u0026quot; - name: Delete Filesystem Snapshot using Name dellemc_unity_filesystem_snapshot: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; username: \u0026quot;{{username}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; snapshot_name: \u0026quot;ansible_test_FS_snap\u0026quot; state: \u0026quot;absent\u0026quot; - name: Delete Filesystem Snapshot using ID dellemc_unity_filesystem_snapshot: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; username: \u0026quot;{{username}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; snapshot_id: \u0026quot;10008000403\u0026quot; state: \u0026quot;absent\u0026quot; Return Values   Key Type Returned Description   changed   bool   always   Whether or not the resource has changed    filesystem_snapshot_details   complex   When filesystem snapshot exists   Details of the filesystem snapshot.    \u0026nbsp; access_type   str  success  Access type of filesystem snapshot.    \u0026nbsp; attached_wwn   str  success  Attached WWN details.    \u0026nbsp; creation_time   str  success  Creation time of filesystem snapshot.    \u0026nbsp; creator_schedule   str  success  Creator schedule of filesystem snapshot.    \u0026nbsp; creator_type   str  success  Creator type for filesystem snapshot.    \u0026nbsp; creator_user   str  success  Creator user for filesystem snapshot.    \u0026nbsp; description   str  success  Description of the filesystem snapshot.    \u0026nbsp; expiration_time   str  success  Date and time after which the filesystem snapshot will expire.    \u0026nbsp; filesystem_id   str  success  Id of the filesystem for which the snapshot exists.    \u0026nbsp; filesystem_name   str  success  Name of the filesystem for which the snapshot exists.    \u0026nbsp; id   str  success  Unique identifier of the filesystem snapshot instance.    \u0026nbsp; is_auto_delete   bool  success  Is the filesystem snapshot is auto deleted or not.    \u0026nbsp; name   str  success  The name of the filesystem snapshot.    \u0026nbsp; nas_server_id   str  success  Id of the NAS server on which filesystem exists.    \u0026nbsp; nas_server_name   str  success  Name of the NAS server on which filesystem exists.    \u0026nbsp; size   int  success  Size of the filesystem snapshot.    Authors  Rajshree Khare (@kharer5) ansible.team@dell.com   Snapshot Module Manage snapshots on the Unity storage system.\nSynopsis Managing snapshots on the Unity storage system includes create snapshot, delete snapshot, update snapshot, get snapshot, map host and unmap host.\nParameters   Parameter Type Required Default Choices Description   snapshot_name  str      The name of the snapshot. Mandatory parameter for creating a snapshot. For all other operations either snapshot name or snapshot id is required.    vol_name  str      The name of the volume for which snapshot is created. For creation of a snapshot either vol_name or cg_name is required. Not required for other operations.    cg_name  str      The name of the Consistency Group for which snapshot is created. For creation of a snapshot either vol_name or cg_name is required. Not required for other operations.    snapshot_id  str      The id of the snapshot. For all operations other than creation either snapshot name or snapshot id is required.    auto_delete  bool      This option specifies whether the snapshot is auto deleted or not. If set to true, snapshot will expire based on the pool auto deletion policy. If set to false, snapshot will not be auto deleted based on the pool auto deletion policy. auto_delete can not be set to True, if expiry_time is specified. If during creation neither auto_delete nor expiry_time is mentioned then snapshot will be created keeping auto_delete as True. Once the expiry_time is set then snapshot cannot be assigned to the auto delete policy.    expiry_time  str      This option is for specifying the date and time after which the snapshot will expire. The time is to be mentioned in UTC timezone. The format is \"MM/DD/YYYY HH:MM\". Year must be in 4 digits.    description  str      The additional information about the snapshot can be provided using this option.    new_snapshot_name  str      New name for the snapshot.    state  str   True     absent present   The state option is used to mention the existence of the snapshot.    host_name  str      The name of the host. Either host_name or host_id is required to map or unmap a snapshot from a host. Snapshot can be attached to multiple hosts.    host_id  str      The id of the host. Either host_name or host_id is required to map or unmap a snapshot from a host Snapshot can be attached to multiple hosts.    host_state  str      mapped unmapped   The host_state option is used to mention the existence of the host for snapshot. It is required when a snapshot is mapped or unmapped from host.    unispherehost  str   True     IP or FQDN of the Unity management server.    username  str   True     The username of the Unity management server.    password  str   True     The password of the Unity management server.    verifycert  bool    True    True False   Boolean variable to specify whether or not to validate SSL certificate. True - Indicates that the SSL certificate should be verified. False - Indicates that the SSL certificate should not be verified.    port  int    443    Port number through which communication happens with Unity management server.    Examples  - name: Create a Snapshot for a CG dellemc_unity_snapshot: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; username: \u0026quot;{{username}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; port: \u0026quot;{{port}}\u0026quot; cg_name: \u0026quot;{{cg_name}}\u0026quot; snapshot_name: \u0026quot;{{cg_snapshot_name}}\u0026quot; description: \u0026quot;{{description}}\u0026quot; auto_delete: False state: \u0026quot;present\u0026quot; - name: Create a Snapshot for a volume with Host attached. dellemc_unity_snapshot: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; username: \u0026quot;{{username}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; port: \u0026quot;{{port}}\u0026quot; vol_name: \u0026quot;{{vol_name}}\u0026quot; snapshot_name: \u0026quot;{{vol_snapshot_name}}\u0026quot; description: \u0026quot;{{description}}\u0026quot; expiry_time: \u0026quot;04/15/2025 16:30\u0026quot; host_name: \u0026quot;{{host_name}}\u0026quot; host_state: \u0026quot;mapped\u0026quot; state: \u0026quot;present\u0026quot; - name: Unmap a host for a Snapshot dellemc_unity_snapshot: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; username: \u0026quot;{{username}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; port: \u0026quot;{{port}}\u0026quot; snapshot_name: \u0026quot;{{vol_snapshot_name}}\u0026quot; host_name: \u0026quot;{{host_name}}\u0026quot; host_state: \u0026quot;unmapped\u0026quot; state: \u0026quot;present\u0026quot; - name: Map snapshot to a host dellemc_unity_snapshot: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; username: \u0026quot;{{username}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; port: \u0026quot;{{port}}\u0026quot; snapshot_name: \u0026quot;{{vol_snapshot_name}}\u0026quot; host_name: \u0026quot;{{host_name}}\u0026quot; host_state: \u0026quot;mapped\u0026quot; state: \u0026quot;present\u0026quot; - name: Update attributes of a Snapshot for a volume dellemc_unity_snapshot: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; username: \u0026quot;{{username}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; snapshot_name: \u0026quot;{{vol_snapshot_name}}\u0026quot; new_snapshot_name: \u0026quot;{{new_snapshot_name}}\u0026quot; description: \u0026quot;{{new_description}}\u0026quot; host_name: \u0026quot;{{host_name}}\u0026quot; host_state: \u0026quot;unmapped\u0026quot; state: \u0026quot;present\u0026quot; - name: Delete Snapshot of CG. dellemc_unity_snapshot: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; username: \u0026quot;{{username}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; snapshot_name: \u0026quot;{{cg_snapshot_name}}\u0026quot; state: \u0026quot;absent\u0026quot; Return Values   Key Type Returned Description   changed   bool   always   Whether or not the resource has changed    snapshot_details   complex   When snapshot exists   Details of the snapshot.    \u0026nbsp; expiration_time   str  success  Date and time after which the snapshot will expire.    \u0026nbsp; hosts_list   dict  success  Contains the name and id of the associated hosts.    \u0026nbsp; id   str  success  Unique identifier of the snapshot instance.    \u0026nbsp; is_auto_delete   str  success  Additional information mentioned for snapshot.    \u0026nbsp; name   str  success  The name of the snapshot.    \u0026nbsp; storage_resource_id   str  success  Id of the storage resource for which the snapshot exists.    \u0026nbsp; storage_resource_name   str  success  Name of the storage resource for which the snapshot exists.    Authors  P Srinivas Rao (@srinivas-rao5) ansible.team@dell.com   SMB Share Module Manage SMB shares on Unity storage system.\nSynopsis Managing SMB Shares on Unity storage system includes create, get, modify, and delete the smb shares.\nParameters   Parameter Type Required Default Choices Description   share_name  str      Name of the SMB share. Required during creation of the SMB share. For all other operations either share_name or share_id is required.    share_id  str      ID of the SMB share. Should not be specified during creation. Id is auto generated. For all other operations either share_name or share_id is required. If share_id is used then no need to pass nas_server/filesystem/snapshot/path.    path  str      Local path to the file system/Snapshot or any existing sub-folder of the file system/Snapshot that is shared over the network. Path is relative to the root of the filesystem. Required for creation of the SMB share.    filesystem_id  str      The ID of the File System. Either filesystem_name or filesystem_id is required for creation of the SMB share for filesystem. If filesystem name is specified, then nas_server_name/nas_server_id is required to uniquely identify the filesystem. filesystem_name and filesystem_id are mutually exclusive parameters.    snapshot_id  str      The ID of the Filesystem Snapshot. Either snapshot_name or snapshot_id is required for creation of the SMB share for a snapshot. If snapshot name is specified, then nas_server_name/nas_server_id is required to uniquely identify the snapshot. snapshot_name and snapshot_id are mutually exclusive parameters.    nas_server_id  str      The ID of the NAS Server. It is not required if share_id is used.    filesystem_name  str      The Name of the File System. Either filesystem_name or filesystem_id is required for creation of the SMB share for filesystem. If filesystem name is specified, then nas_server_name/nas_server_id is required to uniquely identify the filesystem. filesystem_name and filesytem_id are mutually exclusive parameters.    snapshot_name  str      The Name of the Filesystem Snapshot. Either snapshot_name or snapshot_id is required for creation of the SMB share for a snapshot. If snapshot name is specified, then nas_server_name/nas_server_id is required to uniquely identify the snapshot. snapshot_name and snapshot_id are mutually exclusive parameters.    nas_server_name  str      The Name of the NAS Server. It is not required if share_id is used. nas_server_name and nas_server_id are mutually exclusive parameters.    description  str      Description for the SMB share. Optional parameter when creating a share. To modify, pass the new value in description field.    is_abe_enabled  bool      Indicates whether Access-based Enumeration (ABE) for SMB share is enabled. During creation, if not mentioned then default is False.    is_branch_cache_enabled  bool      Indicates whether Branch Cache optimization for SMB share is enabled. During creation, if not mentioned then default is False.    is_continuous_availability_enabled  bool      Indicates whether continuous availability for SMB 3.0 is enabled. During creation, if not mentioned then default is False.    is_encryption_enabled  bool      Indicates whether encryption for SMB 3.0 is enabled at the shared folder level. During creation, if not mentioned then default is False.    offline_availability  str      MANUAL DOCUMENTS PROGRAMS NONE   Defines valid states of Offline Availability. MANUAL- Only specified files will be available offline. DOCUMENTS- All files that users open will be available offline. PROGRAMS- Program will preferably run from the offline cache even when connected to the network. All files that users open will be available offline. NONE- Prevents clients from storing documents and programs in offline cache.    umask  str      The default UNIX umask for new files created on the SMB Share.    state  str   True     absent present   Define whether the SMB share should exist or not. present indicates that the share should exist on the system. absent indicates that the share should not exist on the system.    unispherehost  str   True     IP or FQDN of the Unity management server.    username  str   True     The username of the Unity management server.    password  str   True     The password of the Unity management server.    verifycert  bool    True    True False   Boolean variable to specify whether or not to validate SSL certificate. True - Indicates that the SSL certificate should be verified. False - Indicates that the SSL certificate should not be verified.    port  int    443    Port number through which communication happens with Unity management server.    Notes  When ID/Name of the filesystem/snapshot is passed then nas_server is not required. If passed, then filesystem/snapshot should exist for the mentioned nas_server, else the task will fail.  Examples - name: Create SMB share for a filesystem dellemc_unity_smbshare: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; username: \u0026quot;{{username}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; share_name: \u0026quot;sample_smb_share\u0026quot; filesystem_name: \u0026quot;sample_fs\u0026quot; nas_server_id: \u0026quot;NAS_11\u0026quot; path: \u0026quot;/sample_fs\u0026quot; description: \u0026quot;Sample SMB share created\u0026quot; is_abe_enabled: True is_branch_cache_enabled: True offline_availability: \u0026quot;DOCUMENTS\u0026quot; is_continuous_availability_enabled: True is_encryption_enabled: True umask: \u0026quot;777\u0026quot; state: \u0026quot;present\u0026quot; - name: Modify Attributes of SMB share for a filesystem dellemc_unity_smbshare: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; username: \u0026quot;{{username}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; share_name: \u0026quot;sample_smb_share\u0026quot; nas_server_name: \u0026quot;sample_nas_server\u0026quot; description: \u0026quot;Sample SMB share attributes updated\u0026quot; is_abe_enabled: False is_branch_cache_enabled: False offline_availability: \u0026quot;MANUAL\u0026quot; is_continuous_availability_enabled: \u0026quot;False\u0026quot; is_encryption_enabled: \u0026quot;False\u0026quot; umask: \u0026quot;022\u0026quot; state: \u0026quot;present\u0026quot; - name: Create SMB share for a snapshot dellemc_unity_smbshare: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; username: \u0026quot;{{username}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; share_name: \u0026quot;sample_snap_smb_share\u0026quot; snapshot_name: \u0026quot;sample_snapshot\u0026quot; nas_server_id: \u0026quot;NAS_11\u0026quot; path: \u0026quot;/sample_snapshot\u0026quot; description: \u0026quot;Sample SMB share created for snapshot\u0026quot; is_abe_enabled: True is_branch_cache_enabled: True is_continuous_availability_enabled: True is_encryption_enabled: True umask: \u0026quot;777\u0026quot; state: \u0026quot;present\u0026quot; - name: Modify Attributes of SMB share for a snapshot dellemc_unity_smbshare: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; username: \u0026quot;{{username}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; share_name: \u0026quot;sample_snap_smb_share\u0026quot; snapshot_name: \u0026quot;sample_snapshot\u0026quot; description: \u0026quot;Sample SMB share attributes updated for snapshot\u0026quot; is_abe_enabled: False is_branch_cache_enabled: False offline_availability: \u0026quot;MANUAL\u0026quot; is_continuous_availability_enabled: \u0026quot;False\u0026quot; is_encryption_enabled: \u0026quot;False\u0026quot; umask: \u0026quot;022\u0026quot; state: \u0026quot;present\u0026quot; - name: Get details of SMB share dellemc_unity_smbshare: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; username: \u0026quot;{{username}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; share_id: \u0026quot;{{smb_share_id}}\u0026quot; state: \u0026quot;present\u0026quot; - name: Delete SMB share dellemc_unity_smbshare: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; username: \u0026quot;{{username}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; share_id: \u0026quot;{{smb_share_id}}\u0026quot; state: \u0026quot;absent\u0026quot; Return Values   Key Type Returned Description   changed   bool   always   Whether or not the resource has changed    smb_share_details   complex   When share exists.   The SMB share details.    \u0026nbsp; description   str  success  Additional information about the share.    \u0026nbsp; filesystem_id   str  success  The ID of the Filesystem.    \u0026nbsp; filesystem_name   str  success  The Name of the filesystem    \u0026nbsp; id   str  success  The ID of the SMB share.    \u0026nbsp; is_abe_enabled   bool  success  Whether Access Based enumeration is enforced or not    \u0026nbsp; is_branch_cache_enabled   bool  success  Whether branch cache is enabled or not.    \u0026nbsp; is_continuous_availability_enabled   bool  success  Whether the share will be available continuously or not    \u0026nbsp; is_encryption_enabled   bool  success  Whether encryption is enabled or not.    \u0026nbsp; name   str  success  Name of the SMB share.    \u0026nbsp; nas_server_id   str  success  The ID of the nas_server.    \u0026nbsp; nas_server_name   str  success  The Name of the nas_server.    \u0026nbsp; snapshot_id   str  success  The ID of the Snapshot.    \u0026nbsp; snapshot_name   str  success  The Name of the Snapshot.    \u0026nbsp; umask   str  success  Unix mask for the SMB share    Authors  P Srinivas Rao (@srinivas-rao5) ansible.team@dell.com   Host Module Manage Host operations on Unity.\nSynopsis The Host module contains the following operations Creation of a Host. Addition of initiators to Host. Removal of initiators from Host. Modification of host attributes. Get details of a Host. Deletion of a Host.\nParameters   Parameter Type Required Default Choices Description   host_name  str      Name of the host. Mandatory for host creation.    host_id  str      Unique identifier of the host. host_id is auto generated during creation. Except create, all other operations require either host_id or host_name.    description  str      Host description.    host_os  str      AIX Citrix XenServer HP-UX IBM VIOS Linux Mac OS Solaris VMware ESXi Windows Client Windows Server   Operating system running on the host.    new_host_name  str      New name for the host. Only required in rename host operation.    initiators  list elements: str      List of initiators to be added/removed to/from host.    initiator_state  str      present-in-host absent-in-host   State of the initiator.    state  str   True     present absent   State of the host.    unispherehost  str   True     IP or FQDN of the Unity management server.    username  str   True     The username of the Unity management server.    password  str   True     The password of the Unity management server.    verifycert  bool    True    True False   Boolean variable to specify whether or not to validate SSL certificate. True - Indicates that the SSL certificate should be verified. False - Indicates that the SSL certificate should not be verified.    port  int    443    Port number through which communication happens with Unity management server.    Examples - name: Create empty Host. dellemc_unity_host: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; username: \u0026quot;{{username}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; host_name: \u0026quot;ansible-test-host\u0026quot; host_os: \u0026quot;Linux\u0026quot; description: \u0026quot;ansible-test-host\u0026quot; state: \u0026quot;present\u0026quot; - name: Create Host with Initiators. dellemc_unity_host: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; username: \u0026quot;{{username}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; host_name: \u0026quot;ansible-test-host-1\u0026quot; host_os: \u0026quot;Linux\u0026quot; description: \u0026quot;ansible-test-host-1\u0026quot; initiators: - \u0026quot;iqn.1994-05.com.redhat:c38e6e8cfd81\u0026quot; - \u0026quot;20:00:00:90:FA:13:81:8D:10:00:00:90:FA:13:81:8D\u0026quot; initiator_state: \u0026quot;present-in-host\u0026quot; state: \u0026quot;present\u0026quot; - name: Modify Host using host_id. dellemc_unity_host: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; username: \u0026quot;{{username}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; host_id: \u0026quot;Host_253\u0026quot; new_host_name: \u0026quot;ansible-test-host-2\u0026quot; host_os: \u0026quot;Mac OS\u0026quot; description: \u0026quot;Ansible tesing purpose\u0026quot; state: \u0026quot;present\u0026quot; - name: Add Initiators to Host. dellemc_unity_host: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; username: \u0026quot;{{username}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; host_name: \u0026quot;ansible-test-host-2\u0026quot; initiators: - \u0026quot;20:00:00:90:FA:13:81:8C:10:00:00:90:FA:13:81:8C\u0026quot; initiator_state: \u0026quot;present-in-host\u0026quot; state: \u0026quot;present\u0026quot; - name: Get Host details using host_name. dellemc_unity_host: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; username: \u0026quot;{{username}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; host_name: \u0026quot;ansible-test-host-2\u0026quot; state: \u0026quot;present\u0026quot; - name: Get Host details using host_id. dellemc_unity_host: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; username: \u0026quot;{{username}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; host_id: \u0026quot;Host_253\u0026quot; state: \u0026quot;present\u0026quot; - name: Delete Host. dellemc_unity_host: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; username: \u0026quot;{{username}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; host_name: \u0026quot;ansible-test-host-2\u0026quot; state: \u0026quot;absent\u0026quot; Return Values   Key Type Returned Description   changed   bool   always   Whether or not the resource has changed.    host_details   complex   When host exists.   Details of the host.    \u0026nbsp; description   str  success  Description about the host.    \u0026nbsp; fc_host_initiators   complex  success  Details of the FC initiators associated with the host.    \u0026nbsp; \u0026nbsp; UnityHostInitiatorList   complex  success  FC initiators with system generated unique hash value.    \u0026nbsp; id   str  success  The system ID given to the host.    \u0026nbsp; iscsi_host_initiators   complex  success  Details of the ISCSI initiators associated with the host.    \u0026nbsp; \u0026nbsp; UnityHostInitiatorList   complex  success  ISCSI initiators with sytem genrated unique hash value.    \u0026nbsp; name   str  success  The name of the host.    \u0026nbsp; os_type   str  success  Operating system running on the host.    \u0026nbsp; type   str  success  HostTypeEnum of the host.    Authors  Rajshree Khare (@kharer5) ansible.team@dell.com   Consistency Group Module Manage consistency groups on Unity storage system\nSynopsis Managing the consistency group on the Unity storage system includes creating new consistency group, adding volumes to consistency group, removing volumes from consistency group, mapping hosts to consistency group, unmapping hosts from consistency group, renaming consistency group, modifying attributes of consistency group and deleting consistency group.\nParameters   Parameter Type Required Default Choices Description   cg_name  str      The name of the consistency group. It is mandatory for the create operation. Specify either cg_name or cg_id (but not both) for any operation.    cg_id  str      The ID of the consistency group. It can be used only for get, modify, add/remove volumes, or delete operations.    volumes  list elements: dict      This is a list of volumes. Either the volume ID or name must be provided for adding/removing existing volumes from consistency group. If volumes are given, then vol_state should also be specified. Volumes cannot be added/removed from consistency group, if the consistency group or the volume has snapshots.    \u0026nbsp; vol_id   str      The ID of the volume.    \u0026nbsp; vol_name   str      The name of the volume.    vol_state  str      present-in-group absent-in-group   String variable, describes the state of volumes inside consistency group. If volumes are given, then vol_state should also be specified.    new_cg_name  str      The new name of the consistency group, used in rename operation.    description  str      Description of the consistency group.    snap_schedule  str      Snapshot schedule assigned to the consistency group. Specifying an empty string \"\" removes the existing snapshot schedule from consistency group.    tiering_policy  str      AUTOTIER_HIGH AUTOTIER HIGHEST LOWEST   Tiering policy choices for how the storage resource data will be distributed among the tiers available in the pool.    hosts  list elements: dict      This is a list of hosts. Either the host ID or name must be provided for mapping/unmapping hosts for a consistency group. If hosts are given, then mapping_state should also be specified. Hosts cannot be mapped to a consistency group, if the consistency group has no volumes. When a consistency group is being mapped to the host, users should not use the volume module to map the volumes in the consistency group to hosts.    \u0026nbsp; host_id   str      The ID of the host.    \u0026nbsp; host_name   str      The name of the host.    mapping_state  str      mapped unmapped   String variable, describes the state of hosts inside the consistency group. If hosts are given, then mapping_state should also be specified.    state  str   True     absent present   Define whether the consistency group should exist or not.    unispherehost  str   True     IP or FQDN of the Unity management server.    username  str   True     The username of the Unity management server.    password  str   True     The password of the Unity management server.    verifycert  bool    True    True False   Boolean variable to specify whether or not to validate SSL certificate. True - Indicates that the SSL certificate should be verified. False - Indicates that the SSL certificate should not be verified.    port  int    443    Port number through which communication happens with Unity management server.    Examples - name: Create consistency group dellemc_unity_consistencygroup: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; username: \u0026quot;{{username}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; cg_name: \u0026quot;{{cg_name}}\u0026quot; description: \u0026quot;{{description}}\u0026quot; snap_schedule: \u0026quot;{{snap_schedule1}}\u0026quot; state: \u0026quot;present\u0026quot; - name: Get details of consistency group using id dellemc_unity_consistencygroup: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; username: \u0026quot;{{username}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; cg_id: \u0026quot;{{cg_id}}\u0026quot; state: \u0026quot;present\u0026quot; - name: Add volumes to consistency group dellemc_unity_consistencygroup: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; username: \u0026quot;{{username}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; cg_id: \u0026quot;{{cg_id}}\u0026quot; volumes: - vol_name: \u0026quot;Ansible_Test-3\u0026quot; - vol_id: \u0026quot;sv_1744\u0026quot; vol_state: \u0026quot;{{vol_state_present}}\u0026quot; state: \u0026quot;present\u0026quot; - name: Rename consistency group dellemc_unity_consistencygroup: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; username: \u0026quot;{{username}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; cg_name: \u0026quot;{{cg_name}}\u0026quot; new_cg_name: \u0026quot;{{new_cg_name}}\u0026quot; state: \u0026quot;present\u0026quot; - name: Modify consistency group details dellemc_unity_consistencygroup: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; username: \u0026quot;{{username}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; cg_name: \u0026quot;{{new_cg_name}}\u0026quot; snap_schedule: \u0026quot;{{snap_schedule2}}\u0026quot; tiering_policy: \u0026quot;{{tiering_policy1}}\u0026quot; state: \u0026quot;present\u0026quot; - name: Map hosts to a consistency group dellemc_unity_consistencygroup: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; username: \u0026quot;{{username}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; cg_id: \u0026quot;{{cg_id}}\u0026quot; hosts: - host_name: \u0026quot;10.226.198.248\u0026quot; - host_id: \u0026quot;Host_511\u0026quot; mapping_state: \u0026quot;mapped\u0026quot; state: \u0026quot;present\u0026quot; - name: Unmap hosts from a consistency group dellemc_unity_consistencygroup: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; username: \u0026quot;{{username}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; cg_id: \u0026quot;{{cg_id}}\u0026quot; hosts: - host_id: \u0026quot;Host_511\u0026quot; - host_name: \u0026quot;10.226.198.248\u0026quot; mapping_state: \u0026quot;unmapped\u0026quot; state: \u0026quot;present\u0026quot; - name: Remove volumes from consistency group dellemc_unity_consistencygroup: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; username: \u0026quot;{{username}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; cg_name: \u0026quot;{{new_cg_name}}\u0026quot; volumes: - vol_name: \u0026quot;Ansible_Test-3\u0026quot; - vol_id: \u0026quot;sv_1744\u0026quot; vol_state: \u0026quot;{{vol_state_absent}}\u0026quot; state: \u0026quot;present\u0026quot; - name: Delete consistency group dellemc_unity_consistencygroup: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; username: \u0026quot;{{username}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; cg_name: \u0026quot;{{new_cg_name}}\u0026quot; state: \u0026quot;absent\u0026quot; Return Values   Key Type Returned Description   changed   bool   always   Whether or not the resource has changed    consistency_group_details   complex   When consistency group exists   Details of the consistency group    \u0026nbsp; block_host_access   complex  success  Details of hosts mapped to the consistency group    \u0026nbsp; \u0026nbsp; UnityBlockHostAccessList   complex  success  List of hosts mapped to consistency group    \u0026nbsp; \u0026nbsp; \u0026nbsp; UnityBlockHostAccess   complex  success  Details of host    \u0026nbsp; id   str  success  The system ID given to the consistency group    \u0026nbsp; luns   complex  success  Details of volumes part of consistency group    \u0026nbsp; \u0026nbsp; UnityLunList   complex  success  List of volumes part of consistency group    \u0026nbsp; \u0026nbsp; \u0026nbsp; UnityLun   complex  success  Detail of volume    \u0026nbsp; relocation_policy   str  success  FAST VP tiering policy for the consistency group    \u0026nbsp; snap_schedule   complex  success  Snapshot schedule applied to consistency group    \u0026nbsp; \u0026nbsp; UnitySnapSchedule   complex  success  Snapshot schedule applied to consistency group    \u0026nbsp; \u0026nbsp; \u0026nbsp; id   str  success  The system ID given to the snapshot schedule    \u0026nbsp; \u0026nbsp; \u0026nbsp; name   str  success  The name of the snapshot schedule    \u0026nbsp; snapshots   complex  success  List of snapshots of consistency group    \u0026nbsp; \u0026nbsp; creation_time   str  success  Date and time on which the snapshot was taken    \u0026nbsp; \u0026nbsp; expirationTime   str  success  Date and time after which the snapshot will expire    \u0026nbsp; \u0026nbsp; name   str  success  Name of the snapshot    \u0026nbsp; \u0026nbsp; storageResource   complex  success  Storage resource for which the snapshot was taken    \u0026nbsp; \u0026nbsp; \u0026nbsp; UnityStorageResource   complex  success  Details of the storage resource    Authors  Akash Shendge (@shenda1) ansible.team@dell.com   Snapshot Schedule Module Manage snapshot schedules on Unity storage system\nSynopsis Managing snapshot schedules on Unity storage system includes creating new snapshot schedule, getting details of snapshot schedule, modifying attributes of snapshot schedule, and deleting snapshot schedule.\nParameters   Parameter Type Required Default Choices Description   name  str      The name of the snapshot schedule. Name is mandatory for a create operation. Specify either name or id (but not both) for any operation.    id  str      The ID of the snapshot schedule.    type  str      every_n_hours every_day every_n_days every_week every_month   Type of the rule to be included in snapshot schedule. Type is mandatory for any create or modify operation. Once the snapshot schedule is created with one type it can be modified.    interval  int      Number of hours between snapshots. Applicable only when rule type is 'every_n_hours'.    hours_of_day  list elements: int      Hours of the day when the snapshot will be taken. Applicable only when rule type is 'every_day'.    day_interval  int      Number of days between snapshots. Applicable only when rule type is 'every_n_days'.    days_of_week  list elements: str      SUNDAY MONDAY TUESDAY WEDNESDAY THURSDAY FRIDAY SATURDAY   Days of the week for which the snapshot schedule rule applies. Applicable only when rule type is 'every_week'.    day_of_month  int      Day of the month for which the snapshot schedule rule applies. Applicable only when rule type is 'every_month'. Value should be [1, 31].    hour  int      the hour when the snapshot will be taken. Applicable for 'every_n_days', 'every_week', 'every_month' rule types. For create operation, if 'hour' parameter is not specified, value will be taken as 0. Value should be [0, 23].    minute  int      Minute offset from the hour when the snapshot will be taken. Applicable for all rule types. For a create operation, if 'minute' parameter is not specified, value will be taken as 0. Value should be [0, 59].    desired_retention  int      The number of days/hours for which snapshot will be retained. When auto_delete is True, desired_retention cannot be specified. Maximum desired retention supported is 31 days or 744 hours.    retention_unit  str    hours    hours days   The retention unit for the snapshot.    auto_delete  bool      Indicates whether the system can automatically delete the snapshot.    state  str   True     absent present   Define whether the snapshot schedule should exist or not.    unispherehost  str   True     IP or FQDN of the Unity management server.    username  str   True     The username of the Unity management server.    password  str   True     The password of the Unity management server.    verifycert  bool    True    True False   Boolean variable to specify whether or not to validate SSL certificate. True - Indicates that the SSL certificate should be verified. False - Indicates that the SSL certificate should not be verified.    port  int    443    Port number through which communication happens with Unity management server.    Notes  Snapshot schedule created via Ansible will have only one rule. Modification of rule type is not allowed. Within the same type, other parameters can be modified. If an existing snapshot schedule has more than 1 rule in it, only get and delete operation is allowed.  Examples - name: Create snapshot schedule (Rule Type - every_n_hours) dellemc_unity_snapshotschedule: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; username: \u0026quot;{{username}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; name: \u0026quot;Ansible_Every_N_Hours_Testing\u0026quot; type: \u0026quot;every_n_hours\u0026quot; interval: 6 desired_retention: 24 state: \u0026quot;{{state_present}}\u0026quot; - name: Create snapshot schedule (Rule Type - every_day) dellemc_unity_snapshotschedule: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; username: \u0026quot;{{username}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; name: \u0026quot;Ansible_Every_Day_Testing\u0026quot; type: \u0026quot;every_day\u0026quot; hours_of_day: - 8 - 14 auto_delete: True state: \u0026quot;{{state_present}}\u0026quot; - name: Create snapshot schedule (Rule Type - every_n_days) dellemc_unity_snapshotschedule: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; username: \u0026quot;{{username}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; name: \u0026quot;Ansible_Every_N_Day_Testing\u0026quot; type: \u0026quot;every_n_days\u0026quot; day_interval: 2 desired_retention: 16 retention_unit: \u0026quot;days\u0026quot; state: \u0026quot;{{state_present}}\u0026quot; - name: Create snapshot schedule (Rule Type - every_week) dellemc_unity_snapshotschedule: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; username: \u0026quot;{{username}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; name: \u0026quot;Ansible_Every_Week_Testing\u0026quot; type: \u0026quot;every_week\u0026quot; days_of_week: - MONDAY - FRIDAY hour: 12 minute: 30 desired_retention: 200 state: \u0026quot;{{state_present}}\u0026quot; - name: Create snapshot schedule (Rule Type - every_month) dellemc_unity_snapshotschedule: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; username: \u0026quot;{{username}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; name: \u0026quot;Ansible_Every_Month_Testing\u0026quot; type: \u0026quot;every_month\u0026quot; day_of_month: 17 auto_delete: True state: \u0026quot;{{state_present}}\u0026quot; - name: Get snapshot schedule details using name dellemc_unity_snapshotschedule: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; username: \u0026quot;{{username}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; name: \u0026quot;Ansible_Every_N_Hours_Testing\u0026quot; state: \u0026quot;{{state_present}}\u0026quot; - name: Get snapshot schedule details using id dellemc_unity_snapshotschedule: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; username: \u0026quot;{{username}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; id: \u0026quot;{{id}}\u0026quot; state: \u0026quot;{{state_present}}\u0026quot; - name: Modify snapshot schedule details id dellemc_unity_snapshotschedule: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; username: \u0026quot;{{username}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; id: \u0026quot;{{id}}\u0026quot; type: \u0026quot;every_n_hours\u0026quot; interval: 8 state: \u0026quot;{{state_present}}\u0026quot; - name: Modify snapshot schedule using name dellemc_unity_snapshotschedule: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; username: \u0026quot;{{username}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; name: \u0026quot;Ansible_Every_Day_Testing\u0026quot; type: \u0026quot;every_day\u0026quot; desired_retention: 200 auto_delete: False state: \u0026quot;{{state_present}}\u0026quot; - name: Delete snapshot schedule using id dellemc_unity_snapshotschedule: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; username: \u0026quot;{{username}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; id: \u0026quot;{{id}}\u0026quot; state: \u0026quot;{{state_absent}}\u0026quot; - name: Delete snapshot schedule using name dellemc_unity_snapshotschedule: unispherehost: \u0026quot;{{unispherehost}}\u0026quot; verifycert: \u0026quot;{{verifycert}}\u0026quot; username: \u0026quot;{{username}}\u0026quot; password: \u0026quot;{{password}}\u0026quot; name: \u0026quot;Ansible_Every_Day_Testing\u0026quot; state: \u0026quot;{{state_absent}}\u0026quot; Return Values   Key Type Returned Description   changed   bool   always   Whether or not the resource has changed.    snapshot_schedule_details   complex   When snapshot schedule exists   Details of the snapshot schedule.    \u0026nbsp; id   str  success  The system ID given to the snapshot schedule.    \u0026nbsp; luns   complex  success  Details of volumes for which snapshot schedule applied.    \u0026nbsp; \u0026nbsp; UnityLunList   complex  success  List of volumes for which snapshot schedule applied.    \u0026nbsp; \u0026nbsp; \u0026nbsp; UnityLun   complex  success  Detail of volume.    \u0026nbsp; name   str  success  The name of the snapshot schedule.    \u0026nbsp; rules   complex  success  Details of rules that apply to snapshot schedule.    \u0026nbsp; \u0026nbsp; days_of_month   list  success  Days of the month for which the snapshot schedule rule applies.    \u0026nbsp; \u0026nbsp; days_of_week   complex  success  Days of the week for which the snapshot schedule rule applies.    \u0026nbsp; \u0026nbsp; \u0026nbsp; DayOfWeekEnumList   list  success  Enumeration of days of the week.    \u0026nbsp; \u0026nbsp; hours   list  success  Hourly frequency for the snapshot schedule rule.    \u0026nbsp; \u0026nbsp; id   str  success  The system ID of the rule.    \u0026nbsp; \u0026nbsp; interval   int  success  Number of days or hours between snaps, depending on the rule type.    \u0026nbsp; \u0026nbsp; is_auto_delete   bool  success  Indicates whether the system can automatically delete the snapshot based on pool automatic-deletion thresholds.    \u0026nbsp; \u0026nbsp; minute   int  success  Minute frequency for the snapshot schedule rule.    \u0026nbsp; \u0026nbsp; retention_time   int  success  Period of time in seconds for which to keep the snapshot.    \u0026nbsp; \u0026nbsp; retention_time_in_hours   int  success  Period of time in hours for which to keep the snapshot.    \u0026nbsp; \u0026nbsp; rule_type   str  success  Type of the rule applied to snapshot schedule.    \u0026nbsp; storage_resources   complex  success  Details of storage resources for which snapshot schedule applied.    \u0026nbsp; \u0026nbsp; UnityStorageResourceList   complex  success  List of storage resources for which snapshot schedule applied.    \u0026nbsp; \u0026nbsp; \u0026nbsp; UnityStorageResource   complex  success  Detail of storage resource.    Authors  Akash Shendge (@shenda1) ansible.team@dell.com   ","excerpt":"Ansible Modules for Dell EMC Unity Product Guide 1.2.0 © 2021 Dell Inc. or its subsidiaries. All …","ref":"/ansible-docs/docs/server/platforms/unity/product-guide/","title":"Unity Product Guide"},{"body":"Ansible Modules for Dell EMC Unity Release Notes 1.2.0  © 2021 Dell Inc. or its subsidiaries. All rights reserved. Dell, EMC, and other trademarks are trademarks of Dell Inc. or its subsidiaries. Other trademarks may be trademarks of their respective owners.\n Content These release notes contain supplemental information about Ansible Modules for Dell EMC Unity.\n Revision History Product Description New Features \u0026amp; Enhancements Known Issues Limitations Distribution Documentation  Revision history The table in this section lists the revision history of this document.\nTable 1. Revision history\n   Revision Date Description     01 June 2021 Current release of Ansible Modules for Dell EMC Unity 1.2.0    Product Description The Ansible modules for Dell EMC Unity are used to automate and orchestrate the deployment, configuration, and management of Dell EMC Unity Family systems, including Unity, Unity XT, and the UnityVSA. The capabilities of Ansible modules are managing NFS exports, SMB shares, NAS server, File Systems, File System Snapshots, Quota tree, User quotas for filesystem and quota tree and obtaining Unity system information. The options available for each capability are list, show, create, delete, and modify; except for NAS server for which options available are list \u0026amp; modify.\nNew features \u0026amp; enhancements This release supports the following features -\n  Application Tagging:\n A new HTTP header (Application-Type) is added in Unity REST API in Goshawk release which is used to set REST client name and its version and this information is recorded in Unity logs Ansible modules support application tagging which is used to identify the REST application that makes the request to Unisphere The value of application type parameter from Ansible module is set to Ansible/1.2.0    User quota module supports the following functionalities:\n Create User quota for a Filesystem/Quota tree Get User quota details Modify attributes of User quota Delete User quota    Quota tree module supports the following functionalities:\n Create Quota tree for a Filesystem Get Quota tree details Modify attributes of Quota tree Delete Quota tree    Consistency group module has the following enhancements:\n Map hosts to a new or an existing Consistency group Unmap hosts from a Consistency group    Filesystem module has the following enhancements:\n Set the attributes of Quota config while Filesystem creation Associate an existing snapshot schedule to existing or new Filesystem Remove snapshot schedule from a Filesystem    Volume module has the following enhancements:\n Map multiple hosts to a new or existing volume Unmap multiple hosts from a volume    Gather Facts Module has the following enhancements:\n List of User quota List of Quota tree    Known issues Known issues in this release are listed below:\n  Filesystem creation with quota config\n Setting quota configuration while creating a filesystem may sometime observe delay in fetching the details about the quota config of the new filesystem. The module will throw an error to rerun the task to see expected result.    Mapping and unmapping of hosts for a Consistency group\n Interoperability between Ansible Unity playbooks and Unisphere REST API is not supported for mapping and unmapping of hosts for a consistency group.  WORKAROUND: It is recommended to use Ansible Unity modules consistently for all mapping and unmapping of hosts for a consistency group instead of partially/mutually doing it through Unisphere and Ansible modules.\n     Limitations There are no known limitations.\nDistribution The software package is available for download from the Ansible Modules for Unity GitHub page.\nDocumentation The documentation is available on Ansible Modules for Unity GitHub page. It includes the following:\n README Release Notes (this document) Product Guide  ","excerpt":"Ansible Modules for Dell EMC Unity Release Notes 1.2.0  © 2021 Dell Inc. or its subsidiaries. All …","ref":"/ansible-docs/docs/server/platforms/unity/release-notes/","title":"Unity Release notes"},{"body":"Installation information for all the drivers can be found in the individual drivers page in this section\n","excerpt":"Installation information for all the drivers can be found in the individual drivers page in this …","ref":"/ansible-docs/v1/installation/","title":"Installation"},{"body":"Installation information for all the drivers can be found in the individual drivers page in this section\n","excerpt":"Installation information for all the drivers can be found in the individual drivers page in this …","ref":"/ansible-docs/v2/installation/","title":"Installation"},{"body":"This section provides the details and instructions on how to install the Dell EMC CSI drivers using the provided Helm charts and the Dell CSI Helm Installer.\nDependencies Installing any of the Dell EMC CSI Drivers using Helm requires a few utilities to be installed on the system running the installation.\n   Dependency Usage     kubectl Kubectl is used to validate that the Kubernetes system meets the requirements of the driver.   helm Helm v3 is used as the deployment tool for Charts. Go here to install Helm 3.   sshpass sshpass is used to check certain pre-requisities in worker nodes (in chosen drivers).    \nNote: To use these tools, a valid KUBECONFIG is required. Ensure that either a valid configuration is in the default location, or, that the KUBECONFIG environment variable points to a valid configuration before using these tools.\n","excerpt":"This section provides the details and instructions on how to install the Dell EMC CSI drivers using …","ref":"/ansible-docs/v1/installation/helm/","title":"CSI Driver installation using Helm"},{"body":"This section provides the details and instructions on how to install the Dell EMC CSI drivers using the provided Helm charts and the Dell CSI Helm Installer.\nDependencies Installing any of the Dell EMC CSI Drivers using Helm requires a few utilities to be installed on the system running the installation.\n   Dependency Usage     kubectl Kubectl is used to validate that the Kubernetes system meets the requirements of the driver.   helm Helm v3 is used as the deployment tool for Charts. Go here to install Helm 3.   sshpass sshpass is used to check certain pre-requisities in worker nodes (in chosen drivers).    \nNote: To use these tools, a valid KUBECONFIG is required. Ensure that either a valid configuration is in the default location, or, that the KUBECONFIG environment variable points to a valid configuration before using these tools.\n","excerpt":"This section provides the details and instructions on how to install the Dell EMC CSI drivers using …","ref":"/ansible-docs/v2/installation/helm/","title":"CSI Driver installation using Helm"},{"body":"Users can install the Dell CSI Operator via OperatorHub.io on Kubernetes. The following outlines the process to do so:\nSteps\n  Search DellEMC in storage category in Operatorhub.io.   Click DellEMC Operator.   Check the desired version is selected and click Install. Follow the provided instructions.   Install CSI Drivers via Operator Proceed to this link for further installing the driver using Operator\n","excerpt":"Users can install the Dell CSI Operator via OperatorHub.io on Kubernetes. The following outlines the …","ref":"/ansible-docs/v1/partners/operator/","title":"OperatorHub.io"},{"body":"Users can install the Dell CSI Operator via OperatorHub.io on Kubernetes. The following outlines the process to do so:\nSteps\n  Search DellEMC in storage category in Operatorhub.io.   Click DellEMC Operator.   Check the desired version is selected and click Install. Follow the provided instructions.   Install CSI Drivers via Operator Proceed to this link for further installing the driver using Operator\n","excerpt":"Users can install the Dell CSI Operator via OperatorHub.io on Kubernetes. The following outlines the …","ref":"/ansible-docs/v2/partners/operator/","title":"OperatorHub.io"},{"body":"The Dell EMC CSI Drivers support Red Hat OpenShift. Please see the Supported Platforms table for more details.\nThe CSI drivers can be installed via Helm charts or Dell CSI Operator. The Dell CSI Operator allows for easy installation of the driver via the Openshift UI. The process to install the Operator via the OpenShift UI can be found below.\nInstall Operator via the OpenShift UI Steps\n  Type \u0026ldquo;Dell\u0026rdquo; in the OperatorHub section under Operators, to get the list of available Dell CSI Operators.   Check the version you want to install from the list, you can check the details by clicking it.   Once selected, click \u0026ldquo;Install\u0026rdquo; to proceed with installation process.   You can verify the list of available operators by selecting \u0026ldquo;Installed Operator\u0026rdquo; section.   Select the Dell CSI Operator to get further description.   Install CSI Drivers via Operator Steps\n  Select the particular CSI driver which you want to install, as seen in step 5 above. In this example, CSI Unity is selected.   After clicking \u0026ldquo;Create CSIUnity\u0026rdquo; option in above snippet, you can set relevant parameters in your yaml file, as shown below. Refer to the driver install pages for the Dell CSI Operator for information on the parameters.   You can check the driver installed and node and controller pods running in the Pods section under Workloads.   ","excerpt":"The Dell EMC CSI Drivers support Red Hat OpenShift. Please see the Supported Platforms table for …","ref":"/ansible-docs/v1/partners/redhat/","title":"Red Hat OpenShift"},{"body":"The Dell EMC CSI Drivers support Red Hat OpenShift. Please see the Supported Platforms table for more details.\nThe CSI drivers can be installed via Helm charts or Dell CSI Operator. The Dell CSI Operator allows for easy installation of the driver via the Openshift UI. The process to install the Operator via the OpenShift UI can be found below.\nInstall Operator via the OpenShift UI Steps\n  Type \u0026ldquo;Dell\u0026rdquo; in the OperatorHub section under Operators, to get the list of available Dell CSI Operators.   Check the version you want to install from the list, you can check the details by clicking it.   Once selected, click \u0026ldquo;Install\u0026rdquo; to proceed with installation process.   You can verify the list of available operators by selecting \u0026ldquo;Installed Operator\u0026rdquo; section.   Select the Dell CSI Operator to get further description.   Install CSI Drivers via Operator Steps\n  Select the particular CSI driver which you want to install, as seen in step 5 above. In this example, CSI Unity is selected.   After clicking \u0026ldquo;Create CSIUnity\u0026rdquo; option in above snippet, you can set relevant parameters in your yaml file, as shown below. Refer to the driver install pages for the Dell CSI Operator for information on the parameters.   You can check the driver installed and node and controller pods running in the Pods section under Workloads.   ","excerpt":"The Dell EMC CSI Drivers support Red Hat OpenShift. Please see the Supported Platforms table for …","ref":"/ansible-docs/v2/partners/redhat/","title":"Red Hat OpenShift"},{"body":"Uninstall a Dell CSI driver installed via Helm To uninstall a driver, the csi-uninstall.sh script provides a handy wrapper around the helm utility. The only required argument for uninstallation is the namespace name. For example, to uninstall the driver:\n./csi-uninstall.sh --namespace \u0026lt;driver-namespace\u0026gt; For usage information:\n[dell-csi-helm-installer]# ./csi-uninstall.sh -h Help for ./csi-uninstall.sh Usage: ./csi-uninstall.sh options... Options: Required --namespace[=]\u0026lt;namespace\u0026gt; Kubernetes namespace to uninstall the CSI driver from Optional --release[=]\u0026lt;helm release\u0026gt; Name to register with helm, default value will match the driver name -h Help Uninstall a Dell CSI driver installed via Dell CSI Operator For uninstalling any CSI drivers deployed the Dell CSI Operator, just delete the respective Custom Resources.\nThis can be done using OperatorHub GUI by deleting the CR or via kubectl.\nFor example - To uninstall the driver installed via the operator, delete the Custom Resource(CR)\n# Replace driver-name and driver-namespace with their respective values kubectl delete \u0026lt;driver-name\u0026gt; -n \u0026lt;driver-namespace\u0026gt; ","excerpt":"Uninstall a Dell CSI driver installed via Helm To uninstall a driver, the csi-uninstall.sh script …","ref":"/ansible-docs/v1/uninstall/","title":"Uninstallation"},{"body":"Uninstall a Dell CSI driver installed via Helm To uninstall a driver, the csi-uninstall.sh script provides a handy wrapper around the helm utility. The only required argument for uninstallation is the namespace name. For example, to uninstall the PowerScale driver:\n./csi-uninstall.sh --namespace isilon/\u0026lt;driver-namespace\u0026gt; For usage information:\n[dell-csi-helm-installer]# ./csi-uninstall.sh -h Help for ./csi-uninstall.sh Usage: ./csi-uninstall.sh options... Options: Required --namespace[=]\u0026lt;namespace\u0026gt; Kubernetes namespace to uninstall the CSI driver from Optional --release[=]\u0026lt;helm release\u0026gt; Name to register with helm, default value will match the driver name -h Help Uninstall a Dell CSI driver installed via Dell CSI Operator For uninstalling any CSI drivers deployed the Dell CSI Operator, just delete the respective Custom Resources.\nThis can be done using OperatorHub GUI by deleting the CR or via kubectl.\nFor example - To uninstall a PowerFlex driver installed via the operator, delete the Custom Resource(CR)\n# Replace driver-name and driver-namespace with their respective values $ kubectl delete vxflexos/\u0026lt;driver-name\u0026gt; -n \u0026lt;driver-namespace\u0026gt; ","excerpt":"Uninstall a Dell CSI driver installed via Helm To uninstall a driver, the csi-uninstall.sh script …","ref":"/ansible-docs/v2/uninstall/","title":"Uninstallation"},{"body":"","excerpt":"","ref":"/ansible-docs/v1/upgradation/","title":"Upgrade"},{"body":"","excerpt":"","ref":"/ansible-docs/v2/upgradation/","title":"Upgrade"},{"body":"","excerpt":"","ref":"/ansible-docs/v1/features/","title":"Features"},{"body":"","excerpt":"","ref":"/ansible-docs/v2/features/","title":"Features"},{"body":"The Dell CSI Operator is a Kubernetes Operator, which can be used to install and manage the CSI Drivers provided by Dell EMC for various storage platforms. This operator is available as a community operator for upstream Kubernetes and can be deployed using OperatorHub.io. It is also available as a certified operator for OpenShift clusters and can be deployed using OpenShift Container Platform. Both these methods of installation use OLM (Operator Lifecycle Manager). The operator can also be deployed manually.\nInstallation Dell CSI Operator has been tested and qualified with\n Upstream Kubernetes or OpenShift (see supported versions)  Before you begin If you have installed an old version of the dell-csi-operator which was available with the name CSI Operator, please refer this section before continuing.\nFull list of CSI Drivers and versions supported by the Dell CSI Operator    CSI Driver Version ConfigVersion Kubernetes Version OpenShift Version     CSI PowerMax 1.5 v4 1.17, 1.18, 1.19 4.4, 4.5, 4.6   CSI PowerMax 1.6 v5 1.18, 1.19, 1.20 4.6, 4.7   CSI PowerFlex 1.3 v3 1.17, 1.18, 1.19 4.4, 4.5, 4.6   CSI PowerFlex 1.4 v4 1.18, 1.19, 1.20 4.6, 4.7   CSI PowerScale 1.4 v4 1.17, 1.18, 1.19 4.4, 4.5, 4.6   CSI PowerScale 1.5 v5 1.18, 1.19, 1.20 4.6, 4.7   CSI Unity 1.4 v3 1.17, 1.18, 1.19 4.4, 4.5, 4.6   CSI Unity 1.5 v4 1.18, 1.19, 1.20 4.6, 4.7   CSI PowerStore 1.2 v2 1.17, 1.18, 1.19 4.5, 4.6   CSI PowerStore 1.3 v3 1.18, 1.19, 1.20 4.6, 4.7     Dell CSI Operator can be installed via OLM (Operator Lifecycle Manager) and manual installation.\nInstallation Using Operator Lifecycle Manager dell-csi-operator can be installed using Operator Lifecycle Manager (OLM) on upstream Kubernetes clusters \u0026amp; Red Hat OpenShift Clusters.\nThe installation process involves creation of a Subscription object either via the OperatorHub UI or using kubectl/oc. While creating the Subscription you can set the Approval strategy for the InstallPlan for the Operator to -\n Automatic - If you want the Operator to be automatically installed or upgraded (once an upgrade becomes available) Manual - If you want a Cluster Administrator to manually review and approve the InstallPlan for installation/upgrades  Pre-Requisite for installation with OLM Please run the following commands for creating the required ConfigMap before installing the dell-csi-operator using OLM.\n$ git clone github.com/dell/dell-csi-operator $ cd dell-csi-operator $ tar -czf config.tar.gz driverconfig/ # Replace operator-namespace in the below command with the actual namespace where the operator will be deployed by OLM $ kubectl create configmap dell-csi-operator-config --from-file config.tar.gz -n \u0026lt;operator-namespace\u0026gt; Upstream Kubernetes  For installing via OperatorHub.io on Kubernetes, go to the OperatorHub page.  Red Hat OpenShift Clusters  For installing via OpenShift with the Operator, go to the OpenShift page.  Manual Installation Steps  Clone the Dell CSI Operator repository. Skip this step for Offline Install. And continue using workspace created by untar of dell-csi-operator-bundle.tar.gz. Run bash scripts/install.sh to install the operator  Run the command oc get pods to validate the install completed, should be able to see the operator related pod on default namespace   Custom Resource Definitions As part of the Dell CSI Operator installation, a CRD representing each driver installation is also installed.\nList of CRDs which are installed in API Group storage.dell.com\n csipowermax csiunity csivxflexos csiisilon csipowerstore csipowermaxrevproxy  For installation of the supported drivers, a CustomResource has to be created in your cluster.\nPre-Requisites for installation of the CSI Drivers Pre-requisites for upstream Kubernetes Clusters On upstream Kubernetes clusters, make sure to install\n VolumeSnapshot CRDs  On clusters running v1.20, make sure to install v1 VolumeSnapshot CRDs On clusters running v1.18 \u0026amp; v1.19, make sure to install v1beta1 VolumeSnapshot CRDs   External Volume Snapshot Controller with correct version  Pre-requisites for Red Hat OpenShift Clusters iSCSI If you are installing a CSI driver which is going to use iSCSI as the transport protocol, please follow the following instructions.\nIn Red Hat OpenShift clusters, you can create a MachineConfig object using the console or oc to ensure that the iSCSI daemon starts on all the Red Hat CoreOS nodes. Here is an example of a MachineConfig object:\napiVersion:machineconfiguration.openshift.io/v1kind:MachineConfigmetadata:name:99-iscsidlabels:machineconfiguration.openshift.io/role:workerspec:config:ignition:version:2.2.0systemd:units:- name:\u0026#34;iscsid.service\u0026#34;enabled:trueOnce the MachineConfig object has been deployed, CoreOS will ensure that iscsid.service starts automatically.\nAlternatively, you can check the status of iSCSI service by entering the following command on each worker node in the cluster:\nsudo systemctl status iscsid\nThe service should be up and running (i.e. should be in active state).\nIf the iscsid.service is not running, then perform the following steps on each worker node in the cluster\n Login to worker nodes and check if the file /etc/iscsi/initiatorname.iscsi has been created properly If the file doesn\u0026rsquo;t exist or it doesn\u0026rsquo;t contain a valid ISCSI IQN, then make sure it exists with valid entries Ensure that iscsid service is running - Enable sudo systemctl enable iscsid \u0026amp; restart sudo systemctl restart iscsid iscsid if necessary. Note: If your worker nodes are running on Red Hat CoreOS , you can refer the URL https://coreos.com/os/docs/latest/iscsi.html#enable-automatic-iscsi-login-at-boot for additional information.  MultiPath If you are installing a CSI Driver which requires the installation of the Linux native Multipath software - multipathd, please follow the below instructions\nTo enable multipathd on RedHat CoreOS nodes you need to prepare a working configuration encoded in base64.\necho 'defaults { user_friendly_names yes find_multipaths yes } blacklist { }' | base64 -w0\nUse the base64 encoded string output in the following MachineConfig yaml file (under source section)\napiVersion:machineconfiguration.openshift.io/v1kind:MachineConfigmetadata:name:workers-multipath-conf-defaultlabels:machineconfiguration.openshift.io/role:workerspec:config:ignition:version:2.2.0storage:files:- contents:source:data:text/plain;charset=utf-8;base64,ZGVmYXVsdHMgewp1c2VyX2ZyaWVuZGx5X25hbWVzIHllcwpmaW5kX211bHRpcGF0aHMgeWVzCn0KCmJsYWNrbGlzdCB7Cn0Kverification:{}filesystem:rootmode:400path:/etc/multipath.confAfter deploying thisMachineConfig object, CoreOS will start multipath service automatically.\nAlternatively you can check the status of multipath service by entering the following command in each worker nodes.\nsudo multipath -ll\nIf the above command is not successful, ensure that the /etc/multipath.conf file is present and configured properly. Once the file has been configured correctly, enable the multipath service by running the following command: sudo /sbin/mpathconf –-enable --with_multipathd y\nFinally , you have to restart the service by providing the command sudo systemctl restart multipathd\nFor additional information refer official documentation of multipath configuration.\nReplacing CSI Operator with Dell CSI Operator Dell CSI Operator was previously available, with the name CSI Operator, for both manual and OLM installation.\nCSI Operator has been discontinued and has been renamed to Dell CSI Operator. This is just a name change and as a result, the Kubernetes resources created as part of the Operator deployment will use the name dell-csi-operator instead of csi-operator.\nBefore proceeding with the installation of the new Dell CSI Operator, any existing CSI Operator installation has to be completely removed from the cluster.\nNote - This doesn\u0026rsquo;t impact any of the CSI Drivers which have been installed in the cluster\nIf the old CSI Operator was installed manually, then run the following command from the root of the repository which was used originally for installation\nbash scripts/undeploy.sh  If you don\u0026rsquo;t have the original repository available, then run the following commands\ngit clone https://github.com/dell/dell-csi-operator.git cd dell-csi-operator git checkout csi-operator-v1.0.0 bash scripts/undeploy.sh  Note - Once you have removed the old CSI Operator, then for installing the new Dell CSI Operator, you will need to pull/checkout the latest code\nIf you had installed old CSI Operator using OLM, then please follow un-installation instructions provided by OperatorHub. This will mostly involve:\n* Deleting the CSI Operator Subscription * Deleting the CSI Operator CSV  Install CSI Driver To install CSI drivers using Dell CSI Operator, please refer here\nNOTE: For more information on pre-requisites and parameters, please refer to the sub-pages below for each driver.\n","excerpt":"The Dell CSI Operator is a Kubernetes Operator, which can be used to install and manage the CSI …","ref":"/ansible-docs/v1/installation/operator/","title":"Dell CSI Operator Installation Process"},{"body":"The Dell CSI Operator is a Kubernetes Operator, which can be used to install and manage the CSI Drivers provided by Dell EMC for various storage platforms. This operator is available as a community operator for upstream Kubernetes and can be deployed using OperatorHub.io. It is also available as a certified operator for OpenShift clusters and can be deployed using OpenShift Container Platform. Both these methods of installation use OLM (Operator Lifecycle Manager). The operator can also be deployed manually.\n For installing via OperatorHub.io on Kubernetes, go to the OperatorHub page. For installing via OpenShift with the certified Operator, go to the OpenShift page. For installing manually, follow the instructions below.  Manual Installation Pre-requisites Dell CSI Operator has been tested and qualified with\n Upstream Kubernetes cluster v1.17, v1.18, v1.19 OpenShift Clusters 4.5, 4.6 with RHEL 7.x \u0026amp; RHCOS worker nodes For upstream k8s clusters, make sure to install  Beta VolumeSnapshot CRDs (can be installed using the Operator installation script) External Volume Snapshot Controller     Note- For more insights or detailed pre-requisites refer https://github.com/dell/dell-csi-operator\n Steps  Clone the Dell CSI Operator repository Run \u0026lsquo;bash scripts/install.sh\u0026rsquo; to install the operator  Run the command \u0026lsquo;oc get pods\u0026rsquo; to validate the install completed  Should be able to see the operator related pod on default namespace     Driver Install via Dell CSI Operator For information on how to install the CSI drivers via the Dell CSI Operator, please refer to the sub-pages below for each driver.\n","excerpt":"The Dell CSI Operator is a Kubernetes Operator, which can be used to install and manage the CSI …","ref":"/ansible-docs/v2/installation/operator/","title":"Dell CSI Operator Installation Process"},{"body":"","excerpt":"","ref":"/ansible-docs/v1/troubleshooting/","title":"Troubleshooting"},{"body":"","excerpt":"","ref":"/ansible-docs/v2/troubleshooting/","title":"Troubleshooting"},{"body":"","excerpt":"","ref":"/ansible-docs/v1/release/","title":"Release Notes"},{"body":"","excerpt":"","ref":"/ansible-docs/v2/release/","title":"Release Notes"},{"body":"","excerpt":"","ref":"/ansible-docs/v1/grasp/","title":"Learn"},{"body":"","excerpt":"","ref":"/ansible-docs/v2/grasp/","title":"Learn"},{"body":"Volume Snapshot Feature The Volume Snapshot feature started in alpha (v1alpha1) in Kubernetes 1.13 and then moved to beta (v1beta1) in Kubernetes version 1.17 and generally available (v1) in Kubernetes version 1.20.\nIn order to use Volume Snapshots, ensure the following components with appropriate versions have been deployed to your cluster:\n Kubernetes Volume Snaphshot CRDs Volume Snapshot Controller  ","excerpt":"Volume Snapshot Feature The Volume Snapshot feature started in alpha (v1alpha1) in Kubernetes 1.13 …","ref":"/ansible-docs/v1/concepts/","title":"Concepts"},{"body":"","excerpt":"","ref":"/ansible-docs/v1/partners/","title":"Our Ecosystem Partners"},{"body":"","excerpt":"","ref":"/ansible-docs/v2/partners/","title":"Our Ecosystem Partners"},{"body":"","excerpt":"","ref":"/ansible-docs/docs/","title":"Documentation"},{"body":"","excerpt":"","ref":"/ansible-docs/docs/network/","title":"Network"},{"body":"","excerpt":"","ref":"/ansible-docs/docs/server/","title":"Server"},{"body":"","excerpt":"","ref":"/ansible-docs/docs/storage/","title":"Storage"},{"body":"Installing CSI Driver via Operator CSI Drivers can be installed by creating a CustomResource object in your cluster.\nSample manifest files for each driver CustomResourceDefintion have been provided in the samples folder to help with the installation of the drivers. These files follow the naming convention\n{driver name}_{driver version}_k8s_{k8 version}.yaml  Or\n{driver name}_{driver version}_ops_{OpenShift version}.yaml  For e.g.\n sample/powermax_v140_k8s_117.yaml* \u0026lt;- To install CSI PowerMax driver v1.4.0 on a Kubernetes 1.17 cluster sample/powermax_v140_ops_46.yaml* \u0026lt;- To install CSI PowerMax driver v1.4.0 on an OpenShift 4.6 cluster  Copy the correct sample file and edit the mandatory \u0026amp; any optional parameters specific to your driver installation by following the instructions here\n NOTE: A detailed explanation of the various mandatory and optional fields in the CustomResource is available here. Please make sure to read through and understand the various fields.\n Run the following command to install the CSI driver.\nkubectl create -f \u0026lt;driver-manifest.yaml\u0026gt; Note: If you are using an OLM based installation, the example manifests are available in the OperatorHub UI. You can edit these manifests and install the driver using the OperatorHub UI.\nVerifying the installation Once the driver Custom Resource has been created, you can verify the installation\n  Check if Driver CR got created successfully\nFor e.g. – If you installed the PowerMax driver\n$ kubectl get csipowermax -n \u0026lt;driver-namespace\u0026gt;   Check the status of the Custom Resource to verify if the driver installation was successful\n  If the driver-namespace was set to test-powermax, and the name of the driver is powermax, then run the command kubectl get csipowermax/powermax -n test-powermax -o yaml to get the details of the Custom Resource.\nNote: If the state of the CustomResource is Running then all the driver pods have been successfully installed. If the state is SuccessFul, then it means the driver deployment was successful but some driver pods may not be in a Running state. Please refer to the Troubleshooting section here if you encounter any issues during installation.\nChanges in installation for latest CSI drivers If you are installing the latest versions of the CSI drivers, the driver controller will be installed as a Kubernetes Deployment instead of a Statefulset. These installations can also run multiple replicas for the driver controller pods(not supported for StatefulSets) to support High Availability for the Controller.\nUpdate CSI Drivers The CSI Drivers installed by the Dell CSI Operator can be updated like any Kubernetes resource. This can be achieved in various ways which include –\n Modifying the installation directly via kubectl edit For e.g. - If the name of the installed unity driver is unity, then run # Replace driver-namespace with the namespace where the Unity driver is installed $ kubectl edit csiunity/unity -n \u0026lt;driver-namespace\u0026gt; and modify the installation\n Modify the API object in-place via kubectl patch  NOTE: If you are trying to upgrade the CSI driver from an older version, make sure to modify the configVersion field if required.\nNOTE: Do not try to update the operator by modifying the original CustomResource manifest file and running the kubectl apply -f command. As part of the driver installation, the Operator sets some annotations on the CustomResource object which are further utilized in some workflows (like detecting upgrade of drivers). If you run the kubectl apply -f command to update the driver, these annotations are overwritten and this may lead to failures.\nSupported modifications  Changing environment variable values for driver Adding (supported) environment variables Updating the image of the driver  Unsupported modifications Kubernetes doesn’t allow to update a storage class once it has been created. Any attempt to update a storage class will result in a failure.\n Note – Any attempt to rename a storage class or snapshot class will result in the deletion of older class and creation of a new class.\n Limitations  The Dell CSI Operator can\u0026rsquo;t manage any existing driver installed using Helm charts. If you already have installed one of the DellEMC CSI driver in your cluster and want to use the operator based deployment, uninstall the driver and then redeploy the driver following the installation procedure described above The Dell CSI Operator can\u0026rsquo;t update storage classes as it is prohibited by Kubernetes. Any attempt to do so will cause an error and the driver Custom Resource will be left in a Failed state. Refer the Troubleshooting section to fix the driver CR. The Dell CSI Operator is not fully compliant with the OperatorHub React UI elements and some of the Custom Resource fields may show up as invalid or unsupported in the OperatorHub GUI. To get around this problem, use kubectl/oc commands to get details about the Custom Resource(CR). This issue will be fixed in the upcoming releases of the Dell CSI Operator  Custom Resource Specification Each CSI Driver installation is represented by a Custom Resource.\nThe specification for the Custom Resource is the same for all the drivers.\nBelow is a list of all the mandatory and optional fields in the Custom Resource specification\nMandatory fields configVersion - Configuration version - Refer full list of supported driver for finding out the appropriate config version here replicas - Number of replicas for controller plugin - Must be set to 1 for all drivers\ncommon\nThis field is mandatory and is used to specify common properties for both controller and the node plugin\n image - driver container image imagePullPolicy - Image Pull Policy of the driver image envs - List of environment variables and their values  Optional fields controller - List of environment variables and values which are applicable only for controller\nnode - List of environment variables and values which are applicable only for node\nsideCars - Specification for CSI sidecar containers.\nauthSecret - Name of the secret holding credentials for use by the driver. If not specified, the default secret *-creds must exist in the same namespace as driver\ntlsCertSecret - Name of the TLS cert secret for use by the driver. If not specified, a secret *-certs must exist in the namespace as driver\nstorageclass\nList of Storage Class fields\n name - name of the Storage Class default - Used to specify if the storage class will be marked as default (only set one storage class as default in a cluster) reclaimPolicy - Sets the PersistentVolumeReclaim Policy for the PVCs. Defaults to Delete if not specified parameters - driver specific parameters. Refer individual driver section for more details allowVolumeExpansion - Set to true for allowing volume expansion for PVC volumeBindingMode - Sets the VolumeBindingMode in the Storage Class. If left blank, it will be set to the default value for the driver version you are installing allowedTopologies - Sets the topology keys and values which allows the pods/and volumes to be scheduled on nodes that have access to the storage.  snapshotclass\nList of Snapshot Class specifications\n name - name of the snapshot class parameters - driver specific parameters. Refer individual driver section for more details  forceUpdate\nBoolean value which can be set to true in order to force update the status of the CSI Driver\ntolerations List of tolerations which should be applied to the driver StatefulSet/Deployment and DaemonSet\nIt should be set separately in the controller and node sections if you want separate set of tolerations for them\nnodeSelector Used to specify node selectors for the driver StatefulSet/Deployment and DaemonSet\nHere is a sample specification annotated with comments to explain each field\napiVersion:storage.dell.com/v1kind:CSIPowerMax# Type of the drivermetadata:name:test-powermax# Name of the drivernamespace:test-powermax# Namespace where driver is installedspec:driver:# Used to specify configuration versionconfigVersion:v3# Refer the table containing the full list of supported drivers to find the appropriate config versionreplicas:1forceUpdate:false# Set to true in case you want to force an update of driver statuscommon:# All common specificationimage:\u0026#34;dellemc/csi-powermax:v1.4.0.000R\u0026#34;#driver image for a particular releaseimagePullPolicy:IfNotPresentenvs:- name:X_CSI_POWERMAX_ENDPOINTvalue:\u0026#34;https://0.0.0.0:8443/\u0026#34;- name:X_CSI_K8S_CLUSTER_PREFIXvalue:\u0026#34;XYZ\u0026#34;storageClass:- name:bronzedefault:truereclaimPolicy:Deleteparameters:SYMID:\u0026#34;000000000001\u0026#34;SRP:DEFAULT_SRPServiceLevel:BronzeYou can set the field replicas to a higher number than 1 for the latest driver versions.\nNote - The image field should point to the correct image tag for version of the driver you are installing.\nFor e.g. - If you wish to install v1.4 of the CSI PowerMax driver, use the image tag dellemc/csi-powermax:v1.4.0.000R\nNote - The name of the Storage Class or the Volume Snapshot Class (which are created in the Kubernetes/OpenShift cluster) is created using the name of the driver and the name provided for these classes in the manifest. This is done in order to ensure that these names are unique if there are multiple drivers installed in the same cluster.\nFor e.g. - With the above sample manifest, the name of the storage class which is created in the cluster will be test-powermax-bronze.\nYou can get the name of the StorageClass and SnapshotClass created by the operator by running the commands - kubectl get storageclass and kubectl get volumesnapshotclass\nSideCars Although the sidecars field in the driver specification is optional, it is strongly recommended to not modify any details related to sidecars provided (if present) in the sample manifests. Any modifications to this should be only done after consulting with Dell EMC support.\nModify the driver specification  Choose the correct configVersion. Refer the table containing the full list of supported drivers and versions. Provide the namespace (in metadata section) where you want to install the driver. Provide a name (in metadata section) for the driver. This will be the name of the Custom Resource. Edit the values for mandatory configuration parameters specific to your installation. Edit/Add any values for optional configuration parameters to customize your installation. If you are installing the latest versions of the CSI drivers, the default number of replicas is set to 2. You can increase/decrease this value.  ","excerpt":"Installing CSI Driver via Operator CSI Drivers can be installed by creating a CustomResource object …","ref":"/ansible-docs/v1/installation/operator/installdriver/","title":""},{"body":"PowerScale v1.3 -Release Notes\n-Product Guide\nv1.2 -Release Notes\n-Product Guide\nPowerMax v1.4 -Release Notes\n-Product Guide\nPowerFlex v1.2 -Release Notes\n-Product Guide\nPowerStore v1.1 -Release Notes\n-Product Guide\nUnity v1.3 -Release Notes\n-Product Guide\n","excerpt":"PowerScale v1.3 -Release Notes\n-Product Guide\nv1.2 -Release Notes\n-Product Guide\nPowerMax v1.4 …","ref":"/ansible-docs/v1/archives/","title":"Archives"},{"body":"PowerScale v1.3 -Release Notes\n-Product Guide\nv1.2 -Release Notes\n-Product Guide\nPowerMax v1.4 -Release Notes\n-Product Guide\nPowerFlex v1.2 -Release Notes\n-Product Guide\nPowerStore v1.1 -Release Notes\n-Product Guide\nUnity v1.3 -Release Notes\n-Product Guide\n","excerpt":"PowerScale v1.3 -Release Notes\n-Product Guide\nv1.2 -Release Notes\n-Product Guide\nPowerMax v1.4 …","ref":"/ansible-docs/v2/archives/","title":"Archives"},{"body":"   Before installing the drivers, Dell CSI Operator tries to validate the Custom Resource being created. If some mandatory environment variables are missing or there is a type mismatch, then the Operator will report an error during the reconciliation attempts.\nBecause of this, the status of the Custom Resource will change to \u0026ldquo;Failed\u0026rdquo; and the error captured in the \u0026ldquo;ErrorMessage\u0026rdquo; field in the status.\nFor e.g. - If the PowerMax driver was installed in the namespace test-powermax and has the name powermax, then run the command kubectl get csipowermax/powermax -n test-powermax -o yaml to get the Custom Resource details.\nIf there was an error while installing the driver, then you would see a status like this -\nstatus:status:errorMessage:mandatoryEnv- X_CSI_K8S_CLUSTER_PREFIXnotspecifiedinuserspecstate:FailedThe state of the Custom Resource can also change to Failed because of any other prohibited updates or any failure while installing the driver. In order to recover from this failure, fix the error in the manifest and update/patch the Custom Resource\n  After an update to the driver, the controller pod may not have the latest desired specification\nThe above happens when the controller pod was in a failed state before applying the update. Even though the Dell CSI Operator updates the pod template specification for the StatefulSet, the StatefulSet controller does not apply the update to the pod. This happens because of the unique nature of StatefulSets where the controller tries to retain the last known working state.\nTo get around this problem, the Dell CSI Operator forces an update of the pod specification by deleting the older pod. In case the Dell CSI Operator fails to do so, delete the controller pod to force an update of the controller pod specification\n  The Status of the CSI Driver Custom Resource shows the state of the driver pods after installation. This state will not be updated automatically if there are any changes to the driver pods outside any Operator operations At times because of inconsistencies in fetching data from the Kubernetes cache, state of some driver pods may not be updated correctly in the status. To force an update of the state, you can update the Custom Resource forcefully by setting forceUpdate to true. If all the driver pods are in Available State, then the state of the Custom Resource will be updated as Running\n  ","excerpt":"Before installing the drivers, Dell CSI Operator tries to validate the Custom Resource being …","ref":"/ansible-docs/v1/troubleshooting/operator/","title":"Dell CSI Operator"},{"body":" Dell Technologies Ansible Documentation Network   Server   Storage          ","excerpt":" Dell Technologies Ansible Documentation Network   Server   Storage          ","ref":"/ansible-docs/","title":"Dell Technologies"},{"body":"The Dell CSI Drivers support Docker Enterprise Edition (EE) and deployment on clusters bootstrapped with Docker Universal Control Plane (UCP).\nThe installation process for the drivers on such clusters remains the same as the installation process on regular Kubernetes clusters.\nOn UCP based clusters, kubectl may not be installed by default, it is important that kubectl is installed prior to the installation of the driver.\nThe worker nodes on UCP backed clusters may run any of the OSs which we support with upstream clusters.\nDocker EE UI Examples ","excerpt":"The Dell CSI Drivers support Docker Enterprise Edition (EE) and deployment on clusters bootstrapped …","ref":"/ansible-docs/v1/partners/docker/","title":"Docker EE"},{"body":"The Dell CSI Drivers support Docker Enterprise Edition (EE) and deployment on clusters bootstrapped with Docker Universal Control Plane (UCP).\nThe installation process for the drivers on such clusters remains the same as the installation process on regular Kubernetes clusters.\nOn UCP based clusters, kubectl may not be installed by default, it is important that kubectl is installed prior to the installation of the driver.\nThe worker nodes on UCP backed clusters may run any of the OSs which we support with upstream clusters.\nDocker EE UI Examples ","excerpt":"The Dell CSI Drivers support Docker Enterprise Edition (EE) and deployment on clusters bootstrapped …","ref":"/ansible-docs/v2/partners/docker/","title":"Docker EE"},{"body":"NOTE: This doc version is no longer supported by us. You can check our latest version\n","excerpt":"NOTE: This doc version is no longer supported by us. You can check our latest version","ref":"/ansible-docs/v1/","title":"Documentation"},{"body":"NOTE: This doc version is no longer supported by us. You can check our latest version\n","excerpt":"NOTE: This doc version is no longer supported by us. You can check our latest version","ref":"/ansible-docs/v2/","title":"Documentation"},{"body":"The csi-offline-bundle.sh script can be used to create a package usable for offline installation of the Dell EMC CSI Storage Providers, via either Helm or the Dell CSI Operator.\nThis includes the following drivers:\n PowerFlex PowerMax PowerScale PowerStore Unity  As well as the Dell CSI Operator\n Dell CSI Operator  Dependencies Multiple linux based systems may be required to create and process an offline bundle for use.\n One linux based system, with internet access, will be used to create the bundle. This involved the user cloning a git repository hosted on github.com and then invoking a script that utilizes docker or podman to pull and save container images to file. One linux based system, with access to an image registry, to invoke a script that uses docker or podman to restore container images from file and push them to a registry  If one linux system has both internet access and access to an internal registry, that system can be used for both steps.\nPreparing an offline bundle requires the following utilities:\n   Dependency Usage     docker or podman docker or podman will be used to pull images from public image registries, tag them, and push them to a private registry.    One of these will be required on both the system building the offline bundle as well as the system preparing for installation.    Tested version(s) are docker 19.03+ and podman 1.6.4+   git git will be used to manually clone one of the above repos in order to create and offline bundle.    This is only needed on the system preparing the offline bundle.    Tested version(s) are git 1.8+ but any version should work.    Workflow To perform an offline installation of a driver or the Operator, the following steps should be performed:\n Build an offline bundle Unpacking the offline bundle created in Step 1 and preparing for installation Perform either a Helm installation or Operator installation using the files obtained after unpacking in Step 2  Building an offline bundle This needs to be performed on a linux system with access to the internet as a git repo will need to be cloned, and container images pulled from public registries.\nTo build an offline bundle, the following steps are needed:\n Perform a git clone of the desired repository. For a helm based install, the specific driver repo should be cloned. For an Operator based deployment, the Dell CSI Operator repo should be cloned Run the csi-offline-bundle.sh script with an argument of -c in order to create an offline bundle   For Helm installs, the csi-offline-bundle.sh script will be found in the dell-csi-helm-installer directory For Operator installs, the csi-offline-bundle.sh script will be found in the scripts directory  The script will perform the following steps:\n Determine required images by parsing either the driver Helm charts (if run from a cloned CSI Driver git repository) or the Dell CSI Operator configuration files (if run from a clone of the Dell CSI Operator repository) Perform an image pull of each image required Save all required images to a file by running docker save or podman save Build a tar.gz file containing the images as well as files required to installer the driver and/or Operator  The resulting offline bundle file can be copied to another machine, if necessary, to gain access to the desired image registry.\nFor example, here is the output of a request to build an offline bundle for the Dell CSI Operator:\n[user@anothersystem /home/user]# git clone https://github.com/dell/dell-csi-operator.git [user@anothersystem /home/user]# cd dell-csi-operator [user@system /home/user/dell-csi-operator]# scripts/csi-offline-bundle.sh -c * * Building image manifest file * * Pulling container images dellemc/csi-isilon:v1.2.0 dellemc/csi-isilon:v1.3.0.000R dellemc/csipowermax-reverseproxy:v1.0.0.000R dellemc/csi-powermax:v1.2.0.000R dellemc/csi-powermax:v1.4.0.000R dellemc/csi-powerstore:v1.1.0.000R dellemc/csi-unity:v1.3.0.000R dellemc/csi-vxflexos:v1.1.5.000R dellemc/csi-vxflexos:v1.2.0.000R dellemc/dell-csi-operator:v1.1.0.000R quay.io/k8scsi/csi-attacher:v2.0.0 quay.io/k8scsi/csi-attacher:v2.2.0 quay.io/k8scsi/csi-node-driver-registrar:v1.2.0 quay.io/k8scsi/csi-provisioner:v1.4.0 quay.io/k8scsi/csi-provisioner:v1.6.0 quay.io/k8scsi/csi-resizer:v0.5.0 quay.io/k8scsi/csi-snapshotter:v2.1.1 * * Saving images * * Copying necessary files /dell/git/dell-csi-operator/config /dell/git/dell-csi-operator/deploy /dell/git/dell-csi-operator/samples /dell/git/dell-csi-operator/scripts /dell/git/dell-csi-operator/README.md /dell/git/dell-csi-operator/LICENSE * * Compressing release dell-csi-operator-bundle/ dell-csi-operator-bundle/samples/ ... \u0026lt;listing of files included in bundle\u0026gt; ... dell-csi-operator-bundle/LICENSE dell-csi-operator-bundle/README.md * * Complete Offline bundle file is: /dell/git/dell-csi-operator/dell-csi-operator-bundle.tar.gz Unpacking the offline bundle and preparing for installation This needs to be performed on a linux system with access to an image registry that will host container images. If the registry requires login, that should be done before proceeding.\nTo prepare for driver or Operator installation, the following steps need to be performed:\n Copy the offline bundle file created from the previous step to a system with access to an image registry available to your Kubernetes/OpenShift cluster Expand the bundle file by running tar xvfz \u0026lt;filename\u0026gt; Run the csi-offline-bundle.sh script and supply the -p option as well as the path to the internal registry with the -r option  The script will then perform the following steps:\n Load the required container images into the local system Tag the images according to the user supplied registry information Push the newly tagged images to the registry Modify the Helm charts or Operator configuration to refer to the newly tagged/pushed images  An example of preparing the bundle for installation (192.168.75.40:5000 refers to a image registry accessible to Kubernetes/OpenShift):\n[user@anothersystem /tmp]# tar xvfz dell-csi-operator-bundle.tar.gz dell-csi-operator-bundle/ dell-csi-operator-bundle/samples/ ... \u0026lt;listing of files included in bundle\u0026gt; ... dell-csi-operator-bundle/LICENSE dell-csi-operator-bundle/README.md [user@anothersystem /tmp]# cd dell-csi-operator-bundle [user@anothersystem /tmp/dell-csi-operator-bundle]# scripts/csi-offline-bundle.sh -p -r 192.168.75.40:5000/operator Preparing a offline bundle for installation * * Loading docker images * * Tagging and pushing images dellemc/csi-isilon:v1.2.0 -\u0026gt; 192.168.75.40:5000/operator/csi-isilon:v1.2.0 dellemc/csi-isilon:v1.3.0.000R -\u0026gt; 192.168.75.40:5000/operator/csi-isilon:v1.3.0.000R dellemc/csipowermax-reverseproxy:v1.0.0.000R -\u0026gt; 192.168.75.40:5000/operator/csipowermax-reverseproxy:v1.0.0.000R dellemc/csi-powermax:v1.2.0.000R -\u0026gt; 192.168.75.40:5000/operator/csi-powermax:v1.2.0.000R dellemc/csi-powermax:v1.4.0.000R -\u0026gt; 192.168.75.40:5000/operator/csi-powermax:v1.4.0.000R dellemc/csi-powerstore:v1.1.0.000R -\u0026gt; 192.168.75.40:5000/operator/csi-powerstore:v1.1.0.000R dellemc/csi-unity:v1.3.0.000R -\u0026gt; 192.168.75.40:5000/operator/csi-unity:v1.3.0.000R dellemc/csi-vxflexos:v1.1.5.000R -\u0026gt; 192.168.75.40:5000/operator/csi-vxflexos:v1.1.5.000R dellemc/csi-vxflexos:v1.2.0.000R -\u0026gt; 192.168.75.40:5000/operator/csi-vxflexos:v1.2.0.000R dellemc/dell-csi-operator:v1.1.0.000R -\u0026gt; 192.168.75.40:5000/operator/dell-csi-operator:v1.1.0.000R quay.io/k8scsi/csi-attacher:v2.0.0 -\u0026gt; 192.168.75.40:5000/operator/csi-attacher:v2.0.0 quay.io/k8scsi/csi-attacher:v2.2.0 -\u0026gt; 192.168.75.40:5000/operator/csi-attacher:v2.2.0 quay.io/k8scsi/csi-node-driver-registrar:v1.2.0 -\u0026gt; 192.168.75.40:5000/operator/csi-node-driver-registrar:v1.2.0 quay.io/k8scsi/csi-provisioner:v1.4.0 -\u0026gt; 192.168.75.40:5000/operator/csi-provisioner:v1.4.0 quay.io/k8scsi/csi-provisioner:v1.6.0 -\u0026gt; 192.168.75.40:5000/operator/csi-provisioner:v1.6.0 quay.io/k8scsi/csi-resizer:v0.5.0 -\u0026gt; 192.168.75.40:5000/operator/csi-resizer:v0.5.0 quay.io/k8scsi/csi-snapshotter:v2.1.1 -\u0026gt; 192.168.75.40:5000/operator/csi-snapshotter:v2.1.1 * * Preparing operator files within /tmp/dell-csi-operator-bundle changing: dellemc/csi-isilon:v1.2.0 -\u0026gt; 192.168.75.40:5000/operator/csi-isilon:v1.2.0 changing: dellemc/csi-isilon:v1.3.0.000R -\u0026gt; 192.168.75.40:5000/operator/csi-isilon:v1.3.0.000R changing: dellemc/csipowermax-reverseproxy:v1.0.0.000R -\u0026gt; 192.168.75.40:5000/operator/csipowermax-reverseproxy:v1.0.0.000R changing: dellemc/csi-powermax:v1.2.0.000R -\u0026gt; 192.168.75.40:5000/operator/csi-powermax:v1.2.0.000R changing: dellemc/csi-powermax:v1.4.0.000R -\u0026gt; 192.168.75.40:5000/operator/csi-powermax:v1.4.0.000R changing: dellemc/csi-powerstore:v1.1.0.000R -\u0026gt; 192.168.75.40:5000/operator/csi-powerstore:v1.1.0.000R changing: dellemc/csi-unity:v1.3.0.000R -\u0026gt; 192.168.75.40:5000/operator/csi-unity:v1.3.0.000R changing: dellemc/csi-vxflexos:v1.1.5.000R -\u0026gt; 192.168.75.40:5000/operator/csi-vxflexos:v1.1.5.000R changing: dellemc/csi-vxflexos:v1.2.0.000R -\u0026gt; 192.168.75.40:5000/operator/csi-vxflexos:v1.2.0.000R changing: dellemc/dell-csi-operator:v1.1.0.000R -\u0026gt; 192.168.75.40:5000/operator/dell-csi-operator:v1.1.0.000R changing: quay.io/k8scsi/csi-attacher:v2.0.0 -\u0026gt; 192.168.75.40:5000/operator/csi-attacher:v2.0.0 changing: quay.io/k8scsi/csi-attacher:v2.2.0 -\u0026gt; 192.168.75.40:5000/operator/csi-attacher:v2.2.0 changing: quay.io/k8scsi/csi-node-driver-registrar:v1.2.0 -\u0026gt; 192.168.75.40:5000/operator/csi-node-driver-registrar:v1.2.0 changing: quay.io/k8scsi/csi-provisioner:v1.4.0 -\u0026gt; 192.168.75.40:5000/operator/csi-provisioner:v1.4.0 changing: quay.io/k8scsi/csi-provisioner:v1.6.0 -\u0026gt; 192.168.75.40:5000/operator/csi-provisioner:v1.6.0 changing: quay.io/k8scsi/csi-resizer:v0.5.0 -\u0026gt; 192.168.75.40:5000/operator/csi-resizer:v0.5.0 changing: quay.io/k8scsi/csi-snapshotter:v2.1.1 -\u0026gt; 192.168.75.40:5000/operator/csi-snapshotter:v2.1.1 * * Complete Perform either a Helm installation or Operator installation Now that the required images are available and the Helm Charts/Operator configuration updated, you can proceed by following the usual installation procedure as documented either via Helm or Operator.\nNOTE: Installation should be done using the files that was obtained after unpacking the bundle as the image tags in the manifests are modifed to point to the internal registry.\n","excerpt":"The csi-offline-bundle.sh script can be used to create a package usable for offline installation of …","ref":"/ansible-docs/v1/installation/offline/","title":"Offline Installation of Dell EMC CSI Storage Providers"},{"body":"The csi-offline-bundle.sh script can be used to create a package usable for offline installation of the Dell EMC CSI Storage Providers, via either Helm or the Dell CSI Operator.\nThis includes the following drivers:\n PowerFlex PowerMax PowerScale PowerStore Unity  As well as the Dell CSI Operator\n Dell CSI Operator  Dependencies Multiple linux based systems may be required to create and process an offline bundle for use.\n One linux based system, with internet access, will be used to create the bundle. This involved the user cloning a git repository hosted on github.com and then invoking a script that utilizes docker or podman to pull and save container images to file. One linux based system, with access to an image registry, to invoke a script that uses docker or podman to restore container images from file and push them to a registry  If one linux system has both internet access and access to an internal registry, that system can be used for both steps.\nPreparing an offline bundle requires the following utilities:\n   Dependency Usage     docker or podman docker or podman will be used to pull images from public image registries, tag them, and push them to a private registry.    One of these will be required on both the system building the offline bundle as well as the system preparing for installation.    Tested version(s) are docker 19.03+ and podman 1.6.4+   git git will be used to manually clone one of the above repos in order to create and offline bundle.    This is only needed on the system preparing the offline bundle.    Tested version(s) are git 1.8+ but any version should work.    Workflow To perform an offline installation of a driver or the Operator, the following steps should be performed:\n Build an offline bundle Unpacking an offline bundle and preparing for installation Perform either a Helm installation or Operator installation  Building an offline bundle This needs to be performed on a linux system with access to the internet as a git repo will need to be cloned, and container images pulled from public registries.\nThe build an offline bundle, the following steps are needed:\n Perform a git clone of the desired repository. For a helm based install, the specific driver repo should be cloned. For an Operator based deployment, the Dell CSI Operator repo should be cloned Run the csi-offline-bundle.sh script with an argument of -c in order to create an offline bundle   For Helm installs, the csi-offline-bundle.sh script will be found in the dell-csi-helm-installer directory For Operator installs, the csi-offline-bundle.sh script will be found in the scripts directory  The script will perform the following steps:\n Determine required images by parsing either the driver Helm charts (if run from a cloned CSI Driver git repository) or the Dell CSI Operator configuration files (if run from a clone of the Dell CSI Operator repository) Perform an image pull of each image required Save all required images to a file by running docker save or podman save Build a tar.gz file containing the images as well as files required to installer the driver and/or Operator  The resulting offline bundle file can be copied to another machine, if necessary, to gain access to the desired image registry.\nFor example, here is the output of a request to build an offline bundle for the Dell CSI Operator:\n[user@anothersystem /home/user]# git clone https://github.com/dell/dell-csi-operator.git [user@anothersystem /home/user]# cd dell-csi-operator [user@system /home/user/dell-csi-operator]# scripts/csi-offline-bundle.sh -c * * Building image manifest file * * Pulling container images dellemc/csi-isilon:v1.2.0 dellemc/csi-isilon:v1.3.0.000R dellemc/csipowermax-reverseproxy:v1.0.0.000R dellemc/csi-powermax:v1.2.0.000R dellemc/csi-powermax:v1.4.0.000R dellemc/csi-powerstore:v1.1.0.000R dellemc/csi-unity:v1.3.0.000R dellemc/csi-vxflexos:v1.1.5.000R dellemc/csi-vxflexos:v1.2.0.000R dellemc/dell-csi-operator:v1.1.0.000R quay.io/k8scsi/csi-attacher:v2.0.0 quay.io/k8scsi/csi-attacher:v2.2.0 quay.io/k8scsi/csi-node-driver-registrar:v1.2.0 quay.io/k8scsi/csi-provisioner:v1.4.0 quay.io/k8scsi/csi-provisioner:v1.6.0 quay.io/k8scsi/csi-resizer:v0.5.0 quay.io/k8scsi/csi-snapshotter:v2.1.1 * * Saving images * * Copying necessary files /dell/git/dell-csi-operator/config /dell/git/dell-csi-operator/deploy /dell/git/dell-csi-operator/samples /dell/git/dell-csi-operator/scripts /dell/git/dell-csi-operator/README.md /dell/git/dell-csi-operator/LICENSE * * Compressing release dell-csi-operator-bundle/ dell-csi-operator-bundle/samples/ ... \u0026lt;listing of files included in bundle\u0026gt; ... dell-csi-operator-bundle/LICENSE dell-csi-operator-bundle/README.md * * Complete Offline bundle file is: /dell/git/dell-csi-operator/dell-csi-operator-bundle.tar.gz Unpacking an offline bundle and preparing for installation This needs to be performed on a linux system with access to an image registry that will host container images. If the registry requires login, that should be done before proceeding.\nTo prepare for driver or Operator installation, the following steps need to be performed:\n Copy the offline bundle file to a system with access to an image registry available to your Kubernetes/OpenShift cluster Expand the bundle file by running tar xvfz \u0026lt;filename\u0026gt; Run the csi-offline-bundle.sh script and supply the -p option as well as the path to the internal registry with the -r option  The script will then perform the following steps:\n Load the required container images into the local system Tag the images according to the user supplied registry information Push the newly tagged images to the registry Modify the Helm charts or Operator configuration to refer to the newly tagged/pushed images  An example of preparing the bundle for installation (192.168.75.40:5000 refers to a image registry accessible to Kubernetes/OpenShift):\n[user@anothersystem /tmp]# tar xvfz dell-csi-operator-bundle.tar.gz dell-csi-operator-bundle/ dell-csi-operator-bundle/samples/ ... \u0026lt;listing of files included in bundle\u0026gt; ... dell-csi-operator-bundle/LICENSE dell-csi-operator-bundle/README.md [user@anothersystem /tmp]# cd dell-csi-operator-bundle [user@anothersystem /tmp/dell-csi-operator-bundle]# scripts/csi-offline-bundle.sh -p -r 192.168.75.40:5000/operator Preparing a offline bundle for installation * * Loading docker images * * Tagging and pushing images dellemc/csi-isilon:v1.2.0 -\u0026gt; 192.168.75.40:5000/operator/csi-isilon:v1.2.0 dellemc/csi-isilon:v1.3.0.000R -\u0026gt; 192.168.75.40:5000/operator/csi-isilon:v1.3.0.000R dellemc/csipowermax-reverseproxy:v1.0.0.000R -\u0026gt; 192.168.75.40:5000/operator/csipowermax-reverseproxy:v1.0.0.000R dellemc/csi-powermax:v1.2.0.000R -\u0026gt; 192.168.75.40:5000/operator/csi-powermax:v1.2.0.000R dellemc/csi-powermax:v1.4.0.000R -\u0026gt; 192.168.75.40:5000/operator/csi-powermax:v1.4.0.000R dellemc/csi-powerstore:v1.1.0.000R -\u0026gt; 192.168.75.40:5000/operator/csi-powerstore:v1.1.0.000R dellemc/csi-unity:v1.3.0.000R -\u0026gt; 192.168.75.40:5000/operator/csi-unity:v1.3.0.000R dellemc/csi-vxflexos:v1.1.5.000R -\u0026gt; 192.168.75.40:5000/operator/csi-vxflexos:v1.1.5.000R dellemc/csi-vxflexos:v1.2.0.000R -\u0026gt; 192.168.75.40:5000/operator/csi-vxflexos:v1.2.0.000R dellemc/dell-csi-operator:v1.1.0.000R -\u0026gt; 192.168.75.40:5000/operator/dell-csi-operator:v1.1.0.000R quay.io/k8scsi/csi-attacher:v2.0.0 -\u0026gt; 192.168.75.40:5000/operator/csi-attacher:v2.0.0 quay.io/k8scsi/csi-attacher:v2.2.0 -\u0026gt; 192.168.75.40:5000/operator/csi-attacher:v2.2.0 quay.io/k8scsi/csi-node-driver-registrar:v1.2.0 -\u0026gt; 192.168.75.40:5000/operator/csi-node-driver-registrar:v1.2.0 quay.io/k8scsi/csi-provisioner:v1.4.0 -\u0026gt; 192.168.75.40:5000/operator/csi-provisioner:v1.4.0 quay.io/k8scsi/csi-provisioner:v1.6.0 -\u0026gt; 192.168.75.40:5000/operator/csi-provisioner:v1.6.0 quay.io/k8scsi/csi-resizer:v0.5.0 -\u0026gt; 192.168.75.40:5000/operator/csi-resizer:v0.5.0 quay.io/k8scsi/csi-snapshotter:v2.1.1 -\u0026gt; 192.168.75.40:5000/operator/csi-snapshotter:v2.1.1 * * Preparing operator files within /tmp/dell-csi-operator-bundle changing: dellemc/csi-isilon:v1.2.0 -\u0026gt; 192.168.75.40:5000/operator/csi-isilon:v1.2.0 changing: dellemc/csi-isilon:v1.3.0.000R -\u0026gt; 192.168.75.40:5000/operator/csi-isilon:v1.3.0.000R changing: dellemc/csipowermax-reverseproxy:v1.0.0.000R -\u0026gt; 192.168.75.40:5000/operator/csipowermax-reverseproxy:v1.0.0.000R changing: dellemc/csi-powermax:v1.2.0.000R -\u0026gt; 192.168.75.40:5000/operator/csi-powermax:v1.2.0.000R changing: dellemc/csi-powermax:v1.4.0.000R -\u0026gt; 192.168.75.40:5000/operator/csi-powermax:v1.4.0.000R changing: dellemc/csi-powerstore:v1.1.0.000R -\u0026gt; 192.168.75.40:5000/operator/csi-powerstore:v1.1.0.000R changing: dellemc/csi-unity:v1.3.0.000R -\u0026gt; 192.168.75.40:5000/operator/csi-unity:v1.3.0.000R changing: dellemc/csi-vxflexos:v1.1.5.000R -\u0026gt; 192.168.75.40:5000/operator/csi-vxflexos:v1.1.5.000R changing: dellemc/csi-vxflexos:v1.2.0.000R -\u0026gt; 192.168.75.40:5000/operator/csi-vxflexos:v1.2.0.000R changing: dellemc/dell-csi-operator:v1.1.0.000R -\u0026gt; 192.168.75.40:5000/operator/dell-csi-operator:v1.1.0.000R changing: quay.io/k8scsi/csi-attacher:v2.0.0 -\u0026gt; 192.168.75.40:5000/operator/csi-attacher:v2.0.0 changing: quay.io/k8scsi/csi-attacher:v2.2.0 -\u0026gt; 192.168.75.40:5000/operator/csi-attacher:v2.2.0 changing: quay.io/k8scsi/csi-node-driver-registrar:v1.2.0 -\u0026gt; 192.168.75.40:5000/operator/csi-node-driver-registrar:v1.2.0 changing: quay.io/k8scsi/csi-provisioner:v1.4.0 -\u0026gt; 192.168.75.40:5000/operator/csi-provisioner:v1.4.0 changing: quay.io/k8scsi/csi-provisioner:v1.6.0 -\u0026gt; 192.168.75.40:5000/operator/csi-provisioner:v1.6.0 changing: quay.io/k8scsi/csi-resizer:v0.5.0 -\u0026gt; 192.168.75.40:5000/operator/csi-resizer:v0.5.0 changing: quay.io/k8scsi/csi-snapshotter:v2.1.1 -\u0026gt; 192.168.75.40:5000/operator/csi-snapshotter:v2.1.1 * * Complete Perform either a Helm installation or Operator installation Now that the required images have been made available and the Helm Charts/Operator configuration updated, installation can proceed by following the usual installation procedure as documented.\n","excerpt":"The csi-offline-bundle.sh script can be used to create a package usable for offline installation of …","ref":"/ansible-docs/v2/installation/offline/","title":"Offline Installation of Dell EMC CSI Storage Providers"},{"body":"Release Notes - Dell CSI Operator 1.3.0  Note: There will be a delay in certification of Dell CSI Operator 1.3.0 and it will not be available for download from the Red Hat OpenShift certified catalog. The operator will still be available for download from the Red Hat OpenShift Community Catalog soon after the 1.3.0 release.\n New Features/Changes  Added support for OpenShift 4.6, 4.7 with RHEL and CoreOS worker nodes Added support for Upstream Kubernetes cluster v1.18, v1.19, v1.20 Migrated to Operator SDK 1.0 Added support for CSI Ephemeral Inline Volumes Changed driver controller installation from StatefulSet to Deployment Added Support for multiple replicas for driver controller Deployment Added Support for setting volumeBindingMode for Storage Classes Added Support for setting topology keys for Storage Classes  Fixed Issues There are no fixed issues in this release.\nKnown Issues    Issue Workaround     A warning message will be listed in the events for StorageClasses if the driver is not upgraded after an operator upgrade. This happens because of the fix provided by Kubernetes in 1.20 for one of the known issue. StorageClasses will get updated automatically after 45 mins if there is no driver upgrade, after an operator upgrade.    Support The Dell CSI Operator image is available on Dockerhub and is officially supported by Dell EMC. For any CSI operator and driver issues, questions or feedback, join the Dell EMC Container community.\n","excerpt":"Release Notes - Dell CSI Operator 1.3.0  Note: There will be a delay in certification of Dell CSI …","ref":"/ansible-docs/v1/release/operator/","title":"Operator"},{"body":"Release Notes - Dell CSI Operator 1.2.0  Note: There is a delay in Operator 1.2.0 certification hence it will not be visible in Red Hat OpenShift certified catalogue immediately after release on GitHub.\n New Features/Changes  Added support for OpenShift 4.5, 4.6 with RHEL and CoreOS worker nodes Migrated to Operator SDK 1.0 Added support for CSI Ephemeral Inline Volumes Changed driver controller installation from StatefulSet to Deployment Added Support for multiple replicas for driver controller Deployment Added Support for setting volumeBindingMode for Storage Classes Added Support for setting topology keys for Storage Classes  Fixed Issues There are no fixed issues in this release.\nKnown Issues There are no Known issues in this release.\nSupport The Dell CSI Operator image is available on Dockerhub and is officially supported by Dell EMC. For any CSI operator and driver issues, questions or feedback, join the Dell EMC Container community.\n","excerpt":"Release Notes - Dell CSI Operator 1.2.0  Note: There is a delay in Operator 1.2.0 certification …","ref":"/ansible-docs/v2/release/operator/","title":"Operator"},{"body":"Volume Snapshot Feature The Volume Snapshot feature was introduced in alpha (v1alpha1) in Kubernetes 1.13 and then moved to beta (v1beta1) in Kubernetes 1.17 and is generally available (v1) in Kubernetes version 1.20.\nThe CSI PowerFlex driver version 1.4 supports v1beta1 snapshots on Kubernetes 1.18/1.19 and v1 snapshots on Kubernetes 1.20.\nIn order to use Volume Snapshots, ensure the following components are deployed to your cluster:\n Kubernetes Volume Snaphshot CRDs Volume Snapshot Controller  Volume Snapshot Class During the installation of CSI PowerFlex 1.4 driver, a Volume Snapshot Class is created. This is the only Volume Snapshot Class required and there is no need to create any other Volume Snapshot Class.\nFollowing is the manifest for the Volume Snapshot Class created during installation:\n{{- if eq .Values.kubeversion \u0026quot;v1.20\u0026quot; }} apiVersion: snapshot.storage.k8s.io/v1 {{- else }} apiVersion: snapshot.storage.k8s.io/v1beta1 {{- end}} kind: VolumeSnapshotClass metadata: name: vxflexos-snapclass driver: csi-vxflexos.dellemc.com deletionPolicy: Delete Create Volume Snapshot The following is a sample manifest for creating a Volume Snapshot using the v1beta1 snapshot APIs:\napiVersion: snapshot.storage.k8s.io/v1beta1 kind: VolumeSnapshot metadata: name: pvol0-snap1 namespace: helmtest-vxflexos spec: volumeSnapshotClassName: vxflexos-snapclass source: persistentVolumeClaimName: pvol0 Once the VolumeSnapshot is successfully created by the CSI PowerFlex driver, a VolumeSnapshotContent object is automatically created. Once the status of the VolumeSnapshot object has the readyToUse field set to true , it is available for use.\nFollowing is the relevant section of VolumeSnapshot object status:\nstatus: boundVolumeSnapshotContentName: snapcontent-5a8334d2-eb40-4917-83a2-98f238c4bda creationTime: \u0026quot;2020-07-16T08:42:12Z\u0026quot; readyToUse: true Creating PVCs with Volume Snapshots as Source The following is a sample manifest for creating a PVC with a VolumeSnapshot as a source:\napiVersion: v1 kind: PersistentVolumeClaim metadata: name: restorepvc namespace: helmtest-vxflexos spec: storageClassName: vxflexos dataSource: name: pvol0-snap kind: VolumeSnapshot apiGroup: snapshot.storage.k8s.io accessModes: - ReadWriteOnce resources: requests: storage: 8Gi Volume Expansion Feature The CSI PowerFlex driver version 1.2 and later support expansion of Persistent Volumes. This expansion is done online, that is, when PVC is attached to a node.\nTo use this feature, the storage class used to create the PVC must have the attribute allowVolumeExpansion set to true. The storage classes created during the installation (both using Helm or dell-csi-operator) have the allowVolumeExpansion set to true by default.\nIn case you are creating more storage classes, ensure that this attribute is set to true if you wish to expand any Persistent Volumes created using these new storage classes.\nFollowing is a sample manifest for a storage class which allows for Volume Expansion:\napiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: vxflexos-expand annotations: provisioner: csi-vxflexos.dellemc.com reclaimPolicy: Delete allowVolumeExpansion: true parameters: storagepool: pool volumeBindingMode: WaitForFirstConsumer allowedTopologies: - matchLabelExpressions: - key: csi-vxflexos.dellemc.com/sample values: - csi-vxflexos.dellemc.com To resize a PVC, edit the existing PVC spec and set spec.resources.requests.storage to the intended size.\nFor example, if you have a PVC - pvol0 of size 8Gi, then you can resize it to 16 Gi by updating the PVC:\nspec: accessModes: - ReadWriteOnce resources: requests: storage: 16Gi #update from 8Gi storageClassName: vxflexos volumeMode: Filesystem volumeName: k8s-0e50dada status: accessModes: - ReadWriteOnce capacity: storage: 8Gi phase: Bound NOTE: Kubernetes Volume Expansion feature cannot be used to shrink a volume and volumes cannot be expanded to a value that is not a multiple of 8. If attempted, the driver will round up. For example, if the above PVC was edited to have a size of 20 Gb, the size would actually be expanded to 24 Gb, the next highest multiple of 8.\nVolume Cloning Feature The CSI PowerFlex driver version 1.3 and later support volume cloning. This feature allows specifying existing PVCs in the dataSource field to indicate a user would like to clone a Volume.\nThe source PVC must be bound and available (not in use). Source and destination PVC must be in the same namespace and have the same Storage Class.\nTo clone a volume, you should first have an existing pvc, for example, pvol0:\nkind: PersistentVolumeClaim apiVersion: v1 metadata: name: pvol0 namespace: helmtest-vxflexos spec: storageClassName: vxflexos accessModes: - ReadWriteOnce volumeMode: Filesystem resources: requests: storage: 8Gi The following is a sample manifest for cloning pvol0:\napiVersion: v1 kind: PersistentVolumeClaim metadata: name: clonedpvc namespace: helmtest-vxflexos spec: storageClassName: vxflexos dataSource: name: pvol0 kind: PersistentVolumeClaim accessModes: - ReadWriteOnce resources: requests: storage: 8Gi Raw Block Support The CSI PowerFlex driver version 1.2 and later support Raw Block volumes, which are created using the volumeDevices list in the pod template spec with each entry accessing a volumeClaimTemplate specifying a volumeMode: Block.\nFollowing is an example configuration of Raw Block Outline:\nkind: StatefulSet apiVersion: apps/v1 metadata: name: powerflextest namespace: helmtest-vxflexos spec: ... spec: ... containers: - name: test ... volumeDevices: - devicePath: \u0026quot;/dev/data0\u0026quot; name: pvol volumeClaimTemplates: - metadata: name: pvol spec: accessModes: - ReadWriteOnce volumeMode: Block storageClassName: vxflexos resources: requests: storage: 8Gi Allowable access modes are ReadWriteOnce , ReadWriteMany , and for block devices that have been previously initialized, ReadOnlyMany.\nRaw Block volumes are presented as a block device to the pod by using a bind mount to a block device in the node\u0026rsquo;s file system. The driver does not format or check the format of any file system on the block device. Raw Block volumes do support online Volume Expansion, but it is up to the application to manage reconfiguring the file system (if any) to the new size.\nFor additional information, see the Kubernetes Raw Block Volume Support documentation.\nTopology Support The CSI PowerFlex driver version 1.2 and later support Topology which forces volumes to be placed on worker nodes that have connectivity to the backend storage. This covers use cases where:\n The PowerFlex SDC may not be installed or running on some nodes. Users have chosen to restrict the nodes on which the CSI driver is deployed.  This Topology support does not include customer defined topology, users cannot create their own labels for nodes and storage classes and expect the labels to be honored by the driver.\nTopology Usage To utilize the Topology feature, the storage classes are modified to specify the volumeBindingMode as WaitForFirstConsumer and to specify the desired topology labels within allowedTopologies. This ensures that pod scheduling takes advantage of the topology and be guaranteed that the node selected has access to provisioned volumes.\nStorage Class Example with Topology Support:\napiVersion: storage.k8s.io/v1 kind: StorageClass metadata: annotations: meta.helm.sh/release-name: vxflexos meta.helm.sh/release-namespace: vxflexos storageclass.beta.kubernetes.io/is-default-class: \u0026quot;true\u0026quot; creationTimestamp: \u0026quot;2020-05-27T13:24:55Z\u0026quot; labels: app.kubernetes.io/managed-by: Helm name: vxflexos resourceVersion: \u0026quot;170198\u0026quot; selfLink: /apis/storage.k8s.io/v1/storageclasses/vxflexos uid: abb094e6-2c25-42c1-b82e-bd80372e78b parameters: storagepool: pool provisioner: csi-vxflexos.dellemc.com reclaimPolicy: Delete volumeBindingMode: WaitForFirstConsumer allowedTopologies: - matchLabelExpressions: - key: csi-vxflexos.dellemc.com/6c29fd07674c values: - csi-vxflexos.dellemc.com For additional information, see the Kubernetes Topology documentation.\nNOTE: In the manifest file of the Dell CSI operator, topology can be enabled by specifying the system name or systemid in the allowed topologies field. Volumebindingmode is also set to WaitForFirstConsumer by default.\nController HA The CSI PowerFlex driver version 1.3 and later support multiple controller pods. A Controller pod can be assigned to a worker node or a master node, as long as no other controller pod is currently assigned to the node. To control the number of controller pods, edit:\ncontrollerCount: 2 in your values file to the desired number of controller pods. By default, the driver will deploy with two controller pods, each assigned to a different worker node.\n NOTE: If controller count is greater than the number of available nodes, excess controller pods will be stuck in pending state.\n If you are using the Dell CSI Operator, the value to adjust is:\nreplicas: 1 in your driver yaml in config/samples/\nIf you want to specify where controller pods get assigned, make the following edits to your values file (helm install):\nTo assign controller pods to worker nodes only (Default):\n# \u0026quot;controller\u0026quot; allows to configure controller specific parameters controller: #\u0026quot;controller.nodeSelector\u0026quot; defines what nodes would be selected for pods of controller deployment # Leave as blank to use all nodes nodeSelector: # node-role.kubernetes.io/master: \u0026quot;\u0026quot; # \u0026quot;controller.tolerations\u0026quot; defines tolerations that would be applied to controller deployment # Leave as blank to install controller on worker nodes tolerations: # - key: \u0026quot;node-role.kubernetes.io/master\u0026quot; # operator: \u0026quot;Exists\u0026quot; # effect: \u0026quot;NoSchedule\u0026quot; To assign controller pods to master and worker nodes:\n# \u0026quot;controller\u0026quot; allows to configure controller specific parameters controller: #\u0026quot;controller.nodeSelector\u0026quot; defines what nodes would be selected for pods of controller deployment # Leave as blank to use all nodes nodeSelector: # node-role.kubernetes.io/master: \u0026quot;\u0026quot; # \u0026quot;controller.tolerations\u0026quot; defines tolerations that would be applied to controller deployment # Leave as blank to install controller on worker nodes tolerations: - key: \u0026quot;node-role.kubernetes.io/master\u0026quot; operator: \u0026quot;Exists\u0026quot; effect: \u0026quot;NoSchedule\u0026quot; To assign controller pods to master nodes only:\n# \u0026quot;controller\u0026quot; allows to configure controller specific parameters controller: #\u0026quot;controller.nodeSelector\u0026quot; defines what nodes would be selected for pods of controller deployment # Leave as blank to use all nodes nodeSelector: node-role.kubernetes.io/master: \u0026quot;\u0026quot; # \u0026quot;controller.tolerations\u0026quot; defines tolerations that would be applied to controller deployment # Leave as blank to install controller on worker nodes tolerations: - key: \u0026quot;node-role.kubernetes.io/master\u0026quot; operator: \u0026quot;Exists\u0026quot; effect: \u0026quot;NoSchedule\u0026quot; For configuring Controller HA on the Dell CSI Operator, please refer to the Dell CSI Operator documentation.\nSDC Deployment The CSI PowerFlex driver version 1.3 and later support the automatic deployment of the PowerFlex SDC on Kubernetes nodes which run node portion of CSI driver. The deployment of the SDC kernel module occurs on these nodes with OS platform which support automatic SDC deployment, currently Fedora CoreOS (FCOS) and Red Hat CoreOS (RHCOS). On Kubernetes nodes with OS version not supported by automatic install, you must perform the Manual SDC Deployment steps below. Refer https://hub.docker.com/r/dellemc/sdc for your OS versions.\n On Kubernetes nodes which run node portion of CSI driver, the SDC init container runs prior to the driver being installed. It installs the SDC kernel module on the nodes with OS version which supports automatic SDC deployment . If there is a SDC kernel module installed then the version is checked and updated. Optionally, if the SDC monitor is enabled, another container is started and runs as the monitor. Follow PowerFlex SDC documentation to get monitor metrics. On nodes which do not support automatic SDC deployment by SDC init container, manuall installation steps must be followed. The SDC init container skips installing and you can see this mentioned in the logs by running kubectl logs on the node for SDC. Refer https://hub.docker.com/r/dellemc/sdc for supported OS versions. There is no automated uninstall of SDC kernel module. Follow PowerFlex SDC documentation to manually uninstall the SDC driver from node.  Multiarray Support The CSI PowerFlex driver version 1.4 adds support for managing multiple PowerFlex arrays from the single driver instance. This feature is enabled by default and integrated to even single instance installations.\nTo manage multiple arrays you need to create an array connection configuration that lists multiple arrays.\nCreating array configuration There is a sample json file under the top directory named config.json with the following content:\n[ { \u0026#34;username\u0026#34;: \u0026#34;admin\u0026#34;, # username for connecting to API \u0026#34;password\u0026#34;: \u0026#34;password\u0026#34;, # password for connecting to API \u0026#34;systemID\u0026#34;: \u0026#34;ID1\u0026#34;,\t# system ID for system \u0026#34;endpoint\u0026#34;: \u0026#34;http://127.0.0.1\u0026#34;, # full URL path to the PowerFlex API \u0026#34;insecure\u0026#34;: true, # use insecure connection or not \u0026#34;isDefault\u0026#34;: true, # treat current array as default (would be used by storage class without arrayIP parameter) \u0026#34;mdm\u0026#34;: \u0026#34;10.0.0.1,10.0.0.2\u0026#34; # MDM IP for the system }, { \u0026#34;username\u0026#34;: \u0026#34;admin\u0026#34;, \u0026#34;password\u0026#34;: \u0026#34;password\u0026#34;, \u0026#34;systemID\u0026#34;: \u0026#34;ID2\u0026#34;, \u0026#34;endpoint\u0026#34;: \u0026#34;https://127.0.0.2\u0026#34;, \u0026#34;insecure\u0026#34;: true, \u0026#34;mdm\u0026#34;: \u0026#34;10.0.0.3,10.0.0.4\u0026#34; } ] Here we specify that we want CSI driver to manage two arrays: one with an IP 127.0.0.1 and the other with an IP 127.0.0.2.\nTo use this config we need to create a Kubernetes secret from it. To do so run the following command:\nkubectl create secret generic vxflexos-config -n vxflexos --from-file=config=config.json\nCreating storage classes To be able to provision Kubernetes volumes using a specific array we need to create corresponding storage classes.\nFind the sample yaml files under helm/samples/storageclass. Edit storageclass.yaml if you want ext4 filesystem, and use storageclass-xfs.yaml if you want xfs filesystem. Replace \u0026lt;STORAGE_POOL\u0026gt; with the storage pool you have, and replace \u0026lt;SYSTEM_ID\u0026gt; with the system ID you have.\nThen we need to apply storage classes to Kubernetes using kubectl:\nkubectl create -f storageclass.yaml After that, you can use the storage class for the corresponding array.\nEphemeral Inline Volume The CSI PowerFlex driver version 1.4 supports ephemeral inline CSI volumes. This feature allows CSI volumes to be specified directly in the pod specification.\nAt runtime, nested inline volumes follow the ephemeral lifecycle of their associated pods where the driver handles all phases of volume operations as pods are created and destroyed.\nThe following is a sample manifest (found in csi-vxflexos/test/helm/ephemeral) for creating ephemeral volume in pod manifest with CSI PowerFlex driver.\nkind:PodapiVersion:v1metadata:name:my-csi-app-inline-volumesspec:containers:- name:my-frontendimage:busyboxcommand:[\u0026#34;sleep\u0026#34;,\u0026#34;100000\u0026#34;]volumeMounts:- mountPath:\u0026#34;/data0\u0026#34;name:my-csi-volume- mountPath:\u0026#34;/data1\u0026#34;name:my-csi-volume-xfsvolumes:- name:my-csi-volumecsi:driver:csi-vxflexos.dellemc.comfsType:\u0026#34;ext4\u0026#34;volumeAttributes:volumeName:\u0026#34;my-csi-volume\u0026#34;size:\u0026#34;8Gi\u0026#34;storagepool:samplesystemID:sample- name:my-csi-volume-xfscsi:driver:csi-vxflexos.dellemc.comfsType:\u0026#34;xfs\u0026#34;volumeAttributes:volumeName:\u0026#34;my-csi-volume-xfs\u0026#34;size:\u0026#34;10Gi\u0026#34;storagepool:samplesystemID:sampleThis manifest will create a pod and attach two newly created ephemeral inline csi volumes to it, one ext4 and the other xfs.\nTo run the corresponding helm test, go to csi-vxflexos/test/helm/ephemeral and fill in the values for storagepool and systemID in sample.yaml.\nThen run:\n./testEphemeral.sh this test will deploy the pod with two ephemeral volumes, and write some data to them before deleting the pod.\nWhen creating ephemeral volumes, it is important to specify the following within the volumeAttributes section: volumeName, size, storagepool, and if you want to use a non-default array, systemID.\n","excerpt":"Volume Snapshot Feature The Volume Snapshot feature was introduced in alpha (v1alpha1) in Kubernetes …","ref":"/ansible-docs/v1/features/powerflex/","title":"PowerFlex"},{"body":"The CSI Driver for Dell EMC PowerFlex can be deployed by using the provided Helm v3 charts and installation scripts on both Kubernetes and OpenShift platforms. For more detailed information on the installation scripts, please review the script documentation.\nThe controller section of the Helm chart installs the following components in a Deployment in the namespace vxflexos:\n CSI Driver for Dell EMC PowerFlex Kubernetes External Provisioner, which provisions the volumes Kubernetes External Attacher, which attaches the volumes to the containers Kubernetes External Snapshotter, which provides snapshot support Kubernetes External Resizer, which resizes the volume  The node section of the Helm chart installs the following component in a DaemonSet in the namespace vxflexos:\n CSI Driver for Dell EMC PowerFlex Kubernetes Node Registrar, which handles the driver registration  Prerequisites The following are requirements that must be met before installing the CSI Driver for Dell EMC PowerFlex:\n Install Kubernetes or OpenShift (see supported versions) Install Helm 3 Enable Zero Padding on PowerFlex Configure Mount propagation on container runtime (example: Docker) Install PowerFlex Storage Data Client Volume Snapshot requirements A user must exist on the array with a role \u0026gt;= FrontEndConfigure  Install Helm 3.0 Install Helm 3.0 on the master node before you install the CSI Driver for Dell EMC PowerFlex.\nSteps\nRun the curl https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3 | bash command to install Helm 3.0.\nEnable Zero Padding on PowerFlex Verify that zero padding is enabled on the PowerFlex storage pools that will be used. Use PowerFlex GUI or the PowerFlex CLI to check this setting. See Dell EMC PowerFlex documentation for more information to configure this setting.\nConfigure Mount Propagation on Container Runtime It is required to configure mount propagation on your container runtime on all Kubernetes nodes before installing the CSI Driver for Dell EMC PowerFlex. The following is instruction on how to do this with Docker. If you use another container runtime, follow the recommended instructions from the vendor to configure mount propagation.\nSteps\n The service section of /etc/systemd/system/multi-user.target.wants/docker.service needs to be edited in a few places. First, the Requires entry under the [Unit] header needs have docker.service added to it, as shown. Second, MountFlags=shared needs to be added under the [Service] header. [Unit] ... Requires=docker.socket containerd.service docker.service [Service] ... MountFlags=shared  Restart the docker service with systemctl daemon-reload and systemctl restart docker on all the nodes.  NOTE: Some distribution, like Ubuntu, already has MountFlags set by default.\nInstall PowerFlex Storage Data Client The CSI Driver for PowerFlex requires you to have installed the PowerFlex Storage Data Client (SDC) on all Kubernetes nodes which run node portion of CSI driver. SDC could be installed automatically by CSI driver install on Kubernetes nodes with OS platform which support automatic SDC deployment, currently Fedora CoreOS (FCOS) and Red Hat CoreOS (RHCOS). On Kubernetes nodes with OS version not supported by automatic install, you must perform the Manual SDC Deployment steps below. Refer https://hub.docker.com/r/dellemc/sdc for supported OS versions.\nSDC Deployment The CSI Driver for PowerFlex requires you to have installed the PowerFlex Storage Data Client (SDC) on all Kubernetes nodes which run node portion of CSI driver. SDC could be installed automatically by CSI driver install on Kubernetes nodes with OS platform which support automatic SDC deployment, currently Fedora CoreOS (FCOS) and Red Hat CoreOS (RHCOS).\nOn Kubernetes nodes with OS version not supported by automatic install, you must perform the Manual SDC Deployment steps below. Refer https://hub.docker.com/r/dellemc/sdc for your OS versions.\nOptional: For a typical install, you will pull SDC kernel modules from the Dell EMC ftp site, which is setup by default. Some users might want to mirror this repository to a local location. The PowerFlex KB article (https://www.dell.com/support/kbdoc/en-us/000184206/how-to-use-a-private-repository-for) has instructions on how to do this.\nManual SDC Deployment For detailed PowerFlex installation procedure, see the Dell EMC PowerFlex Deployment Guide. Install the PowerFlex SDC as follows:\nSteps\n Download the PowerFlex SDC from Dell EMC Online support. The filename is EMC-ScaleIO-sdc-*.rpm, where * is the SDC name corresponding to the PowerFlex installation version. Export the shell variable MDM_IP in a comma-separated list using export MDM_IP=xx.xxx.xx.xx,xx.xxx.xx.xx, where xxx represents the actual IP address in your environment. This list contains the IP addresses of the MDMs. Install the SDC per the Dell EMC PowerFlex Deployment Guide:  For Red Hat Enterprise Linux and Cent OS, run rpm -iv ./EMC-ScaleIO-sdc-*.x86_64.rpm, where * is the SDC name corresponding to the PowerFlex installation version.   To add more MDM_IP for multi-array support, run /opt/emc/scaleio/sdc/bin/drv_cfg --add_mdm --ip 10.xx.xx.xx.xx,10.xx.xx.xx  Volume Snapshot Requirements Volume Snapshot CRD\u0026rsquo;s The Kubernetes Volume Snapshot CRDs can be obtained and installed from the external-snapshotter project on Github.\n If on Kubernetes 1.18/1.19 (beta snapshots) use v3.0.3 If on Kubernetes 1.20 (v1 snapshots) use v4.0.0  Volume Snapshot Controller Starting with beta Volume Snapshots in Kubernetes 1.17, the CSI external-snapshotter sidecar is split into two controllers:\n A common snapshot controller A CSI external-snapshotter sidecar  The common snapshot controller must be installed only once in the cluster irrespective of the number of CSI drivers installed in the cluster. On OpenShift clusters 4.4 and later, the common snapshot-controller is pre-installed. In the clusters where it is not present, it can be installed using kubectl and the manifests are available:\n If on Kubernetes 1.18/1.19 (beta snapshots) use v3.0.3 If on Kubernetes 1.20 (v1 snapshots) use v4.0.0  NOTE:\n The manifests available on GitHub install the snapshotter image:  quay.io/k8scsi/csi-snapshotter:v3.0.3 quay.io/k8scsi/csi-snapshotter:v4.0.0   The CSI external-snapshotter sidecar is still installed along with the driver and does not involve any extra configuration.  Install the Driver Steps\n  Run git clone https://github.com/dell/csi-powerflex.git to clone the git repository.\n  Ensure that you have created namespace where you want to install the driver. You can run kubectl create namespace vxflexos to create a new one.\n  Check helm/csi-vxflexos/driver-image.yaml and confirm the driver image points to new image.\n  Collect information from the PowerFlex SDC by executing the get_vxflexos_info.sh script located in the top-level helm directory. This script shows the VxFlex OS system ID and MDM IP addresses. Make a note of the value for these parameters as they must be entered in the config.json file in the top-level directory.\n  Prepare the config.json for driver configuration. The following table lists driver configuration parameters for multiple storage arrays.\n   Parameter Description Required Default     username Username for accessing PowerFlex system true -   password Password for accessing PowerFlex system true -   systemID System name/ID of PowerFlex system true -   endpoint REST API gateway HTTPS endpoint for PowerFlex system true -   insecure Determines if the driver is going to validate certs while connecting to PowerFlex REST API interface true true   isDefault An array having isDefault=true is for backward compatibility. This parameter should occur once in the list false false   mdm mdm defines the MDM(s) that SDC should register with on start. This should be an list of MDM IP addresses or hostnames separated by comma true -    Example: config.json\n[ { \u0026#34;username\u0026#34;: \u0026#34;admin\u0026#34;, \u0026#34;password\u0026#34;: \u0026#34;password\u0026#34;, \u0026#34;systemID\u0026#34;: \u0026#34;ID1\u0026#34;, \u0026#34;endpoint\u0026#34;: \u0026#34;http://127.0.0.1\u0026#34;, \u0026#34;insecure\u0026#34;: true, \u0026#34;isDefault\u0026#34;: true, \u0026#34;mdm\u0026#34;: \u0026#34;10.0.0.1,10.0.0.2\u0026#34; }, { \u0026#34;username\u0026#34;: \u0026#34;admin\u0026#34;, \u0026#34;password\u0026#34;: \u0026#34;password\u0026#34;, \u0026#34;systemID\u0026#34;: \u0026#34;ID2\u0026#34;, \u0026#34;endpoint\u0026#34;: \u0026#34;https://127.0.0.2\u0026#34;, \u0026#34;insecure\u0026#34;: true, \u0026#34;mdm\u0026#34;: \u0026#34;10.0.0.3,10.0.0.4\u0026#34; } ] After editing the file, run the following command to create a secret called vxflexos-config kubectl create secret generic vxflexos-config -n vxflexos --from-file=config=config.json\nUse the following command to replace or update the secret:\nkubectl create secret generic vxflexos-config -n vxflexos --from-file=config=config.json -o yaml --dry-run=client | kubectl replace -f -\nNOTE:\n The user needs to validate the JSON syntax and array related key/values while replacing the vxflexos-creds secret. If you update the secret, you will have to reinstall the driver. System ID, MDM configuration etc. now are taken directly from config.json, and no longer the values file.    If using automated SDC deployment:\n Check the SDC container image is the correct version for your version of PowerFlex.    Copy the default values.yaml file cd helm \u0026amp;\u0026amp; cp csi-vxflexos/values.yaml myvalues.yaml\n  Edit the newly created values file and provide values for the following parameters vi myvalues.yaml:\n     Parameter Description Required Default     volumeNamePrefix Set so that volumes created by the driver have a default prefix. If one PowerFlex/VxFlex OS system is servicing several different Kubernetes installations or users, these prefixes help you distinguish them. No \u0026ldquo;k8s\u0026rdquo;   controllerCount Set to deploy multiple controller instances. If controller count is greater than the number of available nodes, excess pods will be left in pending state. You can increase number of available nodes by configuring the \u0026ldquo;controller\u0026rdquo; section in your values.yaml. For more details on the new controller pod configurations, see the Features section for Powerflex specifics. Yes 2   enablelistvolumesnapshot Set to have snapshots included in the CSI operation ListVolumes. Disabled by default. No FALSE   allowRWOMultiPodAccess Setting allowRWOMultiPodAccess to \u0026ldquo;true\u0026rdquo; will allow multiple pods on the same node to access the same RWO volume. This behavior conflicts with the CSI specification version 1.3. NodePublishVolume description that requires an error to be returned in this case. However some other CSI drivers support this behavior and some customers desire this behavior. Customers use this option at their own risk. No FALSE   controller This section allows configuration of controller specific parameters. To maximize the number of available nodes for controller pods, see this section. For more details on the new controller pod configurations, see the Features section for Powerflex specifics. - -   nodeSelector Defines what nodes would be selected for pods of controller deployment. Leave as blank to use all nodes. Uncomment this section to deploy on master nodes exclusively. No \u0026quot; \u0026quot;   tolerations Defines tolerations that would be applied to controller deployment. Leave as blank to install controller on worker nodes only. If deploying on master nodes is desired, uncomment out this section. No \u0026quot; \u0026quot;   monitor This section allows configuration of the SDC monitoring pod. - -   enabled Set to enable the usage of the monitoring pod. No FALSE   hostNetwork Set whether the monitor pod should run on the host network or not. No TRUE   hostPID Set whether the monitor pod should run in the host namespace or not. No TRUE   podmon Podmon is an optional feature under development and tech preview. Enable this feature only after contact support for additional information - -   enabled  No FALSE    Install the driver using csi-install.sh bash script by running cd ../dell-csi-helm-installer \u0026amp;\u0026amp; ./csi-install.sh --namespace vxflexos --values ../helm/myvalues.yaml  NOTE:\n For detailed instructions on how to run the install scripts, refer to the README.md in the dell-csi-helm-installer folder. This script runs verify-csi-vxflexos.sh script that is present in the same directory. It will validate MDM IP(s) in vxflexos-config secret and creates a new field consumed by init container and sdc-monitor container This script also runs the verify.sh script. You will be prompted to enter the credentials for each of the Kubernetes nodes. The verify.sh script needs the credentials to check if SDC has been configured on all nodes. It is mandatory to run the first installation and installation after changes to MDM configuration in vxflexos-config secret without skipping the verification. After that you can use --skip-verify-node or --skip-verify . (Optional) Enable additional Mount Options - A user is able to specify additional mount options as needed for the driver.  Mount options are specified in storageclass yaml under mountOptions. WARNING: Before utilizing mount options, you must first be fully aware of the potential impact and understand your environment\u0026rsquo;s requirements for the specified option.    Storage Classes Starting in CSI PowerFlex v1.4, dell-csi-helm-installer will not create any storage classes as part of the driver installation. A wide set of annotated storage class manifests have been provided in the helm/samples folder. Please use these samples to create new storage classes to provision storage. See this note for the driving reason behind this change.\nWhat happens to my existing storage classes? Upgrading from CSI PowerFlex v1.4 driver The storage classes created as part of the installation have an annotation - \u0026ldquo;helm.sh/resource-policy\u0026rdquo;: keep set. This ensures that even after an uninstall or upgrade, the storage classes are not deleted. You can continue using these storage classes if you wish so.\nUpgrading from an older version of the driver The storage classes will be deleted if you upgrade the driver. If you wish to continue using those storage classes, you can patch them and apply the annotation \u0026ldquo;helm.sh/resource-policy\u0026rdquo;: keep before performing an upgrade.\nNOTE: If you continue to use the old storage classes, you may not be able to take advantage of any new storage class parameter supported by the driver.\nSteps to create storage class: There are samples storage class yaml files available under helm/samples/storageclass. These can be copied and modified as needed.\n Edit storageclass.yaml if you need ext4 filesystem and storageclass-xfs.yaml if you want xfs filesystem Replace \u0026lt;STORAGE_POOL\u0026gt; with the storage pool you have Replace \u0026lt;SYSTEM_ID\u0026gt; with the system ID you have. Note there are two appearances in the file Edit storageclass.kubernetes.io/is-default-class to true if you want to set it as default, otherwise false. Save the file and create it by using kubectl create -f storageclass.yaml or kubectl create -f storageclass-xfs.yaml  NOTE:\n At least one storage class is required for one array. If you uninstall the driver and reinstall it, you can still face errors if any update in the values.yaml file leads to an update of the storage class(es):   Error: cannot patch \u0026quot;\u0026lt;sc-name\u0026gt;\u0026quot; with kind StorageClass: StorageClass.storage.k8s.io \u0026quot;\u0026lt;sc-name\u0026gt;\u0026quot; is invalid: parameters: Forbidden: updates to parameters are forbidden In case you want to make such updates, ensure to delete the existing storage classes using the kubectl delete storageclass command.\nDeleting a storage class has no impact on a running Pod with mounted PVCs. You will not be able to provision new PVCs until at least one storage class is newly created.\n","excerpt":"The CSI Driver for Dell EMC PowerFlex can be deployed by using the provided Helm v3 charts and …","ref":"/ansible-docs/v1/installation/helm/powerflex/","title":"PowerFlex"},{"body":"Installing PowerFlex CSI Driver via Operator The CSI Driver for Dell EMC PowerFlex v1.4.0 can be installed via the Dell CSI Operator.\nTo deploy the Operator, follow the instructions available here.\nThere are sample manifests provided which can be edited to do an easy installation of the driver. Note that the deployment of the driver using the operator does not use any Helm charts. The installation and configuration parameters will be slightly different from the ones specified via the Helm installer.\nKubernetes Operators make it easy to deploy and manage entire lifecycle of complex Kubernetes applications. Operators use Custom Resource Definitions (CRD) which represents the application and use custom controllers to manage them.\nPrerequisites: SDC Deployment for Operator  This feature deploys the sdc kernel modules on all nodes with the help of an init container. For non supported verisons of the OS also do the manual SDC deployment steps given below. Refer https://hub.docker.com/r/dellemc/sdc for supported versions. Note: When driver is created, MDM value for initContainers in driver CR is set by operator from mdm attributes in driver configuration file, config.json. Example of config.json is below in this document. Do not set MDM value for initContainers in driver CR file manually. Note: To use a sdc-binary module from customer ftp site:  Create a secret, sdc-repo-secret.yaml to contain the credentials for the private repo. To generate the base64 encoding of a credential:    echo-n\u0026lt;credential\u0026gt;|base64-isecret sample to use:\napiVersion:v1kind:Secretmetadata:name:sdc-repo-credsnamespace:vxflexostype:Opaquedata:# set username to the base64 encoded username, sdc default isusername:\u0026lt;usernameinbase64\u0026gt; # set password to the base64 encoded password, sdc default ispassword:\u0026lt;passwordinbase64\u0026gt; Create secret for FTP side by using the command kubectl create -f sdc-repo-secret.yaml. Optionally, enable sdc monitor by uncommenting the section for sidecar in manifest yaml.  Example CR: config/samples/vxflex_v140_ops_46.yaml sideCars:# Uncomment the following section if you want to run the monitoring sidecar- name:sdc-monitorenvs:- name:HOST_PIDvalue:\u0026#34;1\u0026#34;- name:MDMvalue:\u0026#34;10.xx.xx.xx,10.xx.xx.xx\u0026#34;initContainers:- image:dellemc/sdc:3.5.1.1imagePullPolicy:IfNotPresentname:sdcenvs:- name:MDMvalue:\u0026#34;\u0026#34;Manual SDC Deployment For detailed PowerFlex installation procedure, see the Dell EMC PowerFlex Deployment Guide. Install the PowerFlex SDC as follows:\nSteps\n Download the PowerFlex SDC from Dell EMC Online support. The filename is EMC-ScaleIO-sdc-*.rpm, where * is the SDC name corresponding to the PowerFlex installation version. Export the shell variable MDM_IP in a comma-separated list using export MDM_IP=xx.xxx.xx.xx,xx.xxx.xx.xx, where xxx represents the actual IP address in your environment. This list contains the IP addresses of the MDMs. Install the SDC per the Dell EMC PowerFlex Deployment Guide:  For Red Hat Enterprise Linux and Cent OS, run rpm -iv ./EMC-ScaleIO-sdc-*.x86_64.rpm, where * is the SDC name corresponding to the PowerFlex installation version.   To add more MDM_IP for multi-array support, run /opt/emc/scaleio/sdc/bin/drv_cfg --add_mdm --ip 10.xx.xx.xx.xx,10.xx.xx.xx  Install Driver   Create namespace: Run kubectl create namespace \u0026lt;driver-namespace\u0026gt; command using the desired name to create the namespace.\n  Prepare the config.json for driver configuration. The following table lists driver configuration parameters for multiple storage arrays.\n   Parameter Description Required Default     username Username for accessing PowerFlex system true -   password Password for accessing PowerFlex system true -   systemID System name/ID of PowerFlex system true -   endpoint REST API gateway HTTPS endpoint for PowerFlex system true -   insecure Determines if the driver is going to validate certs while connecting to PowerFlex REST API interface true true   isDefault An array having isDefault=true is for backward compatibility. This parameter should occur once in the list false false   mdm mdm defines the MDM(s) that SDC should register with on start. This should be an list of MDM IP addresses or hostnames separated by comma true -    Example: config.json\n[ { \u0026#34;username\u0026#34;: \u0026#34;admin\u0026#34;, \u0026#34;password\u0026#34;: \u0026#34;password\u0026#34;, \u0026#34;systemID\u0026#34;: \u0026#34;ID1\u0026#34;, \u0026#34;endpoint\u0026#34;: \u0026#34;http://127.0.0.1\u0026#34;, \u0026#34;insecure\u0026#34;: true, \u0026#34;isDefault\u0026#34;: true, \u0026#34;mdm\u0026#34;: \u0026#34;10.0.0.1,10.0.0.2\u0026#34; }, { \u0026#34;username\u0026#34;: \u0026#34;admin\u0026#34;, \u0026#34;password\u0026#34;: \u0026#34;password\u0026#34;, \u0026#34;systemID\u0026#34;: \u0026#34;ID2\u0026#34;, \u0026#34;endpoint\u0026#34;: \u0026#34;https://127.0.0.2\u0026#34;, \u0026#34;insecure\u0026#34;: true, \u0026#34;mdm\u0026#34;: \u0026#34;10.0.0.3,10.0.0.4\u0026#34; } ] After editing the file, run the following command to create a secret called vxflexos-config kubectl create secret generic vxflexos-config -n \u0026lt;driver-namespace\u0026gt; --from-file=config=config.json\nUse the following command to replace or update the secret:\nkubectl create secret generic vxflexos-config -n \u0026lt;driver-namespace\u0026gt; --from-file=config=config.json -o yaml --dry-run=client | kubectl replace -f -\nNote:\n The user needs to validate the JSON syntax and array related key/values while replacing the vxflexos-creds secret. If you update the secret, you must reinstall the driver. System ID, MDM configuration etc. now are taken directly from config.json. MDM provided in the input_sample_file.yaml will be overided with MDM values in config.json.    Create a Custom Resource (CR) for PowerFlex using the sample files provided here .\n  Users should configure the parameters in CR. The following table lists the primary configurable parameters of the PowerFlex driver and their default values:\n   Parameter Description Required Default     replicas Controls the amount of controller pods you deploy. If the number of controller pods are greater than number of available nodes, excess pods will become stay in a pending state. Defaults is 2 which allows for Controller high availability. Yes 2   Common parameters for node and controller      X_CSI_VXFLEXOS_ENABLELISTVOLUMESNAPSHOT Enable list volume operation to include snapshots (since creating a volume from a snap actually results in a new snap) No false   X_CSI_VXFLEXOS_ENABLESNAPSHOTCGDELETE Enable this to automatically delete all snapshots in a consistency group when a snap in the group is deleted No false   X_CSI_DEBUG To enable debug mode No false   X_CSI_ALLOW_RWO_MULTI_POD_ACCESS Setting allowRWOMultiPodAccess to \u0026ldquo;true\u0026rdquo; will allow multiple pods on the same node to access the same RWO volume. This behavior conflicts with the CSI specification version 1.3. NodePublishVolume description that requires an error to be returned in this case. However, some other CSI drivers support this behavior and some customers desire this behavior. Customers use this option at their own risk. No false   StorageClass parameters      storagePool Defines the PowerFlex storage pool from which this driver will provision volumes. You must set this for the primary storage pool to be used Yes pool1   allowVolumeExpansion Once the allowed topology is modified in storage class, pods/and volumes will always be scheduled on nodes that have access to the storage No true   allowedTopologies:key This is to enable topology to allow pods/and volumes to always be scheduled on nodes that have access to the storage. You need to replace the X_CSI_VXFLEXOS_SYSTEMNAME in the key with the actual systemname value No X_CSI_VXFLEXOS_SYSTEMNAME   initContainers:value Set the MDM IP\u0026rsquo;s here if installing on CoreOS to enable automatic SDC installation Yes (OpenShift) \u0026ldquo;10.xx.xx.xx,10.xx.xx.xx\u0026rdquo;      Execute the kubectl create -f \u0026lt;input_sample_file.yaml\u0026gt; command to create PowerFlex custom resource. This command will deploy the CSI-PowerFlex driver.\n  ","excerpt":"Installing PowerFlex CSI Driver via Operator The CSI Driver for Dell EMC PowerFlex v1.4.0 can be …","ref":"/ansible-docs/v1/installation/operator/powerflex/","title":"PowerFlex"},{"body":"This section provides multiple methods to test driver functionality in your environment.\nNote: To run the test for CSI Driver for Dell EMC PowerFlex, install Helm 3.\nTest deploying a simple pod with PowerFlex storage Test the deployment workflow of a simple pod on PowerFlex storage.\nPrerequisites\nIn the source code, there is a directory that contains examples of how you can use the driver. To use these examples, you must create a helmtest-vxflexos namespace, using kubectl create namespace helmtest-vxflexos, before you can start testing. HELM 3 must be installed to perform the tests.\nThe starttest.sh script is located in the csi-vxflexos/test/helm directory. This script is used in the following procedure to deploy helm charts that test the deployment of a simple pod.\nSteps\n Navigate to the test/helm directory, which contains the starttest.sh and the 2vols directories. This directory contains a simple Helm chart that will deploy a pod that uses two PowerFlex volumes. NOTE: Helm tests are designed assuming users are using the default storageclass names (vxflexos and vxflexos-xfs). If your storageclass names differ from the default values, such as when deploying with the Dell CSI Operator, please update the templates in 2vols accordingly (located in test/helm/2vols/templates directory). You can use kubectl get sc to check for the storageclass names. Run sh starttest.sh 2vols to deploy the pod. You should see the following:  Normal Pulled 38s kubelet, k8s113a-10-247-102-215.lss.emc.com Successfully pulled image \u0026quot;docker.io/centos:latest\u0026quot; Normal Created 38s kubelet, k8s113a-10-247-102-215.lss.emc.com Created container Normal Started 38s kubelet, k8s113a-10-247-102-215.lss.emc.com Started container /dev/scinib 8125880 36852 7653216 1% /data /dev/scinia 16766976 32944 16734032 1% /data /dev/scinib on /data0 type ext4 (rw,relatime,data=ordered) /dev/scinia on /data1 type xfs (rw,relatime,attr2,inode64,noquota) To stop the test, run sh stoptest.sh 2vols. This script deletes the pods and the volumes depending on the retention setting you have configured.  Results\nAn outline of this workflow is described below:\n The 2vols helm chart contains two PersistentVolumeClaim definitions, one in pvc0.yaml , and the other in pvc1.yaml. They are referenced by the test.yaml which creates the pod. The contents of the Pvc0.yaml file are described below:  kind:PersistentVolumeClaimapiVersion:v1metadata:name:pvolnamespace:helmtest-vxflexosspec:accessModes:- ReadWriteOncevolumeMode:Filesystemresources:requests:storage:8GistorageClassName:vxflexosThe volumeMode: Filesystem requires a mounted file system, and the resources.requests.storage of 8Gi requires an 8 GB file. In this case, the storageClassName: vxflexos directs the system to use one of the pre-defined storage classes created by the CSI Driver for Dell EMC PowerFlex installation process. This step yields a mounted ext4 file system. You can see the storage class definitions in the PowerFlex installation helm chart files storageclass.yaml and storageclass-xfs.yaml. If you compare pvol0.yaml and pvol1.yaml , you will find that the latter uses a different storage class; vxflexos-xfs. This class gives you an xfs file system. To see the volumes you created, run kubectl get persistentvolumeclaim –n helmtest-vxflexos and kubectl describe persistentvolumeclaim –n helmtest-vxflexos. NOTE: For more information about Kubernetes objects like StatefulSet and PersistentVolumeClaim see Kubernetes documentation: Concepts.  Test creating snapshots Test the workflow for snapshot creation.\nSteps\n Start the 2vols container and leave it running.  Helm tests are designed assuming users are using the default storageclass names (vxflexos and vxflexos-xfs). If your storageclass names differ from the default values, such as when deploying with the Operator, update the templates in 2vols accordingly (located in test/helm/2vols/templates directory). You can use kubectl get sc to check for the storageclass names. Helm tests are designed assuming users are using the default snapshotclass name. If your snapshotclass names differ from the default values, update snap1.yaml and snap2.yaml accordingly.   Run sh snaptest.sh to start the test.  This will create a snapshot of each of the volumes in the container using VolumeSnapshot objects defined in snap1.yaml and snap2.yaml. The following are the contents of snap1.yaml:\napiVersion:snapshot.storage.k8s.io/v1alpha1kind:VolumeSnapshotmetadata:name:pvol0-snapnamespace:helmtest-vxflexosspec:snapshotClassName:vxflexos-snapclasssource:name:pvolkind:PersistentVolumeClaimResults\nThe snaptest.sh script will create a snapshot using the definitions in the snap1.yaml file. The spec.source section contains the volume that will be snapped. For example, if the volume to be snapped is pvol0 , then the created snapshot is named pvol0-snap.\nNOTE: The snaptest.sh shell script creates the snapshots, describes them, and then deletes them. You can see your snapshots using kubectl get volumesnapshot -n test.\nNotice that this VolumeSnapshot class has a reference to a snapshotClassName: vxflexos-snapclass. The CSI Driver for Dell EMC PowerFlex installation creates this class as its default snapshot class. You can see its definition in the installation directory file volumesnapshotclass.yaml.\nTest restoring from a snapshot Test the restore operation workflow to restore from a snapshot.\nPrerequisites\nEnsure that you have stopped any previous test instance before performing this procedure.\nSteps\n Run sh snaprestoretest.sh to start the test.  This script deploys the 2vols example, creates a snap of pvol0, and then updates the deployed helm chart from the updateddirectory 2vols+restore. This then adds an additional volume that is created from the snapshot.\nNOTE:\n Helm tests are designed assuming users are using the default storageclass names (vxflexos and vxflexos-xfs). If your storageclass names differ from the default values, such as when deploying with the Dell CSI Operator, update the templates for snap restore tests accordingly (located in test/helm/2vols+restore/template directory). You can use kubectl get sc to check for the storageclass names. Helm tests are designed assuming users are using the default snapshotclass name. If your snapshotclass names differ from the default values, update snap1.yaml and snap2.yaml accordingly.  Results\nAn outline of this workflow is described below:\n The snapshot is taken using snap1.yaml. Helm is called to upgrade the deployment with a new definition, which is found in the 2vols+restore directory. The csi-vxflexos/test/helm/2vols+restore/templates directory contains the newly created createFromSnap.yaml file. The script then creates a PersistentVolumeClaim , which is a volume that is dynamically created from the snapshot. Then the helm deployment is upgraded to contain the newly created third volume. In other words, when the snaprestoretest.sh creates a new volume with data from the snapshot, the restore operation is tested. The contents of the createFromSnap.yaml are described below:  apiVersion:v1kind:PersistentVolumeClaimmetadata:name:restorepvcnamespace:helmtest-vxflexosspec:storageClassName:vxflexosdataSource:name:pvol0-snapkind:VolumeSnapshotapiGroup:snapshot.storage.k8s.ioaccessModes:- ReadWriteOnceresources:requests:storage:8GiNOTE: The spec.dataSource clause, specifies a source VolumeSnapshot named pvol0-snap1 which matches the snapshot\u0026rsquo;s name in snap1.yaml.\n","excerpt":"This section provides multiple methods to test driver functionality in your environment.\nNote: To …","ref":"/ansible-docs/v1/installation/test/powerflex/","title":"Test PowerFlex CSI Driver"},{"body":"Release Notes - CSI PowerFlex v1.4.0 New Features/Changes  Added support for Kubernetes v1.20 Added support for OpenShift 4.7 with RHEL and CoreOS worker nodes Added support for Red Hat Enterprise Linux (RHEL) 8.3 Added support for Fedora CoreOS Added SDC deployment on Fedora CoreOS nodes Added support for Ephemeral Inline Volume Added support multi-mount volumes Added support for managing multiple PowerFlex arrays from one driver Removed storage classes from helm template  Fixed Issues There are no fixed issues in this release.\nKnown Issues    Issue Workaround     Slow volume attached/detach If your Kubernetes 1.18 cluster has a lot of VolumeAttachment objects, the attach/detach operations will be very slow. This is a known issue and affects all CSI plugins. It is tracked here: CSI VolumeAttachment slows pod startup time. To get around this problem you can upgrade to latest Kubernetes/OpenShift patches, which contains a partial fix: 1.18.5+   Installation warning: \u0026ldquo;OpenShift version 4.7, is newer than the version that has been tested. Latest tested version is: 4.6\u0026rdquo; Ignore this warning and continue with the installation. v1.4.0 release of the driver supports OpenShift 4.6/4.7 .    ","excerpt":"Release Notes - CSI PowerFlex v1.4.0 New Features/Changes  Added support for Kubernetes v1.20 Added …","ref":"/ansible-docs/v1/release/powerflex/","title":"PowerFlex"},{"body":"   Symptoms Prevention, Resolution or Workaround     The installation fails with the following error message: Node xxx does not have the SDC installed Install the PowerFlex SDC on listed nodes. The SDC must be installed on all the nodes that needs to pull an image of the driver.   When you run the command kubectl describe pods vxflexos-controller-0 –n vxflexos, the system indicates that the driver image could not be loaded. - If on Kubernetes, edit the daemon.json file found in the registry location and add { \u0026quot;insecure-registries\u0026quot; :[ \u0026quot;hostname.cloudapp.net:5000\u0026quot; ] }\n- If on OpenShift, run the command oc edit image.config.openshift.io/cluster and add registries to yaml file that is displayed when you run the command.   The kubectl logs -n vxflexos vxflexos-controller-0 driver logs shows that the driver is not authenticated. Check the username, password, and the gateway IP address for the PowerFlex system.   The kubectl logs vxflexos-controller-0 -n vxflexos driver logs shows that the system ID is incorrect. Use the get_vxflexos_info.sh to find the correct system ID. Add the system ID to myvalues.yaml script.   Defcontext mount option seems to be ignored, volumes still are not being labeled correctly. Ensure SElinux is enabled on worker node, and ensure your container run time manager is properly configured to be utilized with SElinux.   Mount options that interact with SElinux are not working (like defcontext). Check that your container orchestrator is properly configured to work with SElinux.   Installation of the driver on Kubernetes v1.20 fails with the following error: Error: unable to build kubernetes objects from release manifest: unable to recognize \u0026quot;\u0026quot;: no matches for kind \u0026quot;VolumeSnapshotClass\u0026quot; in version \u0026quot;snapshot.storage.k8s.io/v1\u0026quot; Kubernetes v1.20 requires v1 version of snapshot CRDs. If on Kubernetes 1.20 (v1 snapshots) install CRDs from v4.0.0, see the Volume Snapshot Requirements    ","excerpt":"Symptoms Prevention, Resolution or Workaround     The installation fails with the following error …","ref":"/ansible-docs/v1/troubleshooting/powerflex/","title":"PowerFlex"},{"body":"Volume Snapshot Feature The CSI PowerFlex driver version 1.2 and later support beta snapshots. Earlier versions of the driver supported alpha snapshots.\nVolume Snapshots feature in Kubernetes has moved to beta in Kubernetes version 1.17. It was an alpha feature in earlier releases (1.13 onwards). The snapshot API version has changed from v1alpha1 to v1beta1 with this migration.\nIn order to use Volume Snapshots, ensure the following components are deployed to your cluster:\n Kubernetes Volume Snaphshot CRDs Volume Snapshot Controller  Volume Snapshot Class During the installation of CSI PowerFlex 1.3 driver, a Volume Snapshot Class is created using the new v1beta1 snapshot APIs. This is the only Volume Snapshot Class required and there is no need to create any other Volume Snapshot Class.\nFollowing is the manifest for the Volume Snapshot Class created during installation:\napiVersion: snapshot.storage.k8s.io/v1beta1 kind: VolumeSnapshotClass metadata: name: vxflexos-snapclass driver: csi-vxflexos.dellemc.com deletionPolicy: Delete Create Volume Snapshot The following is a sample manifest for creating a Volume Snapshot using the v1beta1 snapshot APIs:\napiVersion: snapshot.storage.k8s.io/v1beta1 kind: VolumeSnapshot metadata: name: pvol0-snap namespace: helmtest-vxflexos spec: volumeSnapshotClassName: vxflexos-snapclass source: persistentVolumeClaimName: pvol Once the VolumeSnapshot is successfully created by the CSI PowerFlex driver, a VolumeSnapshotContent object is automatically created. Once the status of the VolumeSnapshot object has the readyToUse field set to true , it is available for use.\nFollowing is the relevant section of VolumeSnapshot object status:\nstatus: boundVolumeSnapshotContentName: snapcontent-5a8334d2-eb40-4917-83a2-98f238c4bda creationTime: \u0026quot;2020-07-16T08:42:12Z\u0026quot; readyToUse: true Creating PVCs with Volume Snapshots as Source The following is a sample manifest for creating a PVC with a VolumeSnapshot as a source:\napiVersion: v1 kind: PersistentVolumeClaim metadata: name: restorepvc namespace: helmtest-vxflexos spec: storageClassName: vxflexos dataSource: name: pvol0-snap kind: VolumeSnapshot apiGroup: snapshot.storage.k8s.io accessModes: - ReadWriteOnce resources: requests: storage: 8Gi Volume Expansion Feature The CSI PowerFlex driver version 1.2 and later support expansion of Persistent Volumes. This expansion is done online, that is, when PVC is attached to a node.\nTo use this feature, the storage class used to create the PVC must have the attribute allowVolumeExpansion set to true. The storage classes created during the installation (both using Helm or dell-csi-operator) have the allowVolumeExpansion set to true by default.\nIn case you are creating more storage classes, make sure that this attribute is set to true if you wish to expand any Persistent Volumes created using these new storage classes.\nFollowing is a sample manifest for a storage class which allows for Volume Expansion:\napiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: vxflexos-expand annotations: provisioner: csi-vxflexos.dellemc.com reclaimPolicy: Delete allowVolumeExpansion: true parameters: storagepool: pool volumeBindingMode: WaitForFirstConsumer allowedTopologies: - matchLabelExpressions: - key: csi-vxflexos.dellemc.com/sample values: - csi-vxflexos.dellemc.com To resize a PVC, edit the existing PVC spec and set spec.resources.requests.storage to the intended size.\nFor example, if you have a PVC - pvol0 of size 8Gi, then you can resize it to 16 Gi by updating the PVC:\nspec: accessModes: - ReadWriteOnce resources: requests: storage: 16Gi #update from 8Gi storageClassName: vxflexos volumeMode: Filesystem volumeName: k8s-0e50dada status: accessModes: - ReadWriteOnce capacity: storage: 8Gi phase: Bound NOTE: Kubernetes Volume Expansion feature cannot be used to shrink a volume and volumes cannot be expanded to a value that is not a multiple of 8. If attempted, the driver will round up. For example, if the above PVC was edited to have a size of 20 Gb, the size would actually be expanded to 24 Gb, the closest multiple of 8.\nVolume Cloning Feature The CSI PowerFlex driver version 1.3 and later support volume cloning. This feature allows specifying existing PVCs in the dataSource field to indicate a user would like to clone a Volume.\nThe source PVC must be bound and available (not in use). Source and destination PVC must be in the same namespace and have the same Storage Class.\nTo clone a volume, you should first have an existing pvc, for example, pvol0:\nkind: PersistentVolumeClaim apiVersion: v1 metadata: name: pvol0 namespace: helmtest-vxflexos spec: storageClassName: vxflexos accessModes: - ReadWriteOnce volumeMode: Filesystem resources: requests: storage: 8Gi The following is a sample manifest for cloning pvol0:\napiVersion: v1 kind: PersistentVolumeClaim metadata: name: clonedpvc namespace: helmtest-vxflexos spec: storageClassName: vxflexos dataSource: name: pvol0 kind: PersistentVolumeClaim accessModes: - ReadWriteOnce resources: requests: storage: 8Gi Raw Block Support The CSI PowerFlex driver version 1.2 and later support Raw Block volumes, which are created using the volumeDevices list in the pod template spec with each entry accessing a volumeClaimTemplate specifying a volumeMode: Block.\nFollowing is an example configuration of Raw Block Outline:\nkind: StatefulSet apiVersion: apps/v1 metadata: name: powerflextest namespace: helmtest-vxflexos spec: ... spec: ... containers: - name: test ... volumeDevices: - devicePath: \u0026quot;/dev/data0\u0026quot; name: pvol volumeClaimTemplates: - metadata: name: pvol spec: accessModes: - ReadWriteOnce volumeMode: Block storageClassName: vxflexos resources: requests: storage: 8Gi Allowable access modes are ReadWriteOnce , ReadWriteMany , and for block devices that have been previously initialized, ReadOnlyMany.\nRaw Block volumes are presented as a block device to the pod by using a bind mount to a block device in the node\u0026rsquo;s file system. The driver does not format or check the format of any file system on the block device. Raw Block volumes do support online Volume Expansion, but it is up to the application to manage reconfiguring the file system (if any) to the new size.\nFor additional information, see the Kubernetes Raw Block Volume Support documentation.\nTopology Support The CSI PowerFlex driver version 1.2 and later support Topology which forces volumes to be placed on worker nodes that have connectivity to the backend storage. This covers use cases where:\n The PowerFlex SDC may not be installed or running on some nodes. Users have chosen to restrict the nodes on which the CSI driver is deployed.  This Topology support does not include customer defined topology, users cannot create their own labels for nodes and storage classed and expect the labels to be honored by the driver.\nTopology Usage To utilize the Topology feature, the storage classes are modified to specify the volumeBindingMode as WaitForFirstConsumer and to specify the desired topology labels within allowedTopologies. This ensures that pod scheduling takes advantage of the topology and be guaranteed that the node selected has access to provisioned volumes.\nStorage Class Example with Topology Support:\napiVersion: storage.k8s.io/v1 kind: StorageClass metadata: annotations: meta.helm.sh/release-name: vxflexos meta.helm.sh/release-namespace: vxflexos storageclass.beta.kubernetes.io/is-default-class: \u0026quot;true\u0026quot; creationTimestamp: \u0026quot;2020-05-27T13:24:55Z\u0026quot; labels: app.kubernetes.io/managed-by: Helm name: vxflexos resourceVersion: \u0026quot;170198\u0026quot; selfLink: /apis/storage.k8s.io/v1/storageclasses/vxflexos uid: abb094e6-2c25-42c1-b82e-bd80372e78b parameters: storagepool: pool provisioner: csi-vxflexos.dellemc.com reclaimPolicy: Delete volumeBindingMode: WaitForFirstConsumer allowedTopologies: - matchLabelExpressions: - key: csi-vxflexos.dellemc.com/6c29fd07674c values: - csi-vxflexos.dellemc.com For additional information, see the Kubernetes Topology documentation.\nNOTE In the manifest file of the Dell CSI operator, topology can be enabled by specifying the system name or systemid in the allowed topologies field. Volumebindingmode is also set to WaitForFirstConsumer by default.\nController HA The CSI PowerFlex driver version 1.3 and later support multiple controller pods. A Controller pod can be assigned to a worker node or a master node, as long as no other controller pod is currently assigned to the node. To control the number of controller pods, edit:\ncontrollerCount: 2 in your values file to the desired number of controller pods. By default, the driver will deploy with two controller pods, each assigned to a different worker node.\n NOTE: If controller count is greater than the number of available nodes, excess controller pods will be stuck in pending state.\n If you\u0026rsquo;re using the Dell CSI Operator, the value to adjust is:\nreplicas: 1 in your driver yaml in config/samples/\nIf you want to specify where controller pods get assigned, make the following edits to your values file (helm install):\nTo assign controller pods to worker nodes only (Default):\n# \u0026quot;controller\u0026quot; allows to configure controller specific parameters controller: #\u0026quot;controller.nodeSelector\u0026quot; defines what nodes would be selected for pods of controller deployment # Leave as blank to use all nodes nodeSelector: # node-role.kubernetes.io/master: \u0026quot;\u0026quot; # \u0026quot;controller.tolerations\u0026quot; defines tolerations that would be applied to controller deployment # Leave as blank to install controller on worker nodes tolerations: # - key: \u0026quot;node-role.kubernetes.io/master\u0026quot; # operator: \u0026quot;Exists\u0026quot; # effect: \u0026quot;NoSchedule\u0026quot; To assign controller pods to master and worker nodes:\n# \u0026quot;controller\u0026quot; allows to configure controller specific parameters controller: #\u0026quot;controller.nodeSelector\u0026quot; defines what nodes would be selected for pods of controller deployment # Leave as blank to use all nodes nodeSelector: # node-role.kubernetes.io/master: \u0026quot;\u0026quot; # \u0026quot;controller.tolerations\u0026quot; defines tolerations that would be applied to controller deployment # Leave as blank to install controller on worker nodes tolerations: - key: \u0026quot;node-role.kubernetes.io/master\u0026quot; operator: \u0026quot;Exists\u0026quot; effect: \u0026quot;NoSchedule\u0026quot; To assign controller pods to master nodes only:\n# \u0026quot;controller\u0026quot; allows to configure controller specific parameters controller: #\u0026quot;controller.nodeSelector\u0026quot; defines what nodes would be selected for pods of controller deployment # Leave as blank to use all nodes nodeSelector: node-role.kubernetes.io/master: \u0026quot;\u0026quot; # \u0026quot;controller.tolerations\u0026quot; defines tolerations that would be applied to controller deployment # Leave as blank to install controller on worker nodes tolerations: - key: \u0026quot;node-role.kubernetes.io/master\u0026quot; operator: \u0026quot;Exists\u0026quot; effect: \u0026quot;NoSchedule\u0026quot; For configuring Controller HA on the Dell CSI Operator, please refer to the Dell CSI Operator documentation.\nAutomated SDC Deployment The CSI PowerFlex driver version 1.3 and later support the automatic deployment of the PowerFlex SDC on Red Hat CoreOS (RHCOS) nodes in an OpenShift cluster. Only RHCOS is supported at this time. The deployment of the SDC kernel module on RHCOS nodes is done via an init container. Automated installation is supported in both via Helm and Dell CSI Operator based installs. The following describes further details of this feature:\n On RHCOS nodes, the SDC init container runs prior to the driver being installed. It installs the SDC kernel module on the node. If there is a SDC kernel module installed then the version is checked and updated. Optionally, if the SDC monitor is enabled, another container is started and runs as the monitor. Follow PowerFlex SDC documentation to get monitor metrics. On non-RHCOS nodes, the SDC init container skips installing and you can see this mentioned in the logs by running kubectl logs on the node for SDC There is no automated uninstall of SDC kernel module. Follow PowerFlex SDC documentation to manually uninstall the SDC driver from node.  ","excerpt":"Volume Snapshot Feature The CSI PowerFlex driver version 1.2 and later support beta snapshots. …","ref":"/ansible-docs/v2/features/powerflex/","title":"PowerFlex"},{"body":"The CSI Driver for Dell EMC PowerFlex can be deployed by using the provided Helm v3 charts and installation scripts on both Kubernetes and OpenShift platforms. For more detailed information on the installation scripts, please review the script documentation.\nThe controller section of the Helm chart installs the following components in a Deployment in the namespace vxflexos:\n CSI Driver for Dell EMC PowerFlex Kubernetes External Provisioner, which provisions the volumes Kubernetes External Attacher, which attaches the volumes to the containers Kubernetes External Snapshotter, which provides snapshot support Kubernetes External Resizer, which resizes the volume  The node section of the Helm chart installs the following component in a DaemonSet in the namespace vxflexos:\n CSI Driver for Dell EMC PowerFlex Kubernetes Node Registrar, which handles the driver registration  Prerequisites The following are requirements must be met before installing the CSI Driver for Dell EMC PowerFlex:\n Install Kubernetes (1.17, 1.18, 1.19) or OpenShift (4.5 or 4.6) Install Helm 3 Enable Zero Padding on PowerFlex Configure Mount propagation on container runtime (i.e. Docker) Install PowerFlex Storage Data Client Volume Snapshot requirements A user must exist on the array with a role \u0026gt;= FrontEndConfigure  Install Helm 3.0 Install Helm 3.0 on the master node before you install the CSI Driver for Dell EMC PowerFlex.\nSteps\nRun the curl https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3 | bash command to install Helm 3.0.\nEnable Zero Padding on PowerFlex Verify that zero padding is enabled on the PowerFlex storage pools that will be used. Use PowerFlex GUI or the PowerFlex CLI to check this setting. See Dell EMC PowerFlex documentation for more information to configure this setting.\nConfigure Mount Propagation on Container Runtime It is required to configure mount propagation on your container runtime on all Kubernetes nodes before installing the CSI Driver for Dell EMC PowerFlex. The following is instruction on how to do this with Docker. If you use another container runtime, follow the recommended instructions from the vendor to configure mount propagation.\nSteps\n Edit the service section of /etc/systemd/system/multi-user.target.wants/docker.service file to add the following lines: docker.service [Service]... MountFlags=shared  Restart the docker service with systemctl daemon-reload and systemctl restart docker on all the nodes.  Note: Some distribution, like Ubuntu, already has MountFlags set by default\nInstall PowerFlex Storage Data Client The CSI Driver for PowerFlex requires you to have installed the PowerFlex Storage Data Client (SDC) on all worker nodes. If installing on Red Hat CoreOS (RHCOS) nodes on OpenShift you can install using the automated SDC deployment feature. If installing on non-RHCOS nodes, you must install SDC manually.\nAutomatic SDC Deployment The automated deployment of the SDC runs by default when installing the driver. It installs an SDC container to faciliate the installation. While the install is automated there are a few configuration options for this feature. Those are referenced in the Install the Driver section. More details on how the automatic SDC deployment works can be found in the Feature section of this site on the PowerFlex page.\nOptional: For a typical install, you will pull SDC kernel modules from the Dell EMC ftp site, which is setup by default. Some users might want to mirror this repository to a local location. The PowerFlex documentation has instructions on how to do this. If a mirror is used, you need to create an SDC repo secret for managing the credentials to the mirror. Details on how to create the secret are in the Install the Driver section.\nManually SDC Deployment For detailed PowerFlex installation procedure, see the Dell EMC PowerFlex Deployment Guide. Install the PowerFlex SDC as follows:\nSteps\n Download the PowerFlex SDC from Dell EMC Online support. The filename is EMC-ScaleIO-sdc-*.rpm, where * is the SDC name corresponding to the PowerFlex installation version. Export the shell variable MDM_IP in a comma-separated list using export MDM_IP=xx.xxx.xx.xx,xx.xxx.xx.xx, where xxx represents the actual IP address in your environment. This list contains the IP addresses of the MDMs. Install the SDC per the Dell EMC PowerFlex Deployment Guide:  For Red Hat Enterprise Linux and Cent OS, run rpm -iv ./EMC-ScaleIO-sdc-*.x86_64.rpm, where * is the SDC name corresponding to the PowerFlex installation version.    Volume Snapshot requirements Volume Snapshot CRD\u0026rsquo;s The Kubernetes Volume Snapshot CRDs can be obtained and installed from the external-snapshotter project on Github.\nYou can also install the CRDs by supplying the option --snapshot-crd while installing the driver using the csi-install.sh script.\nVolume Snapshot Controller Starting with beta Volume Snapshots in Kubernetes 1.17, the CSI external-snapshotter sidecar is split into two controllers:\n A common snapshot controller A CSI external-snapshotter sidecar  The common snapshot controller must be installed only once in the cluster irrespective of the number of CSI drivers installed in the cluster. On OpenShift clusters 4.4 and later, the common snapshot-controller is pre-installed. In the clusters where it is not present, it can be installed using kubectl and the manifests available on GitHub.\nNOTE:\n The manifests available on GitHub install v3.0.3 of the snapshotter image - quay.io/k8scsi/csi-snapshotter:v3.0.3 Dell recommends using v3.0.3 image of the snapshot-controller - quay.io/k8scsi/snapshot-controller:v3.0.3 The CSI external-snapshotter sidecar is still installed along with the driver and does not involve any extra configuration.  Install the Driver Steps\n Run git clone https://github.com/dell/csi-powerflex.git to clone the git repository. Ensure that you have created namespace where you want to install the driver. You can run kubectl create namespace vxflexos to create a new one. Check helm/csi-vxflexos/driver-image.yaml and confirm the driver image points to new image. Edit the helm/secret.yaml, point to correct namespace and replace the values for the username and password parameters. These values can be obtained using base64 encoding as described in the following example: echo -n \u0026#34;myusername\u0026#34; | base64 echo -n \u0026#34;mypassword\u0026#34; | base64 where myusername \u0026amp; mypassword are credentials for a user with PowerFlex priviledges.\n Create the secret by running kubectl create -f secret.yaml If not using automated SDC deployment, create a dummy SDC repo secret file: kubectl create -f sdc-repo-secret.yaml If using automated SDC deployment:  Check the SDC container image is the correct version for your version of PowerFlex. Create a secret for the SDC repo credentials and provide the URL for the repo.  To create the secret, you must update the details in helm/sdc-repo-secret.yaml file and running kubectl create -f sdc-repo-secret.yaml. To set the repo URL, you must set the repoUrl parameter in the myvalues.yaml file.     Collect information from the PowerFlex SDC by executing the get_vxflexos_info.sh script located in the top-level helm directory. This script shows the VxFlex OS system ID and MDM IP addresses. Make a note of the value for these parameters as they must be entered in the myvalues.yaml file.  NOTE: Your SDC might have multiple VxFlex OS systems registered. Ensure that you choose the correct values.   Copy the default values.yaml file cd helm \u0026amp;\u0026amp; cp csi-vxflexos/values.yaml myvalues.yaml Edit the newly created values file and provide values for the following parameters vi myvalues.yaml:     Parameter Description Required Default     systemName Set to the PowerFlex/VxFlex OS system name or system ID to be used with the driver. Yes \u0026ldquo;systemname\u0026rdquo;   restGateway Set to the URL of your system’s REST API Gateway. You can obtain this value from the PowerFlex administrator. Yes \u0026ldquo;https://123.0.0.1\u0026rdquo;   storagePool Set to a default (existing) storage pool name in your PowerFlex/VxFlex OS system. Yes \u0026ldquo;sp\u0026rdquo;   volumeNamePrefix Set so that volumes created by the driver have a default prefix. If one PowerFlex/VxFlex OS system is servicing several different Kubernetes installations or users, these prefixes help you distinguish them. No \u0026ldquo;k8s\u0026rdquo;   controllerCount Set to deploy multiple controller instances. If controller count is greater than the number of available nodes, excess pods will be left in pending state. You can increase number of available nodes by configuring the \u0026ldquo;controller\u0026rdquo; section in your values.yaml. For more details on the new controller pod configurations, see the Features section for Powerflex specifics. Yes 2   enablelistvolumesnapshot Set to have snapshots included in the CSI operation ListVolumes. Disabled by default. No FALSE   StorageClass Helm charts create a Kubernetes StorageClass while deploying CSI Driver for Dell EMC PowerFlex. This section includes relevant variables. - -   name Defines the name of the Kubernetes storage class that the Helm charts will create. For example, the vxflexos base name will be used to generate names such as vxflexos and vxflexos-xfs. No \u0026ldquo;vxflexos\u0026rdquo;   isDefault Sets the newly created storage class as default for Kubernetes. Set this value to true only if you expect PowerFlex to be your principle storage provider, as it will be used in PersitentVolumeClaims where no storageclass is provided. After installation, you can add custom storage classes, if desired. No TRUE   reclaimPolicy Defines whether the volumes will be retained or deleted when the assigned pod is destroyed. The valid values for this variable are Retain or Delete. No \u0026ldquo;Delete\u0026rdquo;   controller This section allows configuration of controller specific parameters. To maximize the number of available nodes for controller pods, see this section. For more details on the new controller pod configurations, see the Features section for Powerflex specifics. - -   nodeSelector Defines what nodes would be selected for pods of controller deployment. Leave as blank to use all nodes. Uncomment this section to deploy on master nodes exclusively. No \u0026quot; \u0026quot;   tolerations Defines tolerations that would be applied to controller deployment. Leave as blank to install controller on worker nodes only. If deploying on master nodes is desired, uncomment out this section. No \u0026quot; \u0026quot;   monitor This section allows configuration of the SDC monitoring pod. - -   enabled Set to enable the usage of the monitoring pod. No FALSE   hostNetwork Set whether the monitor pod should run on the host network or not. No TRUE   hostPID Set whether the monitor pod should run in the host namespace or not. No TRUE   sdcKernelMirror [RHCOS only] The PowerFlex SDC may need to pull a new module that is known to work with newer Linux kernels. The default location of this mirror os at ftp.emc.com. The PowerFlex documentation has instructions for methods to mirror this repository to a local location if necessary. - -   repoUrl Set the URL of the ftp mirror containing SDC kernel modules. Only ftp locations are allowed. A blank string signifies the default mirror, which is \u0026ldquo;ftp://ftp.emc.com\u0026rdquo;. No \u0026quot; \u0026quot;   11. Install the driver using csi-install.sh bash script by running cd ../dell-csi-helm-installer \u0026amp;\u0026amp; ./csi-install.sh --namespace vxflexos --values ../helm/myvalues.yaml       NOTE:\n For detailed instructions on how to run the install scripts, refer to the README.md in the dell-csi-helm-installer folder. This script also runs the verify.sh script that is present in the same directory. You will be prompted to enter the credentials for each of the Kubernetes nodes. The verify.sh script needs the credentials to check if SDC has been configured on all nodes. You can also skip the verification step by specifiying the --skip-verify-node option. (Optional) Enable additional Mount Options - A user is able to specify additional mount options as needed for the driver.  Mount options are specified in storageclass yaml under mountOptions. WARNING: Before utilizing mount options, you must first be fully aware of the potential impact and understand your environment\u0026rsquo;s requirements for the specified option.    Storage Classes As part of the driver installation, a set of storage classes is created along with the driver pods. This is done to demonstrate how storage classes need to be created to consume storage from Dell EMC storage arrays.\nThe StorageClass object in Kubernetes is immutable and can\u0026rsquo;t be modified once created. It creates challenges when we need to change or update a parameter, for example when a version of the driver introduces new configurable parameters for the storage classes. To avoid issues during upgrades, future releases of the drivers will have the installation separated from the creation of Storage Classes. In preparation for that, starting in Q4 of 2020, an annotation \u0026quot;helm.sh/resource-policy\u0026quot;: keep is applied to the storage classes created by the dell-csi-helm-installer.\nBecause of this annotation, these storage classes are not going to be deleted even after the driver has been uninstalled. This annotation has been applied to give you an opportunity to keep using these storage classes even with a future release of the driver. In case you wish to not use these storage classes, you will need to delete them by using the kubectl delete storageclass command.\nNOTE: If you uninstall the driver and reinstall it, you can still face errors if any update in the values.yaml file leads to an update of the storage class(es):\n Error: cannot patch \u0026quot;\u0026lt;sc-name\u0026gt;\u0026quot; with kind StorageClass: StorageClass.storage.k8s.io \u0026quot;\u0026lt;sc-name\u0026gt;\u0026quot; is invalid: parameters: Forbidden: updates to parameters are forbidden In case you want to make such updates, make sure to delete the existing storage classes using the kubectl delete storageclass command.\nDeleting a storage class has no impact on a running Pod with mounted PVCs. You won\u0026rsquo;t be able to provision new PVCs until at least one storage class is newly created.\n","excerpt":"The CSI Driver for Dell EMC PowerFlex can be deployed by using the provided Helm v3 charts and …","ref":"/ansible-docs/v2/installation/helm/powerflex/","title":"PowerFlex"},{"body":"Installing PowerFlex CSI Driver via Operator The CSI Driver for Dell EMC PowerFlex can be installed via the Dell CSI Operator.\nTo deploy the Operator, follow the instructions available here.\nThere are sample manifests provided which can be edited to do an easy installation of the driver. Note that the deployment of the driver using the operator does not use any Helm charts. The installation and configuration parameters will be slightly different from the ones specified via the Helm installer.\nKubernetes Operators make it easy to deploy and manage entire lifecycle of complex Kubernetes applications. Operators use Custom Resource Definitions (CRD) which represents the application and use custom controllers to manage them.\nPrerequisites: Automated SDC Deployment for Operator  This applies to OpenShift with RHCOS Nodes Only. This feature deploys the sdc kernel modules on CoreOS nodes with the help of an init container. Required: MDM value need to be provided in CR file for the sdc init container to work. Expect error if not in proper format. To use a specific image from ftp site, pass in repo url, repo password and repo username.  Repo username and repo password are to be encrypted by a secret and passed in. Create secret for FTP side by using the command kubectl create -f sdc-repo-secret.yaml.   Optionally, enable sdc monitor by uncommenting the section for sidecar in manifest yaml.  Example CR: config/samples/vxflexos_v130_ops_46.yaml #sideCars:# Uncomment the following section if you want to run the monitoring sidecar# - name: sdc-monitor# envs:# - name: HOST_PID# value: \u0026#34;1\u0026#34;initContainers:- image:dellemc/sdc:3.6.0.176-3.5.1000.176imagePullPolicy:IfNotPresentname:sdcenvs:- name:MDMvalue:\u0026#34;10.xx.xx.xx,10.xx.xx.xx\u0026#34;Install Driver  Create namespace: Run kubectl create namespace \u0026lt;driver-namespace\u0026gt; command using the desired name to create the namespace. Create PowerFlex credentials: Create a file called vxflexos-creds.yaml with the following content apiVersion:v1kind:Secretmetadata:name:vxflexos-creds# Replace driver-namespace with the namespace where driver is being deployednamespace:\u0026lt;driver-namespace\u0026gt; type: Opaquedata:# set username to the base64 encoded usernameusername:\u0026lt;base64username\u0026gt; # set password to the base64 encoded passwordpassword:\u0026lt;base64password\u0026gt;Replace the values for the username and password parameters. These values can be optioned using base64 encoding as described in the following example:\necho -n \u0026quot;myusername\u0026quot; | base64 echo -n \u0026quot;mypassword\u0026quot; | base64 Run kubectl create -f vxflexos-creds.yaml command to create the secret.\n Create a Custom Resource (CR) for PowerFlex using the sample files provided here . Users should configure the parameters in CR. The following table lists the primary configurable parameters of the PowerFlex driver and their default values:    Parameter Description Required Default     replicas Controls the amount of controller pods you deploy. If controller pods is greater than number of available nodes, excess pods will become stuck in pending. Defaults is 2 which allows for Controller high availability. Yes 2   Common parameters for node and controller      X_CSI_VXFLEXOS_SYSTEMNAME Defines the name of the PowerFlex system from which volumes will be provisioned. This must either be set to the PowerFlex system name or system ID Yes systemname   X_CSI_VXFLEXOS_ENDPOINT Defines the PowerFlex REST API endpoint, with full URL, typically leveraging HTTPS. You must set this for your PowerFlex installations REST gateway Yes https://127.0.0.1   CSI_ENDPOINT The UNIX socket address for handling gRPC calls No /var/run/csi/csi.sock   X_CSI_VXFLEXOS_ENABLELISTVOLUMESNAPSHOT Enable list volume operation to include snapshots (since creating a volume from a snap actually results in a new snap) No false   X_CSI_VXFLEXOS_ENABLESNAPSHOTCGDELETE Enable this to automatically delete all snapshots in a consistency group when a snap in the group is deleted No false   X_CSI_DEBUG To enable debug mode No false   StorageClass parameters      storagePool Defines the PowerFlex storage pool from which this driver will provision volumes. You must set this for the primary storage pool to be used Yes pool1   allowVolumeExpansion Once the allowed topology is modified in storage class, pods/and volumes will always be scheduled on nodes that have access to the storage No true   allowedTopologies:key This is to enable topology to allow pods/and volumes to always be scheduled on nodes that have access to the storage. You need to replace the X_CSI_VXFLEXOS_SYSTEMNAME in the key with the actual systemname value No X_CSI_VXFLEXOS_SYSTEMNAME   initContainers:value Set the MDM IP\u0026rsquo;s here if installing on CoreOS to enable automatic SDC installation Yes (OpenShift) \u0026ldquo;10.xx.xx.xx,10.xx.xx.xx\u0026rdquo;     Execute the kubectl create -f \u0026lt;input_sample_file.yaml\u0026gt; command to create PowerFlex custom resource. This command will deploy the CSI-PowerFlex driver.  ","excerpt":"Installing PowerFlex CSI Driver via Operator The CSI Driver for Dell EMC PowerFlex can be installed …","ref":"/ansible-docs/v2/installation/operator/powerflex/","title":"PowerFlex"},{"body":"This section provides multiple methods to test driver functionality in your environment.\nNote: To run the test for CSI Driver for Dell EMC PowerFlex, install Helm 3.\nTest deploying a simple pod with PowerFlex storage Test the deployment workflow of a simple pod on PowerFlex storage.\nPrerequisites\nIn the source code, there is a directory that contains examples of how you can use the driver. To use these examples, you must create a helmtest-vxflexos namespace, using kubectl create namespace helmtest-vxflexos, before you can start testing. HELM 3 must be installed to perform the tests.\nThe starttest.sh script is located in the csi-vxflexos/test/helm directory. This script is used in the following procedure to deploy helm charts that test the deployment of a simple pod.\nSteps\n Navigate to the test/helm directory, which contains the starttest.sh and the 2vols directories. This directory contains a simple Helm chart that will deploy a pod that uses two PowerFlex volumes. NOTE: Helm tests are designed assuming users are using the default storageclass names (vxflexos and vxflexos-xfs). If your storageclass names differ from the default values, such as when deploying with the Dell CSI Operator, please update the templates in 2vols accordingly (located in test/helm/2vols/templates directory). You can use kubectl get sc to check for the storageclass names. Run sh starttest.sh 2vols to deploy the pod. You should see the following:  Normal Pulled 38s kubelet, k8s113a-10-247-102-215.lss.emc.com Successfully pulled image \u0026quot;docker.io/centos:latest\u0026quot; Normal Created 38s kubelet, k8s113a-10-247-102-215.lss.emc.com Created container Normal Started 38s kubelet, k8s113a-10-247-102-215.lss.emc.com Started container /dev/scinib 8125880 36852 7653216 1% /data /dev/scinia 16766976 32944 16734032 1% /data /dev/scinib on /data0 type ext4 (rw,relatime,data=ordered) /dev/scinia on /data1 type xfs (rw,relatime,attr2,inode64,noquota) To stop the test, run sh stoptest.sh 2vols. This script deletes the pods and the volumes depending on the retention setting you have configured.  Results\nAn outline of this workflow is described below:\n The 2vols helm chart contains two PersistentVolumeClaim definitions, one in pvc0.yaml , and the other in pvc1.yaml. They are referenced by the test.yaml which creates the pod. The contents of the Pvc0.yaml file are described below:  kind: PersistentVolumeClaim apiVersion: v1 metadata: name: pvol namespace: helmtest-vxflexos spec: accessModes: - ReadWriteOnce volumeMode: Filesystem resources: requests: storage: 8Gi storageClassName: vxflexos The volumeMode: Filesystem requires a mounted file system, and the resources.requests.storage of 8Gi requires an 8 GB file. In this case, the storageClassName: vxflexos directs the system to use one of the pre-defined storage classes created by the CSI Driver for Dell EMC PowerFlex installation process. This step yields a mounted ext4 file system. You can see the storage class definitions in the PowerFlex installation helm chart files storageclass.yaml and storageclass-xfs.yaml. If you compare pvol0.yaml and pvol1.yaml , you will find that the latter uses a different storage class; vxflexos-xfs. This class gives you an xfs file system. To see the volumes you created, run kubectl get persistentvolumeclaim –n helmtest-vxflexos and kubectl describe persistentvolumeclaim –n helmtest-vxflexos. NOTE: For more information about Kubernetes objects like StatefulSet and PersistentVolumeClaim see Kubernetes documentation: Concepts.  Test creating snapshots Test the workflow for snapshot creation.\nSteps\n Start the 2vols container and leave it running.  Helm tests are designed assuming users are using the default storageclass names (vxflexos and vxflexos-xfs). If your storageclass names differ from the default values, such as when deploying with the Operator, update the templates in 2vols accordingly (located in test/helm/2vols/templates directory). You can use kubectl get sc to check for the storageclass names. Helm tests are designed assuming users are using the default snapshotclass name. If your snapshotclass names differ from the default values, update snap1.yaml and snap2.yaml accordingly.   Run sh snaptest.sh to start the test.  This will create a snapshot of each of the volumes in the container using VolumeSnapshot objects defined in snap1.yaml and snap2.yaml. The following are the contents of snap1.yaml:\napiVersion: snapshot.storage.k8s.io/v1alpha1 kind: VolumeSnapshot metadata: name: pvol0-snap namespace: helmtest-vxflexos spec: snapshotClassName: vxflexos-snapclass source: name: pvol kind: PersistentVolumeClaim Results\nThe snaptest.sh script will create a snapshot using the definitions in the snap1.yaml file. The spec.source section contains the volume that will be snapped. For example, if the volume to be snapped is pvol0 , then the created snapshot is named pvol0-snap.\nNOTE: The snaptest.sh shell script creates the snapshots, describes them, and then deletes them. You can see your snapshots using kubectl get volumesnapshot -n test.\nNotice that this VolumeSnapshot class has a reference to a snapshotClassName: vxflexos-snapclass. The CSI Driver for Dell EMC PowerFlex installation creates this class as its default snapshot class. You can see its definition in the installation directory file volumesnapshotclass.yaml.\nTest restoring from a snapshot Test the restore operation workflow to restore from a snapshot.\nPrerequisites\nEnsure that you have stopped any previous test instance before performing this procedure.\nSteps\n Run sh snaprestoretest.sh to start the test.  This script deploys the 2vols example, creates a snap of pvol0, and then updates the deployed helm chart from the updateddirectory 2vols+restore. This then adds an additional volume that is created from the snapshot.\nNOTE:\n Helm tests are designed assuming users are using the default storageclass names (vxflexos and vxflexos-xfs). If your storageclass names differ from the default values, such as when deploying with the Dell CSI Operator, update the templates for snap restore tests accordingly (located in test/helm/2vols+restore/template directory). You can use kubectl get sc to check for the storageclass names. Helm tests are designed assuming users are using the default snapshotclass name. If your snapshotclass names differ from the default values, update snap1.yaml and snap2.yaml accordingly.  Results\nAn outline of this workflow is described below:\n The snapshot is taken using snap1.yaml. Helm is called to upgrade the deployment with a new definition, which is found in the 2vols+restore directory. The csi-vxflexos/test/helm/2vols+restore/templates directory contains the newly created createFromSnap.yaml file. The script then creates a PersistentVolumeClaim , which is a volume that is dynamically created from the snapshot. Then the helm deployment is upgraded to contain the newly created third volume. In other words, when the snaprestoretest.sh creates a new volume with data from the snapshot, the restore operation is tested. The contents of the createFromSnap.yaml are described below:  apiVersion: v1 kind: PersistentVolumeClaim metadata: name: restorepvc namespace: helmtest-vxflexos spec: storageClassName: vxflexos dataSource: name: pvol0-snap kind: VolumeSnapshot apiGroup: snapshot.storage.k8s.io accessModes: - ReadWriteOnce resources: requests: storage: 8Gi NOTE: The spec.dataSource clause, specifies a source VolumeSnapshot named pvol0-snap1 which matches the snapshot\u0026rsquo;s name in snap1.yaml.\n","excerpt":"This section provides multiple methods to test driver functionality in your environment.\nNote: To …","ref":"/ansible-docs/v2/installation/test/powerflex/","title":"Test PowerFlex CSI Driver"},{"body":"Release Notes - CSI PowerFlex v1.3.0 New Features/Changes  Added support for OpenShift 4.5/4.6 with RHEL and CoreOS worker nodes Added automatic SDC deployment on OpenShift CoreOS nodes Added support for Red Hat Enterprise Linux (RHEL) 7.9 Added support for Ubuntu 20.04 Added support for volume cloning Added support for Controller high availability (multiple-controllers)  Fixed Issues There are no fixed issues in this release.\nKnown Issues    Issue Workaround     Slow volume attached/detach If your Kubernetes 1.17 or 1.18 cluster has a lot of VolumeAttachment objects, the attach/detach operations will be very slow. This is a known issue and affects all CSI plugins. It is tracked here: CSI VolumeAttachment slows pod startup time. To get around this problem you can upgrade to latest Kubernetes/OpenShift patches, which contains a partial fix: 1.17.8+, 1.18.5+    ","excerpt":"Release Notes - CSI PowerFlex v1.3.0 New Features/Changes  Added support for OpenShift 4.5/4.6 with …","ref":"/ansible-docs/v2/release/powerflex/","title":"PowerFlex"},{"body":"   Symptoms Prevention, Resolution or Workaround     The installation fails with the following error message: Node xxx does not have the SDC installed Install the PowerFlex SDC on listed nodes. The SDC must be installed on all the nodes that needs to pull an image of the driver.   When you run the command kubectl describe pods vxflexos-controller-0 –n vxflexos, the system indicates that the driver image could not be loaded. - If on Kubernetes, edit the daemon.json file found in the registry location and add { \u0026quot;insecure-registries\u0026quot; :[ \u0026quot;hostname.cloudapp.net:5000\u0026quot; ] }\n- If on OpenShift, run the command oc edit image.config.openshift.io/cluster and add registries to yaml file that is displayed when you run the command.   The kubectl logs -n vxflexos vxflexos-controller-0 driver logs shows that the driver is not authenticated. Check the username, password, and the gateway IP address for the PowerFlex system.   The kubectl logs vxflexos-controller-0 -n vxflexos driver logs shows that the system ID is incorrect. Use the get_vxflexos_info.sh to find the correct system ID. Add the system ID to myvalues.yaml script.   Defcontext mount option seems to be ignored, volumes still are not being labeled correctly. Ensure SElinux is enabled on worker node, and ensure your container run time manager is properly configured to be   utilized with SElinux.    Mount options that interact with SElinux are not working (like defcontext). Check that your container orchestrator is properly configured to work with SElinux.    ","excerpt":"Symptoms Prevention, Resolution or Workaround     The installation fails with the following error …","ref":"/ansible-docs/v2/troubleshooting/powerflex/","title":"PowerFlex"},{"body":"Volume Snapshot Feature The Volume Snapshot feature was introduced in alpha (v1alpha1) in Kubernetes 1.13 and then moved to beta (v1beta1) in Kubernetes version 1.17 and was generally available (v1) in Kubernetes version 1.20.\nThe CSI PowerMax driver version 1.6 supports v1beta1 snapshots on Kubernetes 1.18/1.19 and v1 snapshots on Kubernetes 1.20.\nIn order to use Volume Snapshots, ensure the following components have been deployed to your cluster:\n Kubernetes Volume Snapshot CRDs Volume Snapshot Controller  Volume Snapshot Class During the installation of the CSI PowerMax 1.6 driver, a Volume Snapshot Class is created. This is the only Volume Snapshot Class you will need and there is no need to create any other Volume Snapshot Class.\nThe following is the manifest for the Volume Snapshot Class created during installation (using the default driver name):\napiVersion: snapshot.storage.k8s.io/v1 deletionPolicy: Delete kind: VolumeSnapshotClass metadata: name: powermax-snapclass driver: csi-powermax.dellemc.com Note: The apiVersion for VolumeSnapshotClass object created on clusters running Kubernetes versions \u0026lt; 1.20 will be v1beta1\nCreating Volume Snapshots The following is a sample manifest for creating a Volume Snapshot using the v1 snapshot APIs:\napiVersion:snapshot.storage.k8s.io/v1kind:VolumeSnapshotmetadata:name:pvol0-snap1spec:volumeSnapshotClassName:powermax-snapclasssource:persistentVolumeClaimName:pvol0After the VolumeSnapshot has been successfully created by the CSI PowerMax driver, a VolumeSnapshotContent object is automatically created. When the status of the VolumeSnapshot object has the readyToUse field set to true , it is available for use.\nThe following is the relevant section of VolumeSnapshot object status:\nstatus:boundVolumeSnapshotContentName:snapcontent-5a8334d2-eb40-4917-83a2-98f238c4bdacreationTime:\u0026#34;2020-07-16T08:42:12Z\u0026#34;readyToUse:trueCreating PVCs with VolumeSnapshots as Source The following is a sample manifest for creating a PVC with a VolumeSnapshot as a source:\napiVersion:v1kind:PersistentVolumeClaimmetadata:name:pmax-restore-pvc-demonamespace:testspec:storageClassName:powermaxdataSource:name:pmax-snapshot-demokind:VolumeSnapshotapiGroup:snapshot.storage.k8s.ioaccessModes:- ReadWriteOnceresources:requests:storage:8GiCreating PVCs with PVCs as source This is a sample manifest for creating a PVC with another PVC as a source:\napiVersion:v1kind:PersistentVolumeClaimmetadata:name:pmax-clone-pvc-demonamespace:testspec:storageClassName:powermaxdataSource:name:pmax-pvc-demokind:PersistentVolumeClaimaccessModes:- ReadWriteOnceresources:requests:storage:8GiiSCSI CHAP With version 1.3.0, support has been added for unidirectional Challenge Handshake Authentication Protocol (CHAP) for iSCSI. To enable CHAP authentication:\n Create secret powermax-creds with the key chapsecret set to the iSCSI CHAP secret. If the secret exists, delete and re-create the secret with this newly added key. Set the parameter enableCHAP in my-powermax-settings.yaml to true.  The driver uses the provided chapsecret to configure the iSCSI node database on each node with iSCSI access.\nWhen the driver is installed and all the node plug-ins have initialized successfully, the storage administrator must enable CHAP authentication using the following Solutions Enabler (SYMCLI) commands:\nsymaccess -sid \u0026lt;symid\u0026gt; -iscsi \u0026lt;host iqn\u0026gt; set chap -cred \u0026lt;host IQN\u0026gt; -secret \u0026lt;CHAP secret\u0026gt;\nWhere is the name of the iSCSI initiator of a host IQN, and is the chapsecret that is used at the time of the installation of the driver.\nNOTE: The host IQN is also used as the username when setting up the CHAP credentials.\nCHAP support for PowerMax With unidirectional CHAP, the PowerMax array challenges the host initiator during the initial link negotiation process and expects to receive a valid credential and CHAP secret in response.\nWhen challenged, the host initiator transmits a CHAP credential and CHAP secret to the storage array. The storage array looks for this credential and CHAP secret which stored in the host initiator group. When a positive authentication occurs, the PowerMax array sends an acceptance message to the host. However, if the PowerMax array fails to find any record of the credential/secret pair, it sends a rejection message, and the link is closed.\nCustom Driver Name (experimental feature) With version 1.3.0 of the driver, a custom name can be assigned to the driver at the time of installation. This enables installation of the CSI driver in a different namespace and installation of multiple CSI drivers for Dell EMC PowerMax in the same Kubernetes/OpenShift cluster.\nTo use this experimental feature, set the following values under customDriverName in my-powermax-settings.yaml.\n Value: Set this to the custom name of the driver. Enabled: Set this to true in case you want to enable this feature. The driver helm chart installation uses the values above to: Configure the driver name which is used for communication with other Kubernetes components. Configure the provisioner value in the storage class template. Configure the snapshotter value in the snapshot class template.  If enabled, the driver name is in the following format: \u0026lt;namespace\u0026gt;.\u0026lt;driver name\u0026gt;.dellemc.com\nFor example, if the driver name is set to driver and it is installed in the namespace powermax , then the name that is used for the driver (and the provisioner/snapshotter) is powermax.driver.dellemc.com\nNOTE: If not enabled, the name is set to csi-powermax.dellemc.com by default (without any namespace prefix).\nInstall multiple drivers NOTE: This is an experimental feature and should be used with extreme caution after consulting with Dell EMC Support.\nTo install multiple CSI Drivers for Dell EMC PowerMax in a single Kubernetes cluster, you can take advantage of the custom driver name feature. There are a few important restrictions which should be strictly adhered to:\n Only one driver can be installed in a single namespace Different drivers should not connect to a single Unisphere server Different drivers should not be used to manage a single PowerMax array Storage class and snapshot class names must be unique across installations  To install multiple CSI drivers, follow these steps:\n Create (or use) a new namespace. Ensure that all the pre-requisites are met:  powermax-creds secret is created in this namespace Optional) powermax-certs secret is created in this namespace   Update my-powermax-settings.yaml with the required values. Run the csi-install.sh script to install the driver.  Volume expansion Starting in v1.4, the CSI PowerMax driver supports expansion of Persistent Volumes (PVs). This expansion is done online, that is, when the PVC is attached to any node.\nTo use this feature, the storage class that is used to create the PVC must have the attribute allowVolumeExpansion set to true. The storage classes created during the installation (both using Helm or dell-csi-operator) have the allowVolumeExpansion set to true by default.\nIf you are creating more storage classes, ensure that this attribute is set to true to expand any PVs created using these new storage classes.\nThis is a sample manifest for a storage class which allows for Volume Expansion.\napiVersion:storage.k8s.io/v1kind:StorageClassmetadata:name:powermax-expand-scannotations:storageclass.beta.kubernetes.io/is-default-class:falseprovisioner:csi-powermax.dellemc.comreclaimPolicy:DeleteallowVolumeExpansion:true#Set this attribute to true if you plan to expand any PVCscreatedusingthisstorageclassparameters:SYMID:\u0026#34;000000000001\u0026#34;SRP:\u0026#34;DEFAULT_SRP\u0026#34;ServiceLevel:\u0026#34;Bronze\u0026#34;To resize a PVC, edit the existing PVC spec and set spec.resources.requests.storage to the intended size. For example, if you have a PVC - pmax-pvc-demo of size 5 Gi, then you can resize it to 10 Gi by updating the PVC.\nkind:PersistentVolumeClaimapiVersion:v1metadata:name:pmax-pvc-demonamespace:testspec:accessModes:- ReadWriteOncevolumeMode:Filesystemresources:requests:storage:10Gi#Updated size from 5Gi to 10GistorageClassName:powermax-expand-scNOTE: The Kubernetes Volume Expansion feature can only be used to increase the size of volume, it cannot be used to shrink a volume.\nRaw block support Starting in v1.4, CSI PowerMax driver supports raw block volumes.\nRaw Block volumes are created using the volumeDevices list in the Pod template spec with each entry accessing a volumeClaimTemplate specifying a volumeMode: Block. An example configuration is outlined here:\nkind:StatefulSetapiVersion:apps/v1metadata:name:powermaxtestnamespace:{{.Values.namespace}}spec:...spec:...containers:- name:test...volumeDevices:- devicePath:\u0026#34;/dev/data0\u0026#34;name:pvolvolumeClaimTemplates:- metadata:name:pvolspec:accessModes:- ReadWriteOncevolumeMode:BlockstorageClassName:powermaxresources:requests:storage:8GiAllowable access modes are ReadWriteOnce, ReadWriteMany, and for block devices that have been previously initialized, ReadOnlyMany.\nRaw Block volumes are presented as a block device to the Pod by using a bind mount to a block device in the node\u0026rsquo;s file system. The driver does not format or check the format of any file system on the block device. Raw Block volumes support online Volume Expansion, but it is up to the application to manage reconfiguring the file system (if any) to the new size.\nFor additional information, see the website: Kubernetes\nCSI PowerMax Reverse Proxy To get the maximum performance out of the CSI driver for PowerMax and Unisphere forPowerMax REST APIs, starting with v1.4 of the driver, you can deploy the optional CSI PowerMax Reverse Proxy application.\nCSI PowerMax Reverse Proxy is a (go) HTTPS server which acts as a reverse proxy for the Unisphere forPowerMax RESTAPI interface. Any RESTAPI request sent from the driver to the reverse proxy is forwarded to the Unisphere server and the response is routed back to the driver.\nThe Reverse Proxy helps regulate the maximum number of requests which can be sent to the Unisphere RESTAPI at a given time across all driver controller and node Pods. This helps with better queuing of CSI requests and performance of the CSI PowerMax driver.\nOptionally you can specify an alternate (backup) Unisphere server and if the primary Unisphere server is not reachable or does not respond, the proxy will redirect the calls to this alternate Unisphere.\nInstallation CSI PowerMax Reverse Proxy is installed as a Kubernetes deployment in the same namespace as the driver.\nIt is also configured as a Kubernetes \u0026ldquo;NodePort\u0026rdquo; service. If the CSI PowerMax driver has been configured to use this service, then it will connect to the IP address and port exposed by the Kubernetes service instead of directly connecting to the Unisphere server.\nPrerequisite CSI PowerMax Reverse Proxy is a HTTPS server and has to be configured with an SSL certificate and a private key.\nThe certificate and key are provided to the proxy via a Kubernetes TLS secret (in the same namespace). The SSL certificate must be a X.509 certificate encoded in PEM format. The certificates can be obtained via a Certificate Authority or can be self-signed and generated by a tool such as openssl.\nHere is an example to generate a private key and use that to sign an SSL certificate using the openssl tool:\nopenssl genrsa -out tls.key 2048 openssl req -new -x509 -sha256 -key tls.key -out tls.crt -days 3650 kubectl create secret -n \u0026lt;namespace\u0026gt; tls revproxy-certs --cert=tls.crt --key=tls.key kubectl create secret -n \u0026lt;namespace\u0026gt; tls csirevproxy-tls-secret --cert=tls.crt -- key=tls.key Using Helm installer A new section, csireverseproxy, in the my-powermax-settings.yaml file can be used to deploy and configure the CSI PowerMax Reverse Proxy.\nThe new Helm chart is configured as a sub chart for the CSI PowerMax helm chart. If it is enabled (using the enabled parameter in the csireverseproxy section of the my-powermax-settings.yaml file), the install script automatically installs the CSI PowerMax Reverse Proxy and configures the CSI PowerMax driver to use this service.\nUsing Dell CSI Operator Starting with the v1.1.0 release of the Dell CSI Operator, a new Custom Resource Definition can be used to install CSI PowerMax Reverse Proxy.\nThis Custom Resource has to be created in the same namespace as the CSI PowerMax driver and it has to be created before the driver Custom Resource. To use the service, the driver Custom Resource manifest must be configured with the service name \u0026ldquo;powermax-reverseproxy\u0026rdquo;. For complete installation instructions for the CSI PowerMax driver and the CSI PowerMax Reverse Proxy, see the Dell CSI Operator documentation.\nUser-friendly hostnames Users can set a value for the nodeNameTemplate in my-powermax-settings.yaml during the installation of the driver so that the driver can use this value to decide the name format of hosts to create or update in the PowerMax array for the nodes in a Kubernetes cluster. The hostname value in nodeNameTemplate should always be contained between two \u0026lsquo;%\u0026rsquo; characters. String prefixing first \u0026lsquo;%\u0026rsquo; and string suffixing second \u0026lsquo;%\u0026rsquo; is used as is before and after every node identifier.\nAlso, there is a new setting, modifyHostName, which could be set to true if you want the driver to rename the existing Hosts/IG for the host initiators on the PowerMax array. The new name uses the default naming convention (csi-\u0026lt;ClusterPrefix\u0026gt;-\u0026lt;HostName\u0026gt;*) or the nodeNameTemplate if it was specified.\nFor example, if nodeNameTemplate is abc-%foo%-hostname and nodename is worker1 , then the host ID is created or updated as abc-worker1-hostname. This change will happen for all nodes in a cluster with the respective node name.\nNOTE: nodeNameTemplate can contain alphanumeric characters [a - z, A - Z, 0 - 9], \u0026lsquo;-\u0026rsquo; and \u0026lsquo;_\u0026rsquo;, other characters are not allowed.\nController HA Starting with version 1.5, the CSI PowerMax driver supports running multiple replicas of controller Pod. At any time, only one controller Pod is active(leader), and the rest are on standby. In case of a failure, one of the standby Pods becomes active and takes the position of leader. This is achieved by using native leader election mechanisms utilizing kubernetes leases. Additionally by leveraging pod anti-affinity, no two controller Pods are ever scheduled on the same node.\nTo increase or decrease the number of controller Pods, edit the following value in values.yaml file:\ncontrollerCount: 2  NOTE: The default value for controllerCount is 2. We recommend not changing this unless it is really necessary. Also, if the controller count is greater than the number of available nodes (where the Pods can be scheduled), some controller Pods will remain in the Pending state\n If you are using the dell-csi-operator, adjust the following value in your Custom Resource manifest\nreplicas: 2 For more details about configuring Controller HA using the Dell CSI Operator, see the Dell CSI Operator documentation.\nNodeSelectors and Tolerations Starting with version 1.5, the CSI PowerMax driver helm installer allows you to specify a set of nodeSelectors and tolerations which can be applied on the driver controller Deployment and driver node Daemonset. There are two new sections in the values file - controller and node - where you can specify these values separately for the controller and node Pods.\ncontroller If you want to apply nodeSelectors and tolerations for the controller Pods, edit the controller section in the values file.\nHere are some examples:\n To schedule controller Pods to worker nodes only (Default):  controller:nodeSelector:tolerations: Set the following values for controller Pods to tolerate the taint NoSchedule on master nodes:  controller:nodeSelector:tolerations:- key:\u0026#34;node-role.kubernetes.io/master\u0026#34;operator:\u0026#34;Exists\u0026#34;effect:\u0026#34;NoSchedule\u0026#34; Set the following values for controller pods to be only scheduled on nodes labelled as master (node-role.kubernetes.io/master):  controller:nodeSelector:node-role.kubernetes.io/master:\u0026#34;\u0026#34;tolerations:- key:\u0026#34;node-role.kubernetes.io/master\u0026#34;operator:\u0026#34;Exists\u0026#34;effect:\u0026#34;NoSchedule\u0026#34;node If you want to apply nodeSelectors and tolerations for the node Pods, edit the node section in the values file.\nThe values file already includes a set of default tolerations and you can add and remove tolerations to this list\n# \u0026#34;node\u0026#34; allows to configure node specific parametersnode:# \u0026#34;node.nodeSelector\u0026#34; defines what nodes would be selected for Pods of node daemonset# Leave as blank to use all nodesnodeSelector:# node-role.kubernetes.io/master: \u0026#34;\u0026#34;# \u0026#34;node.tolerations\u0026#34; defines tolerations that would be applied to node daemonset# Add/Remove tolerations as per requirement# Leave as blank if you wish to not apply any tolerationstolerations:- key:\u0026#34;node.kubernetes.io/memory-pressure\u0026#34;operator:\u0026#34;Exists\u0026#34;effect:\u0026#34;NoExecute\u0026#34;- key:\u0026#34;node.kubernetes.io/disk-pressure\u0026#34;operator:\u0026#34;Exists\u0026#34;effect:\u0026#34;NoExecute\u0026#34;- key:\u0026#34;node.kubernetes.io/network-unavailable\u0026#34;operator:\u0026#34;Exists\u0026#34;effect:\u0026#34;NoExecute\u0026#34;Topology Support Starting from version 1.5, the CSI PowerMax driver supports topology-aware volume provisioning which helps Kubernetes scheduler place PVCs on worker nodes which have access to backend storage. When used with nodeSelectors which can be specified for the driver node Pods, it provides an effective way to provision applications on nodes which have access to the PowerMax array.\nAfter a successful installation of the driver, if a node Pod is running successfully on a worker node, the following topology keys are created for a specific PowerMax array:\n csi-powermax.dellemc.com/\\\u0026lt;array-id\\\u0026gt; If the worker node has Fibre Channel connectivity to the PowerMax array - csi-powermax.dellemc.com/\\\u0026lt;array-id\\\u0026gt;.fc If the worker node has ISCSI connectivity to the PowerMax array - csi-powermax.dellemc.com/\\\u0026lt;array-id\\\u0026gt;.iscsi  The values for all these keys are always set to the name of the provisioner which is usually csi-powermax.dellemc.com.\n NOTE: The Topology support does not include any customer-defined topology, that is, users cannot create their own labels for nodes and storage classes and expect the labels to be honored by the driver.\n Topology Usage To use the Topology feature, the storage classes must be modified as follows:\n volumeBindingMode must be set to WaitForFirstConsumer allowedTopologies should be set to one or more topology keys described in the previous section  For example, a PVC created using the following storage class will always be scheduled on nodes which have FC connectivity to the PowerMax array 000000000001\napiVersion:storage.k8s.io/v1kind:StorageClassmetadata:name:powermax-fcparameters:SRP:\u0026#34;SRP_1\u0026#34;SYMID:\u0026#34;000000000001\u0026#34;ServiceLevel:\u0026lt;ServiceLevel\u0026gt;#Insert Service Level Nameprovisioner:csi-powermax.dellemc.comreclaimPolicy:DeletevolumeBindingMode:WaitForFirstConsumerallowVolumeExpansion:trueallowedTopologies:- matchLabelExpressions:- key:csi-powermax.dellemc.com/000000000001values:- csi-powermax.dellemc.com- key:csi-powermax.dellemc.com/000000000001.fcvalues:- csi-powermax.dellemc.comIn the above example if you remove the entry for the key csi-powermax.dellemc.com/000000000001.fc, then the PVCs created using this storage class will be scheduled on any worker node with access to the PowerMax array 000000000001 irrespective of the transport protocol\n NOTE: The storage classes created during the driver installation (via Helm) do not contain any topology keys and have the volumeBindingMode set to Immediate. A set of sample storage class definitions to enable topology -aware volume provisioning has been provided in the csi-powermax/helm/samples/storageclass folder\n For additional information on how to use Topology aware Volume Provisioning, see the Kubernetes Topology documentation.\nIf you are using dell-csi-operator to create storage classes while installing the CSI PowerMax 1.5 driver, you can set the allowedTopologies value appropriately. volumeBindingMode is set to WaitForFirstConsumer if not specified explicitly.\n","excerpt":"Volume Snapshot Feature The Volume Snapshot feature was introduced in alpha (v1alpha1) in Kubernetes …","ref":"/ansible-docs/v1/features/powermax/","title":"PowerMax"},{"body":"The CSI Driver for Dell EMC PowerMax can be deployed by using the provided Helm v3 charts and installation scripts on both Kubernetes and OpenShift platforms. For more detailed information on the installation scripts, see the script documentation.\nThe controller section of the Helm chart installs the following components in a Deployment in the powermax namespace:\n CSI Driver for Dell EMC PowerMax Kubernetes External Provisioner, which provisions the volumes Kubernetes External Attacher, which attaches the volumes to the containers Kubernetes External Snapshotter, which provides snapshot support Kubernetes External Resizer, which resizes the volume  The node section of the Helm chart installs the following component in a DaemonSet in the namespace powermax:\n CSI Driver for Dell EMC PowerMax Kubernetes Node Registrar, which handles the driver registration  Prerequisites The following requirements must be met before installing the CSI Driver for Dell EMC PowerMax:\n Install Kubernetes or OpenShift (see supported versions) Install Helm 3 Fibre Channel requirements iSCSI requirements Certificate validation for Unisphere REST API calls Configure Mount propagation on container runtime (that is, Docker) Linux multipathing requirements Volume Snapshot requirements  Install Helm 3 Install Helm 3 on the master node before you install the CSI Driver for Dell EMC PowerMax.\nSteps\nRun the curl https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3 | bash command to install Helm 3.\nFibre Channel Requirements CSI Driver for Dell EMC PowerMax supports Fibre Channel communication. Ensure that the following requirements are met before you install the CSI Driver:\n Zoning of the Host Bus Adapters (HBAs) to the Fibre Channel port director must be completed. Ensure that the HBA WWNs (initiators) appear on the list of initiators that are logged into the array. If the number of volumes that will be published to nodes is high, then configure the maximum number of LUNs for your HBAs on each node. See the appropriate HBA document to configure the maximum number of LUNs.  iSCSI Requirements The CSI Driver for Dell EMC PowerMax supports iSCSI connectivity. These requirements are applicable for the nodes that use iSCSI initiator to connect to the PowerMax arrays.\nSet up the iSCSI initiators as follows:\n All Kubernetes nodes must have the iscsi-initiator-utils package installed. Ensure that the iSCSI initiators are available on all the nodes where the driver node plugin will be installed. Kubernetes nodes should have access (network connectivity) to an iSCSI director on the Dell EMC PowerMax array that has IP interfaces. Manually create IP routes for each node that connects to the Dell EMC PowerMax if required. Ensure that the iSCSI initiators on the nodes are not a part of any existing Host (Initiator Group) on the Dell EMC PowerMax array. The CSI Driver needs the port group names containing the required iSCSI director ports. These port groups must be set up on each Dell EMC PowerMax array. All the port groups names supplied to the driver must exist on each Dell EMC PowerMax with the same name.  For information about configuring iSCSI, see Dell EMC PowerMax documentation on Dell EMC Support.\nCertificate validation for Unisphere REST API calls As part of the CSI driver installation, the CSI driver requires a secret with the name powermax-certs present in the namespace powermax. This secret contains the X509 certificates of the CA which signed the Unisphere SSL certificate in PEM format. This secret is mounted as a volume in the driver container. In earlier releases, if the install script did not find the secret, it created an empty secret with the same name. From the 1.2.0 release, the secret volume has been made optional. The install script no longer attempts to create an empty secret.\nThe CSI driver exposes an install parameter skipCertificateValidation which determines if the driver performs client-side verification of the Unisphere certificates. The skipCertificateValidation parameter is set to true by default, and the driver does not verify the Unisphere certificates.\nIf the skipCertificateValidation parameter is set to false and a previous installation attempt created an empty secret, then this secret must be deleted and re-created using the CA certs.\nIf the Unisphere certificate is self-signed or if you are using an embedded Unisphere, then perform the following steps:\n  To fetch the certificate, run openssl s_client -showcerts -connect [Unisphere IP]:8443 \u0026lt;/dev/null\u0026gt; /dev/null | openssl x509 -outform PEM \u0026gt; ca_cert.pem\nNOTE: The IP address varies for each user.\n  To create the secret, run kubectl create secret generic powermax-certs --from-file=ca_cert.pem -n powermax\n  Ports in port group There are no restrictions around how many ports can be present in the iSCSI port groups provided to the driver.\nThe same applies to Fibre Channel where there are no restrictions on the number of FA directors a host HBA can be zoned to. See the best practices for host connectivity to Dell EMC PowerMax to ensure that you have multiple paths to your data volumes.\nConfigure Mount Propagation on Container Runtime You must configure mount propagation on your container runtime on all Kubernetes nodes before installing the CSI Driver for Dell EMC PowerMax. The following steps explain how to do this with Docker. If you use another container runtime please follow the recommended instructions from the vendor to configure mount propagation.\nSteps\n Edit the service section of /etc/systemd/system/multi-user.target.wants/docker.service file to add the following lines: docker.service [Service]... MountFlags=shared  Restart the docker service with systemctl daemon-reload and systemctl restart docker on all the nodes. Restart the docker service with systemctl daemon-reload and systemctl restart docker on all the nodes.  NOTE: Some distribution, like Ubuntu, already has MountFlags set by default.\nLinux multipathing requirements CSI Driver for Dell EMC PowerMax supports Linux multipathing. Configure Linux multipathing before installing the CSI Driver.\nSet up Linux multipathing as follows:\n All the nodes must have the Device Mapper Multipathing package installed.\nNOTE: When this package is installed it creates a multipath configuration file which is located at /etc/multipath.conf. Please ensure that this file always exists. Enable multipathing using mpathconf --enable --with_multipathd y Enable user_friendly_names and find_multipaths in the multipath.conf file.  Volume Snapshot Requirements Volume Snapshot CRD\u0026rsquo;s The Kubernetes Volume Snapshot CRDs can be obtained and installed from the external-snapshotter project on Github.\n If on Kubernetes 1.18/1.19 (beta snapshots) use v3.0.3 If on Kubernetes 1.20 (v1 snapshots) use v4.0.0  Volume Snapshot Controller Starting with beta Volume Snapshots in Kubernetes 1.17, the CSI external-snapshotter sidecar is split into two controllers:\n A common snapshot controller A CSI external-snapshotter sidecar  The common snapshot controller must be installed only once in the cluster irrespective of the number of CSI drivers installed in the cluster. On OpenShift clusters 4.4 and later, the common snapshot-controller is pre-installed. In the clusters where it is not present, it can be installed using kubectl and the manifests are available:\n If on Kubernetes 1.18/1.19 (beta snapshots) use v3.0.3 If on Kubernetes 1.20 (v1 snapshots) use v4.0.0  NOTE:\n The manifests available on GitHub install the snapshotter image:  quay.io/k8scsi/csi-snapshotter:v3.0.3 quay.io/k8scsi/csi-snapshotter:v4.0.0   The CSI external-snapshotter sidecar is still installed along with the driver and does not involve any extra configuration.  Install the Driver Steps\n Run git clone https://github.com/dell/csi-powermax.git to clone the git repository. This will include the Helm charts and dell-csi-helm-installer scripts. Ensure that you have created a namespace where you want to install the driver. You can run kubectl create namespace powermax to create a new one Edit the `helm/secret.yaml file, point to the correct namespace and replace the values for the username and password parameters. These values can be obtained using base64 encoding as described in the following example: echo -n \u0026#34;myusername\u0026#34; | base64 echo -n \u0026#34;mypassword\u0026#34; | base64 where myusername and mypassword are credentials for a user with PowerMax priviledges.\n Create the secret by running kubectl create -f helm/secret.yaml If you are going to install the new CSI PowerMax ReverseProxy service, create a TLS secret with the name - csireverseproxy-tls-secret which holds an SSL certificate and the corresponding private key in the namespace where you are installing the driver. Copy the default values.yaml file `cd helm \u0026amp;\u0026amp; cp csi-powermax/values.yaml my-powermax-settings.yaml Edit the newly created file and provide values for the following parameters vi my-powermax-settings.yaml     Parameter Description Required Default     unisphere Specifies the URL of the Unisphere for PowerMax server. If using the CSI PowerMax Reverse Proxy, leave this value unchanged at https://127.0.0.1:8443. Yes \u0026ldquo;https://127.0.0.1:8443\u0026rdquo;   clusterPrefix Prefix that is used during the creation of various masking-related entities (Storage Groups, Masking Views, Hosts, and Volume Identifiers) on the array. The value that you specify here must be unique. Ensure that no other CSI PowerMax driver is managing the same arrays that are configured with the same prefix. The maximum length for this prefix is three characters. Yes \u0026ldquo;ABC\u0026rdquo;   controller Allows configuration of the controller-specific parameters. - -   node Allows configuration of the node-specific parameters. - -   tolerations Add tolerations as per requirement No -   nodeSelector Add node selectors as per requirement No -   defaultFsType Used to set the default FS type for external provisioner Yes ext4   portGroups List of comma-separated port group names. Any port group that is specified here must be present on all the arrays that the driver manages. For iSCSI Only \u0026ldquo;PortGroup1, PortGroup2, PortGroup3\u0026rdquo;   arrayWhitelist List of comma-separated array IDs. If this parameter remains empty, the driver manages all the arrays that are managed by the Unisphere instance that is configured for the driver. Specify the IDs of the arrays that you want to manage, using the driver. No Empty   symmetrixID Specify a Dell EMC PowerMax array that the driver manages. This value is used to create a default storage class. Yes \u0026ldquo;000000000000\u0026rdquo;   storageResourcePool This parameter must mention one of the SRPs on the PowerMax array that the symmetrixID specifies. This value is used to create the default storage class. Yes \u0026ldquo;SRP_1\u0026rdquo;   serviceLevel This parameter must mention one of the Service Levels on the PowerMax array. This value is used to create the default storage class. Yes \u0026ldquo;Bronze\u0026rdquo;   skipCertificateValidation Skip client-side TLS verification of Unisphere certificates No \u0026ldquo;True\u0026rdquo;   transportProtocol Set preferred transport protocol for the Kubernetes cluster which helps the driver choose between FC and iSCSI when a node has both FC and iSCSI connectivity to a PowerMax array. No Empty   nodeNameTemplate Used to specify a template which will be used by the driver to create Host/IG names on the PowerMax array. To use the default naming convention, then leave this value empty. No Empty   csireverseproxy This section refers to configuration options for CSI PowerMax Reverse Proxy - -   enabled Boolean parameter which indicates if CSI PowerMax Reverse Proxy is going to be configured and installed.\nNOTE: If not enabled, then there is no requirement to configure any of the following values. No \u0026ldquo;False\u0026rdquo;   port Specify the port number that is used by the NodePort service created by the CSI PowerMax Reverse Proxy installation No 2222   primary Mandatory section for Reverse Proxy - -   unisphere This must specify the URL of the Unisphere for PowerMax server Yes, if using Reverse Proxy \u0026ldquo;https://0.0.0.0:8443\u0026rdquo;   skipCertificateValidation This parameter should be set to false if you want to do client-side TLS verification of Unisphere for PowerMax SSL certificates. It is set to true by default. No \u0026ldquo;True\u0026rdquo;   certSecret The name of the secret in the same namespace containing the CA certificates of the Unisphere server Yes, if skipCertificateValidation is set to false Empty   backup Optional section for Reverse Proxy. Specify Unisphere server address which the Reverse Proxy can fall back to if the primary Unisphere is unreachable or unresponsive.\nNOTE: If you do not want to specify a backup Unisphere server, then remove the backup section from the file - -   unisphere Specify the IP address of the Unisphere for PowerMax server which manages the arrays being used by the CSI driver No \u0026ldquo;https://0.0.0.0:8443\u0026rdquo;   skipCertificateValidation This parameter should be set to false if you want to do client-side TLS verification of Unisphere for PowerMax SSL certificates. It is set to true by default. No \u0026ldquo;True\u0026rdquo;   certSecret The name of the secret in the same namespace containing the CA certificates of the Unisphere server No Empty    Install the driver using csi-install.sh bash script by running cd ../dell-csi-helm-installer \u0026amp;\u0026amp; ./csi-install.sh --namespace powermax --values ../helm/my-powermax-settings.yaml  Note:\n For detailed instructions on how to run the install scripts, see the readme document in the dell-csi-helm-installer folder. This script also runs the verify.sh script in the same directory. You will be prompted to enter the credentials for each of the Kubernetes nodes. The verify.sh script needs the credentials to check if the iSCSI initiators have been configured on all nodes. You can also skip the verification step by specifying the --skip-verify-node option  Storage Classes Starting in CSI PowerMax v1.6, dell-csi-helm-installer will not create any storage classes as part of the driver installation. A wide set of annotated storage class manifests has been provided in the helm/samples folder. Please use these samples to create new storage classes to provision storage. See this note for the driving reason behind this change.\nWhat happens to my existing storage classes? Upgrading from CSI PowerMax v1.5 driver The storage classes created as part of the installation have an annotation - \u0026ldquo;helm.sh/resource-policy\u0026rdquo;: keep set. This ensures that even after an uninstall or upgrade, the storage classes are not deleted. You can continue using these storage classes if you wish so.\nUpgrading from an older version of the driver The storage classes will be deleted if you upgrade the driver. If you wish to continue using those storage classes, you can patch them and apply the annotation \u0026ldquo;helm.sh/resource-policy\u0026rdquo;: keep before performing an upgrade.\nNote: If you continue to use the old storage classes, you may not be able to take advantage of any new storage class parameter supported by the driver.\n","excerpt":"The CSI Driver for Dell EMC PowerMax can be deployed by using the provided Helm v3 charts and …","ref":"/ansible-docs/v1/installation/helm/powermax/","title":"PowerMax"},{"body":"Installing PowerMax CSI Driver via Operator The CSI Driver for Dell EMC PowerMax can be installed via the Dell CSI Operator.\nTo deploy the Operator, follow the instructions available here.\nThere are sample manifests provided which can be edited to do an easy installation of the driver. Please note that the deployment of the driver using the operator does not use any Helm charts and the installation and configuration parameters will be slightly different from the ones specified via the Helm installer.\nKubernetes Operators make it easy to deploy and manage entire the lifecycle of complex Kubernetes applications. Operators use Custom Resource Definitions (CRD) which represents the application and use custom controllers to manage them.\nPrerequisite Create secret for client-side TLS verification (Optional) Create a secret named powermax-certs in the namespace where the CSI PowerMax driver will be installed. This is an optional step and is only required if you are setting the env variable X_CSI_POWERMAX_SKIP_CERTIFICATE_VALIDATION to false. See the detailed documentation on how to create this secret here.\nInstall Driver  Create namespace: Run kubectl create namespace \u0026lt;driver-namespace\u0026gt; using the desired name to create the namespace. Create PowerMax credentials: Create a file called powermax-creds.yaml with the following content apiVersion:v1kind:Secretmetadata:name:powermax-creds# Replace driver-namespace with the namespace where driver is being deployednamespace:\u0026lt;driver-namespace\u0026gt; type: Opaquedata:# set username to the base64 encoded usernameusername:\u0026lt;base64username\u0026gt; # set password to the base64 encoded passwordpassword:\u0026lt;base64password\u0026gt; # Uncomment the following key if you wish to use ISCSI CHAP authentication (v1.3.0 onwards)# chapsecret: \u0026lt;base64 CHAP secret\u0026gt;Replace the values for the username and password parameters. These values can be obtained using base64 encoding as described in the following example:\necho -n \u0026quot;myusername\u0026quot; | base64 echo -n \u0026quot;mypassword\u0026quot; | base64 # If mychapsecret is the ISCSI CHAP secret echo -n \u0026quot;mychapsecret\u0026quot; | base64 Run the kubectl create -f powermax-creds.yaml command to create the secret.\n Create a Custom Resource (CR) for PowerMax using the sample files provided here. Users should configure the parameters in CR. The following table lists the primary configurable parameters of the PowerMax driver and their default values:    Parameter Description Required Default     replicas Controls the number of controller Pods you deploy. If controller Pods are greater than the number of available nodes, excess Pods will become stuck in pending. The default is 2 which allows for Controller high availability. Yes 2   Common parameters for node and controller      X_CSI_K8S_CLUSTER_PREFIX Define a prefix that is appended to all resources created in the array; unique per K8s/CSI deployment; max length - 3 characters Yes XYZ   X_CSI_POWERMAX_ENDPOINT IP address of the Unisphere for PowerMax Yes https://0.0.0.0:8443   X_CSI_TRANSPORT_PROTOCOL Choose what transport protocol to use (ISCSI, FC, auto or None) Yes auto   X_CSI_POWERMAX_PORTGROUPS List of comma-separated port groups (ISCSI only). Example: \u0026ldquo;PortGroup1,PortGroup2\u0026rdquo; No -   X_CSI_POWERMAX_ARRAYS List of comma-separated array id(s) which will be managed by the driver No -   X_CSI_POWERMAX_PROXY_SERVICE_NAME Name of CSI PowerMax ReverseProxy service. Leave blank if not using reverse proxy No -   X_CSI_GRPC_MAX_THREADS Number of concurrent grpc requests allowed per client No 4   X_CSI_POWERMAX_DRIVER_NAME Set custom CSI driver name. For more details on this feature see the related documentation No -   X_CSI_IG_NODENAME_TEMPLATE Template used for creating hosts on PowerMax. Example: \u0026ldquo;a-b-c-%foo%-xyz\u0026rdquo; where the text between the % symbols(foo) is replaced by the actual host name No -   X_CSI_IG_MODIFY_HOSTNAME Determines if the node plugin can rename any existing host on the PowerMax array. Use it with the node name template to rename the existing hosts No false   X_CSI_POWERMAX_DEBUG Determines if HTTP Request/Response is logged No false   Node parameters      X_CSI_POWERMAX_ISCSI_ENABLE_CHAP Enable ISCSI CHAP authentication. For more details on this feature see the related documentation No false   StorageClass parameters      SYMID Symmetrix ID Yes 000000000001   SRP Storage Resource Pool Name Yes DEFAULT_SRP   ServiceLevel Service Level No Bronze   FsType File System type (xfs/ext4) xfs    allowVolumeExpansion After the allowed topology is modified in storage class, Pods/and volumes will always be scheduled on nodes that have access to the storage No false   allowedTopologies:key This is to enable topology to allow Pods/and volumes to always be scheduled on nodes that have access to the storage. You need to specify the PowerMax array ID and append .fc or .iscsi at the end of it to specify a protocol. For more details on this feature see the related documentation No \u0026ldquo;000000000001\u0026rdquo;     Execute the following command to create PowerMax custom resource:kubectl create -f \u0026lt;input_sample_file.yaml\u0026gt;. The above command will deploy the CSI-PowerMax driver.  CSI PowerMax ReverseProxy CSI PowerMax ReverseProxy is an optional component which can be installed along with the CSI PowerMax driver. For more details on this feature see the related documentation.\nWhen you install CSI PowerMax ReverseProxy, dell-csi-operator will create a Deployment and ClusterIP service as part of the installation\nNote - To use the ReverseProxy with CSI PowerMax driver, the ReverseProxy service should be created before you install the CSIPowerMax driver.\nPre-requisites Create a TLS secret which holds a SSL certificate and a private key which is required by the reverse proxy server. Use a tool such as openssl to generate this secret using the example below:\n openssl genrsa -out tls.key 2048 openssl req -new -x509 -sha256 -key tls.key -out tls.crt -days 3650 kubectl create secret -n powermax tls revproxy-certs --cert=tls.crt --key=tls.key Set the following parameters in the CSI PowerMaxReverseProxy Spec tlsSecret : Provide the name of the TLS secret. If using the above example, it should be set to revproxy-certs\nconfig : This section contains the details of the Reverse Proxy configuration\nmode : This value is set to Linked by default. Do not change this value\nlinkConfig : This section contains the configuration of the Linked mode\nprimary : This section holds details for the primary Unisphere which the Reverse Proxy will connect to backup : This optional section holds details for a backup Unisphere which the Reverse Proxy can connect to if the primary Unisphere is unreachable\nurl : URL of the Unisphere server skipCertificateValidation: This setting determines if the client-side Unisphere certificate validation is required certSecret: Secret name which holds the CA certificates which was used to sign Unisphere SSL certificates. Mandatory if skipCertificateValidation is set to false\nHere is a sample manifest with each field annotated. A copy of this manifest is provided in the samples folder\napiVersion:storage.dell.com/v1kind:CSIPowerMaxRevProxymetadata:name:powermax-reverseproxy# \u0026lt;- Name of the CSIPowerMaxRevProxy objectnamespace:test-powermax# \u0026lt;- Set the namespace to where you will install the CSI PowerMax driverspec:# Image for CSI PowerMax ReverseProxyimage:dellemc/csipowermax-reverseproxy:v1.0.0.000R# \u0026lt;- CSI PowerMax Reverse Proxy image imagePullPolicy:Always# TLS secret which contains SSL certificate and private key for the Reverse Proxy servertlsSecret:csirevproxy-tls-secretconfig:# Mode for the proxy - only supported mode for now is \u0026#34;Linked\u0026#34;mode:LinkedlinkConfig:primary:url:https://0.0.0.0:8443#Unisphere URLskipCertificateValidation:true# This setting determines if client side Unisphere certificate validation is to be skippedcertSecret:\u0026#34;\u0026#34;# Provide this value if skipCertificateValidation is set to falsebackup:# This is an optional field and lets you configure a backup unisphere which can be used by proxy serverurl:https://0.0.0.0:8443#Unisphere URLskipCertificateValidation:trueInstallation Copy the sample file - powermax_reverseproxy.yaml from the samples folder or use the sample available in the OperatorHub UI\nEdit and input all required parameters and then use the OperatorHub UI or run the following command to install the CSI PowerMax Reverse Proxy service:\nkubectl create -f powermax_reverseproxy.yaml  You can query for the deployment and service created as part of the installation using the following commands:\nkubectl get deployment -n \u0026lt;namespace\u0026gt; kubectl get svc -n \u0026lt;namespace\u0026gt;  ","excerpt":"Installing PowerMax CSI Driver via Operator The CSI Driver for Dell EMC PowerMax can be installed …","ref":"/ansible-docs/v1/installation/operator/powermax/","title":"PowerMax"},{"body":"This section provides multiple methods to test driver functionality in your environment. The tests are validated using bash as the default shell.\nNote: To run the test for CSI Driver for Dell EMC PowerMax, install Helm 3.\nThe csi-powermax repository includes examples of how you can use the CSI Driver for Dell EMC PowerMax. These examples automate the creation of Pods using the default storage classes that were created during installation. The shell scripts are used to automate the installation and uninstallation of helm charts for the creation of Pods with different number of volumes. To test the installation of the CSI driver, perform these tests:\n Volume clone test Volume test Snapshot test  Volume test Use this procedure to perform a volume test.\n  Create a namespace with the name test.\n  Run the cd csi-powermax/test/helm command to go to the csi-powermax/test/helm directory, which contains the starttest.sh script and the 2vols directories.\n  Run the starttest.sh script and provide it with a test name. The following is a sample command that can be used to run the 2vols test: ./starttest.sh -t 2vols -n test\nThis script installs a helm chart that creates a Pod with a container, creates two PVCs, and mounts them into the created container. You can now log in to the newly created container and check the mounts.\n  Run the ./stoptest.sh -t 2vols -n test script to stop the test. This script deletes the Pods and the PVCs created during the test and uninstalls the helm chart.\n   NOTE: Helm tests have been designed assuming that users are using the default storageclass names (powermax and powermax-xfs). If your storageclass names differ from the default values, such as when deploying with the Operator, update the templates in 2vols accordingly (located in test/helm/2vols/templates/ directory). You can use kubectl get sc to check for the storageclass names.\n Volume clone test Use this procedure to perform a volume clone test.\n Create a namespace with the name test. Run the cd csi-powermax/test/helm command to go to the csi-powermax/test/helm directory, which contains the volumeclonetest.sh script. Run the volumeclonetest.sh script using the following command: bash volumeclonetest.sh  This script does the following:\n Installs a helm chart that creates a Pod with a container, creates two PVCs, and mounts them into the created container. Then it creates a file on one of the PVCs and calculates its checksum. After that, it uses that PVC as the data source to create a new PVC and mounts it on the same container. It checks if the file that existed in the source PVC also exists in the new PVC, calculates its checksum and compares it to the checksum previously calculated. Finally, it cleans up all the resources that are created as part of the test.  Snapshot test Use this procedure to perform a snapshot test.\n Create a namespace with the name test. Run the cd csi-powermax/test/helm command to go to the csi-powermax/test/helm directory, which contains the snaprestoretest.shscript. Run the snaprestoretest.sh script by running the command : bash snaprestoretest.sh  This script does the following:\n Installs a helm chart that creates a Pod with a container, creates two PVCs, and mounts them into the created container. Writes some data to one of the PVCs. After that, it creates a snapshot on that PVC and uses it as a data source to create a new PVC. It mounts the newly created PVC to the container created earlier and then lists the contents of the source and the target PVCs. Cleans up all the resources that were created as part of the test.   NOTE: This test has been designed assuming that users are using the snapshot class name powermax-snapclass which is created by the Helm-based installer. If you have an operator-based deployment, the name of the snapshot class will differ. You must update the snapshot class name in the file betaSnap1.yaml present in the test/helm folder based on your method of deployment. To get a list of volume snapshot classes, run the command - kubectl get volumesnapshotclass\n Volume Expansion test Use this procedure to perform a volume expansion test.\n Create a namespace with the name test Run the cd csi-powermax/test/helm command to go to the csi-powermax/test/helm directory, which contains the volumeexpansiontest.shscript. Run the volumeexpansiontest.sh script by running the command : bash volumeexpansiontest.sh  This script does the following:\n Installs a helm chart that creates a Pod with a container, creates one PVC and mounts it into the created container Writes some data to the PVC After that, it calculates the checksum of the written data, expands the PVC and then recalculates the checksum Cleans up all the resources that were created as part of the test  ","excerpt":"This section provides multiple methods to test driver functionality in your environment. The tests …","ref":"/ansible-docs/v1/installation/test/powermax/","title":"Test PowerMax CSI Driver"},{"body":"Release Notes - CSI PowerMax v1.6.0 New Features/Changes  Added support for Kubernetes v1.20 Added support for OpenShift 4.7 with RHEL and CoreOS worker nodes Added support for Red Hat Enterprise Linux (RHEL) 8.3 Removed storage classes from helm template  Fixed Issues There are no fixed issues in this release.\nKnown Issues    Issue Workaround     Slow volume attached/detach If your Kubernetes 1.18 cluster has a lot of VolumeAttachment objects, the attach/detach operations will be very slow. This is a known issue and affects all CSI plugins. It is tracked here: CSI VolumeAttachment slows pod startup time. To get around this problem you can upgrade to latest Kubernetes/OpenShift patches, which contains a partial fix: 1.18.5+   Delete Volume fails with error message: volume is part of masking view This issue is due to limitations in Unisphere and occurs when Unisphere is overloaded. Currently, there is no workaround for this but can be avoided by making sure Unisphere is not overloaded during such operations. The Unisphere team is assessing a fix for this in a future Unisphere release   Driver installation warning: \u0026ldquo;OpenShift version 4.7, is newer than the version that has been tested. Latest tested version is: 4.6\u0026rdquo; Ignore this warning and continue with the installation. v1.6.0 release of the driver supports OpenShift 4.6/4.7 .    ","excerpt":"Release Notes - CSI PowerMax v1.6.0 New Features/Changes  Added support for Kubernetes v1.20 Added …","ref":"/ansible-docs/v1/release/powermax/","title":"PowerMax"},{"body":"   Symptoms Prevention, Resolution or Workaround     Warning about feature gates Double check that you have applied all the features to the indicated processes. Restart kubelet when remediated.   kubectl describe pod powermax-controller-\u0026lt;xyz\u0026gt; –n \u0026lt;namespace\u0026gt; indicates the driver image could not be loaded You may need to put an insecure-registries entry in /etc/docker/daemon.json or login to the docker registry   kubectl logs powermax-controller-\u0026lt;xyz\u0026gt; –n \u0026lt;namespace\u0026gt; driver logs shows the driver cannot authenticate Check your secret’s username and password   kubectl logs powermax-controller-\u0026lt;xyz\u0026gt; –n \u0026lt;namespace\u0026gt; driver logs shows the driver failed to connect to the U4P because it couldn’t verify the certificates Check the powermax-certs secret and ensure it is not empty or it has the valid certificates    ","excerpt":"   Symptoms Prevention, Resolution or Workaround     Warning about feature gates Double check that …","ref":"/ansible-docs/v1/troubleshooting/powermax/","title":"PowerMax"},{"body":"Volume Snapshot Feature The CSI PowerMax driver supports beta snapshots. Driver versions prior to version 1.4 supported alpha snapshots.\nThe Volume Snapshots feature in Kubernetes has moved to beta in Kubernetes version 1.17. It was an alpha feature in earlier releases (1.13 onwards). The snapshot API version has changed from v1alpha1 to v1beta1 with this migration.\nIn order to use Volume Snapshots, ensure the following components have been deployed to your cluster:\n Kubernetes Volume Snaphshot CRDs Volume Snapshot Controller  Volume Snapshot Class Starting CSI PowerMax 1.4 driver, a Volume Snapshot Class is created using the new v1beta1 snapshot APIs. This is the only Volume Snapshot Class you will need and there is no need to create any other Volume Snapshot Class.\nFollowing is the manifest for the Volume Snapshot Class created during installation (using the default driver name):\napiVersion: snapshot.storage.k8s.io/v1beta1 deletionPolicy: Delete kind: VolumeSnapshotClass metadata: name: powermax-snapclass driver: csi-powermax.dellemc.com Creating Volume Snapshots The following is a sample manifest for creating a Volume Snapshot using the v1beta1 snapshot APIs:\napiVersion: snapshot.storage.k8s.io/v1beta1 kind: VolumeSnapshot metadata: name: pmax-snapshot-demo namespace: test spec: volumeSnapshotClassName: powermax-snapclass source: persistentVolumeClaimName: pmax-pvc-demo Once the VolumeSnapshot has been successfully created by the CSI PowerMax driver, a VolumeSnapshotContent object is automatically created. Once the status of the VolumeSnapshot object has the readyToUse field set to true , it is available for use.\nFollowing is the relevant section of VolumeSnapshot object status:\nstatus: boundVolumeSnapshotContentName: snapcontent-5a8334d2-eb40-4917-83a2-98f238c4bda creationTime: \u0026quot;2020-07-16T08:42:12Z\u0026quot; readyToUse: true Creating PVCs with VolumeSnapshots as Source The following is a sample manifest for creating a PVC with a VolumeSnapshot as a source:\napiVersion: v1 kind: PersistentVolumeClaim metadata: name: pmax-restore-pvc-demo namespace: test spec: storageClassName: powermax dataSource: name: pmax-snapshot-demo kind: VolumeSnapshot apiGroup: snapshot.storage.k8s.io accessModes: - ReadWriteOnce resources: requests: storage: 8Gi Creating PVCs with PVCs as source This is a sample manifest for creating a PVC with another PVC as a source:\napiVersion: v1 kind: PersistentVolumeClaim metadata: name: pmax-clone-pvc-demo namespace: test spec: storageClassName: powermax dataSource: name: pmax-pvc-demo kind: PersistentVolumeClaim accessModes: - ReadWriteOnce resources: requests: storage: 8Gi iSCSI CHAP With version 1.3.0, support has been added for unidirectional Challenge Handshake Authentication Protocol (CHAP) for iSCSI. To enable CHAP authentication:\n Create secret powermax-creds with the key chapsecret set to the iSCSI CHAP secret. If the secret exists, delete and re-create the secret with this newly added key. Set the parameter enableCHAP in my-powermax-settings.yaml to true.  The driver uses the provided chapsecret to configure the iSCSI node database on each node with iSCSI access.\nWhen the driver is installed and all the node plug-ins have initialized successfully, the storage administrator must enable CHAP authentication using the following Solutions Enabler (SYMCLI) commands:\nsymaccess -sid \u0026lt;symid\u0026gt; -iscsi \u0026lt;host iqn\u0026gt; set chap -cred \u0026lt;host IQN\u0026gt; -secret \u0026lt;CHAP secret\u0026gt;\nWhere is the name of the iSCSI initiator of a host IQN, and is the chapsecret that is used at the time of the installation of the driver.\nNOTE: The host IQN is also used as the username when setting up the CHAP credentials.\nCHAP support for PowerMax With unidirectional CHAP, the PowerMax array challenges the host initiator during the initial link negotiation process and expects to receive a valid credential and CHAP secret in response.\nWhen challenged, the host initiator transmits a CHAP credential and CHAP secret to the storage array. The storage array looks for this credential and CHAP secret which stored in the host initiator initiator group. When a positive authentication occurs, the PowerMax array sends an acceptance message to the host. However, if the PowerMax array fails to find any record of the credential/secret pair, it sends a rejection message, and the link is closed.\nCustom Driver Name (Experimental feature) With version 1.3.0 of the driver, a custom name can be assigned to the driver at the time of installation. This enables installation of the CSI driver in a different namespace and installation of multiple CSI drivers for Dell EMC PowerMax in the same Kubernetes/OpenShift cluster.\nTo use this experimental feature, set the following values under customDriverName in my-powermax-settings.yaml.\n Value: Set this to the custom name of the driver. Enabled: Set this to true in case you want to enable this feature. The driver helm chart installation uses the values above to: Configure the driver name which is used for communication with other Kubernetes components. Configure the provisioner value in the storage class template. Configure the snapshotter value in the snapshot class template.  If enabled, the driver name is in the following format: \u0026lt;namespace\u0026gt;.\u0026lt;driver name\u0026gt;.dellemc.com\nFor example, if the driver name is set to driver and it is installed in the namespace powermax , then the name that is used for the driver (and the provisioner/snapshotter) is powermax.driver.dellemc.com\nNOTE: If not enabled, the name is set to csi-powermax.dellemc.com by default (without any namespace prefix).\nInstall multiple drivers NOTE: This is an experimental feature and should be used with extreme caution after consulting with Dell EMC Support.\nTo install multiple CSI Drivers for Dell EMC PowerMax in a single Kubernetes cluster, you can take advantage of the custom driver name feature. There are a few important restrictions which should be strictly adhered to:\n Only one driver can be installed in a single namespace Different drivers should not connect to a single Unisphere server Different drivers should not be used to manage a single PowerMax array Storage class and snapshot class names must be unique across installations  To install multiple CSI drivers, follow these steps:\n Create (or use) a new namespace. Ensure that all the pre-requisites are met:  powermax-creds secret is created in this namespace Optional) powermax-certs secret is created in this namespace   Update my-powermax-settings.yaml with the required values. Run the csi-install.sh script to install the driver.  Volume expansion Starting with v1.4, the CSI PowerMax driver supports expansion of Persistent Volumes (PVs). This expansion is done online, that is, when the PVC is attached to any node.\nTo use this feature, the storage class that is used to create the PVC must have the attribute allowVolumeExpansion set to true. The storage classes created during the installation (both using Helm or dell-csi-operator) have the allowVolumeExpansion set to true by default.\nIf you are creating more storage classes, ensure that this attribute is set to true to expand any PVs created using these new storage classes.\nThis is a sample manifest for a storage class which allows for Volume Expansion.\napiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: powermax-expand-sc annotations: storageclass.beta.kubernetes.io/is-default-class: false provisioner: csi-powermax.dellemc.com reclaimPolicy: Delete allowVolumeExpansion: true #Set this attribute to true if you plan to expand any PVCs created using this storage class parameters: SYMID: \u0026quot;000000000001\u0026quot; SRP: \u0026quot;DEFAULT_SRP\u0026quot; ServiceLevel: \u0026quot;Bronze\u0026quot; To resize a PVC, edit the existing PVC spec and set spec.resources.requests.storage to the intended size. For example, if you have a PVC - pmax-pvc-demo of size 5Gi, then you can resize it to 10 Gi by updating the PVC.\nkind: PersistentVolumeClaim apiVersion: v1 metadata: name: pmax-pvc-demo namespace: test spec: accessModes: - ReadWriteOnce volumeMode: Filesystem resources: requests: storage: 10Gi #Updated size from 5Gi to 10Gi storageClassName: powermax-expand-sc NOTE: The Kubernetes Volume Expansion feature can only be used to increase the size of volume, it cannot be used to shrink a volume.\nRaw block support Starting v1.4, CSI PowerMax driver supports raw block volumes.\nRaw Block volumes are created using the volumeDevices list in the Pod template spec with each entry accessing a volumeClaimTemplate specifying a volumeMode: Block. An example configuration is outlined here:\nkind: StatefulSet apiVersion: apps/v1 metadata: name: powermaxtest namespace: {{ .Values.namespace }} spec: ... spec: ... containers: - name: test ... volumeDevices: - devicePath: \u0026quot;/dev/data0\u0026quot; name: pvol volumeClaimTemplates: - metadata: name: pvol spec: accessModes: - ReadWriteOnce volumeMode: Block storageClassName: powermax resources: requests: storage: 8Gi Allowable access modes are ReadWriteOnce, ReadWriteMany, and for block devices that have been previously initialized, ReadOnlyMany.\nRaw Block volumes are presented as a block device to the Pod by using a bind mount to a block device in the node\u0026rsquo;s file system. The driver does not format or check the format of any file system on the block device. Raw Block volumes support online Volume Expansion, but it is up to the application to manage reconfiguring the file system (if any) to the new size.\nFor additional information, see the website: Kubernetes\nCSI PowerMax Reverse Proxy To get the maximum performance out of the CSI driver for PowerMax and Unisphere forPowerMax REST APIs, starting with v1.4 of the driver, you can deploy the optional CSI PowerMax Reverse Proxy application.\nCSI PowerMax Reverse Proxy is a (go) HTTPS server which acts as a reverse proxy for the Unisphere forPowerMax RESTAPI interface. Any RESTAPI request sent from the driver to the reverse proxy is forwarded to the Unisphere server and the response is routed back to the driver.\nThe Reverse Proxy helps regulate the maximum number of requests which can be sent to the Unisphere RESTAPI at a given time across all driver controller and node Pods. This helps with better queuing of CSI requests and performance of the CSI PowerMax driver.\nOptionally you can specify an alternate (backup) Unisphere server and if the primary Unisphere server is not reachable or does not respond, the proxy will redirect the calls to this alternate Unisphere.\nInstallation CSI PowerMax Reverse Proxy is installed as a Kubernetes deployment in the same namespace as the driver.\nIt is also configured as a Kubernetes \u0026ldquo;NodePort\u0026rdquo; service. If the CSI PowerMax driver has been configured to use this service, then it will connect to the IP address and port exposed by the Kubernetes service instead of directly connecting to the Unisphere server.\nPrerequisite CSI PowerMax Reverse Proxy is a HTTPS server and has to be configured with an SSL certificate and a private key.\nThe certificate and key are provided to the proxy via a Kubernetes TLS secret (in the same namespace). The SSL certificate must be a X.509 certificate encoded in PEM format. The certificates can be obtained via a Certificate Authority or can be self-signed and generated by a tool such as openssl.\nHere is an example to generate a private key and use that to sign an SSL certificate using the openssl tool:\nopenssl genrsa -out tls.key 2048 openssl req -new -x509 -sha256 -key tls.key -out tls.crt -days 3650 kubectl create secret -n \u0026lt;namespace\u0026gt; tls revproxy-certs --cert=tls.crt --key=tls.key kubectl create secret -n \u0026lt;namespace\u0026gt; tls csirevproxy-tls-secret --cert=tls.crt -- key=tls.key Using Helm installer A new section, csireverseproxy, in the my-powermax-settings.yaml file can be used to deploy and configure the CSI PowerMax Reverse Proxy.\nThe new Helm chart is configured as a sub chart for the CSI PowerMax helm chart. If it is enabled (using the enabled parameter in the csireverseproxy section of the my-powermax-settings.yaml file), the install script automatically installs the CSI PowerMax Reverse Proxy and configures the CSI PowerMax driver to use this service.\nUsing Dell CSI Operator Starting with the v1.1.0 release of the Dell CSI Operator, a new Custom Resource Definition can be used to install CSI PowerMax Reverse Proxy.\nThis Custom Resource has to be created in the same namespace as the CSI PowerMax driver and it has to be created before the driver Custom Resource. To use the service, the driver Custom Resource manifest must be configured with the service name \u0026ldquo;powermax-reverseproxy\u0026rdquo;. For complete installation instructions for the CSI PowerMax driver and the CSI PowerMax Reverse Proxy, see the Dell CSI Operator documentation.\nUser-friendly hostnames Users can set a value for the nodeNameTemplate in my-powermax-settings.yaml during the installation of the driver so that the driver can use this value to decide the name format of hosts to create or update in the PowerMax array for the nodes in a Kubernetes cluster. The hostname value in nodeNameTemplate should always be contained between two \u0026lsquo;%\u0026rsquo; characters. String prefixing first \u0026lsquo;%\u0026rsquo; and string suffixing second \u0026lsquo;%\u0026rsquo; is used as is before and after every node identifier.\nAlso, there is a new setting, modifyHostName, which could be set to true if you want the driver to rename the existing Hosts/IG for the host initiators on the PowerMax array. The new name uses the default naming convention (csi-\u0026lt;ClusterPrefix\u0026gt;-\u0026lt;HostName\u0026gt;*) or the nodeNameTemplate if it was specified.\nFor example, if nodeNameTemplate is abc-%foo%-hostname and nodename is worker1 , then the host ID is created or updated as abc-worker1-hostname. This change will happen for all nodes in a cluster with the respective node name.\nNOTE: nodeNameTemplate can contain alphanumeric characters [a - z, A - Z, 0 - 9], \u0026lsquo;-\u0026rsquo; and \u0026lsquo;_\u0026rsquo;, other characters are not allowed.\nController HA Starting with version 1.5, the CSI PowerMax driver supports running multiple replicas of controller pod. At any time, only one controller pod is active(leader), and the rest are on standby. In case of a failure, one of the standby pods becomes active and takes the position of leader. This is achieved by using native leader election mechanisms utilizing kubernetes leases. Additionally by leveraging pod anti-affinity, no two controller pods are ever scheduled on the same node.\nTo increase or decrease the number of controller pods, edit the following value in values.yaml file:\ncontrollerCount: 2  NOTE: The default value for controllerCount is 2. We recommend to not change this unless really required. Also, if controller count is greater than the number of available nodes (where the pods can be scheduled), some controller pods will remain in Pending state\n If you\u0026rsquo;re using the dell-csi-operator, adjust the following value in your Custom Resource manifest\nreplicas: 2 For more details about configuring Controller HA using the Dell CSI Operator, please refer to the Dell CSI Operator documentation.\nNodeSelectors and Tolerations Starting with version 1.5, the CSI PowerMax driver helm installer allows you to specify a set of nodeSelectors and tolerations which can be applied on the driver controller Deployment \u0026amp; driver node Daemonset. There are two new sections in the values file - controller \u0026amp; node - where you can specify these values separately for the controller and node pods.\ncontroller If you want to apply nodeSelectors \u0026amp; tolerations for the controller pods, edit the controller section in the values file.\nHere are some examples:\n To schedule controller pods to worker nodes only (Default):  controller: nodeSelector: tolerations:  Set the following values for controller pods to tolerate the taint NoSchedule on master nodes:  controller: nodeSelector: tolerations: - key: \u0026quot;node-role.kubernetes.io/master\u0026quot; operator: \u0026quot;Exists\u0026quot; effect: \u0026quot;NoSchedule\u0026quot;  Set the following values for controller pods to be only scheduled on nodes labelled as master (node-role.kubernetes.io/master):  controller: nodeSelector: node-role.kubernetes.io/master: \u0026quot;\u0026quot; tolerations: - key: \u0026quot;node-role.kubernetes.io/master\u0026quot; operator: \u0026quot;Exists\u0026quot; effect: \u0026quot;NoSchedule\u0026quot; node If you want to apply nodeSelectors \u0026amp; tolerations for the node pods, edit the node section in the values file.\nThe values file already includes a set of default tolerations and you can add/remove tolerations to this list\n# \u0026quot;node\u0026quot; allows to configure node specific parameters node: # \u0026quot;node.nodeSelector\u0026quot; defines what nodes would be selected for pods of node daemonset # Leave as blank to use all nodes nodeSelector: # node-role.kubernetes.io/master: \u0026quot;\u0026quot; # \u0026quot;node.tolerations\u0026quot; defines tolerations that would be applied to node daemonset # Add/Remove tolerations as per requirement # Leave as blank if you wish to not apply any tolerations tolerations: - key: \u0026quot;node.kubernetes.io/memory-pressure\u0026quot; operator: \u0026quot;Exists\u0026quot; effect: \u0026quot;NoExecute\u0026quot; - key: \u0026quot;node.kubernetes.io/disk-pressure\u0026quot; operator: \u0026quot;Exists\u0026quot; effect: \u0026quot;NoExecute\u0026quot; - key: \u0026quot;node.kubernetes.io/network-unavailable\u0026quot; operator: \u0026quot;Exists\u0026quot; effect: \u0026quot;NoExecute\u0026quot; Topology Support Starting from version 1.5, the CSI PowerMax driver supports Topology aware Volume Provisioning which helps Kubernetes scheduler place PVCs on worker nodes which have access to backend storage. When used in conjunction with nodeSelectors which can be specified for the driver node pods, it provides an effective way to provision applications on nodes which have access to the PowerMax array.\nAfter a successful installation of the driver, if a node pod is running successfully on a worker node, the following topology keys are created for a specific PowerMax array:\n csi-powermax.dellemc.com/\\\u0026lt;array-id\\\u0026gt; If the worker node has Fibre Channel connectivity to the PowerMax array - csi-powermax.dellemc.com/\\\u0026lt;array-id\\\u0026gt;.fc If the worker node has ISCSI connectivity to the PowerMax array - csi-powermax.dellemc.com/\\\u0026lt;array-id\\\u0026gt;.iscsi  The values for all these keys are always set to the name of the provisioner which is usually csi-powermax.dellemc.com.\n NOTE: The Topology support does not include any customer defined topology i.e. users cannot create their own labels for nodes and storage classes and expect the labels to be honored by the driver.\n Topology Usage In order to utilize the Topology feature, the storage classes must be modified as follows:\n volumeBindingMode must be set to WaitForFirstConsumer allowedTopologies should be set to one or more topology keys described in the previous section  For e.g. - A PVC created using the following storage class will always be scheduled on nodes which have FC connectivity to the PowerMax array 000000000001\napiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: powermax-fc parameters: SRP: \u0026quot;SRP_1\u0026quot; SYMID: \u0026quot;000000000001\u0026quot; ServiceLevel: \u0026lt;Service Level\u0026gt; #Insert Service Level Name provisioner: csi-powermax.dellemc.com reclaimPolicy: Delete volumeBindingMode: WaitForFirstConsumer allowVolumeExpansion: true allowedTopologies: - matchLabelExpressions: - key: csi-powermax.dellemc.com/000000000001 values: - csi-powermax.dellemc.com - key: csi-powermax.dellemc.com/000000000001.fc values: - csi-powermax.dellemc.com In the above example if you remove the entry for the key csi-powermax.dellemc.com/000000000001.fc, then the PVCs created using this storage class will be scheduled on any worker node with access to the PowerMax array 000000000001 irrespective of the transport protocol\n NOTE: The storage classes created during the driver installation (via Helm) do not contain any topology keys and have the volumeBindingMode set to Immediate. A set of sample storage class definitions to enable topology aware volume provisioning has been provided in the csi-powermax/helm/samples/storageclass folder\n For additional information on how to use Topology aware Volume Provisioning, see the Kubernetes Topology documentation.\nIf you are using dell-csi-operator to create storage classes while installing the CSI PowerMax 1.5 driver, you can set the allowedTopologies value appropriately. volumeBindingMode is set to WaitForFirstConsumer if not specified explicitly.\n","excerpt":"Volume Snapshot Feature The CSI PowerMax driver supports beta snapshots. Driver versions prior to …","ref":"/ansible-docs/v2/features/powermax/","title":"PowerMax"},{"body":"The CSI Driver for Dell EMC PowerMax can be deployed by using the provided Helm v3 charts and installation scripts on both Kubernetes and OpenShift platforms. For more detailed information on the installation scripts, review the script documentation.\nThe controller section of the Helm chart installs the following components in a Deployment in the powermax namespace:\n CSI Driver for Dell EMC PowerMax Kubernetes External Provisioner, which provisions the volumes Kubernetes External Attacher, which attaches the volumes to the containers Kubernetes External Snapshotter, which provides snapshot support Kubernetes External Resizer, which resizes the volume  The node section of the Helm chart installs the following component in a DaemonSet in the namespace powermax:\n CSI Driver for Dell EMC PowerMax Kubernetes Node Registrar, which handles the driver registration  Prerequisites The following requirements must be met before installing the CSI Driver for Dell EMC PowerMax:\n Install Kubernetes (1.17, 1.18, 1.19) or OpenShift (4.5 or 4.6) Install Helm 3 Fibre Channel requirements iSCSI requirements Certificate validation for Unisphere REST API calls Configure Mount propagation on container runtime (that is, Docker) Linux multipathing requirements Volume Snapshot requirements  Install Helm 3 Install Helm 3 on the master node before you install the CSI Driver for Dell EMC PowerMax.\nSteps\nRun the curl https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3 | bash command to install Helm 3.\nFibre Channel Requirements CSI Driver for Dell EMC PowerMax supports Fibre Channel communication. Ensure that the following requirements are met before you install the CSI Driver:\n Zoning of the Host Bus Adapters (HBAs) to the Fibre Channel port director must be completed. Ensure that the HBA WWNs (initiators) appear on the list of initiators that are logged into the array. If number of volumes that will be published to nodes is high, then configure the maximum number of LUNs for your HBAs on each node. See the appropriate HBA document to configure the maximum number of LUNs.  iSCSI Requirements The CSI Driver for Dell EMC PowerMax supports iSCSI connectivity. These requirements are applicable for the nodes that use iSCSI initiator to connect to the PowerMax arrays.\nSet up the iSCSI initiators as follows:\n All Kubernetes nodes must have the iscsi-initiator-utils package installed. Ensure that the iSCSI initiators are available on all the nodes where the driver node plugin will be installed. Kubernetes nodes should have access (network connectivity) to an iSCSI director on the Dell EMC PowerMax array that has IP interfaces. Manually create IP routes for each node that connects to the Dell EMC PowerMax if required. Ensure that the iSCSI initiators on the nodes are not a part of any existing Host (Initiator Group) on the Dell EMC PowerMax array. The CSI Driver needs the port group names containing the required iSCSI director ports. These port groups must be set up on each Dell EMC PowerMax array. All the port groups names supplied to the driver must exist on each Dell EMC PowerMax with the same name.  For information about configuring iSCSI, see Dell EMC PowerMax documentation on Dell EMC Support.\nCertificate validation for Unisphere REST API calls As part of the CSI driver installation, the CSI driver requires a secret with the name powermax-certs present in the namespace powermax. This secret contains the X509 certificates of the CA which signed the Unisphere SSL certificate in PEM format. This secret is mounted as a volume in the driver container. In earlier releases, if the install script did not find the secret, it created an empty secret with the same name. From the 1.2.0 release, the secret volume has been made optional. The install script no longer attempts to create an empty secret.\nThe CSI driver exposes an install parameter skipCertificateValidation which determines if the driver performs client-side verification of the Unisphere certificates. The skipCertificateValidation parameter is set to true by default, and the driver does not verify the Unisphere certificates.\nIf the skipCertificateValidation parameter is set to false and a previous installation attempt created an empty secret, then this secret must be deleted and re-created using the CA certs.\nIf the Unisphere certificate is self-signed or if you are using an embedded Unisphere, then perform the following steps:\n  To fetch the certificate, run openssl s_client -showcerts -connect [Unisphere IP]:8443 \u0026lt;/dev/null\u0026gt; /dev/null | openssl x509 -outform PEM \u0026gt; ca_cert.pem\nNOTE: The IP address varies for each user.\n  To create the secret, run kubectl create secret generic powermax-certs --from-file=ca_cert.pem -n powermax\n  Ports in port group There are no restrictions around how many ports can be present in the iSCSI port groups provided to the driver.\nThe same applies to Fibre Channel where there are no restrictions on the number of FA directors a host HBA can be zoned to. See the best practices for host connectivity to Dell EMC PowerMax to ensure that you have multiple paths to your data volumes.\nConfigure Mount Propagation on Container Runtime You must configure mount propagation on your container runtime on all Kubernetes nodes before installing the CSI Driver for Dell EMC PowerMax. The following steps explain how to do this with Docker. If you use another container runtime please follow the recommended instructions from the vendor to configure mount propagation.\nSteps\n Edit the service section of /etc/systemd/system/multi-user.target.wants/docker.service file to add the following lines: docker.service [Service]... MountFlags=shared  Restart the docker service with systemctl daemon-reload and systemctl restart docker on all the nodes. Restart the docker service with systemctl daemon-reload and systemctl restart docker on all the nodes.  Linux multipathing requirements CSI Driver for Dell EMC PowerMax supports Linux multipathing. Configure Linux multipathing before installing the CSI Driver.\nSet up Linux multipathing as follows:\n All the nodes must have Device Mapper Multipathing package installed.\nNOTE: When this package is installed it creates a multipath configuration file which is located at /etc/multipath.conf. Please ensure that this file always exists. Enable multipathing using mpathconf --enable --with_multipathd y Enable user_friendly_names and find_multipaths in the multipath.conf file.  Volume Snapshot requirements Volume Snapshot CRDs The Kubernetes Volume Snapshot CRDs can be obtained and installed from the external-snapshotter project on Github.\nAlternately, you can install the CRDs by supplying the option \u0026ndash;snapshot-crd while installing the driver using the csi-install.sh script.\nVolume Snapshot Controller Starting with the beta Volume Snapshots, the CSI external-snapshotter sidecar is split into two controllers:\n A common snapshot controller A CSI external-snapshotter sidecar  The common snapshot controller must be installed only once in the cluster irrespective of the number of CSI drivers installed in the cluster. On OpenShift clusters 4.4 and later, the common snapshot-controller is pre-installed. In the clusters where it is not present, it can be installed using kubectl and the manifests available on GitHub.\nNOTE:\n The manifests available on the GitHub repository for snapshot controller will install v3.0.3 of the snapshotter controller - (k8s.gcr.io/sig-storage/snapshot-controller:v3.0.3) Dell EMC recommends using the v3.0.3 image of the CSI external snapshotter - (k8s.gcr.io/sig-storage/csi-snapshotter:v3.0.3) The CSI external-snapshotter sidecar is still installed with the driver and does not involve any extra configuration.  Install the Driver Steps\n Run git clone https://github.com/dell/csi-powermax.git to clone the git repository. This will include the Helm charts and dell-csi-helm-installer scripts. Ensure that you have created a namespace where you want to install the driver. You can run kubectl create namespace powermax to create a new one Edit the `helm/secret.yaml, point to the correct namespace and replace the values for the username and password parameters. These values can be obtained using base64 encoding as described in the following example: echo -n \u0026#34;myusername\u0026#34; | base64 echo -n \u0026#34;mypassword\u0026#34; | base64 where myusername and mypassword are credentials for a user with PowerMax priviledges.\n Create the secret by running kubectl create -f helm/secret.yaml If you are going to install the new CSI PowerMax ReverseProxy service, create a TLS secret with the name - csireverseproxy-tls-secret which holds a SSL certificate and the corresponding private key in the namespace where you are installing the driver. Copy the default values.yaml file `cd helm \u0026amp;\u0026amp; cp csi-powermax/values.yaml my-powermax-settings.yaml Edit the newly created file and provide values for the following parameters vi my-powermax-settings.yaml     Parameter Description Required Default     unisphere Specifies the URL of the Unisphere for PowerMax server. If using the CSI PowerMax Reverse Proxy, leave this value unchanged at https://127.0.0.1:8443. Yes \u0026ldquo;https://127.0.0.1:8443\u0026rdquo;   clusterPrefix Prefix that is used during the creation of various masking-related entities (Storage Groups, Masking Views, Hosts, and Volume Identifiers) on the array. The value that you specify here must be unique. Ensure that no other CSI PowerMax driver is managing the same arrays that are configured with the same prefix. The maximum length for this prefix is three characters. Yes \u0026ldquo;ABC\u0026rdquo;   controller Allows configuration of the controller-specific parameters. - -   node Allows configuration of the node-specific parameters. - -   tolerations Add tolerations as per requirement No -   nodeSelector Add node selectors as per requirement No -   defaultFsType Used to set the default FS type for external provisioner Yes ext4   portGroups List of comma-separated port group names. Any port group that is specified here must be present on all the arrays that the driver manages. For iSCSI Only \u0026ldquo;PortGroup1, PortGroup2, PortGroup3\u0026rdquo;   arrayWhitelist List of comma-separated array IDs. If this parameter remains empty, the driver manages all the arrays that are managed by the Unisphere instance that is configured for the driver. Specify the IDs of the arrays that you want to manage, using the driver. No Empty   symmetrixID Specify a Dell EMC PowerMax array that the driver manages. This value is used to create a default storage class. Yes \u0026ldquo;000000000000\u0026rdquo;   storageResourcePool Must mention one of the SRPs on the PowerMax array that the symmetrixID specifies. This value is used to create the default storage class. Yes \u0026ldquo;SRP_1\u0026rdquo;   serviceLevel This parameter must mention one of the Service Levels on the PowerMax array. This value is used to create the default storage class. Yes \u0026ldquo;Bronze\u0026rdquo;   skipCertificateValidation Skip client-side TLS verification of Unisphere certificates No \u0026ldquo;True\u0026rdquo;   transportProtocol Set preferred transport protocol for the Kubernetes cluster which helps the driver choose between FC and iSCSI when a node has both FC and iSCSI connectivity to a PowerMax array. No Empty   nodeNameTemplate Used to specify a template which will be used by the driver to create Host/IG names on the PowerMax array. To use the default naming convention, then leave this value empty. No Empty   csireverseproxy This section refers to configuration options for CSI PowerMax Reverse Proxy - -   enabled Boolean parameter which indicates if CSI PowerMax Reverse Proxy is going to be configured and installed.\nNOTE: If not enabled, then there is no requirement to configure any of the following values. No \u0026ldquo;False\u0026rdquo;   port Specify the port number that is used by the NodePort service created by the CSI PowerMax Reverse Proxy installation No 2222   primary Mandatory section for Reverse Proxy - -   unisphere This must specify the URL of the Unisphere for PowerMax server Yes, if using Reverse Proxy \u0026ldquo;https://0.0.0.0:8443\u0026rdquo;   skipCertificateValidation This parameter should be set to false if you want to do client-side TLS verification of Unisphere for PowerMax SSL certificates. It is set to true by default. No \u0026ldquo;True\u0026rdquo;   certSecret The name of the secret in the same namespace containing the CA certificates of the Unisphere server Yes, if skipCertificateValidation is set to false Empty   backup Optional section for Reverse Proxy. Specify Unisphere server address which the Reverse Proxy can fall back to if the primary Unisphere is unreachable or unresponsive.\nNOTE: If you do not want to specify a backup Unisphere server, then remove the backup section from the file - -   unisphere Specify the IP address of the Unisphere for PowerMax server which manages the arrays being used by the CSI driver No \u0026ldquo;https://0.0.0.0:8443\u0026rdquo;   skipCertificateValidation This parameter should be set to false if you want to do client side TLS verification of Unisphere for PowerMax SSL certificates. It is set to true by default. No \u0026ldquo;True\u0026rdquo;   certSecret The name of the secret in the same namespace containing the CA certificates of the Unisphere server No Empty    Install the driver using csi-install.sh bash script by running cd ../dell-csi-helm-installer \u0026amp;\u0026amp; ./csi-install.sh --namespace powermax --values ../helm/my-powermax-settings.yaml  Note:\n For detailed instructions on how to run the install scripts, refer to the readme document in the dell-csi-helm-installer folder. This script also runs the verify.sh script in the same directory. You will be prompted to enter the credentials for each of the Kubernetes nodes. The verify.sh script needs the credentials to check if the iSCSI initiators have been configured on all nodes. You can also skip the verification step by specifying the --skip-verify-node option  Storage Classes As part of the driver installation, a set of storage classes is created along with the driver pods. This is done to demonstrate how storage classes need to be created to consume storage from Dell EMC storage arrays.\nThe StorageClass object in Kubernetes is immutable and can\u0026rsquo;t be modified once created. It creates challenges when we need to change or update a parameter, for example when a version of the driver introduces new configurable parameters for the storage classes. To avoid issues during upgrades, future releases of the drivers will have the installation separated from the creation of Storage Classes. In preparation for that, starting in Q4 of 2020, an annotation \u0026quot;helm.sh/resource-policy\u0026quot;: keep is applied to the storage classes created by the dell-csi-helm-installer.\nBecause of this annotation, these storage classes are not going to be deleted even after the driver has been uninstalled. This annotation has been applied to give you an opportunity to keep using these storage classes even with a future release of the driver. In case you wish to not use these storage classes, you will need to delete them by using the kubectl delete storageclass command.\nNOTE: If you uninstall the driver and reinstall it, you can still face errors if any update in the values.yaml file leads to an update of the storage class(es):\n Error: cannot patch \u0026quot;\u0026lt;sc-name\u0026gt;\u0026quot; with kind StorageClass: StorageClass.storage.k8s.io \u0026quot;\u0026lt;sc-name\u0026gt;\u0026quot; is invalid: parameters: Forbidden: updates to parameters are forbidden In case you want to make such updates, make sure to delete the existing storage classes using the kubectl delete storageclass command.\nDeleting a storage class has no impact on a running Pod with mounted PVCs. You won\u0026rsquo;t be able to provision new PVCs until at least one storage class is newly created.\n","excerpt":"The CSI Driver for Dell EMC PowerMax can be deployed by using the provided Helm v3 charts and …","ref":"/ansible-docs/v2/installation/helm/powermax/","title":"PowerMax"},{"body":"Installing PowerMax CSI Driver via Operator The CSI Driver for Dell EMC PowerMax can be installed via the Dell CSI Operator.\nTo deploy the Operator, follow the instructions available here.\nThere are sample manifests provided which can be edited to do an easy installation of the driver. Please note that the deployment of the driver using the operator does not use any Helm charts and the installation and configuration parameters will be slightly different from the ones specified via the Helm installer.\nKubernetes Operators make it easy to deploy and manage entire lifecycle of complex Kubernetes applications. Operators use Custom Resource Definitions (CRD) which represents the application and use custom controllers to manage them.\nPrerequisite Create secret for client-side TLS verification (Optional) Create a secret named powermax-certs in the namespace where the CSI PowerMax driver will be installed. This is an optional step and is only required if you are setting the env variable X_CSI_POWERMAX_SKIP_CERTIFICATE_VALIDATION to false. Please refer detailed documentation on how to create this secret here.\nInstall Driver  Create namespace: Run kubectl create namespace \u0026lt;driver-namespace\u0026gt; using the desired name to create the namespace. Create PowerMax credentials: Create a file called powermax-creds.yaml with the following content apiVersion:v1kind:Secretmetadata:name:powermax-creds# Replace driver-namespace with the namespace where driver is being deployednamespace:\u0026lt;driver-namespace\u0026gt; type: Opaquedata:# set username to the base64 encoded usernameusername:\u0026lt;base64username\u0026gt; # set password to the base64 encoded passwordpassword:\u0026lt;base64password\u0026gt; # Uncomment the following key if you wish to use ISCSI CHAP authentication (v1.3.0 onwards)# chapsecret: \u0026lt;base64 CHAP secret\u0026gt;Replace the values for the username and password parameters. These values can be obtained using base64 encoding as described in the following example:\necho -n \u0026quot;myusername\u0026quot; | base64 echo -n \u0026quot;mypassword\u0026quot; | base64 # If mychapsecret is the ISCSI CHAP secret echo -n \u0026quot;mychapsecret\u0026quot; | base64 Run kubectl create -f powermax-creds.yaml command to create the secret.\n Create a Custom Resource (CR) for PowerMax using the sample files provided here. Users should configure the parameters in CR. The following table lists the primary configurable parameters of the PowerMax driver and their default values:    Parameter Description Required Default     replicas Controls the amount of controller Pods you deploy. If controller Pods are greater than number of available nodes, excess Pods will become stuck in pending. Defaults is 2 which allows for Controller high availability. Yes 2   Common parameters for node and controller      X_CSI_K8S_CLUSTER_PREFIX Define a prefix that is appended onto all resources created in the array; unique per K8s/CSI deployment; max length - 3 characters Yes XYZ   X_CSI_POWERMAX_ENDPOINT IP address of the Unisphere for PowerMax Yes https://0.0.0.0:8443   X_CSI_TRANSPORT_PROTOCOL Choose what transport protocol to use (ISCSI, FC, auto or None) Yes auto   X_CSI_POWERMAX_PORTGROUPS List of comma-separated port groups (ISCSI only). Example: \u0026ldquo;PortGroup1,PortGroup2\u0026rdquo; No -   X_CSI_POWERMAX_ARRAYS List of comma-separated array id(s) which will be managed by the driver No -   X_CSI_POWERMAX_PROXY_SERVICE_NAME Name of CSI PowerMax ReverseProxy service. Leave blank if not using reverse proxy No -   X_CSI_GRPC_MAX_THREADS Number of concurrent grpc requests allowed per client No 4   X_CSI_POWERMAX_DRIVER_NAME Set custom CSI driver name. For more details on this feature review related documentation No -   X_CSI_IG_NODENAME_TEMPLATE Template used for creating hosts on PowerMax. Example: \u0026ldquo;a-b-c-%foo%-xyz\u0026rdquo; where the text between the % symbols(foo) is replaced by the actual host name No -   X_CSI_IG_MODIFY_HOSTNAME Determines if node plugin can rename any existing host on the PowerMax array. Use it with the node name template to rename the existing hosts No false   X_CSI_POWERMAX_DEBUG Determines if HTTP Request/Response is logged No false   Node parameters      X_CSI_POWERMAX_ISCSI_ENABLE_CHAP Enable ISCSI CHAP authentication. For more details on this feature review the related documentation No false   StorageClass parameters      SYMID Symmetrix ID Yes 000000000001   SRP Storage Resource Pool Name Yes DEFAULT_SRP   ServiceLevel Service Level No Bronze   FsType File System type (xfs/ext4) xfs    allowVolumeExpansion After the allowed topology is modified in storage class, pods/and volumes will always be scheduled on nodes that have access to the storage No false   allowedTopologies:key This is to enable topology to allow pods/and volumes to always be scheduled on nodes that have access to the storage. You need to specify the PowerMax array ID and append .fc or .iscsi at the end of it to specify a protocol. For more details on this feature review the related documentation No \u0026ldquo;000000000001\u0026rdquo;     Execute the following command to create PowerMax custom resource:kubectl create -f \u0026lt;input_sample_file.yaml\u0026gt;. The above command will deploy the CSI-PowerMax driver.  CSI PowerMax ReverseProxy CSI PowerMax ReverseProxy is an optional component which can be installed along with the CSI PowerMax driver. For more details on this feature review the related documentation.\nWhen you install CSI PowerMax ReverseProxy, dell-csi-operator is going to create a Deployment and ClusterIP service as part of the installation\nNote - If you wish to use the ReverseProxy with CSI PowerMax driver, the ReverseProxy service should be created before you install the CSIPowerMax driver.\nPre-requisites Create a TLS secret which holds a SSL certificate and a private key which is required by the reverse proxy server. Use a tool like openssl to generate this secret using the example below:\n openssl genrsa -out tls.key 2048 openssl req -new -x509 -sha256 -key tls.key -out tls.crt -days 3650 kubectl create secret -n powermax tls revproxy-certs --cert=tls.crt --key=tls.key Set the following parameters in the CSI PowerMaxReverseProxy Spec tlsSecret : Provide the name of the TLS secret. If using the above example, it should be set to revproxy-certs\nconfig : This section contains the details of the Reverse Proxy configuration\nmode : This value is set to Linked by default. Do not change this value\nlinkConfig : This section contains the configuration of the Linked mode\nprimary : This section holds details for the primary Unisphere which the Reverse Proxy will connect to backup : This optional section holds details for a backup Unisphere which the Reverse Proxy can connect to if Primary Unisphere is unreachable\nurl : URL of the Unisphere server skipCertificateValidation: This setting determines if the client-side Unisphere certificate validation is required certSecret: Secret name which holds the CA certificates which was used to sign Unisphere SSL certificates. Mandatory if skipCertificateValidation is set to false\nHere is a sample manifest with each field annotated. A copy of this manifest is provided in the samples folder\napiVersion: storage.dell.com/v1 kind: CSIPowerMaxRevProxy metadata: name: powermax-reverseproxy # \u0026lt;- Name of the CSIPowerMaxRevProxy object namespace: test-powermax # \u0026lt;- Set the namespace to where you will install the CSI PowerMax driver spec: # Image for CSI PowerMax ReverseProxy image: dellemc/csipowermax-reverseproxy:v1.0.0.000R # \u0026lt;- CSI PowerMax Reverse Proxy image imagePullPolicy: Always # TLS secret which contains SSL certificate and private key for the Reverse Proxy server tlsSecret: csirevproxy-tls-secret config: # Mode for the proxy - only supported mode for now is \u0026quot;Linked\u0026quot; mode: Linked linkConfig: primary: url: https://0.0.0.0:8443 #Unisphere URL skipCertificateValidation: true # This setting determines if client side Unisphere certificate validation is to be skipped certSecret: \u0026quot;\u0026quot; # Provide this value if skipCertificateValidation is set to false backup: # This is an optional field and lets you configure a backup unisphere which can be used by proxy server url: https://0.0.0.0:8443 #Unisphere URL skipCertificateValidation: true Installation Copy the sample file - powermax_reverseproxy.yaml from the samples folder or use the sample available in the OperatorHub UI\nEdit and input all required parameters and then use the OperatorHub UI or run the following command to install the CSI PowerMax Reverse Proxy service\nkubectl create -f powermax_reverseproxy.yaml  You can query for the deployment and service created as part of the installation using the following commands:\nkubectl get deployment -n \u0026lt;namespace\u0026gt; kubectl get svc -n \u0026lt;namespace\u0026gt;  ","excerpt":"Installing PowerMax CSI Driver via Operator The CSI Driver for Dell EMC PowerMax can be installed …","ref":"/ansible-docs/v2/installation/operator/powermax/","title":"PowerMax"},{"body":"This section provides multiple methods to test driver functionality in your environment. The tests are validated using bash as the default shell.\nNote: To run the test for CSI Driver for Dell EMC PowerMax, install Helm 3.\nThe csi-powermax repository includes examples of how you can use the CSI Driver for Dell EMC PowerMax. These examples automate the creation of Pods using the default storage classes that were created during installation. The shell scripts are used to automate the installation and uninstallation of helm charts for the creation of Pods with different number of volumes. To test the installation of the CSI driver, perform these tests:\n Volume clone test Volume test Snapshot test  Volume test Use this procedure to perform a volume test.\n  Create a namespace with the name test.\n  Run the cd csi-powermax/test/helm command to go to the csi-powermax/test/helm directory, which contains the starttest.sh script and the 2vols directories.\n  Run the starttest.sh script and provide it with a test name. The following is a sample command that can be used to run the 2vols test: ./starttest.sh -t 2vols -n test\nThis script installs a helm chart that creates a Pod with a container, creates two PVCs, and mounts them into the created container. You can now log in to the newly created container and check the mounts.\n  Run the ./stoptest.sh -t 2vols -n test script to stop the test. This script deletes the Pods and the PVCs created during the test and uninstalls the helm chart.\n   NOTE: Helm tests have been designed assuming that users are using the default storageclass names (powermax and powermax-xfs). If your storageclass names differ from the default values, such as when deploying with the Operator, update the templates in 2vols accordingly (located in test/helm/2vols/templates/ directory). You can use kubectl get sc to check for the storageclass names.\n Volume clone test Use this procedure to perform a volume clone test.\n Create a namespace with the name test. Run the cd csi-powermax/test/helm command to go to the csi-powermax/test/helm directory, which contains the volumeclonetest.sh script. Run the volumeclonetest.sh script using the following command: bash volumeclonetest.sh  This script does the following:\n Installs a helm chart that creates a Pod with a container, creates two PVCs, and mounts them into the created container. Then it creates a file on one of the PVCs and calculates its checksum. After that, it uses that PVC as the data source to create a new PVC and mounts it on the same container. It checks if the file that existed in the source PVC also exists in the new PVC, calculates its checksum and compares it to the checksum previously calculated. Finally, it cleans up all the resources that are created as part of the test.  Snapshot test Use this procedure to perform a snapshot test.\n Create a namespace with the name test. Run the cd csi-powermax/test/helm command to go to the csi-powermax/test/helm directory, which contains the snaprestoretest.shscript. Run the snaprestoretest.sh script by running the command : bash snaprestoretest.sh  This script does the following:\n Installs a helm chart that creates a Pod with a container, creates two PVCs, and mounts them into the created container. Writes some data to one of the PVCs. After that, it creates a snapshot on that PVC and uses it as a data source to create a new PVC. It mounts the newly created PVC to the container created earlier and then lists the contents of the source and the target PVCs. Cleans up all the resources that were created as part of the test.   NOTE: This test has been designed assuming that users are using the snapshot class name powermax-snapclass which is created by the Helm-based installer. If you have an operator-based deployment, the name of the snapshot class will differ. You must update the snapshot class name in the file betaSnap1.yaml present in the test/helm folder based on your method of deployment. To get a list of volume snapshot classes, run the command - kubectl get volumesnapshotclass\n Volume Expansion test Use this procedure to perform a volume expansion test.\n Create a namespace with the name test Run the cd csi-powermax/test/helm command to go to the csi-powermax/test/helm directory, which contains the volumeexpansiontest.shscript. Run the volumeexpansiontest.sh script by running the command : bash volumeexpansiontest.sh  This script does the following:\n Installs a helm chart that creates a Pod with a container, creates one PVC and mounts it into the created container Writes some data to the PVC After that, it calculates the checksum of the written data, expands the PVC and then recalculates the checksum Cleans up all the resources that were created as part of the test  ","excerpt":"This section provides multiple methods to test driver functionality in your environment. The tests …","ref":"/ansible-docs/v2/installation/test/powermax/","title":"Test PowerMax CSI Driver"},{"body":"Release Notes - CSI PowerMax v1.5.0 New Features/Changes  Added support for OpenShift 4.5/4.6 with RHEL and CoreOS worker nodes Added support for Red Hat Enterprise Linux (RHEL) 7.9 Added support for Ubuntu 20.04 Added support for Docker EE 3.1 Added support for Controller high availability (multiple-controllers) Added support for Topology Added support for mount options Changed driver base image to UBI 8.x  Fixed Issues There are no fixed issues in this release.\nKnown Issues    Issue Workaround     Slow volume attached/detach If your Kubernetes 1.17 or 1.18 cluster has a lot of VolumeAttachment objects, the attach/detach operations will be very slow. This is a known issue and affects all CSI plugins. It is tracked here: CSI VolumeAttachment slows pod startup time. To get around this problem you can upgrade to latest Kubernetes/OpenShift patches, which contains a partial fix: 1.17.8+, 1.18.5+   Delete Volume fails with error message: volume is part of masking view This issue is due to limitations in Unisphere and occurs when Unisphere is overloaded. Currently, there is no workaround for this but can be avoided by making sure Unisphere is not overloaded during such operations. The Unisphere team is assessing a fix for this in a future Unisphere release    ","excerpt":"Release Notes - CSI PowerMax v1.5.0 New Features/Changes  Added support for OpenShift 4.5/4.6 with …","ref":"/ansible-docs/v2/release/powermax/","title":"PowerMax"},{"body":"   Symptoms Prevention, Resolution or Workaround     Warning about feature gates Double check that you have applied all the features to the indicated processes. Restart kubelet when remediated.   kubectl describe pod powermax-controller-\u0026lt;xyz\u0026gt; –n \u0026lt;namespace\u0026gt; indicates the driver image could not be loaded You may need to put an insecure-registries entry in /etc/docker/daemon.json or login to the docker registry   kubectl logs powermax-controller-\u0026lt;xyz\u0026gt; –n \u0026lt;namespace\u0026gt; driver logs shows the driver cannot authenticate Check your secret’s username and password   kubectl logs powermax-controller-\u0026lt;xyz\u0026gt; –n \u0026lt;namespace\u0026gt; driver logs shows the driver failed to connect to the U4P because it couldn’t verify the certificates Check the powermax-certs secret and ensure it is not empty or it has the valid certificates    ","excerpt":"   Symptoms Prevention, Resolution or Workaround     Warning about feature gates Double check that …","ref":"/ansible-docs/v2/troubleshooting/powermax/","title":"PowerMax"},{"body":"Multicluster support You can connect single CSI-PowerScale driver with multiple PowerScale clusters. Pre-Requisistes:\n Creation of secret.json with credentials related to one or more Clusters. Creation of (at least) one Custom Storage classes for each non-default clusters. Creation of custom-volumesnapshot classes, if corresponding isiPaths differ in custom storage classes. Inclusion of cluster name in volume handle, if you want to provision existing static volumes.  Consuming existing volumes with static provisioning You can use existent volumes from PowerScale array as Persistent Volumes in your Kubernetes, perform the following steps:\n Open your volume in One FS, and take a note of volume-id. Create PersistentVolume and use this volume-id as a volumeHandle in the manifest. Modify other parameters according to your needs. In the following example, the PowerScale cluster accessZone is assumed as \u0026lsquo;System\u0026rsquo;, cluster name is assumed as \u0026lsquo;pscale-cluster\u0026rsquo; and volume\u0026rsquo;s internal name as \u0026lsquo;isilonvol\u0026rsquo;. The volume-handle shoulb be in the format of \u0026lt;volume_name\u0026gt;=_=_=\u0026lt;export_id\u0026gt;=_=_==_=_=\u0026lt;cluster_name\u0026gt;  apiVersion:v1kind:PersistentVolumemetadata:name:isilonstaticpvnamespace:defaultspec:capacity:storage:5GiaccessModes:- ReadWriteManypersistentVolumeReclaimPolicy:RetainstorageClassName:isiloncsi:driver:csi-isilon.dellemc.comvolumeAttributes:Path:\u0026#34;/ifs/data/csi/isilonvol\u0026#34;Name:\u0026#34;isilonvol\u0026#34;AzServiceIP:\u0026#39;XX.XX.XX.XX\u0026#39;volumeHandle:isilonvol=_=_=652=_=_=System=_=_=pscale-clusterclaimRef:name:isilonstaticpvcnamespace:defaultCreate PersistentVolumeClaim to use this PersistentVolume.  apiVersion:v1kind:PersistentVolumeClaimmetadata:name:isilonstaticpvcnamespace:defaultspec:accessModes:- ReadWriteManyresources:requests:storage:5GivolumeName:isilonstaticpvstorageClassName:isilonThen use this PVC as a volume in a pod.  apiVersion:v1kind:Podmetadata:name:static-prov-podspec:containers:- name:testimage:docker.io/centos:latestcommand:[\u0026#34;/bin/sleep\u0026#34;,\u0026#34;3600\u0026#34;]volumeMounts:- mountPath:\u0026#34;/data0\u0026#34;name:pvolvolumes:- name:pvolpersistentVolumeClaim:claimName:isilonstaticpvcAfter the pod becomes Ready and Running, you can start to use this pod and volume.  Volume Snapshot Feature The CSI PowerScale driver version 1.3 and later supports managing beta snapshots.\nIn order to use Volume Snapshots, ensure the following components have been deployed to your cluster:\n Kubernetes Volume Snaphshot CRDs Volume Snapshot Controller   For general use, update the snapshot controller YAMLs with an appropriate namespace before installing. For example, on a Vanilla Kubernetes cluster, update the namespace from default to kube-system before issuing the kubectl create command.\n Volume Snapshot Class During the installation of CSI PowerScale driver version 1.3 and later, a Volume Snapshot Class is created using the new recommended snapshot APIs (depends upon Kubernetes version). This is the Volume Snapshot Class created for the default isiPath provided in my-isilon-settings.yaml (which is created based on values.yaml). For additional custom storage classes, separate custom volume snapshot class should be created (only if the isiPath is different from default storage class).\nFollowing are the manifests for the Volume Snapshot Class created during installation:\n VolumeSnapshotClass - v1  # For kubernetes version 20 (v1 snaps)# This is a sample manifest for creating snapshotclass with IsiPath other than default# pvc is created with sc which has some different IsiPath e.g. /ifs/custom# to create a snapshot for this pvc volumesnapshotclass must also be initilized with same IsiPath (i.e. /ifs/custom ) to work snapshot featureapiVersion:snapshot.storage.k8s.io/v1kind:VolumeSnapshotClassmetadata:name:\u0026#34;isilon-snapclass-custom\u0026#34;driver:csi-isilon.dellemc.com#The deletionPolicy of a volume snapshot class can either be Retain or Delete#If the deletionPolicy is Delete, then the underlying storage snapshot is deleted along with the VolumeSnapshotContent object.#If the deletionPolicy is Retain, then both the underlying snapshot and VolumeSnapshotContent remaindeletionPolicy:Deleteparameters:#IsiPath should match with respective storageClass IsiPathIsiPath:\u0026#34;/ifs/custom\u0026#34;VolumeSnapshotClass - beta  # For kubernetes version 18 and 19 (beta snaps)# This is a sample manifest for creating snapshotclass with IsiPath other than default# pvc is created with sc which has some different IsiPath e.g. /ifs/custom# to create a snapshot for this pvc volumesnapshotclass must also be initilized with same IsiPath (i.e. /ifs/custom ) to work snapshot featureapiVersion:snapshot.storage.k8s.io/v1beta1kind:VolumeSnapshotClassmetadata:name:\u0026#34;isilon-snapclass-custom\u0026#34;driver:csi-isilon.dellemc.com#The deletionPolicy of a volume snapshot class can either be Retain or Delete#If the deletionPolicy is Delete, then the underlying storage snapshot is deleted along with the VolumeSnapshotContent object.#If the deletionPolicy is Retain, then both the underlying snapshot and VolumeSnapshotContent remaindeletionPolicy:Deleteparameters:#IsiPath should match with respective storageClass IsiPathIsiPath:\u0026#34;/ifs/custom\u0026#34;### Create Volume SnapshotThe following is a sample manifest for creating a Volume Snapshot using the v1beta1 snapshot APIs:\napiVersion:snapshot.storage.k8s.io/v1beta1kind:VolumeSnapshotmetadata:name:pvcsnapnamespace:defaultspec:volumeSnapshotClassName:isilon-snapclasssource:persistentVolumeClaimName:autotestvolumeOnce the VolumeSnapshot has been successfully created by the CSI PowerScale driver, a VolumeSnapshotContent object is automatically created. Once the status of the VolumeSnapshot object has the readyToUse field set to true , it is available for use.\nFollowing is the relevant section of VolumeSnapshot object status:\nstatus:boundVolumeSnapshotContentName:snapcontent-xxxxxxxxxxxxxcreationTime:\u0026#34;2020-07-16T08:42:12Z\u0026#34;readyToUse:trueCreating PVCs with Volume Snapshots as Source The following is a sample manifest for creating a PVC with a VolumeSnapshot as a source:\napiVersion:v1kind:PersistentVolumeClaimmetadata:name:createfromsnapnamespace:defaultspec:storageClassName:isilondataSource:name:newsnapkind:VolumeSnapshotapiGroup:snapshot.storage.k8s.ioaccessModes:- ReadWriteManyresources:requests:storage:5GiVolume Expansion The CSI PowerScale driver version 1.3 and later supports the expansion of Persistent Volumes (PVs). This expansion can be done either online (for example, when a PVC is attached to a node) or offline (for example, when a PVC is not attached to any node).\nTo use this feature, the storage class that is used to create the PVC must have the attribute allowVolumeExpansion set to true. The storage classes created during the installation (both using Helm or dell-csi-operator) have the allowVolumeExpansion set to true by default.\nIf you are creating more storage classes, ensure that this attribute is set to true to expand any PVs created using these new storage classes.\nThe following is a sample manifest for a storage class which allows for Volume Expansion:\napiVersion:storage.k8s.io/v1kind:StorageClassmetadata:name:isilon-expand-scannotations:storageclass.beta.kubernetes.io/is-default-class:\u0026#34;false\u0026#34;provisioner:\u0026#34;csi-isilon.dellemc.com\u0026#34;reclaimPolicy:Deleteparameters:ClusterName:\u0026lt;clusterNamespecifiedinsecret.json\u0026gt; AccessZone: SystemisiPath:\u0026#34;/ifs/data/csi\u0026#34;AzServiceIP :\u0026#39;XX.XX.XX.XX\u0026#39;rootClientEnabled:\u0026#34;true\u0026#34;allowVolumeExpansion:truevolumeBindingMode:ImmediateTo resize a PVC, edit the existing PVC spec and set spec.resources.requests.storage to the intended size. For example, if you have a PVC isilon-pvc-demo of size 3Gi, then you can resize it to 30Gi by updating the PVC.\napiVersion:v1kind:PersistentVolumeClaimmetadata:name:isilon-pvc-demospec:accessModes:- ReadWriteOnceresources:requests:storage:30Gi# Updated size from 3Gi to 30GistorageClassName:isilon-expand-sc The Kubernetes Volume Expansion feature can only be used to increase the size of a volume. It cannot be used to shrink a volume.\n Volume Cloning Feature The CSI PowerScale driver version 1.3 and later supports volume cloning. This allows specifying existing PVCs in the dataSource field to indicate a user would like to clone a Volume.\nSource and destination PVC must be in the same namespace and have the same Storage Class.\nTo clone a volume, you should first have an existing PVC:\napiVersion:v1kind:PersistentVolumeClaimmetadata:name:existing-pvcspec:accessModes:- ReadWriteManyresources:requests:storage:5GistorageClassName:isilonThe following is a sample manifest for cloning:\nkind:PersistentVolumeClaimapiVersion:v1metadata:name:volume-from-volumenamespace:defaultspec:accessModes:- ReadWriteManyvolumeMode:Filesystemresources:requests:storage:50GistorageClassName:isilondataSource:kind:PersistentVolumeClaimname:existing-pvcapiGroup:\u0026#34;\u0026#34;Controller HA The CSI PowerScale driver version 1.4.0 and later supports running multiple replicas of controller pod. At any time, only one controller pod is active(leader), and the rest are on standby. In case of a failure, one of the standby pods becomes active and takes the position of leader. This is achieved by using native leader election mechanisms utilizing kubernetes leases.\nAdditionally by leveraging pod anti-affinity, no two controller pods are ever scheduled on the same node.\nTo increase or decrease the number of controller pods, edit the following value in myvalues.yaml file:\ncontrollerCount: 2  NOTE: The default value for controllerCount is 2. It is recommended to not change this unless really required. Also, if controller count is greater than the number of available nodes (where the pods can be scheduled), some controller pods will remain in Pending state.\n If you are using the dell-csi-operator, adjust the following value in your Custom Resource manifest\nreplicas: 2 For more details about configuring Controller HA using the Dell CSI Operator, refer to the Dell CSI Operator documentation.\nEphemeral Inline Volume The CSI PowerScale driver version 1.4.0 and later supports CSI ephemeral inline volumes.\nThis feature serves use cases for data volumes whose content and lifecycle are tied to a pod. For example, a driver might populate a volume with dynamically created secrets that are specific to the application running in the pod. Such volumes need to be created together with a pod and can be deleted as part of pod termination (ephemeral). They get defined as part of the pod spec (inline).\nAt runtime, nested inline volumes follow the lifecycle of their associated pods where the driver handles all phases of volume operations as pods are created and destroyed.\nThe following is a sample manifest for creating CSI ephemeral Inline Volume in pod manifest with CSI PowerScale driver.\nkind:PodapiVersion:v1metadata:name:my-csi-app-inline-volumespec:containers:- name:my-frontendimage:busyboxcommand:[\u0026#34;sleep\u0026#34;,\u0026#34;100000\u0026#34;]volumeMounts:- mountPath:\u0026#34;/data\u0026#34;name:my-csi-volumevolumes:- name:my-csi-volumecsi:driver:csi-isilon.dellemc.comvolumeAttributes:size:\u0026#34;2Gi\u0026#34;ClusterName:\u0026#34;cluster1\u0026#34;This manifest creates a pod in given cluster and attach newly created ephemeral inline csi volume to it.\nTopology Topology Support The CSI PowerScale driver version 1.4.0 and later supports Topology by default which forces volumes to be placed on worker nodes that have connectivity to the backend storage, as a result of which the nodes which have access to PowerScale Array are appropriately labelled. The driver leverages these labels to ensure that the driver components (controller, node) are spawned only on nodes wherein these labels exist.\nThis covers use cases where:\nThe CSI PowerScale driver may not be installed or running on some nodes where Users have chosen to restrict the nodes on accessing the PowerScale storage array.\nWe support CustomTopology which enables users to apply labels for nodes - \u0026ldquo;csi-isilon.dellemc.com/XX.XX.XX.XX=csi-isilon.dellemc.com\u0026rdquo; and expect the labels to be honored by the driver.\nWhen “enableCustomTopology” is set to “true”, CSI driver fetches custom labels “csi-isilon.dellemc.com/XX.XX.XX.XX=csi-isilon.dellemc.com” applied on worker nodes, and use them to initialize node pod with custom PowerScale FQDN/IP. Note: Only a single cluster can be configured as part of secret.json for custom topology.\nTopology Usage To utilize the Topology feature, create a custom StorageClass with volumeBindingMode set to WaitForFirstConsumer and specify the desired topology labels within allowedTopologies field of this custom storage class. This ensures that Pod scheduling takes advantage of the topology and the selected node has access to provisioned volumes.\nStorage Class Example with Topology Support:\n# This is a sample manifest for utilizing the topology feature and mount options.# PVCs created using this storage class will be scheduled # only on the nodes with access to Isilon# Change all instances of \u0026lt;ISILON_IP\u0026gt; to the IP of the PowerScale OneFS API server# Provide mount options through \u0026#34;mountOptions\u0026#34; attribute # to create PVCs with mount options.apiVersion:storage.k8s.io/v1kind:StorageClassmetadata:name:isilonprovisioner:csi-isilon.dellemc.comreclaimPolicy:DeleteallowVolumeExpansion:trueparameters:AccessZone:SystemIsiPath:\u0026#34;/ifs/data/csi\u0026#34;# AccessZone groupnet service IP. Update AzServiceIP in values.yaml if different than isiIP.#AzServiceIP : 192.168.2.1# When a PVC is being created, it takes the storage class\u0026#39; value of \u0026#34;storageclass.rootClientEnabled\u0026#34;, # which determines, when a node mounts the PVC, in NodeStageVolume, whether to add the k8s node to # the \u0026#34;Root clients\u0026#34; field (when true) or \u0026#34;Clients\u0026#34; field (when false) of the NFS export RootClientEnabled:\u0026#34;false\u0026#34;# Name of PowerScale cluster where pv will be provisioned# This name should match with name of one of the cluster configs in isilon-creds secret# If this parameter is not specified, then default cluster config in isilon-creds secret will be considered if available#ClusterName: \u0026#34;\u0026lt;cluster_name\u0026gt;\u0026#34;# volumeBindingMode controls when volume binding and dynamic provisioning should occur.# Immediate mode indicates that volume binding and dynamic provisioning occurs once the PersistentVolumeClaim is created# WaitForFirstConsumer mode will delay the binding and provisioning of a PersistentVolume# until a Pod using the PersistentVolumeClaim is createdvolumeBindingMode:WaitForFirstConsumer# allowedTopologies helps scheduling pod on worker nodes which matches all of below expressions# If enableCustomTopology is set to true in helm values.yaml, then do not specify allowedTopologiesallowedTopologies:- matchLabelExpressions:- key:csi-isilon.dellemc.com/\u0026lt;ISILON_IP\u0026gt; values:- csi-isilon.dellemc.commountOptions:[\u0026#34;\u0026lt;mountOption1\u0026gt;\u0026#34;,\u0026#34;\u0026lt;mountOption2\u0026gt;\u0026#34;,...,\u0026#34;\u0026lt;mountOptionN\u0026gt;\u0026#34;]For additional information, see the Kubernetes Topology documentation.\nSupport for Docker EE The CSI Driver for Dell EMC PowerScale supports Docker EE and deployment on clusters bootstrapped with UCP (Universal Control Plane) 3.3.5. *UCP version 3.3.5 supports kubernetes 1.20 and CSI driver can be installed on UCP 3.3.5 with Helm.\nThe installation process for the driver on such clusters remains the same as the installation process on upstream clusters.\nOn UCP based clusters, kubectl may not be installed by default, it is important that kubectl is installed prior to the installation of the driver.\nThe worker nodes in UCP backed clusters may run any of the OSs which we support with upstream clusters.\nSupport custom networks for NFS I/O traffic When allowedNetworks is specified for using custom networks to handle NFS traffic, and a user already has workloads scheduled, there is a possibility that it might lead to backwards compatibility issues. For example, ControllerUnPublish might not be able to completely remove clients from the NFS exports of previously created pods. Also, previous workload will still be using default network and not custom networks, for previous workloads to use custom networks recreation of pods required.\n","excerpt":"Multicluster support You can connect single CSI-PowerScale driver with multiple PowerScale clusters. …","ref":"/ansible-docs/v1/features/powerscale/","title":"PowerScale"},{"body":"The CSI Driver for Dell EMC PowerScale can be deployed by using the provided Helm v3 charts in upstream Kubernetes. For more detailed information on the installation scripts, review the script documentation.\nThe controller section of the Helm chart installs the following components in a Deployment:\n CSI Driver for PowerScale Kubernetes External Provisioner, which provisions the volumes Kubernetes External Attacher, which attaches the volumes to the containers Kubernetes External Snapshotter, which provides snapshot support Kubernetes External Resizer, which resizes the volume  The node section of the Helm chart installs the following component in a Daemon Set:\n CSI Driver for PowerScale Kubernetes Node Registrar, which handles the driver registration  Prerequisites Before you install CSI Driver for PowerScale, verify the requirements that are mentioned in this topic are installed and configured.\nRequirements  Install Kubernetes or OpenShift (see supported versions) Configure Docker service Install Helm v3 Install volume snapshot components Deploy PowerScale driver using Helm  NOTE: There is no feature gate that needs to be set explicitly for CSI drivers version 1.17 and later. All the required feature gates are either beta/GA.\nConfigure Docker service The mount propagation in Docker must be configured on all Kubernetes nodes before installing CSI Driver for PowerScale.\nProcedure  Edit the service section of /etc/systemd/system/multi-user.target.wants/docker.service file as follows: [Service] ... MountFlags=shared  Restart the Docker service with the following commands: systemctl daemon-reload systemctl restart docker   NOTE: Some distribution, like Ubuntu, already has MountFlags set by default.\nInstall volume snapshot components Install Snapshot CRDs  For Kubernetes 1.18 and 1.19, SnapShot CRDs versioned 3.0.3 (https://github.com/kubernetes-csi/external-snapshotter/tree/v3.0.3/client/config/crd), must be installed. For Kubernetes 1.20, SnapShot CRDs versioned 4.0.0 (https://github.com/kubernetes-csi/external-snapshotter/tree/v4.0.0/client/config/crd) must be installed.  Install Snapshot Controller  For Kubernetes 1.18 and 1.19, Snapshot controller versioned 3.0.3 (https://github.com/kubernetes-csi/external-snapshotter/tree/v3.0.3/deploy/kubernetes/snapshot-controller) must be installed. For Kubernetes 1.20, Snapshot controller versioned 4.0.0 (https://github.com/kubernetes-csi/external-snapshotter/tree/v4.0.0/deploy/kubernetes/snapshot-controller) must be installed.  Install CSI Driver for PowerScale Before you begin\n You must clone the source code from git repository. In the dell-csi-helm-installer directory, there will be two shell scripts, csi-install.sh and csi-uninstall.sh. These scripts handle some of the pre and post operations that cannot be performed in the helm chart.  Steps\n  Collect information from the PowerScale Systems like IP address,IsiPath, username and password. Make a note of the value for these parameters as they must be entered in the secret.json.\n  Copy the helm/csi-isilon/values.yaml into a new location with name say my-isilon-settings.yaml, to customize settings for installation.\n  Edit my-isilon-settings.yaml to set the following parameters for your installation: The following table lists the primary configurable parameters of the PowerScale driver Helm chart and their default values. More detailed information can be found in the values.yaml file in this repository.\n   Parameter Description Required Default     certSecretCount Represents number of certificate secrets, which user is going to create for ssl authentication. (isilon-cert-0..isilon-cert-(n-1)); Minimum value should be 1 true 1   isiPort \u0026ldquo;isiPort\u0026rdquo; defines the HTTPs port number of the PowerScale OneFS API server false 8080   allowedNetworks \u0026ldquo;allowedNetworks\u0026rdquo; defines list of networks which can be used for NFS I/O traffic, CIDR format must be used false -   isiInsecure \u0026ldquo;isiInsecure\u0026rdquo; specifies whether the PowerScale OneFS API server\u0026rsquo;s certificate chain and host name must be verified. This value will affect the default storage class implementation false true   isiAccessZone The name of the access zone a volume can be created in false System   volumeNamePrefix \u0026ldquo;volumeNamePrefix\u0026rdquo; defines a string prepended to each volume created by the CSI driver. false k8s   controllerCount \u0026ldquo;controllerCount\u0026rdquo; defines the number of CSI PowerScale controller nodes to deploy to the Kubernetes release. true 2   enableDebug Indicates whether debug level logs should be logged false true   verbose Indicates what content of the OneFS REST API message should be logged in debug level logs false 1   enableQuota Indicates whether the provisioner should attempt to set (later unset) quota on a newly provisioned volume. This requires SmartQuotas to be enabled. false true   noProbeOnStart Indicates whether the controller/node should probe during initialization false false   isiPath The default base path for the volumes to be created, this will be used if a storage class does not have the IsiPath parameter specified false /ifs/data/csi   autoProbe Enable auto probe. false true   nfsV3 Specify whether to set the version to v3 when mounting an NFS export. If the value is \u0026ldquo;false\u0026rdquo;, then the default version supported will be used (that is, the mount command will not explicitly specify \u0026ldquo;-o vers=3\u0026rdquo; option). This flag has now been deprecated and will be removed in a future release. Use the StorageClass.mountOptions if you want to specify \u0026lsquo;vers=3\u0026rsquo; as a mount option. false false   enableCustomTopology Indicates PowerScale FQDN/IP which will be fetched from node label and the same will be used by controller and node pod to establish connection to Array. This requires enableCustomTopology to be enabled. false false   Controller parameters Set nodeSelector and tolerations for controller     nodeSelector Define nodeSelector for the controllers, if required false    tolerations Define tolerations for the controllers, if required false     NOTES\n User should provide all boolean values with double quotes. This applicable only for my-isilon-settings.yaml. Example: \u0026ldquo;true\u0026rdquo;/\u0026ldquo;false\u0026rdquo; ControllerCount parameter value should not exceed number of nodes in the Kubernetes cluster. Otherwise some of the controller pods will be in \u0026ldquo;Pending\u0026rdquo; state till new nodes are available for scheduling. The installer will exit with a WARNING on the same. Whenever certSecretCount parameter changes in myvalues.yaml user needs to reinstall the driver.    Create namespace Run kubectl create namespace isilon to create the isilon namespace. Specify the same namespace name while installing the driver.\nNOTE: CSI PowerScale also supports installation of driver in custom namespace.\n  Create a secret file for the OneFS credentials by editing the secret.json present under helm directory. This secret.json can be used for adding the credentials of one or more OneFS storage arrays.The following table lists driver configuration parameters for a single storage array.\n   Parameter Description Required Default     isiIP \u0026ldquo;isiIP\u0026rdquo; defines the HTTPs endpoint of the PowerScale OneFS API server true -   clusterName PoweScale cluster against which volume CRUD operations are performed through this secret. This is a logical name. true -   username Username for accessing PowerScale OneFS system true -   password Password for accessing PowerScale OneFS system true -   isDefaultCluster defines whether this storage array should be the default.This entry should be present only for one OneFS array and that array will be marked default for existing volumes. true false   Optional parameters Following parameters are Optional, if provided , will override default values of values.yaml     isiPort isiPort defines the HTTPs port number of the PowerScale OneFS API server false -   isiInsecure \u0026ldquo;isiInsecure\u0026rdquo; specifies whether the PowerScale OneFS API server\u0026rsquo;s certificate chain and host name should be verified. false false   isiPath The base path for the volumes to be created. Note: isiPath value provided in the storage class will take the highest precedence while creating PVC true -    The username specified in secret.json must be from the authentication providers of PowerScale. The user must have enough privileges to perform the actions. The suggested privileges are as follows:\nISI_PRIV_LOGIN_PAPI ISI_PRIV_NFS ISI_PRIV_QUOTA ISI_PRIV_SNAPSHOT ISI_PRIV_IFS_RESTORE ISI_PRIV_NS_IFS_ACCESS After editing the file, run the following command to create a secret called isilon-creds  kubectl create secret generic isilon-creds -n isilon --from-file=config=secret.json\nNOTES:\n If any key/value is present in both secret.json and my-isilon-settings.yaml, then the values provided secret.json will take precedence. If any key/value is present in both my-isilon-settings.yaml/secret.json and storageClass, then the values provided in storageClass parameters will take precedence. User has to validate the JSON syntax and array related key/values while replacing or appending the isilon-creds secret. The driver will continue to use previous values in case of an error found in the JSON file.    Install OneFS CA certificates by following the instructions from next section, if you want to validate OneFS API server\u0026rsquo;s certificates. If not, create an empty secret using the following command and empty secret should be created for the successful CSI Driver for Dell EMC Powerscale installation.\nkubectl create -f emptysecret.yaml   Install the driver using csi-install.sh bash script by running cd ../dell-csi-helm-installer \u0026amp;\u0026amp; ./csi-install.sh --namespace isilon --values ../helm/my-isilon-settings.yaml\n  In the case of OpenShift, the driver installation will fail because of a lack of privileges over clusterRole. To resolve this issue the command oc adm policy add-scc-to-user privileged -z isilon-node -n isilon and re-install the driver. This solution will be added in next release.\n  Certificate validation for OneFS REST API calls The CSI driver exposes an install parameter \u0026lsquo;isiInsecure\u0026rsquo; which determines if the driver performs client-side verification of the OneFS certificates. The \u0026lsquo;isiInsecure\u0026rsquo; parameter is set to true by default and the driver does not verify the OneFS certificates.\nIf the \u0026lsquo;isiInsecure\u0026rsquo; is set to false, then the secret isilon-certs must contain the CA certificate for OneFS. If this secret is an empty secret, then the validation of the certificate fails, and the driver fails to start.\nIf the \u0026lsquo;isiInsecure\u0026rsquo; parameter is set to false and a previous installation attempt to create the empty secret, then this secret must be deleted and re-created using the CA certs. If the OneFS certificate is self-signed, then perform the following steps:\nProcedure  To fetch the certificate, run openssl s_client -showcerts -connect [OneFS IP] \u0026lt;/dev/null 2\u0026gt;/dev/null | openssl x509 -outform PEM \u0026gt; ca_cert_0.pem To create the certs secret, run kubectl create secret generic isilon-certs-0 --from-file=cert-0=ca_cert_0.pem -n isilon Use the following command to replace the secret  kubectl create secret generic isilon-certs-0 -n isilon --from-file=cert-0=ca_cert_0.pem -o yaml --dry-run | kubectl replace -f -  NOTES:\n The OneFS IP can be with or without port , depends upon the configuration of OneFS API server. Above said commands is based on the namespace \u0026lsquo;isilon\u0026rsquo; It is highly recommended that ca_cert.pem file(s) having the naming convention as ca_cert_number.pem (example: ca_cert_0, ca_cert_1), where this number starts from 0 and grows as number of OneFS arrays grows. The cert secret created out of these pem files should have the naming convention as isilon-certs-number (example: isilon-certs-0, isilon-certs-1 etc.); The number should start from zero and should grow in incremental order. The number of the secrets created out of pem files should match certSecretCount value in myvalues.yaml or my-isilon-settings.yaml.  Dynamic update of array details via secret.json CSI Driver for Dell EMC PowerScale now provides supports for Multi cluster. Now user can link the single CSI Driver to multiple OneFS Clusters by updating secret.json. User can now update the isilon-creds secret by editing the secret.json and executing following command:\nkubectl create secret generic isilon-creds -n isilon --from-file=config=secret.json -o yaml --dry-run=client | kubectl replace -f - Storage Classes Storage Classes are an essential Kubernetes construct for Storage provisioning. To know more about Storage Classes, please refer: https://kubernetes.io/docs/concepts/storage/storage-classes/\nStarting from v1.5 of the driver, Storage Classes would no longer be created along with the installation of the driver. A wide set of annotated storage class manifests have been provided in the helm/samples/storageclass folder. Please use these samples to create new storage classes to provision storage.\nStarting in CSI PowerScale v1.5, dell-csi-helm-installer will not create any storage classes as part of the driver installation. A wide set of annotated storage class manifests have been provided in the helm/samples/storageclass folder. Please use these samples to create new storage classes to provision storage.\nWhat happens to my existing storage classes? Upgrading from CSI PowerScale v1.4 driver The storage classes created as part of the installation have an annotation - \u0026ldquo;helm.sh/resource-policy\u0026rdquo;: keep set. This ensures that even after an uninstall or upgrade, the storage classes are not deleted. You can continue using these storage classes if you wish so. Since in CSI-PowerScale 1.5 Multi array is supported. The existing storage class (of 1.4) should be treated as default storage class.\nUpgrading from an older version of the driver It is strongly recommended to upgrade older versions of CSI-PowerScale to CSI-PowerScale 1.4 before upgrading to 1.5.\nSteps to create storage class: There are samples storage class yaml files available under helm/samples/storageclass. These can be copied and modified as needed.\nNOTE:\n At least one storage class is required for one array. If you uninstall the driver and reinstall it, you can still face errors if any update in the values.yaml file leads to an update of the storage class(es):   Error: cannot patch \u0026quot;\u0026lt;sc-name\u0026gt;\u0026quot; with kind StorageClass: StorageClass.storage.k8s.io \u0026quot;\u0026lt;sc-name\u0026gt;\u0026quot; is invalid: parameters: Forbidden: updates to parameters are forbidden In case you want to make such updates, ensure to delete the existing storage classes using the kubectl delete storageclass command.\nDeleting a storage class has no impact on a running Pod with mounted PVCs. You will not be able to provision new PVCs until at least one storage class is newly created.\n","excerpt":"The CSI Driver for Dell EMC PowerScale can be deployed by using the provided Helm v3 charts in …","ref":"/ansible-docs/v1/installation/helm/isilon/","title":"PowerScale"},{"body":"Installing PowerScale CSI Driver via Operator The CSI Driver for Dell EMC PowerScale can be installed via the Dell CSI Operator.\nTo deploy the Operator, follow the instructions available here.\nThere are sample manifests provided which can be edited to do an easy installation of the driver. Note that the deployment of the driver using the operator does not use any Helm charts and the installation and configuration parameters will be slightly different from the ones specified via the Helm installer.\nKubernetes Operators make it easy to deploy and manage entire lifecycle of complex Kubernetes applications. Operators use Custom Resource Definitions (CRD) which represents the application and use custom controllers to manage them.\nListing installed drivers with the CSI Isilon CRD User can query for csi-powerscale driver using the following command: kubectl get csiisilon --all-namespaces\nInstall Driver   Create namespace Run kubectl create namespace isilon to create the isilon namespace. Note that the namespace can be any user defined name , in this example, we assume that the namespace is \u0026lsquo;isilon\u0026rsquo;.\n  Create isilon-creds Create a json file called isilon-creds.json with the following content:\n{ \u0026#34;isilonClusters\u0026#34;: [ { \u0026#34;clusterName\u0026#34;: \u0026#34;cluster1\u0026#34;, \u0026#34;username\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;password\u0026#34;: \u0026#34;password\u0026#34;, \u0026#34;isiIP\u0026#34;: \u0026#34;1.2.3.4\u0026#34;, \u0026#34;isDefaultCluster\u0026#34;: true }, { \u0026#34;clusterName\u0026#34;: \u0026#34;cluster2\u0026#34;, \u0026#34;username\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;password\u0026#34;: \u0026#34;password\u0026#34;, \u0026#34;isiIP\u0026#34;: \u0026#34;1.2.3.5\u0026#34;, \u0026#34;isiPort\u0026#34;: \u0026#34;8080\u0026#34;, \u0026#34;isiInsecure\u0026#34;: true, \u0026#34;isiPath\u0026#34;: \u0026#34;/ifs/data/csi\u0026#34; } ] } Replace the values for the given keys as per your environment. This username / password value need not be encoded. You can refer here for more information about isilon secret parameters.\n  Create isilon-certs- secret Please refer this section for creating cert-secrets. Run kubectl create -f isilon-creds.yaml command to create the secret.\n  Create a CR (Custom Resource) for PowerScale using the sample files provided here.\n  Users should configure the parameters in CR. The following table lists the primary configurable parameters of the PowerScale driver and their default values:\n   Parameter Description Required Default     Common parameters for node and controller      CSI_ENDPOINT The UNIX socket address for handling gRPC calls No /var/run/csi/csi.sock   X_CSI_DEBUG To enable debug mode No false   X_CSI_ISI_ENDPOINT HTTPs endpoint of the PowerScale OneFS API server Yes    X_CSI_ISI_INSECURE Specifies whether SSL security needs to be enabled for communication between PowerScale and CSI Driver No true   X_CSI_ISI_PATH Base path for the volumes to be created Yes    X_CSI_ISI_AUTOPROBE To enable auto probing for driver No true   X_CSI_ISILON_NO_PROBE_ON_START Indicates whether the controller/node should probe during initialization Yes    Controller parameters      X_CSI_MODE Driver starting mode No controller   X_CSI_ISI_ACCESS_ZONE Name of the access zone a volume can be created in No System   X_CSI_ISI_QUOTA_ENABLED To enable SmartQuotas Yes    Node parameters      X_CSI_ISILON_NFS_V3 Set the version to v3 when mounting an NFS export. If the value is \u0026ldquo;false\u0026rdquo;, then the default version supported will be used Yes    X_CSI_MODE Driver starting mode No node      Execute the following command to create PowerScale custom resource: kubectl create -f \u0026lt;input_sample_file.yaml\u0026gt; . This command will deploy the CSI-PowerScale driver in the namespace specified in input yaml file.\n  ","excerpt":"Installing PowerScale CSI Driver via Operator The CSI Driver for Dell EMC PowerScale can be …","ref":"/ansible-docs/v1/installation/operator/isilon/","title":"PowerScale"},{"body":"This section provides multiple methods to test driver functionality in your environment.\nNote: To run the test for CSI Driver for Dell EMC PowerScale, install Helm 3.\nTest deploying a simple pod with PowerScale storage Test the deployment workflow of a simple pod on PowerScale storage.\n  Creating a storage class:\nCreate a file storageclass.yaml using sample yaml file located at helm/samples/storageclass\nExecute the following command to create storage class:\nkubectl create -f $PWD/storageclass.yaml Result: After executing the above command storage class will be created in the default namespace, and the user can see the storage class by executing kubectl get sc. Note: Verify system for the new storage class.\n  Creating a volume:\nCreate a file pvc.yaml using sample yaml files located at test/sample_files/\nExecute the following command to create volume:\nkubectl create -f $PWD/pvc.yaml Result: After executing the above command PVC will be created in the default namespace, and the user can see the pvc by executing kubectl get pvc. Note: Verify system for the new volume. Note that the status of the volume can be either Bound or Pending depending on the VolumeBindingMode specified on storage class.\n  Attach the volume to Host\nTo attach a volume to a host, create a new application(Pod) and use the PVC created above in the Pod. This scenario is explained using the Nginx application. Create nginx.yaml using sample yaml files located at test/sample_files/.\nExecute the following command to mount the volume to Kubernetes node:\nkubectl create -f $PWD/nginx.yaml Result: After executing the above command, new nginx pod will be successfully created and started in the default namespace. Note: Verify PowerScale system for host to be part of clients/rootclients field of export created for volume and used by nginx application.\n  Create Snapshot\nThe following procedure will create a snapshot of the volume in the container using VolumeSnapshot objects defined in snap.yaml. The sample file for snapshot creation is located at test/sample_files/\nExecute the following command to create snapshot:\nkubectl create -f $PWD/snap.yaml The spec.source section contains the volume that will be snapped in the default namespace. For example, if the volume to be snapped is testvolclaim1, then the created snapshot is named testvolclaim1-snap1. Verify the PowerScale system for newly created snapshot.\nNote:\n User can see the snapshots using kubectl get volumesnapshot Notice that this VolumeSnapshot class has a reference to a snapshotClassName:isilon-snapclass. The CSI Driver for PowerScale installation creates this class as its default snapshot class. You can see its definition using kubectl get volumesnapshotclasses isilon-snapclass -o yaml. The value of IsiPath in default VolumeSnapshotClass is taken from values.yaml. If user wants different path, she has to create custom volumesnapshot class with required IsiPath in parameters section. Sample VolumeSnapshotClass file is present under helm/samples/volumesnapshotclass    Create Volume from Snapshot\nThe following procedure will create a new volume from a given snapshot which is specified in spec dataSource field.\nThe sample file for volume creation from snapshot is located under test/sample_files/\nExecute the following command to create snapshot:\nkubectl create -f $PWD/volume_from_snap.yaml Verify the PowerScale system for newly created volume from snapshot.\n  Delete Snapshot\nExecute the following commands to delete the snapshot:\nkubectl get volumesnapshot kubectl delete volumesnapshot pvcsnap   Create new volume from existing volume(volume clone)\nThe following procedure will create a new volume from another existing volume which is specified in spec dataSource field.\nThe sample file for volume creation from volume is located at test/sample_files/\nExecute the following command to create snapshot:\nkubectl create -f $PWD/volume_from_volume.yaml Verify the PowerScale system for new created volume from volume.\n  To Unattach the volume from Host\nDelete the nginx application to unattach the volume from host:\nkubectl delete -f nginx.yaml\n  To delete the volume\nkubectl get pvc kubectl delete pvc testvolclaim1 kubectl get pvc   ","excerpt":"This section provides multiple methods to test driver functionality in your environment.\nNote: To …","ref":"/ansible-docs/v1/installation/test/powerscale/","title":"Test PowerScale CSI Driver"},{"body":"Release Notes - CSI Driver for PowerScale v1.5.0 New Features/Changes  Added support for Kubernetes 1.20 Added support for OpenShift 4.7 with RHEL and CoreOS worker nodes Added support for Red Hat Enterprise Linux (RHEL) 8.x Added multi-cluster support through single instance of driver installation Added support for custom networks for NFS I/O traffic SSH permissions are no longer required. You can safely revoke the privilege ISI_PRIV_LOGIN_SSH for the CSI driver user.  Fixed Issues There are no Fixed issues in this release.\nKnown Issues    Issue Resolution or workaround, if known     Creating snapshot fails if the parameter IsiPath in volume snapshot class and related storage class are not the same. The driver uses the incorrect IsiPath parameter and tries to locate the source volume due to the inconsistency. Ensure IsiPath in VolumeSnapshotClass yaml and related storageClass yaml are the same.   While deleting a volume, if there are files or folders created on the volume that are owned by different users. If the Isilon credentials used are for a nonprivileged Isilon user, the delete volume action fails. It is due to the limitation in Linux permission control. To perform the delete volume action, the user account must be assigned a role that has the privilege ISI_PRIV_IFS_RESTORE. The user account must have the following set of privileges to ensure that all the CSI Isilon driver capabilities work properly:\n* ISI_PRIV_LOGIN_PAPI\n* ISI_PRIV_NFS\n* ISI_PRIV_QUOTA\n* ISI_PRIV_SNAPSHOT\n* ISI_PRIV_IFS_RESTORE\n* ISI_PRIV_NS_IFS_ACCESS\nIn some cases, ISI_PRIV_BACKUP is also required, for example, when files owned by other users have mode bits set to 700.   If hostname is mapped to loopback IP in /etc/hosts file, and pods are created using 1.3.0.1 release, after upgrade to 1.4.0 there is a possibility of \u0026ldquo;localhost\u0026rdquo; as stale entry in export We recommend you not to map hostname to loopback IP in /etc/hosts file   If the length of the nodeID exceeds 128 characters, driver fails to update CSINode object and installation fails. This is due to a limitation set by CSI spec which doesn\u0026rsquo;t allow nodeID to be greater than 128 characters. The CSI PowerScale driver uses the hostname for building the nodeID which is set in CSINode resource object, hence we recommend not having very long hostnames in order to avoid this issue. This current limitation of 128 characters is likely to be relaxed in future kubernetes versions as per this issue in the community: https://github.com/kubernetes-sigs/gcp-compute-persistent-disk-csi-driver/issues/581   Driver installation warning: \u0026ldquo;OpenShift version 4.7, is newer than the version that has been tested. Latest tested version is: 4.6\u0026rdquo; Ignore this warning and continue with the installation. v1.5.0 release of the driver supports OpenShift 4.6/4.7 .    ","excerpt":"Release Notes - CSI Driver for PowerScale v1.5.0 New Features/Changes  Added support for Kubernetes …","ref":"/ansible-docs/v1/release/powerscale/","title":"PowerScale"},{"body":"Here are some installation failures that might be encountered and how to mitigate them.\n   Symptoms Prevention, Resolution or Workaround     The kubectl logs isilon-controller-0 -n isilon -c driver logs shows the driver cannot authenticate Check your secret\u0026rsquo;s username and password for corresponding cluster   The kubectl logs isilon-controller-0 -n isilon -c driver logs shows the driver failed to connect to the Isilon because it couldn\u0026rsquo;t verify the certificates Check the isilon-certs- secret and ensure it is not empty and it has the valid certificates. Set isiInsecure: \u0026quot;true\u0026quot; for insecure connection. SSL validation is recommended in production environment.   The kubectl logs isilon-controller-0 -n isilon -c driver logs shows the driver error: create volume failed, Access denied. create directory as requested This situation can happen when the user who created the base path is different from the user configured for the driver. Make sure the user used to deploy CSI-Driver must have enough rights on the base path (i.e. isiPath) to perform all operations.    ","excerpt":"Here are some installation failures that might be encountered and how to mitigate them.\n   Symptoms …","ref":"/ansible-docs/v1/troubleshooting/powerscale/","title":"PowerScale"},{"body":"Consuming existing volumes with static provisioning You can use existent volumes from PowerScale array as Persistent Volumes in your Kubernetes, perform the following steps:\n Open your volume in One FS, and take a note of volume-id. Create PersistentVolume and use this volume-id as a volumeHandle in the manifest. Modify other parameters according to your needs.  apiVersion:v1kind:PersistentVolumemetadata:name:isilonstaticpvnamespace:defaultspec:capacity:storage:5GiaccessModes:- ReadWriteManypersistentVolumeReclaimPolicy:RetainstorageClassName:isiloncsi:driver:csi-isilon.dellemc.comvolumeAttributes:Path:\u0026#34;/ifs/data/csi/isilonvol\u0026#34;Name:\u0026#34;isilonvol\u0026#34;AzServiceIP:\u0026#39;XX.XX.XX.XX\u0026#39;volumeHandle:isilonvol=_=_=652=_=_=SystemclaimRef:name:isilonstaticpvcnamespace:defaultCreate PersistentVolumeClaim to use this PersistentVolume.  apiVersion:v1kind:PersistentVolumeClaimmetadata:name:isilonstaticpvcnamespace:defaultspec:accessModes:- ReadWriteManyresources:requests:storage:5GivolumeName:isilonstaticpvstorageClassName:isilonThen use this PVC as a volume in a pod.  apiVersion:v1kind:Podmetadata:name:static-prov-podspec:containers:- name:testimage:docker.io/centos:latestcommand:[\u0026#34;/bin/sleep\u0026#34;,\u0026#34;3600\u0026#34;]volumeMounts:- mountPath:\u0026#34;/data0\u0026#34;name:pvolvolumes:- name:pvolpersistentVolumeClaim:claimName:isilonstaticpvcAfter the pod becomes Ready and Running, you can start to use this pod and volume.  Volume Snapshot Feature The CSI PowerScale driver version 1.3 and later supports managing beta snapshots.\nIn order to use Volume Snapshots, ensure the following components have been deployed to your cluster:\n Kubernetes Volume Snaphshot CRDs Volume Snapshot Controller   For general use, update the snapshot controller YAMLs with an appropriate namespace before installing. For example, on a Vanilla Kubernetes cluster, update the namespace from default to kube-system before issuing the kubectl create command.\n Volume Snapshot Class During the installation of CSI PowerScale driver version 1.3 and later, a Volume Snapshot Class is created using the new v1beta1 snapshot APIs. This is the only Volume Snapshot Class required and there is no need to create any other Volume Snapshot Class.\nFollowing is the manifest for the Volume Snapshot Class created during installation:\napiVersion:snapshot.storage.k8s.io/v1beta1kind:VolumeSnapshotClassmetadata:name:isilon-snapclassdriver:csi-isilon.dellemc.comdeletionPolicy:DeleteCreate Volume Snapshot The following is a sample manifest for creating a Volume Snapshot using the v1beta1 snapshot APIs:\napiVersion:snapshot.storage.k8s.io/v1beta1kind:VolumeSnapshotmetadata:name:pvcsnapnamespace:defaultspec:volumeSnapshotClassName:isilon-snapclasssource:persistentVolumeClaimName:autotestvolumeOnce the VolumeSnapshot has been successfully created by the CSI PowerScale driver, a VolumeSnapshotContent object is automatically created. Once the status of the VolumeSnapshot object has the readyToUse field set to true , it is available for use.\nFollowing is the relevant section of VolumeSnapshot object status:\nstatus:boundVolumeSnapshotContentName:snapcontent-xxxxxxxxxxxxxcreationTime:\u0026#34;2020-07-16T08:42:12Z\u0026#34;readyToUse:trueCreating PVCs with Volume Snapshots as Source The following is a sample manifest for creating a PVC with a VolumeSnapshot as a source:\napiVersion:v1kind:PersistentVolumeClaimmetadata:name:createfromsnapnamespace:defaultspec:storageClassName:isilondataSource:name:newsnapkind:VolumeSnapshotapiGroup:snapshot.storage.k8s.ioaccessModes:- ReadWriteManyresources:requests:storage:5GiVolume Expansion The CSI PowerScale driver version 1.3 and later supports the expansion of Persistent Volumes (PVs). This expansion can be done either online (for example, when a PVC is attached to a node) or offline (for example, when a PVC is not attached to any node).\nTo use this feature, the storage class that is used to create the PVC must have the attribute allowVolumeExpansion set to true. The storage classes created during the installation (both using Helm or dell-csi-operator) have the allowVolumeExpansion set to true by default.\nIf you are creating more storage classes, ensure that this attribute is set to true to expand any PVs created using these new storage classes.\nThe following is a sample manifest for a storage class which allows for Volume Expansion:\napiVersion:storage.k8s.io/v1kind:StorageClassmetadata:name:isilon-expand-scannotations:storageclass.beta.kubernetes.io/is-default-class:\u0026#34;false\u0026#34;provisioner:\u0026#34;csi-isilon.dellemc.com\u0026#34;reclaimPolicy:Deleteparameters:AccessZone:SystemisiPath:\u0026#34;/ifs/data/csi\u0026#34;AzServiceIP :\u0026#39;XX.XX.XX.XX\u0026#39;rootClientEnabled:\u0026#34;true\u0026#34;allowVolumeExpansion:truevolumeBindingMode:ImmediateTo resize a PVC, edit the existing PVC spec and set spec.resources.requests.storage to the intended size. For example, if you have a PVC isilon-pvc-demo of size 3Gi, then you can resize it to 30Gi by updating the PVC.\napiVersion:v1kind:PersistentVolumeClaimmetadata:name:isilon-pvc-demospec:accessModes:- ReadWriteOnceresources:requests:storage:30Gi# Updated size from 3Gi to 30GistorageClassName:isilon-expand-sc The Kubernetes Volume Expansion feature can only be used to increase the size of a volume. It cannot be used to shrink a volume.\n Volume Cloning Feature The CSI PowerScale driver version 1.3 and later supports volume cloning. This allows specifying existing PVCs in the dataSource field to indicate a user would like to clone a Volume.\nSource and destination PVC must be in the same namespace and have the same Storage Class.\nTo clone a volume, you should first have an existing PVC:\napiVersion:v1kind:PersistentVolumeClaimmetadata:name:existing-pvcspec:accessModes:- ReadWriteManyresources:requests:storage:5GistorageClassName:isilonThe following is a sample manifest for cloning:\nkind:PersistentVolumeClaimapiVersion:v1metadata:name:volume-from-volumenamespace:defaultspec:accessModes:- ReadWriteManyvolumeMode:Filesystemresources:requests:storage:50GistorageClassName:isilondataSource:kind:PersistentVolumeClaimname:existing-pvcapiGroup:\u0026#34;\u0026#34;Controller HA The CSI PowerScale driver version 1.4.0 and later supports running multiple replicas of controller pod. At any time, only one controller pod is active(leader), and the rest are on standby. In case of a failure, one of the standby pods becomes active and takes the position of leader. This is achieved by using native leader election mechanisms utilizing kubernetes leases.\nAdditionally by leveraging pod anti-affinity, no two controller pods are ever scheduled on the same node.\nTo increase or decrease the number of controller pods, edit the following value in myvalues.yaml file:\ncontrollerCount: 2  NOTE: The default value for controllerCount is 2. It is recommended to not change this unless really required. Also, if controller count is greater than the number of available nodes (where the pods can be scheduled), some controller pods will remain in Pending state.\n If you are using the dell-csi-operator, adjust the following value in your Custom Resource manifest\nreplicas: 2 For more details about configuring Controller HA using the Dell CSI Operator, refer to the Dell CSI Operator documentation.\nEphemeral Inline Volume The CSI PowerScale driver version 1.4.0 and later supports CSI ephemeral inline volumes.\nThis feature serves use cases for data volumes whose content and lifecycle are tied to a pod. For example, a driver might populate a volume with dynamically created secrets that are specific to the application running in the pod. Such volumes need to be created together with a pod and can be deleted as part of pod termination (ephemeral). They get defined as part of the pod spec (inline).\nAt runtime, nested inline volumes follow the lifecycle of their associated pods where the driver handles all phases of volume operations as pods are created and destroyed.\nThe following is a sample manifest for creating CSI ephemeral Inline Volume in pod manifest with CSI PowerScale driver.\nkind:PodapiVersion:v1metadata:name:my-csi-app-inline-volumespec:containers:- name:my-frontendimage:busyboxcommand:[\u0026#34;sleep\u0026#34;,\u0026#34;100000\u0026#34;]volumeMounts:- mountPath:\u0026#34;/data\u0026#34;name:my-csi-volumevolumes:- name:my-csi-volumecsi:driver:csi-isilon.dellemc.comvolumeAttributes:size:\u0026#34;2Gi\u0026#34;This manifest will create a pod and attach newly created ephemeral inline csi volume to it.\nTopology Topology Support The CSI PowerScale driver version 1.4.0 and later supports Topology by default which forces volumes to be placed on worker nodes that have connectivity to the backend storage, as a result of which the nodes which have access to PowerScale Array are appropriately labelled. The driver leverages these labels to ensure that the driver components (controller, node) are spawned only on nodes wherein these labels exist.\nThis covers use cases where:\nThe CSI PowerScale driver may not be installed or running on some nodes where Users have chosen to restrict the nodes on accessing the PowerScale storage array.\nWe support CustomTopology which enables users to apply labels for nodes - \u0026ldquo;csi-isilon.dellemc.com/XX.XX.XX.XX=csi-isilon.dellemc.com\u0026rdquo; and expect the labels to be honored by the driver.\nWhen “enableCustomTopology” is set to “true”, CSI driver fetches custom labels “csi-isilon.dellemc.com/XX.XX.XX.XX=csi-isilon.dellemc.com” applied on worker nodes, and use them to initialize node pod with custom PowerScale FQDN/IP.\nTopology Usage To utilize the Topology feature, create a custom StorageClass with volumeBindingMode set to WaitForFirstConsumer and specify the desired topology labels within allowedTopologies field of this custom storage class. This ensures that Pod scheduling takes advantage of the topology and the selected node has access to provisioned volumes.\nStorage Class Example with Topology Support:\n# This is a sample manifest for utilizing the topology feature and mount options.# PVCs created using this storage class will be scheduled # only on the nodes with access to Isilon# Change all instances of \u0026lt;ISILON_IP\u0026gt; to the IP of the PowerScale OneFS API server# Provide mount options through \u0026#34;mountOptions\u0026#34; attribute # to create PVCs with mount options.apiVersion:storage.k8s.io/v1kind:StorageClassmetadata:name:isilonprovisioner:csi-isilon.dellemc.comreclaimPolicy:DeleteallowVolumeExpansion:trueparameters:AccessZone:SystemIsiPath:\u0026#34;/ifs/data/csi\u0026#34;# AccessZone groupnet service IP. Update AzServiceIP in values.yaml if different than isiIP.#AzServiceIP : 192.168.2.1# When a PVC is being created, it takes the storage class\u0026#39; value of \u0026#34;storageclass.rootClientEnabled\u0026#34;, # which determines, when a node mounts the PVC, in NodeStageVolume, whether to add the k8s node to # the \u0026#34;Root clients\u0026#34; field (when true) or \u0026#34;Clients\u0026#34; field (when false) of the NFS export RootClientEnabled:\u0026#34;false\u0026#34;# volumeBindingMode controls when volume binding and dynamic provisioning should occur.# Immediate mode indicates that volume binding and dynamic provisioning occurs once the PersistentVolumeClaim is created# WaitForFirstConsumer mode will delay the binding and provisioning of a PersistentVolume# until a Pod using the PersistentVolumeClaim is createdvolumeBindingMode:WaitForFirstConsumer# allowedTopologies helps scheduling pod on worker nodes which matches all of below expressions# If enableCustomTopology is set to true in helm values.yaml, then do not specify allowedTopologiesallowedTopologies:- matchLabelExpressions:- key:csi-isilon.dellemc.com/\u0026lt;ISILON_IP\u0026gt; values:- csi-isilon.dellemc.commountOptions:[\u0026#34;\u0026lt;mountOption1\u0026gt;\u0026#34;,\u0026#34;\u0026lt;mountOption2\u0026gt;\u0026#34;,...,\u0026#34;\u0026lt;mountOptionN\u0026gt;\u0026#34;]For additional information, see the Kubernetes Topology documentation.\n","excerpt":"Consuming existing volumes with static provisioning You can use existent volumes from PowerScale …","ref":"/ansible-docs/v2/features/powerscale/","title":"PowerScale"},{"body":"The CSI Driver for Dell EMC PowerScale can be deployed by using the provided Helm v3 charts and installation scripts on both Kubernetes and OpenShift platforms. For more detailed information on the installation scripts, review the script documentation.\nThe controller section of the Helm chart installs the following components in a Deployment:\n CSI Driver for PowerScale Kubernetes External Provisioner, which provisions the volumes Kubernetes External Attacher, which attaches the volumes to the containers Kubernetes External Snapshotter, which provides snapshot support Kubernetes External Resizer, which resizes the volume  The node section of the Helm chart installs the following component in a Daemon Set:\n CSI Driver for PowerScale Kubernetes Node Registrar, which handles the driver registration  Prerequisites Before you install CSI Driver for PowerScale, verify the requirements that are mentioned in this topic are installed and configured.\nRequirements  Install Kubernetes. Configure Docker service Install Helm v3 Install volume snapshot components Deploy PowerScale driver using Helm  Note: There is no feature gate that needs to be set explicitly for CSI drivers version 1.17 and later. All the required feature gates are either beta/GA.\nConfigure Docker service The mount propagation in Docker must be configured on all Kubernetes nodes before installing CSI Driver for PowerScale.\nProcedure  Edit the service section of /etc/systemd/system/multi-user.target.wants/docker.service file as follows: [Service] ... MountFlags=shared  Restart the Docker service with systemctl daemon-reload and systemctl daemon-reload systemctl restart docker   Install volume snapshot components Install Snapshot Beta CRDs To install snapshot CRDs specify --snapshot-crd flag to driver installation script dell-csi-helm-installer/csi-install.sh during driver installation.\nInstall Common Snapshot Controller, if not already installed for the cluster.\n The manifests available on GitHub install v3.0.3 of the snapshotter image - quay.io/k8scsi/csi-snapshotter:v3.0.3 Dell recommends using v3.0.3 image of the snapshot-controller - quay.io/k8scsi/snapshot-controller:v3.0.3  Install CSI Driver for PowerScale Before you begin\n You must clone the source code from git repository. In the dell-csi-helm-installer directory, there should be two shell scripts, csi-install.sh and csi-uninstall.sh. These scripts handle some of the pre and post operations that cannot be performed in the helm chart.  Steps\n  Collect information from the PowerScale Systems like IP address, username and password. Make a note of the value for these parameters as they must be entered in the secret.yaml and values file.\n  Copy the helm/csi-isilon/values.yaml into a new location with name say my-isilon-settings.yaml, to customize settings for installation.\n  Edit my-isilon-settings.yaml to set the following parameters for your installation: The following table lists the primary configurable parameters of the PowerScale driver Helm chart and their default values. More detailed information can be found in the values.yaml file in this repository.\n   Parameter Description Required Default     isiIP \u0026ldquo;isiIP\u0026rdquo; defines the HTTPs endpoint of the PowerScale OneFS API server true -   isiPort \u0026ldquo;isiPort\u0026rdquo; defines the HTTPs port number of the PowerScale OneFS API server false 8080   isiInsecure \u0026ldquo;isiInsecure\u0026rdquo; specifies whether the PowerScale OneFS API server\u0026rsquo;s certificate chain and host name should be verified. false true   isiAccessZone The name of the access zone a volume can be created in false System   volumeNamePrefix \u0026ldquo;volumeNamePrefix\u0026rdquo; defines a string prepended to each volume created by the CSI driver. false k8s   controllerCount \u0026ldquo;controllerCount\u0026rdquo; defines the number of CSI PowerScale controller nodes to deploy to the Kubernetes release. true 2   enableDebug Indicates whether debug level logs should be logged false true   verbose Indicates what content of the OneFS REST API message should be logged in debug level logs false 1   enableQuota Indicates whether the provisioner should attempt to set (later unset) quota on a newly provisioned volume. This requires SmartQuotas to be enabled. false true   noProbeOnStart Indicates whether the controller/node should probe during initialization false false   isiPath The default base path for the volumes to be created, this will be used if a storage class does not have the IsiPath parameter specified false /ifs/data/csi   autoProbe Enable auto probe. false true   nfsV3 Specify whether to set the version to v3 when mounting an NFS export. If the value is \u0026ldquo;false\u0026rdquo;, then the default version supported will be used (that is, the mount command will not explicitly specify \u0026ldquo;-o vers=3\u0026rdquo; option). This flag has now been deprecated and will be removed in a future release. Use the StorageClass.mountOptions if you want to specify \u0026lsquo;vers=3\u0026rsquo; as a mount option. false false   enableCustomTopology Indicates PowerScale FQDN/IP which will be fetched from node label and the same will be used by controller and node pod to establish connection to Array. This requires enableCustomTopology to be enabled. false false   Storage Class parameters Following parameters are related to Storage Class     name \u0026ldquo;storageClass.name\u0026rdquo; defines the name of the storage class to be defined. false isilon   isDefault \u0026ldquo;storageClass.isDefault\u0026rdquo; defines whether the primary storage class should be the default. false true   reclaimPolicy \u0026ldquo;storageClass.reclaimPolicy\u0026rdquo; defines what will happen when a volume is removed from the Kubernetes API. Valid values are \u0026ldquo;Retain\u0026rdquo; and \u0026ldquo;Delete\u0026rdquo;. false Delete   accessZone The Access Zone where the Volume would be created false System   AzServiceIP Access Zone service IP if different from isiIP, specify here and refer in storageClass false    rootClientEnabled When a PVC is being created, it takes the storage class\u0026rsquo; value of \u0026ldquo;storageclass.rootClientEnabled\u0026rdquo; false false   Controller parameters Set nodeSelector and tolerations for controller     nodeSelector Define nodeSelector for the controllers, if required false    tolerations Define tolerations for the controllers, if required false     Note: User should provide all boolean values with double quotes. This is applicable only for my-isilon-settings.yaml. Example: \u0026ldquo;true\u0026rdquo;/\u0026ldquo;false\u0026rdquo;\nNote: controllerCount parameter value should not exceed number of nodes in the kubernetes cluster. Otherwise some of the controller pods will be in \u0026ldquo;Pending\u0026rdquo; state till new nodes are available for scheduling. The installer will exit with a WARNING on the same.\n  Create namespace Run kubectl create namespace isilon to create the isilon namespace. Specify the same namespace name while installing the driver.\nNote: CSI PowerScale also supports installation of driver in custom namespace.\n  Create a secret file for the OneFS credentials by editing the secret.yaml present under helm directory. Replace the values for the username and password parameters. Use the following command to convert username/password to base64 encoded string:\necho -n 'admin' | base64 echo -n 'password' | base64 Run kubectl create -f secret.yaml to create the secret.\nNote: The username specified in secret.yaml must be from the authentication providers of PowerScale. The user must have enough privileges to perform the actions. The suggested privileges are as follows:\nISI_PRIV_LOGIN_PAPI ISI_PRIV_NFS ISI_PRIV_QUOTA ISI_PRIV_SNAPSHOT ISI_PRIV_IFS_RESTORE ISI_PRIV_NS_IFS_ACCESS ISI_PRIV_LOGIN_SSH   Install OneFS CA certificates by following the instructions from next section, if you want to validate OneFS API server\u0026rsquo;s certificates. If not, create an empty secret using the following command and empty secret should be created for the successful CSI Driver for Dell EMC Powerscale installation.\nkubectl create -f emptysecret.yaml   Install the driver using csi-install.sh bash script by running cd ../dell-csi-helm-installer \u0026amp;\u0026amp; ./csi-install.sh --namespace isilon --values ../helm/myvalues.yaml\n  Certificate validation for OneFS REST API calls The CSI driver exposes an install parameter \u0026lsquo;isiInsecure\u0026rsquo; which determines if the driver performs client-side verification of the OneFS certificates. The \u0026lsquo;isiInsecure\u0026rsquo; parameter is set to true by default and the driver does not verify the OneFS certificates.\nIf the \u0026lsquo;isiInsecure\u0026rsquo; is set to false, then the secret isilon-certs must contain the CA certificate for OneFS. If this secret is an empty secret, then the validation of the certificate fails, and the driver fails to start.\nIf the \u0026lsquo;isiInsecure\u0026rsquo; parameter is set to false and a previous installation attempt to create the empty secret, then this secret must be deleted and re-created using the CA certs. If the OneFS certificate is self-signed, then perform the following steps:\nProcedure  To fetch the certificate, run openssl s_client -showcerts -connect [OneFS IP] \u0026lt;/dev/null 2\u0026gt;/dev/null | openssl x509 -outform PEM \u0026gt; ca_cert.pem To create the secret, run kubectl create secret generic isilon-certs --from-file=ca_cert.pem -n isilon   Storage Classes As part of the driver installation, a set of storage classes is created along with the driver pods. This is done to demonstrate how storage classes need to be created to consume storage from Dell EMC storage arrays.\nThe StorageClass object in Kubernetes is immutable and can\u0026rsquo;t be modified once created. It creates challenges when we need to change or update a parameter, for example when a version of the driver introduces new configurable parameters for the storage classes. To avoid issues during upgrades, future releases of the drivers will have the installation separated from the creation of Storage Classes. In preparation for that, starting in Q4 of 2020, an annotation \u0026quot;helm.sh/resource-policy\u0026quot;: keep is applied to the storage classes created by the dell-csi-helm-installer.\nBecause of this annotation, these storage classes are not going to be deleted even after the driver has been uninstalled. This annotation has been applied to give you an opportunity to keep using these storage classes even with a future release of the driver. In case you wish to not use these storage classes, you will need to delete them by using the kubectl delete storageclass command.\nNOTE: If you uninstall the driver and reinstall it, you can still face errors if any update in the values.yaml file leads to an update of the storage class(es):\n Error: cannot patch \u0026quot;\u0026lt;sc-name\u0026gt;\u0026quot; with kind StorageClass: StorageClass.storage.k8s.io \u0026quot;\u0026lt;sc-name\u0026gt;\u0026quot; is invalid: parameters: Forbidden: updates to parameters are forbidden In case you want to make such updates, make sure to delete the existing storage classes using the kubectl delete storageclass command.\nDeleting a storage class has no impact on a running Pod with mounted PVCs. You won\u0026rsquo;t be able to provision new PVCs until at least one storage class is newly created.\n","excerpt":"The CSI Driver for Dell EMC PowerScale can be deployed by using the provided Helm v3 charts and …","ref":"/ansible-docs/v2/installation/helm/isilon/","title":"PowerScale"},{"body":"Installing PowerScale CSI Driver via Operator The CSI Driver for Dell EMC PowerScale can be installed via the Dell CSI Operator.\nTo deploy the Operator, follow the instructions available here.\nThere are sample manifests provided which can be edited to do an easy installation of the driver. Note that the deployment of the driver using the operator does not use any Helm charts and the installation and configuration parameters will be slightly different from the ones specified via the Helm installer.\nKubernetes Operators make it easy to deploy and manage entire lifecycle of complex Kubernetes applications. Operators use Custom Resource Definitions (CRD) which represents the application and use custom controllers to manage them.\nListing installed drivers with the CSI Isilon CRD User can query for csi-powerscale driver using the following command: kubectl get csiisilon --all-namespaces\nInstall Driver  Create namespace Run kubectl create namespace isilon to create the isilon namespace. Create isilon-creds Create a file called isilon-creds.yaml with the following content: apiVersion:v1kind:Secretmetadata:name:isilon-credsnamespace:isilontype:Opaquedata:# set username to the base64 encoded usernameusername:\u0026lt;base64username\u0026gt; # set password to the base64 encoded passwordpassword:\u0026lt;base64password\u0026gt;Replace the values for the username and password parameters. These values can be optioned using base64 encoding as described in the following example:\necho -n \u0026quot;myusername\u0026quot; | base64 echo -n \u0026quot;mypassword\u0026quot; | base64 Run kubectl create -f isilon-creds.yaml command to create the secret.\n Create a CR (Custom Resource) for PowerScale using the sample files provided here. Users should configure the parameters in CR. The following table lists the primary configurable parameters of the PowerScale driver and their default values:    Parameter Description Required Default     Common parameters for node and controller      CSI_ENDPOINT The UNIX socket address for handling gRPC calls No /var/run/csi/csi.sock   X_CSI_DEBUG To enable debug mode No false   X_CSI_ISI_ENDPOINT HTTPs endpoint of the PowerScale OneFS API server Yes    X_CSI_ISI_INSECURE Specifies whether SSL security needs to be enabled for communication between PowerScale and CSI Driver No true   X_CSI_ISI_PATH Base path for the volumes to be created Yes    X_CSI_ISI_AUTOPROBE To enable auto probing for driver No true   X_CSI_ISILON_NO_PROBE_ON_START Indicates whether the controller/node should probe during initialization Yes    Controller parameters      X_CSI_MODE Driver starting mode No controller   X_CSI_ISI_ACCESS_ZONE Name of the access zone a volume can be created in No System   X_CSI_ISI_QUOTA_ENABLED To enable SmartQuotas Yes    Node parameters      X_CSI_ISILON_NFS_V3 Set the version to v3 when mounting an NFS export. If the value is \u0026ldquo;false\u0026rdquo;, then the default version supported will be used Yes    X_CSI_MODE Driver starting mode No node     Execute the following command to create PowerScale custom resource: kubectl create -f \u0026lt;input_sample_file.yaml\u0026gt; . This command will deploy the CSI-PowerScale driver.  ","excerpt":"Installing PowerScale CSI Driver via Operator The CSI Driver for Dell EMC PowerScale can be …","ref":"/ansible-docs/v2/installation/operator/isilon/","title":"PowerScale"},{"body":"This section provides multiple methods to test driver functionality in your environment.\nNote: To run the test for CSI Driver for Dell EMC PowerScale, install Helm 3.\nTest deploying a simple pod with PowerScale storage Test the deployment workflow of a simple pod on PowerScale storage.\n  Creating a volume:\nCreate a file pvc.yaml using sample yaml files located at test/sample_files/\nExecute the following command to create volume\nkubectl create -f $PWD/pvc.yaml Result: After executing the above command PVC will be created in the default namespace, and the user can see the pvc by executing kubectl get pvc. Note: Verify system for the new volume\n  Attach the volume to Host\nTo attach a volume to a host, create a new application(Pod) and use the PVC created above in the Pod. This scenario is explained using the Nginx application. Create nginx.yaml using sample yaml files located at test/sample_files/.\nExecute the following command to mount the volume to Kubernetes node\nkubectl create -f $PWD/nginx.yaml Result: After executing the above command, new nginx pod will be successfully created and started in the default namespace. Note: Verify PowerScale system for host to be part of clients/rootclients field of export created for volume and used by nginx application.\n  Create Snapshot\nThe following procedure will create a snapshot of the volume in the container using VolumeSnapshot objects defined in snap.yaml. The sample file for snapshot creation is located at test/sample_files/\nExecute the following command to create snapshot\nkubectl create -f $PWD/snap.yaml The spec.source section contains the volume that will be snapped in the default namespace. For example, if the volume to be snapped is testvolclaim1, then the created snapshot is named testvolclaim1-snap1. Verify the PowerScale system for newly created snapshot.\nNote:\n User can see the snapshots using kubectl get volumesnapshot Notice that this VolumeSnapshot class has a reference to a snapshotClassName:isilon-snapclass. The CSI Driver for PowerScale installation creates this class as its default snapshot class. You can see its definition using kubectl get volumesnapshotclasses isilon-snapclass -o yaml.    Create Volume from Snapshot\nThe following procedure will create a new volume from a given snapshot which is specified in spec dataSource field.\nThe sample file for volume creation from snapshot is located under test/sample_files/\nExecute the following command to create snapshot\nkubectl create -f $PWD/volume_from_snap.yaml Verify the PowerScale system for newly created volume from snapshot.\n  Delete Snapshot\nExecute the following commands to delete the snapshot\nkubectl get volumesnapshot kubectl delete volumesnapshot testvolclaim1-snap1   Create new volume from existing volume(volume clone)\nThe following procedure will create a new volume from another existing volume which is specified in spec dataSource field.\nThe sample file for volume creation from volume is located at test/sample_files/\nExecute the following command to create snapshot\nkubectl create -f $PWD/volume_from_volume.yaml Verify the PowerScale system for new created volume from volume.\n  To Unattach the volume from Host\nDelete the nginx application to unattach the volume from host\nkubectl delete -f nginx.yaml\n  To delete the volume\nkubectl get pvc kubectl delete pvc testvolclaim1 kubectl get pvc   ","excerpt":"This section provides multiple methods to test driver functionality in your environment.\nNote: To …","ref":"/ansible-docs/v2/installation/test/powerscale/","title":"Test PowerScale CSI Driver"},{"body":"Release Notes - CSI Driver for PowerScale v1.4.0 New Features/Changes  Added support for OpenShift 4.6 with RHEL and CoreOS worker nodes Added support for Red Hat Enterprise Linux (RHEL) 7.9 Added support for Ubuntu 20.04 Added support for Controller high availability (multiple-controllers) Added Topology support Added support for CSI Ephemeral Inline Volumes Added support for mount options Enhancements to volume creation from data source Enhanced support for Docker EE 3.1  Fixed Issues    Problem summary Found in version Resolved in version     POD creation fails in OpenShift and Kubernetes environments, if hostname is not an FQDN v1.3.0 v1.4.0   When creating volume from a snapshot or volume from volume, the owner of the new files or folders that are copied from the source snapshot is the Isilon user who is specified in secret.yaml. So the original owner of a file or folder might not be the owner of the newly created file or folder.  v1.4.0    Known Issues    Issue Resolution or workaround, if known     Creating snapshot fails if the parameter IsiPath in volume snapshot class and related storage class are not the same. The driver uses the incorrect IsiPath parameter and tries to locate the source volume due to the inconsistency. Ensure IsiPath in VolumeSnapshotClass yaml and related storageClass yaml are the same.   While deleting a volume, if there are files or folders created on the volume that are owned by different users. If the Isilon credentials used are for a nonprivileged Isilon user, the delete volume action fails. It is due to the limitation in Linux permission control. To perform the delete volume action, the user account must be assigned a role that has the privilege ISI_PRIV_IFS_RESTORE. The user account must have the following set of privileges to ensure that all the CSI Isilon driver capabilities work properly:\n* ISI_PRIV_LOGIN_PAPI\n* ISI_PRIV_NFS\n* ISI_PRIV_QUOTA\n* ISI_PRIV_SNAPSHOT\n* ISI_PRIV_IFS_RESTORE\n* ISI_PRIV_NS_IFS_ACCESS\n* ISI_PRIV_LOGIN_SSH\nIn some cases, ISI_PRIV_BACKUP is also required, for example, when files owned by other users have mode bits set to 700.   If hostname is mapped to loopback IP in /etc/hosts file, and pods are created using 1.3.0.1 release, after upgrade to 1.4.0 there is a possibility of \u0026ldquo;localhost\u0026rdquo; as stale entry in export We recommend you not to map hostname to loopback IP in /etc/hosts file   If the length of the nodeID exceeds 128 characters, driver fails to update CSINode object and installation fails. This is due to a limitation set by CSI spec which doesn\u0026rsquo;t allow nodeID to be greater than 128 characters. The CSI PowerScale driver uses the hostname for building the nodeID which is set in CSINode resource object, hence we recommend not having very long hostnames in order to avoid this issue. This current limitation of 128 characters is likely to be relaxed in future kubernetes versions as per this issue in the community: https://github.com/kubernetes-sigs/gcp-compute-persistent-disk-csi-driver/issues/581    ","excerpt":"Release Notes - CSI Driver for PowerScale v1.4.0 New Features/Changes  Added support for OpenShift …","ref":"/ansible-docs/v2/release/powerscale/","title":"PowerScale"},{"body":"Here are some installation failures that might be encountered and how to mitigate them.\n   Symptoms Prevention, Resolution or Workaround     The kubectl logs isilon-controller-0 -n isilon -c driver logs shows the driver cannot authenticate Check your secret\u0026rsquo;s username and password   The kubectl logs isilon-controller-0 -n isilon -c driver logs shows the driver failed to connect to the Isilon because it couldn\u0026rsquo;t verify the certificates Check the isilon-certs secret and ensure it is not empty and it has the valid certificates. Set isiInsecure: \u0026quot;true\u0026quot; for insecure connection. SSL validation is recommended in production environment.    ","excerpt":"Here are some installation failures that might be encountered and how to mitigate them.\n   Symptoms …","ref":"/ansible-docs/v2/troubleshooting/powerscale/","title":"PowerScale"},{"body":"Creating volumes and consuming them Create a file simple.yaml using sample yaml files located at tests/simple/\nThis command creates a statefulset that consumes three volumes of default storage classes\nkubectl create -f tests/simple/simple.yaml After executing this command 3 PVC and statefulset are created in the testpowerstore namespace. You can check created PVCs by running kubectl get pvc -n testpowerstore and check statefulset\u0026rsquo;s pods by running kubectl get pods -n testpowerstore Pod should be Ready and Running\n If Pod is in CrashLoopback or PVCs is in Pending state then driver installation is not successful, check logs of node and controller\n Deleting volumes To delete volumes, pod and statefulset run\nkubectl delete -f tests/simple/simple.yaml Consuming existing volumes with static provisioning You can use existent volumes from PowerStore array as Persistent Volumes in your Kubernetes, perform the following steps:\n Open your volume in PowerStore Management UI, and take a note of volume-id. The volume link must look similar to https://\u0026lt;powerstore.api.ip\u0026gt;/#/storage/volumes/0055558c-5ae1-4ed1-b421-6f5a9475c19f/capacity, where the volume-id is 0055558c-5ae1-4ed1-b421-6f5a9475c19f. Create PersistentVolume and use this volume-id as a volumeHandle in the manifest. Modify other parameters according to your needs.  apiVersion:v1kind:PersistentVolumemetadata:name:existingvolspec:accessModes:- ReadWriteOncecapacity:storage:30Gicsi:driver:csi-powerstore.dellemc.comvolumeHandle:0055558c-5ae1-4ed1-b421-6f5a9475c19fpersistentVolumeReclaimPolicy:RetainstorageClassName:powerstorevolumeMode:FilesystemCreate PersistentVolumeClaim to use this PersistentVolume.  apiVersion:v1kind:PersistentVolumeClaimmetadata:name:pvolspec:accessModes:- ReadWriteOncevolumeMode:Filesystemresources:requests:storage:30GistorageClassName:powerstoreThen use this PVC as a volume in a pod.  apiVersion:v1kind:Podmetadata:name:static-prov-podspec:containers:- name:testimage:quay.io/centos/centos:latestcommand:[\u0026#34;/bin/sleep\u0026#34;,\u0026#34;3600\u0026#34;]volumeMounts:- mountPath:\u0026#34;/data0\u0026#34;name:pvolvolumes:- name:pvolpersistentVolumeClaim:claimName:pvolAfter the pod is Ready and Running, you can start to use this pod and volume.  Volume Snapshot Feature The CSI PowerStore driver version 1.1 and later supports managing beta snapshots.\nTo use Volume Snapshots, ensure the following components have been deployed to your cluster:\n Kubernetes Volume Snaphshot CRDs Volume Snapshot Controller  You can install them by running the following commands:\ngit clone https://github.com/kubernetes-csi/external-snapshotter/ cd ./external-snapshotter git checkout release-\u0026lt;your-version\u0026gt; kubectl create -f client/config/crd kubectl create -f deploy/kubernetes/snapshot-controller  For general use, update the snapshot controller YAMLs with an appropriate namespace before installing. For example, on a Vanilla Kubernetes cluster, update the namespace from default to kube-system before issuing the kubectl create command.\n Volume Snapshot Class During the installation of CSI PowerStore driver version 1.1 and later, a Volume Snapshot Class is created. This is the only Volume Snapshot Class required and there is no need to create any other Volume Snapshot Class.\nFollowing is the manifest for the Volume Snapshot Class created during installation:\napiVersion:snapshot.storage.k8s.io/v1# or v1beta1, depends on your k8s versionkind:VolumeSnapshotClassmetadata:name:powerstore-snapshotdriver:csi-powerstore.dellemc.comdeletionPolicy:DeleteCreate Volume Snapshot The following is a sample manifest for creating a Volume Snapshot using the v1 (or v1beta1) snapshot APIs:\napiVersion:snapshot.storage.k8s.io/v1# or v1beta1, depends on your k8s versionkind:VolumeSnapshotmetadata:name:pvol0-snapnamespace:testpowerstorespec:volumeSnapshotClassName:powerstore-snapshotsource:persistentVolumeClaimName:pvolOnce the VolumeSnapshot is successfully created by the CSI PowerStore driver, a VolumeSnapshotContent object is automatically created. Once the status of the VolumeSnapshot object has the readyToUse field set to true , it is available for use.\nFollowing is the relevant section of VolumeSnapshot object status:\nstatus:boundVolumeSnapshotContentName:snapcontent-5a8334d2-eb40-4917-83a2-98f238c4bdacreationTime:\u0026#34;2020-07-16T08:42:12Z\u0026#34;readyToUse:trueCreating PVCs with Volume Snapshots as Source The following is a sample manifest for creating a PVC with a VolumeSnapshot as a source:\napiVersion:v1kind:PersistentVolumeClaimmetadata:name:restorepvcnamespace:testpowerstorespec:storageClassName:powerstoredataSource:name:pvol0-snapkind:VolumeSnapshotapiGroup:snapshot.storage.k8s.ioaccessModes:- ReadWriteOnceresources:requests:storage:8GiiSCSI CHAP The CSI PowerStore driver Version 1.3.0 extends Challenge Handshake Authentication Protocol (CHAP) support by adding automatic credentials generation.\nThis means that you no longer need to provide chapsecret/chapuser credentials, they will be automatically generated by the driver for each host.\nTo enable this feature you need to set connection.enableCHAP to true when installing with helm or set X_CSI_POWERSTORE_ENABLE_CHAP to true in your PowerStore CustomResource when installing using operator.\nThe driver uses the generated chapsecret to configure the iSCSI node database on each node with iSCSI access.\nWhen creating new host on powerstore array driver will populate host chap credentials with generated values. When re-using already existing hosts driver should override existing CHAP credentials with newly generated ones.\nVolume Expansion The CSI PowerStore driver version 1.1 and later supports the expansion of Persistent Volumes (PVs). This expansion can be done either online (for example, when a PV is attached to a node) or offline (for example, when a PV is not attached to any node).\nTo use this feature, the storage class that is used to create the PV must have the attribute allowVolumeExpansion set to true. The storage classes created during the installation (both using Helm or dell-csi-operator) have the allowVolumeExpansion set to true by default.\nIf you are creating more storage classes, ensure that this attribute is set to true to expand any PVs created using these new storage classes.\nThe following is a sample manifest for a storage class which allows for Volume Expansion:\napiVersion:storage.k8s.io/v1kind:StorageClassmetadata:name:powerstore-expand-scannotations:storageclass.beta.kubernetes.io/is-default-class:falseprovisioner:csi-powerstore.dellemc.comreclaimPolicy:DeleteallowVolumeExpansion:true# Set this attribute to true if you plan to expand any PVCs created using this storage classparameters:FsType:xfsTo resize a PVC, edit the existing PVC spec and set spec.resources.requests.storage to the intended size. For example, if you have a PVC pstore-pvc-demo of size 3Gi, then you can resize it to 30Gi by updating the PVC.\napiVersion:v1kind:PersistentVolumeClaimmetadata:name:pstore-pvc-demonamespace:testspec:accessModes:- ReadWriteOncevolumeMode:Filesystemresources:requests:storage:30Gi# Updated size from 3Gi to 30GistorageClassName:powerstore-expand-sc The Kubernetes Volume Expansion feature can only be used to increase the size of a volume. It cannot be used to shrink a volume.\n Raw block support CSI PowerStore driver supports managing Raw Block volumes since version 1.1\nRaw Block volumes are created using the volumeDevices list in the pod template spec with each entry accessing a volumeClaimTemplate specifying a volumeMode: Block. An example configuration is outlined here:\napiVersion:apps/v1kind:StatefulSetmetadata:name:powerstoretestnamespace:{{.Values.namespace}}spec:...spec:...containers:- name:test...volumeDevices:- devicePath:\u0026#34;/dev/data0\u0026#34;name:pvolvolumeClaimTemplates:- metadata:name:pvolspec:accessModes:- ReadWriteOncevolumeMode:BlockstorageClassName:powerstoreresources:requests:storage:8GiAllowable access modes are ReadWriteOnce, ReadWriteMany, and for block devices that have been previously initialized, ReadOnlyMany.\nRaw Block volumes are presented as a block device to the pod by using a bind mount to a block device in the node\u0026rsquo;s file system. The driver does not format or check the format of any file system on the block device. Raw Block volumes do support online Volume Expansion, but it is up to the application to manage reconfiguring the file system (if any) to the new size.\nFor additional information, see the kubernetes website.\nVolume Cloning Feature The CSI PowerStore driver version 1.1 and later supports volume cloning. This allows specifying existing PVCs in the dataSource field to indicate a user would like to clone a Volume.\nSource and destination PVC must be in the same namespace and have the same Storage Class.\nTo clone a volume, you should first have an existing pvc, for example, pvol0:\nkind:PersistentVolumeClaimapiVersion:v1metadata:name:pvol0namespace:testpowerstorespec:storageClassName:powerstoreaccessModes:- ReadWriteOncevolumeMode:Filesystemresources:requests:storage:8GiThe following is a sample manifest for cloning pvol0:\napiVersion:v1kind:PersistentVolumeClaimmetadata:name:clonedpvcnamespace:testpowerstorespec:storageClassName:powerstoredataSource:name:pvol0kind:PersistentVolumeClaimaccessModes:- ReadWriteOnceresources:requests:storage:8GiEphemeral Inline Volume The CSI PowerStore driver version 1.2 supports ephemeral inline CSI volumes. This feature allows CSI volumes to be specified directly in the pod specification.\nAt runtime, nested inline volumes follow the ephemeral lifecycle of their associated pods where the driver handles all phases of volume operations as pods are created and destroyed.\nThe following is a sample manifest for creating ephemeral volume in pod manifest with CSI PowerStore driver.\nkind:PodapiVersion:v1metadata:name:powerstore-inline-volumespec:containers:- name:test-containerimage:quay.io/centos/centoscommand:[\u0026#34;sleep\u0026#34;,\u0026#34;3600\u0026#34;]volumeMounts:- mountPath:\u0026#34;/data\u0026#34;name:volumevolumes:- name:volumecsi:driver:csi-powerstore.dellemc.comfsType:\u0026#34;ext4\u0026#34;volumeAttributes:size:\u0026#34;20Gi\u0026#34;This manifest creates a pod and attach newly created ephemeral inline csi volume to it.\nTo create NFS volume you need to provide nasName: parameters that points to the name of your NAS Server in pod manifest like so\nvolumes:- name:volumecsi:driver:csi-powerstore.dellemc.comfsType:\u0026#34;nfs\u0026#34;volumeAttributes:size:\u0026#34;20Gi\u0026#34;nasName:\u0026#34;csi-nas-name\u0026#34;Controller HA The CSI PowerStore driver version 1.2 introduces controller HA feature. Instead of StatefulSet controller pods deployed as a Deployment.\nBy default number of replicas is set to 2, you can set controller.replicas parameter to 1 in my-powerstore-settings.yaml if you want to disable controller HA for your installation. When installing via Operator you can change replicas parameter in spec.driver section in your PowerStore Custom Resource.\nWhen multiple replicas of controller pods are in cluster, each sidecar (attacher, provisioner, resizer, snapshotter) tries to get a lease so only one instance of each sidecar would be active in the cluster at a time.\nDriver pod placement You can configure where driver controller and worker pods should be placed. To configure use nodeSelector and tolerations mechanisms you can configure in your my-powerstore-settings.yaml\nFor example, you can specify tolerations to assign driver controller pods on controller nodes too:\n# \u0026#34;controller\u0026#34; allows to configure controller specific parameterscontroller:# \u0026#34;controller.nodeSelector\u0026#34; defines what nodes would be selected for pods of controller deploymentnodeSelector:# \u0026#34;controller.tolerations\u0026#34; defines tolerations that would be applied to controller deploymenttolerations:- key:\u0026#34;node-role.kubernetes.io/master\u0026#34;operator:\u0026#34;Exists\u0026#34;effect:\u0026#34;NoSchedule\u0026#34;If you want to assign controller pods ONLY on controller nodes you need to configure nodeSelector:\n# \u0026#34;controller\u0026#34; allows to configure controller specific parameterscontroller:# \u0026#34;controller.nodeSelector\u0026#34; defines what nodes would be selected for pods of controller deploymentnodeSelector:node-role.kubernetes.io/master:\u0026#34;\u0026#34;# \u0026#34;controller.tolerations\u0026#34; defines tolerations that would be applied to controller deploymenttolerations:- key:\u0026#34;node-role.kubernetes.io/master\u0026#34;operator:\u0026#34;Exists\u0026#34;effect:\u0026#34;NoSchedule\u0026#34;As mentioned earlier, you can configure where node driver pods would be assinged in the similar way in node section of my-powerstore-settings.yaml\nTopology The CSI PowerStore driver version 1.2 and later supports Topology which forces volumes to be placed on worker nodes that have connectivity to the backend storage. This covers use cases where users have chosen to restrict the nodes on which the CSI driver is deployed.\nThis Topology support does not include customer defined topology, users cannot create their own labels for nodes, they should use whatever labels are returned by driver and applied automatically by Kubernetes on its nodes.\nTopology Usage To use the Topology features user must create their own storage classes similar to those that can be found in helm/samples/storageclass folder.\nThe following is one of example storage class manifest:\napiVersion:storage.k8s.io/v1kind:StorageClassmetadata:name:powerstore-fcprovisioner:csi-powerstore.dellemc.comreclaimPolicy:DeleteallowVolumeExpansion:truevolumeBindingMode:WaitForFirstConsumerallowedTopologies:- matchLabelExpressions:- key:csi-powerstore.dellemc.com/127.0.0.1-fcvalues:- \u0026#34;true\u0026#34;This example matches all nodes where driver has a connection to PowerStore with IP of 127.0.0.1 via FibreChannel. Similar examples can be found in mentioned folder for NFS and iSCSI.\nYou can check what labels your nodes contain by running kubectl get nodes --show-labels\n Notice that volumeBindingMode: is set to WaitForFirstConsumer this is required for topology feature to work.\n For any additional information about topology, see the Kubernetes Topology documentation.\nReuse PowerStore hostname The CSI PowerStore driver version 1.2 and later can automatically detect if the current node was already registered as Host on storage array before. It will check if Host initiators and node initiators (FC or iSCSI) match. If they do, the driver will not create a new storage class and will take the existing name of the Host as nodeID.\nMultiarray support The CSI PowerStore driver version 1.3.0 adds support for managing multiple PowerStore arrays from the single driver instance. This feature is enabled by default and integrated to even single instance installations.\nTo manage multiple arrays you need to create an array connection configuration that lists multiple arrays.\nCreating array configuration Create a file called config.yaml and populate it with the following content\narrays:- endpoint:\u0026#34;https://10.0.0.1/api/rest\u0026#34;# full URL path to the PowerStore APIusername:\u0026#34;user\u0026#34;# username for connecting to APIpassword:\u0026#34;password\u0026#34;# password for connecting to APIinsecure:true# use insecure connection or notdefault:true# treat current array as a default (would be used by storage classes without arrayIP parameter)block-protocol:\u0026#34;ISCSI\u0026#34;# what SCSI transport protocol use on node side (FC, ISCSI, None, or auto)nas-name:\u0026#34;nas-server\u0026#34;# what NAS should be used for NFS volumes- endpoint:\u0026#34;https://10.0.0.2/api/rest\u0026#34;username:\u0026#34;user\u0026#34;password:\u0026#34;password\u0026#34;insecure:trueblock-protocol:\u0026#34;FC\u0026#34;Here we specify that we want to CSI driver to manage two arrays: one with an IP 10.0.0.1 and the other with an IP 10.0.0.2, we want to connect to the first array with iSCSI protocol and with FC to the second array. Also, we want to be able to create NFS-based volume so we provide the name of the NAS to the first array.\nTo use this config we need to create a Kubernetes secret from it, to do so create a file called secret.yaml in the same folder and populate it with the following content:\napiVersion:v1kind:Secretmetadata:name:powerstore-confignamespace:\u0026lt;driver-namespace\u0026gt;type:Opaquedata:config:CONFIG_YAMLApply the secret by running following command:\nsed \u0026#34;s/CONFIG_YAML/`cat config.yaml | base64 -w0`/g\u0026#34; secret.yaml | kubectl apply -f - Creating storage classes To be able to provision Kubernetes volumes using a specific array we need to create corresponding storage classes.\nCreate file storageclass.yaml and populate it with the following content:\napiVersion:storage.k8s.io/v1kind:StorageClassmetadata:name:powerstore-1provisioner:csi-powerstore.dellemc.comreclaimPolicy:DeleteallowVolumeExpansion:truevolumeBindingMode:WaitForFirstConsumerparameters:arrayIP:\u0026#34;10.0.0.1\u0026#34;FsType:\u0026#34;ext4\u0026#34;---apiVersion:storage.k8s.io/v1kind:StorageClassmetadata:name:powerstore-2provisioner:csi-powerstore.dellemc.comreclaimPolicy:DeleteallowVolumeExpansion:truevolumeBindingMode:WaitForFirstConsumerparameters:arrayIP:\u0026#34;10.0.0.2\u0026#34;FsType:\u0026#34;xfs\u0026#34;Here we specify two storage classes: one of them uses the first array and ext4 filesystem, and the other uses the second array and xfs filesystem.\nThen we need to apply storage classes to Kubernetes using kubectl:\nkubectl create -f storageclass.yaml After that, you can use powerstore-1 storage class to create volumes on first array and powerstore-2 storage class to create volumes on the second array.\nDynamic secret change detection CSI PowerStore driver version 1.3.0 adds the ability to detect changes to array configuration Kubernetes secret. This essentially means that you can change credentials for your PowerStore arrays in-flight (without restarting the driver).\nTo do so just change your configuration file config.yaml and apply it again using the following command:\nsed \u0026#34;s/CONFIG_YAML/`cat config.yaml | base64 -w0`/g\u0026#34; secret.yaml | kubectl apply -f - After Kubernetes remounts secret to driver containers (this usually takes around one minute), a driver should detect the change and start using this new configuration information.\nConfiguring custom access to NFS exports CSI PowerStore driver Version 1.3.0 adds the ability to configure NFS access to nodes that use dedicated storage networks.\nTo enable this feature you need to specify externalAccess parameter in your helm values.yaml file or X_CSI_POWERSTORE_EXTERNAL_ACCESS variable when creating CustomResource using an operator.\nThe value of that parameter will be added as an additional entry to NFS Export host access.\nFor example the following notation:\nexternalAccess:\u0026#34;10.0.0.0/24\u0026#34;This would mean that we allow for NFS Export created by driver to be consumed by address range 10.0.0.0-10.0.0.255.\n","excerpt":"Creating volumes and consuming them Create a file simple.yaml using sample yaml files located at …","ref":"/ansible-docs/v1/features/powerstore/","title":"PowerStore"},{"body":"The CSI Driver for Dell EMC PowerStore can be deployed by using the provided Helm v3 charts and installation scripts on both Kubernetes and OpenShift platforms. For more detailed information on the installation scripts, review the script documentation.\nThe controller section of the Helm chart installs the following components in a Deployment in the namespace csi-powerstore:\n CSI Driver for Dell EMC PowerStore Kubernetes External Provisioner, which provisions the volumes Kubernetes External Attacher, which attaches the volumes to the containers Kubernetes External Snapshotter, which provides snapshot support Kubernetes External Resizer, which resizes the volume  The node section of the Helm chart installs the following component in a DaemonSet in the namespace csi-powerstore:\n CSI Driver for Dell EMC PowerStore Kubernetes Node Registrar, which handles the driver registration  Prerequisites The following are requirements to be met before installing the CSI Driver for Dell EMC PowerStore:\n Install Kubernetes or OpenShift (see supported versions) Install Helm 3 If you plan to use either the Fibre Channel or iSCSI protocol, refer to either Fibre Channel requirements or Set up the iSCSI Initiator sections below. You can use NFS volumes without FC or iSCSI configuration.   You can use either the Fibre Channel or iSCSI protocol, but you do not need both.\n  If you want to use preconfigured iSCSI/FC hosts be sure to check that they are not part of any host group\n  Linux native multipathing requirements Configure Mount propagation on container runtime (i.e. Docker) Volume Snapshot requirements The nonsecure registries are defined in Docker or other container runtimes, for CSI drivers that are hosted in a non-secure location. You can access your cluster with kubectl and helm. Ensure that your nodes support mounting NFS volumes.  Install Helm 3.0 Install Helm 3.0 on the master node before you install the CSI Driver for Dell EMC PowerFlex.\nSteps\nRun the curl https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3 | bash command to install Helm 3.0.\nFibre Channel requirements Dell EMC PowerStore supports Fibre Channel communication. If you use the Fibre Channel protocol, ensure that the following requirement is met before you install the CSI Driver for Dell EMC PowerStore:\n Zoning of the Host Bus Adapters (HBAs) to the Fibre Channel port director must be done.  Set up the iSCSI Initiator The CSI Driver for Dell EMC PowerStore v1.3 supports iSCSI connectivity.\nIf you use the iSCSI protocol, set up the iSCSI initiators as follows:\n Ensure that the iSCSI initiators are available on both Controller and Worker nodes. Kubernetes nodes must have access (network connectivity) to an iSCSI director on the Dell EMC PowerStore array that has IP interfaces. Manually create IP routes for each node that connects to the Dell EMC PowerStore. All Kubernetes nodes must have the iscsi-initiator-utils package for CentOS/RHEL or open-iscsi package for Ubuntu installed, and the iscsid service must be enabled and running. To do this, run the systemctl enable --now iscsid command. Ensure that the unique initiator name is set in /etc/iscsi/initiatorname.iscsi.  For information about configuring iSCSI, see Dell EMC PowerStore documentation on Dell EMC Support.\nLinux multipathing requirements Dell EMC PowerStore supports Linux multipathing. Configure Linux multipathing before installing the CSI Driver for Dell EMC PowerStore.\nSet up Linux multipathing as follows:\n Ensure that all nodes have the Device Mapper Multipathing package installed.   You can install it by running yum install device-mapper-multipath on CentOS or apt install multipath-tools on Ubuntu. This package should create a multipath configuration file located in /etc/multipath.conf.\n  Enable multipathing using the mpathconf --enable --with_multipathd y command. Enable user_friendly_names and find_multipaths in the multipath.conf file. Ensure that the multipath command for multipath.conf is available on all Kubernetes nodes.  Configure Mount Propagation on Container Runtime It is required to configure mount propagation on your container runtime on all Kubernetes nodes before installing the CSI Driver for Dell EMC PowerStore. The following is instruction on how to do this with Docker. If you use another container runtime please follow the recommended instructions from the vendor to configure mount propagation.\nSteps\n Edit the service section of /etc/systemd/system/multi-user.target.wants/docker.service file to add the following lines: docker.service [Service]... MountFlags=shared  Restart the docker service with systemctl daemon-reload and systemctl restart docker on all the nodes.  NOTE: Some distribution, like Ubuntu, already has MountFlags set by default.\nVolume Snapshot Requirements Volume Snapshot CRD\u0026rsquo;s The Kubernetes Volume Snapshot CRDs can be obtained and installed from the external-snapshotter project on Github.\n If on Kubernetes 1.18/1.19 (beta snapshots) use v3.0.3 If on Kubernetes 1.20 (v1 snapshots) use v4.0.0  Volume Snapshot Controller The beta Volume Snapshots in Kubernetes version 1.17 and later, the CSI external-snapshotter sidecar is split into two controllers:\n A common snapshot controller A CSI external-snapshotter sidecar  The common snapshot controller must be installed only once in the cluster irrespective of the number of CSI drivers installed in the cluster. On OpenShift clusters 4.4 and later, the common snapshot-controller is pre-installed. In the clusters where it is not present, it can be installed using kubectl and the manifests are available:\n If on Kubernetes 1.18/1.19 (beta snapshots) use v3.0.3 If on Kubernetes 1.20 (v1 snapshots) use v4.0.0  NOTE:\n The manifests available on GitHub install the snapshotter image:  quay.io/k8scsi/csi-snapshotter:v3.0.3 quay.io/k8scsi/csi-snapshotter:v4.0.0   The CSI external-snapshotter sidecar is still installed along with the driver and does not involve any extra configuration.  Installation example You can install CRDs and default snapshot controller by running following commands:\ngit clone https://github.com/kubernetes-csi/external-snapshotter/ cd ./external-snapshotter git checkout release-\u0026lt;your-version\u0026gt; kubectl create -f client/config/crd kubectl create -f deploy/kubernetes/snapshot-controller NOTE:\n It is recommended to use v3.0.3 version of snapshotter/snapshot-controller when using Kubernetes v1.18, v1.19 When using Kubernetes v1.20 it is recommended to use v4.0.0 version of snapshotter/snapshot-controller. The CSI external-snapshotter sidecar is still installed along with the driver and does not involve any extra configuration.  Install the Driver Steps\n  Run git clone https://github.com/dell/csi-powerstore.git to clone the git repository.\n  Ensure that you have created namespace where you want to install the driver. You can run kubectl create namespace csi-powerstore to create a new one.\n  Check helm/csi-powerstore/driver-image.yaml and confirm the driver image points to new image.\n  Edit helm/secret.yaml, correct namespace field to point to your desired namespace.\n  Edit helm/config.yaml file and configure connection information for your PowerStore arrays changing following parameters:\n endpoint: defines the full URL path to the PowerStore API. username, password: defines credentials for connecting to array. insecure: defines if we should use insecure connection or not. default: defines if we should treat the current array as a default. block-protocol: defines what SCSI transport protocol we should use (FC, ISCSI, None, or auto). nas-name: defines what NAS should be used for NFS volumes.  Add more blocks similar to above for each PowerStore array if necessary.\n  Create storage classes using ones from helm/samples/storageclass folder as an example and apply them to the Kubernetes cluster by running kubectl create -f \u0026lt;path_to_storageclass_file\u0026gt;\n If you do not specify arrayIP parameter in the storage class then the array that was specified as the default would be used for provisioning volumes.\n   Create the secret by running sed \u0026quot;s/CONFIG_YAML/`cat helm/config.yaml | base64 -w0`/g\u0026quot; helm/secret.yaml | kubectl apply -f -\n  Copy the default values.yaml file cd dell-csi-helm-installer \u0026amp;\u0026amp; cp ../helm/csi-powerstore/values.yaml ./my-powerstore-settings.yaml\n  Edit the newly created values file and provide values for the following parameters vi my-powerstore-settings.yaml:\n     Parameter Description Required Default     volumeNamePrefix Defines the string added to each volume that the CSI driver creates No \u0026ldquo;csi\u0026rdquo;   nodeNamePrefix Defines the string added to each node that the CSI driver registers No \u0026ldquo;csi-node\u0026rdquo;   nodeIDPath Defines a path to file with a unique identifier identifying the node in the Kubernetes cluster No \u0026ldquo;/etc/machine-id\u0026rdquo;   externalAccess Defines additional entries for hostAccess of NFS volumes, single IP address and subnet are valid entries No \u0026quot; \u0026quot;   connection.enableCHAP Defines whether the driver should use CHAP for iSCSI connections or not No False   controller.nodeSelector Defines what nodes would be selected for pods of controller deployment Yes \u0026quot; \u0026quot;   controller.tolerations Defines tolerations that would be applied to controller deployment Yes \u0026quot; \u0026quot;   controller.replicas Defines number of replicas of controller deployment Yes 2   node.nodeSelector Defines what nodes would be selected for pods of node daemonset Yes \u0026quot; \u0026quot;   node.tolerations Defines tolerations that would be applied to node daemonset Yes \u0026quot; \u0026quot;    Install the driver using csi-install.sh bash script by running ./csi-install.sh --namespace csi-powerstore --values ./my-powerstore-settings.yaml  After that the driver should be installed, you can check the condition of driver pods by running kubectl get all -n csi-powerstore    NOTE:\n For detailed instructions on how to run the install scripts, refer to the readme document in the dell-csi-helm-installer folder. By default, the driver scans available SCSI adapters and tries to register them with the storage array under the SCSI hostname using nodeNamePrefix and the ID read from the file pointed to by nodeIDPath. If an adapter is already registered with the storage under a different hostname, the adapter is not used by the driver. A hostname the driver uses for registration of adapters is in the form \u0026lt;nodeNamePrefix\u0026gt;-\u0026lt;nodeID\u0026gt;-\u0026lt;nodeIP\u0026gt;. By default, these are csi-node and the machine ID read from the file /etc/machine-id. To customize the hostname, for example if you want to make them more user friendly, adjust nodeIDPath and nodeNamePrefix accordingly. For example, you can set nodeNamePrefix to k8s and nodeIDPath to /etc/hostname to produce names such as k8s-worker1-192.168.1.2. (Optional) Enable additional Mount Options - A user is able to specify additional mount options as needed for the driver.  Mount options are specified in storageclass yaml under mountOptions. WARNING: Before utilizing mount options, you must first be fully aware of the potential impact and understand your environment\u0026rsquo;s requirements for the specified option.    Storage Classes The CSI driver for Dell EMC PowerStore version 1.3 and later, dell-csi-helm-installer will not create any storage classes as part of the driver installation. A wide set of annotated storage class manifests have been provided in the helm/samples folder. Please use these samples to create new storage classes to provision storage. See this note for the driving reason behind this change.\nWhat happens to my existing storage classes? Upgrading from CSI PowerStore v1.2 driver The storage classes created as part of the installation have an annotation - \u0026ldquo;helm.sh/resource-policy\u0026rdquo;: keep set. This ensures that even after an uninstall or upgrade, the storage classes are not deleted. You can continue using these storage classes if you wish so.\nUpgrading from an older version of the driver The storage classes will be deleted if you upgrade the driver. If you wish to continue using those storage classes, you can patch them and apply the annotation \u0026ldquo;helm.sh/resource-policy\u0026rdquo;: keep before performing an upgrade.\nNOTE: If you continue to use the old storage classes, you may not be able to take advantage of any new storage class parameter supported by the driver.\nSteps to create storage class:\nThere are samples storage class yaml files available under helm/samples/storageclass. These can be copied and modified as needed.\n Edit the sample storage class yaml file and update following parameters:   arrayIP: specifies what array driver should use to provision volumes, if not specified driver will use array specified as default in helm/config.yaml FsType: specifies what filesystem type driver should use, possible variants ext4, xfs, nfs, if not specified driver will use ext4 by default allowedTopologies (Optional): If you want you can also add topology constraints.  allowedTopologies:- matchLabelExpressions:- key:csi-powerstore.dellemc.com/12.34.56.78-iscsi# replace \u0026#34;-iscsi\u0026#34; with \u0026#34;-fc\u0026#34; or \u0026#34;-nfs\u0026#34; at the end to use FC or NFS enabled hosts# replace \u0026#34;12.34.56.78\u0026#34; with PowerStore endpoint IPvalues:- \u0026#34;true\u0026#34;Create your storage class by using kubectl:  kubectl create -f \u0026lt;path_to_storageclass_file\u0026gt; NOTE: Deleting a storage class has no impact on a running Pod with mounted PVCs. You won\u0026rsquo;t be able to provision new PVCs until at least one storage class is newly created.\n","excerpt":"The CSI Driver for Dell EMC PowerStore can be deployed by using the provided Helm v3 charts and …","ref":"/ansible-docs/v1/installation/helm/powerstore/","title":"PowerStore"},{"body":"Installing PowerStore CSI Driver via Operator The CSI Driver for Dell EMC PowerStore can be installed via the Dell CSI Operator.\nTo deploy the Operator, follow the instructions available here.\nThere are sample manifests provided which can be edited to do an easy installation of the driver. Note: The deployment of the driver using the operator does not use any Helm charts. The installation and configuration parameters will be slightly different from the ones specified via the Helm installer.\nKubernetes Operators make it easy to deploy and manage entire lifecycle of complex Kubernetes applications. Operators use Custom Resource Definitions (CRD) which represents the application and use custom controllers to manage them.\nInstall Driver   Create namespace:\nRun kubectl create namespace \u0026lt;driver-namespace\u0026gt; using the desired name to create the namespace.\n  Create PowerStore array connection config:\nCreate a file called config.yaml with the following content\narrays:- endpoint:\u0026#34;https://10.0.0.1/api/rest\u0026#34;# full URL path to the PowerStore APIusername:\u0026#34;user\u0026#34;# username for connecting to APIpassword:\u0026#34;password\u0026#34;# password for connecting to APIinsecure:true# use insecure connection or notdefault:true# treat current array as a default (would be used by storage classes without arrayIP parameter)block-protocol:\u0026#34;auto\u0026#34;# what SCSI transport protocol use on node side (FC, ISCSI, None, or auto)nas-name:\u0026#34;nas-server\u0026#34;# what NAS should be used for NFS volumesChange the parameters with relevant values for your PowerStore array.\nAdd more blocks similar to above for each PowerStore array if necessary.\n  Create Kubernetes secret:\nCreate a file called secret.yaml in same folder as config.yaml with following content\napiVersion:v1kind:Secretmetadata:name:powerstore-confignamespace:\u0026lt;driver-namespace\u0026gt;type:Opaquedata:config:CONFIG_YAMLCombine both files and create Kubernetes secret by running following command:\nsed \u0026#34;s/CONFIG_YAML/`cat config.yaml | base64 -w0`/g\u0026#34; secret.yaml | kubectl apply -f -   Create a Custom Resource (CR) for PowerStore using the sample files provided here.\n  Users must configure the parameters in CR. The following table lists the primary configurable parameters of the PowerStore driver and their default values:\n   Parameter Description Required Default     replicas Controls the number of controller pods you deploy. If the number of controller pods is greater than the number of available nodes, the excess pods will be pending state till new nodes are available for scheduling. Default is 2 which allows for Controller high availability. Yes 2   namespace Specifies namespace where the drive will be installed Yes \u0026ldquo;test-powerstore\u0026rdquo;   Common parameters for node and controller      X_CSI_POWERSTORE_NODE_NAME_PREFIX Prefix to add to each node registered by the CSI driver Yes \u0026ldquo;csi-node\u0026rdquo;   X_CSI_FC_PORTS_FILTER_FILE_PATH To set path to the file which provides a list of WWPN which should be used by the driver for FC connection on this node No \u0026ldquo;/etc/fc-ports-filter\u0026rdquo;   Controller parameters      X_CSI_POWERSTORE_EXTERNAL_ACCESS allows specifying additional entries for hostAccess of NFS volumes. Both single IP address and subnet are valid entries No \u0026quot; \u0026quot;   Node parameters      X_CSI_POWERSTORE_ENABLE_CHAP Set to true if you want to enable iSCSI CHAP feature No false   StorageClass parameters      FsType Specifies what filesystem type driver should use, possible variants ext4, xfs, nfs No \u0026ldquo;ext4\u0026rdquo;   arrayIP Specifies what array driver should use to provision volumes No \u0026ldquo;default\u0026rdquo;   allowedTopologies:key This is to enable topology to allow pods/and volumes to always be scheduled on nodes that have access to the storage. You need to replace the \u0026ldquo;127.0.0.1-nfs\u0026rdquo; portion in the key with PowerStore endpoint IP with its value and append -nfs, -fc or -iscsi at the end of it No \u0026ldquo;127.0.0.1-nfs\u0026rdquo;      Execute the following command to create PowerStore custom resource:kubectl create -f \u0026lt;input_sample_file.yaml\u0026gt;. The above command will deploy the CSI-PowerStore driver.\n After that the driver should be installed, you can check the condition of driver pods by running kubectl get all -n \u0026lt;driver-namespace\u0026gt;    ","excerpt":"Installing PowerStore CSI Driver via Operator The CSI Driver for Dell EMC PowerStore can be …","ref":"/ansible-docs/v1/installation/operator/powerstore/","title":"PowerStore"},{"body":"In the repository, a simple test manifest exists that creates three different PersistentVolumeClaims using default ext4, xfs and nfs storage classes and automatically mounts them to the pod.\n It assumes that you\u0026rsquo;ve created the same basic three storage classes from helm/samples/storageclass folder without changing their names. If you\u0026rsquo;ve created different storage classes please edit tests/simple/simple.yaml and change PersistentVolumeClaim definitions to point to correct storage classes.\n Steps\n To run this test, run the kubectl command from the root directory of the repository: kubectl create -f ./tests/simple/   You can find all the created resources in testpowerstore namespace.\n Check if the pod is created and Ready and Running by running:\nkubectl get all -n testpowerstore If it\u0026rsquo;s in CrashLoopback state then the driver installation wasn\u0026rsquo;t successful. Check the logs of the node and the controller.\n  Go into the created container and verify that everything is mounted correctly.\n  After verifying, you can uninstall the testing PVCs and StatefulSet.\nkubectl delete -f ./tests/simple/   ","excerpt":"In the repository, a simple test manifest exists that creates three different PersistentVolumeClaims …","ref":"/ansible-docs/v1/installation/test/powerstore/","title":"Test PowerStore CSI Driver"},{"body":"Release Notes - CSI PowerStore v1.3.0 New Features/Changes  Added support for Kubernetes v1.20 Added support for OpenShift 4.7 with RHEL and CoreOS worker nodes Added support for Red Hat Enterprise Linux (RHEL) 8.3 Added support for managing multiple PowerStore arrays from one driver Added support for configuring custom IPs/sub-networks for NFS exports Added support for automatic generation of CHAP credentials Changed code structure of the project Removed storage classes from helm template  Fixed Issues There are no fixed issues in this release.\nKnown Issues    Issue Workaround     Slow volume attached/detach If your Kubernetes 1.17 or 1.18 cluster has a lot of VolumeAttachment objects, the attach/detach operations will be very slow. This is a known issue and affects all CSI plugins. It is tracked here: CSI VolumeAttachment slows pod startup time. To get around this problem you can upgrade to latest Kubernetes/OpenShift patches, which contains a partial fix: 1.17.8+, 1.18.5+   Topology related node labels are not removed automatically. Currently, when the driver is uninstalled, topology related node labels are not getting removed automatically. There is an open issue in the Kubernetes to fix this. Until the fix is released you need to manually remove the node labels    ","excerpt":"Release Notes - CSI PowerStore v1.3.0 New Features/Changes  Added support for Kubernetes v1.20 Added …","ref":"/ansible-docs/v1/release/powerstore/","title":"PowerStore"},{"body":"   Symptoms Prevention, Resolution or Workaround     When you run the command kubectl describe pods powerstore-controller-\u0026lt;suffix\u0026gt; –n csi-powerstore, the system indicates that the driver image could not be loaded. - If on Kubernetes, edit the daemon.json file found in the registry location and add { \u0026quot;insecure-registries\u0026quot; :[ \u0026quot;hostname.cloudapp.net:5000\u0026quot; ] } - If on OpenShift, run the command oc edit image.config.openshift.io/cluster and add registries to yaml file that is displayed when you run the command.   The kubectl logs -n csi-powerstore powerstore-node-\u0026lt;suffix\u0026gt; driver logs shows that the driver can\u0026rsquo;t connect to PowerStore API. Check if you\u0026rsquo;ve created secret with correct credentials   Installation of the driver on Kubernetes v1.20 fails with the following error: Error: unable to build kubernetes objects from release manifest: unable to recognize \u0026quot;\u0026quot;: no matches for kind \u0026quot;VolumeSnapshotClass\u0026quot; in version \u0026quot;snapshot.storage.k8s.io/v1\u0026quot; Kubernetes v1.20 requires v1 version of snapshot CRDs. If on Kubernetes 1.20 (v1 snapshots) install CRDs from v4.0.0, see the Volume Snapshot Requirements    ","excerpt":"Symptoms Prevention, Resolution or Workaround     When you run the command kubectl describe pods …","ref":"/ansible-docs/v1/troubleshooting/powerstore/","title":"PowerStore"},{"body":"Creating volumes and consuming them Create a file simple.yaml using sample yaml files located at tests/simple/\nThis command will create a statefulset that consumes three volumes of default storage classes\nkubectl create -f tests/simple/simple.yaml After executing this command 3 PVC and statefulset will be created in the testpowerstore namespace. You can check created PVCs by running kubectl get pvc -n testpowerstore and check statefulset\u0026rsquo;s pods by running kubectl get pods -n testpowerstore Pod should be Ready and Running\n If Pod is in CrashLoopback or PVCs is in Pending state then driver installation is not successful, check logs of node and controller\n Deleting volumes To delete volumes, pod and statefulset run\nkubectl delete -f tests/simple/simple.yaml Consuming existing volumes with static provisioning You can use existent volumes from PowerStore array as Persistent Volumes in your Kubernetes, perform the following steps:\n Open your volume in PowerStore Management UI, and take a note of volume-id. The volume link must look similar to https://\u0026lt;powerstore.api.ip\u0026gt;/#/storage/volumes/0055558c-5ae1-4ed1-b421-6f5a9475c19f/capacity, where the volume-id is 0055558c-5ae1-4ed1-b421-6f5a9475c19f. Create PersistentVolume and use this volume-id as a volumeHandle in the manifest. Modify other parameters according to your needs.  apiVersion:v1kind:PersistentVolumemetadata:name:existingvolspec:accessModes:- ReadWriteOncecapacity:storage:30Gicsi:driver:csi-powerstore.dellemc.comvolumeHandle:0055558c-5ae1-4ed1-b421-6f5a9475c19fpersistentVolumeReclaimPolicy:RetainstorageClassName:powerstorevolumeMode:FilesystemCreate PersistentVolumeClaim to use this PersistentVolume.  apiVersion:v1kind:PersistentVolumeClaimmetadata:name:pvolspec:accessModes:- ReadWriteOncevolumeMode:Filesystemresources:requests:storage:30GistorageClassName:powerstoreThen use this PVC as a volume in a pod.  apiVersion:v1kind:Podmetadata:name:static-prov-podspec:containers:- name:testimage:docker.io/centos:latestcommand:[\u0026#34;/bin/sleep\u0026#34;,\u0026#34;3600\u0026#34;]volumeMounts:- mountPath:\u0026#34;/data0\u0026#34;name:pvolvolumes:- name:pvolpersistentVolumeClaim:claimName:pvolAfter the pod is Ready and Running, you can start to use this pod and volume.  Volume Snapshot Feature The CSI PowerStore driver version 1.1 and later supports managing beta snapshots.\nTo use Volume Snapshots, ensure the following components have been deployed to your cluster:\n Kubernetes Volume Snaphshot CRDs Volume Snapshot Controller  You can install them by running following commands:\ngit clone https://github.com/kubernetes-csi/external-snapshotter/ cd ./external-snapshotter git checkout release-3.0 kubectl create -f client/config/crd kubectl create -f deploy/kubernetes/snapshot-controller  For general use, update the snapshot controller YAMLs with an appropriate namespace before installing. For example, on a Vanilla Kubernetes cluster, update the namespace from default to kube-system before issuing the kubectl create command.\n Volume Snapshot Class During the installation of CSI PowerStore driver version 1.1 and later, a Volume Snapshot Class is created using the new v1beta1 snapshot APIs. This is the only Volume Snapshot Class required and there is no need to create any other Volume Snapshot Class.\nFollowing is the manifest for the Volume Snapshot Class created during installation:\napiVersion:snapshot.storage.k8s.io/v1beta1kind:VolumeSnapshotClassmetadata:name:powerstore-snapshotdriver:csi-powerstore.dellemc.comdeletionPolicy:DeleteCreate Volume Snapshot The following is a sample manifest for creating a Volume Snapshot using the v1beta1 snapshot APIs:\napiVersion:snapshot.storage.k8s.io/v1beta1kind:VolumeSnapshotmetadata:name:pvol0-snapnamespace:testpowerstorespec:volumeSnapshotClassName:powerstore-snapshotsource:persistentVolumeClaimName:pvolOnce the VolumeSnapshot has been successfully created by the CSI PowerStore driver, a VolumeSnapshotContent object is automatically created. Once the status of the VolumeSnapshot object has the readyToUse field set to true , it is available for use.\nFollowing is the relevant section of VolumeSnapshot object status:\nstatus:boundVolumeSnapshotContentName:snapcontent-5a8334d2-eb40-4917-83a2-98f238c4bdacreationTime:\u0026#34;2020-07-16T08:42:12Z\u0026#34;readyToUse:trueCreating PVCs with Volume Snapshots as Source The following is a sample manifest for creating a PVC with a VolumeSnapshot as a source:\napiVersion:v1kind:PersistentVolumeClaimmetadata:name:restorepvcnamespace:testpowerstorespec:storageClassName:powerstoredataSource:name:pvol0-snapkind:VolumeSnapshotapiGroup:snapshot.storage.k8s.ioaccessModes:- ReadWriteOnceresources:requests:storage:8GiiSCSI CHAP The CSI PowerStore driver Version 1.2.0 adds support for unidirectional Challenge Handshake Authentication Protocol (CHAP) for iSCSI protocol.\nTo enable CHAP authentication:\n Create secret powerstore-creds with the key chapsecret and chapuser set to base64 values. chapsecret must be between 12 and 60 symbols. If the secret exists, delete and re-create the secret with this newly added key. Set the parameter connection.enableCHAP in my-powerstore-settings.yaml to true.  The driver uses the provided chapsecret to configure the iSCSI node database on each node with iSCSI access.\nWhen creating new host on powerstore array driver will populate host chap credentials with provided values. When re-using already existing hosts be sure to check that provided credentials in powerstore-creds match earlier preconfigured host credentials.\nVolume Expansion The CSI PowerStore driver version 1.1 and later supports the expansion of Persistent Volumes (PVs). This expansion can be done either online (for example, when a PV is attached to a node) or offline (for example, when a PV is not attached to any node).\nTo use this feature, the storage class that is used to create the PV must have the attribute allowVolumeExpansion set to true. The storage classes created during the installation (both using Helm or dell-csi-operator) have the allowVolumeExpansion set to true by default.\nIf you are creating more storage classes, ensure that this attribute is set to true to expand any PVs created using these new storage classes.\nThe following is a sample manifest for a storage class which allows for Volume Expansion:\napiVersion:storage.k8s.io/v1kind:StorageClassmetadata:name:powerstore-expand-scannotations:storageclass.beta.kubernetes.io/is-default-class:falseprovisioner:csi-powerstore.dellemc.comreclaimPolicy:DeleteallowVolumeExpansion:true# Set this attribute to true if you plan to expand any PVCs created using this storage classparameters:FsType:xfsTo resize a PVC, edit the existing PVC spec and set spec.resources.requests.storage to the intended size. For example, if you have a PVC pstore-pvc-demo of size 3Gi, then you can resize it to 30Gi by updating the PVC.\napiVersion:v1kind:PersistentVolumeClaimmetadata:name:pstore-pvc-demonamespace:testspec:accessModes:- ReadWriteOncevolumeMode:Filesystemresources:requests:storage:30Gi# Updated size from 3Gi to 30GistorageClassName:powerstore-expand-sc The Kubernetes Volume Expansion feature can only be used to increase the size of a volume. It cannot be used to shrink a volume.\n Raw block support CSI PowerStore driver supports managing Raw Block volumes since version 1.1\nRaw Block volumes are created using the volumeDevices list in the pod template spec with each entry accessing a volumeClaimTemplate specifying a volumeMode: Block. An example configuration is outlined here:\napiVersion:apps/v1kind:StatefulSetmetadata:name:powerstoretestnamespace:{{.Values.namespace}}spec:...spec:...containers:- name:test...volumeDevices:- devicePath:\u0026#34;/dev/data0\u0026#34;name:pvolvolumeClaimTemplates:- metadata:name:pvolspec:accessModes:- ReadWriteOncevolumeMode:BlockstorageClassName:powerstoreresources:requests:storage:8GiAllowable access modes are ReadWriteOnce, ReadWriteMany, and for block devices that have been previously initialized, ReadOnlyMany.\nRaw Block volumes are presented as a block device to the pod by using a bind mount to a block device in the node\u0026rsquo;s file system. The driver does not format or check the format of any file system on the block device. Raw Block volumes do support online Volume Expansion, but it is up to the application to manage reconfiguring the file system (if any) to the new size.\nFor additional information, see the kubernetes website.\nVolume Cloning Feature The CSI PowerStore driver version 1.1 and later supports volume cloning. This allows specifying existing PVCs in the dataSource field to indicate a user would like to clone a Volume.\nSource and destination PVC must be in the same namespace and have the same Storage Class.\nTo clone a volume, you should first have an existing pvc, for example, pvol0:\nkind:PersistentVolumeClaimapiVersion:v1metadata:name:pvol0namespace:testpowerstorespec:storageClassName:powerstoreaccessModes:- ReadWriteOncevolumeMode:Filesystemresources:requests:storage:8GiThe following is a sample manifest for cloning pvol0:\napiVersion:v1kind:PersistentVolumeClaimmetadata:name:clonedpvcnamespace:testpowerstorespec:storageClassName:powerstoredataSource:name:pvol0kind:PersistentVolumeClaimaccessModes:- ReadWriteOnceresources:requests:storage:8GiEphemeral Inline Volume The CSI PowerStore driver version 1.2 supports ephemeral inline CSI volumes. This feature allows CSI volumes to be specified directly in the pod specification.\nAt runtime, nested inline volumes follow the ephemeral lifecycle of their associated pods where the driver handles all phases of volume operations as pods are created and destroyed.\nThe following is a sample manifest for creating ephemeral volume in pod manifest with CSI PowerStore driver.\nkind:PodapiVersion:v1metadata:name:powerstore-inline-volumespec:containers:- name:test-containerimage:busyboxcommand:[\u0026#34;sleep\u0026#34;,\u0026#34;3600\u0026#34;]volumeMounts:- mountPath:\u0026#34;/data\u0026#34;name:volumevolumes:- name:volumecsi:driver:csi-powerstore.dellemc.comfsType:\u0026#34;ext4\u0026#34;volumeAttributes:size:\u0026#34;20Gi\u0026#34;This manifest will create a pod and attach newly created ephemeral inline csi volume to it.\nTo create NFS volume you need to provide nasName: parameters that points to the name of your NAS Server in pod manifest like so\nvolumes:- name:volumecsi:driver:csi-powerstore.dellemc.comfsType:\u0026#34;nfs\u0026#34;volumeAttributes:size:\u0026#34;20Gi\u0026#34;nasName:\u0026#34;csi-nas-name\u0026#34;Controller HA The CSI PowerStore driver version 1.2 introduces controller HA feature. Instead of StatefulSet controller pods deployed as a Deployment.\nBy default number of replicas set to 2, you can set controller.replicas parameter to 1 in my-powerstore-settings.yaml if you want to disable controller HA for your installation. When installing via Operator you can change replicas parameter in spec.driver section in your PowerStore Custom Resource.\nWhen multiple replicas of controller pods are in cluster each sidecar (attacher, provisioner, resizer, snapshotter) tries to get a lease so only one instance of each sidecar would be active in the cluster at a time.\nDriver pod placement You can configure where driver controller and worker pods should be placed. To configure use nodeSelector and tolerations mechanisms you can configure in your my-powerstore-settings.yaml\nFor example, you can specify tolerations to assign driver controller pods on controller nodes too:\n# \u0026#34;controller\u0026#34; allows to configure controller specific parameterscontroller:# \u0026#34;controller.nodeSelector\u0026#34; defines what nodes would be selected for pods of controller deploymentnodeSelector:# \u0026#34;controller.tolerations\u0026#34; defines tolerations that would be applied to controller deploymenttolerations:- key:\u0026#34;node-role.kubernetes.io/master\u0026#34;operator:\u0026#34;Exists\u0026#34;effect:\u0026#34;NoSchedule\u0026#34;If you want to assign controller pods ONLY on controller nodes you need to configure nodeSelector:\n# \u0026#34;controller\u0026#34; allows to configure controller specific parameterscontroller:# \u0026#34;controller.nodeSelector\u0026#34; defines what nodes would be selected for pods of controller deploymentnodeSelector:node-role.kubernetes.io/master:\u0026#34;\u0026#34;# \u0026#34;controller.tolerations\u0026#34; defines tolerations that would be applied to controller deploymenttolerations:- key:\u0026#34;node-role.kubernetes.io/master\u0026#34;operator:\u0026#34;Exists\u0026#34;effect:\u0026#34;NoSchedule\u0026#34;As mentioned earlier, you can configure where node driver pods would be assinged in the similar way in node section of my-powerstore-settings.yaml\nTopology The CSI PowerStore driver version 1.2 and later supports Topology which forces volumes to be placed on worker nodes that have connectivity to the backend storage. This covers use cases where users have chosen to restrict the nodes on which the CSI driver is deployed.\nThis Topology support does not include customer defined topology, users cannot create their own labels for nodes, they should use whatever labels are returned by driver and applied automatically by Kubernetes on its nodes.\nTopology Usage To use the Topology feaure user needs to create their own storage classes similar to those that can be found in helm/samples/storageclass folder.\nThe following is one of example storage class manifest:\napiVersion:storage.k8s.io/v1kind:StorageClassmetadata:name:powerstore-fcprovisioner:csi-powerstore.dellemc.comreclaimPolicy:DeleteallowVolumeExpansion:truevolumeBindingMode:WaitForFirstConsumerallowedTopologies:- matchLabelExpressions:- key:csi-powerstore.dellemc.com/127.0.0.1-fcvalues:- \u0026#34;true\u0026#34;This example will match all nodes where driver has a connection to PowerStore with IP of 127.0.0.1 via FibreChannel. Similar examples can be found in mentioned folder for NFS and iSCSI.\nYou can check what labels your nodes contain by running kubectl get nodes --show-labels\n Notice that volumeBindingMode: is set to WaitForFirstConsumer this is required for topology feature to work.\n For any additional information about topology, see the Kubernetes Topology documentation.\nReuse PowerStore hostname The CSI PowerStore driver version 1.2 and later can automatically detect if the current node was already registered as Host on storage array before. It will check if Host initiators and node initiators (FC or iSCSI) match. If they do, the driver will not create a new storage class and will take the existing name of the Host as nodeID.\n","excerpt":"Creating volumes and consuming them Create a file simple.yaml using sample yaml files located at …","ref":"/ansible-docs/v2/features/powerstore/","title":"PowerStore"},{"body":"The CSI Driver for Dell EMC PowerStore can be deployed by using the provided Helm v3 charts and installation scripts on both Kubernetes and OpenShift platforms. For more detailed information on the installation scripts, please review the script documentation.\nThe controller section of the Helm chart installs the following components in a Deployment in the namespace csi-powerstore:\n CSI Driver for Dell EMC PowerStore Kubernetes External Provisioner, which provisions the volumes Kubernetes External Attacher, which attaches the volumes to the containers Kubernetes External Snapshotter, which provides snapshot support Kubernetes External Resizer, which resizes the volume  The node section of the Helm chart installs the following component in a DaemonSet in the namespace csi-powerstore:\n CSI Driver for Dell EMC PowerStore Kubernetes Node Registrar, which handles the driver registration  Prerequisites The following are requirements to be met before installing the CSI Driver for Dell EMC PowerStore:\n Install Kubernetes (1.17, 1.18, 1.19) or OpenShift (4.5 or 4.6) Install Helm 3 If you plan to use either the Fibre Channel or iSCSI protocol, refer to either Fibre Channel requirements or Set up the iSCSI Initiator sections below. You can use NFS volumes without FC or iSCSI configuration.   You can use either the Fibre Channel or iSCSI protocol, but you do not need both.\n  If you want to use preconfigured iSCSI/FC hosts be sure to check that they are not part of any host group\n  Linux native multipathing requirements Configure Mount propagation on container runtime (i.e. Docker) Volume Snapshot requirements The nonsecure registries are defined in Docker or other container runtime, for CSI drivers that are hosted in a nonsecure location. You can access your cluster with kubectl and helm.  Install Helm 3.0 Install Helm 3.0 on the master node before you install the CSI Driver for Dell EMC PowerFlex.\nSteps\nRun the curl https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3 | bash command to install Helm 3.0.\nFibre Channel requirements Dell EMC PowerStore supports Fibre Channel communication. If you will use the Fibre Channel protocol, ensure that the following requirement is met before you install the CSI Driver for Dell EMC PowerStore:\n Zoning of the Host Bus Adapters (HBAs) to the Fibre Channel port director must be done.  Set up the iSCSI Initiator The CSI Driver for Dell EMC PowerStore v1.2 supports iSCSI connectivity.\nIf you will use the iSCSI protocol, set up the iSCSI initiators as follows:\n Make sure that the iSCSI initiators are available on both Controller and Worker nodes. Kubernetes nodes should have access (network connectivity) to an iSCSI director on the Dell EMC PowerStore array that has IP interfaces. Manually create IP routes for each node that connects to the Dell EMC PowerStore. All Kubernetes nodes must have the iscsi-initiator-utils package for CentOS/RHEL or open-iscsi package for Ubuntu installed, and the iscsid service must be enabled and running. To do this, run the systemctl enable --now iscsid command. Make sure that the unique initiator name is set in /etc/iscsi/initiatorname.iscsi.  For information about configuring iSCSI, see Dell EMC PowerStore documentation on Dell EMC Support.\nLinux multipathing requirements Dell EMC PowerStore supports Linux multipathing. Configure Linux multipathing before installing the CSI Driver for Dell EMC PowerStore.\nSet up Linux multipathing as follows:\n Ensure that all nodes have the Device Mapper Multipathing package installed.   You can install it by running yum install device-mapper-multipath on CentOS or apt install multipath-tools on Ubuntu. This package should create a multipath configuration file located in /etc/multipath.conf.\n  Enable multipathing using the mpathconf --enable --with_multipathd y command. Enable user_friendly_names and find_multipaths in the multipath.conf file. Ensure that the multipath command for multipath.conf is available on all Kubernetes nodes.  Configure Mount Propagation on Container Runtime It is required to configure mount propagation on your container runtime on all Kubernetes nodes before installing the CSI Driver for Dell EMC PowerStore. The following is instruction on how to do this with Docker. If you use another container runtime please follow the recommended instructions from the vendor to configure mount propagation.\nSteps\n Edit the service section of /etc/systemd/system/multi-user.target.wants/docker.service file to add the following lines: docker.service [Service]... MountFlags=shared  Restart the docker service with systemctl daemon-reload and systemctl restart docker on all the nodes.  Volume Snapshot requirements Volume Snapshot CRD\u0026rsquo;s The Kubernetes Volume Snapshot CRDs can be obtained and installed from the external-snapshotter project on Github.\nAlternately, you can install the CRDs by supplying the option --snapshot-crd while installing the driver using the csi-install.sh script.\nVolume Snapshot Controller Starting with beta Volume Snapshots in Kubernetes 1.17, the CSI external-snapshotter sidecar is split into two controllers:\n A common snapshot controller A CSI external-snapshotter sidecar  The common snapshot controller must be installed only once in the cluster irrespective the number of CSI drivers installed in the cluster. On OpenShift clusters 4.4 onwards, the common snapshot-controller is pre-installed. In the clusters where it is not present, it can be installed using kubectl and the manifests available on GitHub.\nNOTE:\n The manifests available on GitHub install v3.0.3 of the snapshotter image - quay.io/k8scsi/csi-snapshotter:v3.0.3 Dell EMC recommends using v3.0.3 image of the snapshot-controller - quay.io/k8scsi/snapshot-controller:v3.0.3 The CSI external-snapshotter sidecar is still installed along with the driver and does not involve any extra configuration.  Install the Driver Steps\n Run git clone https://github.com/dell/csi-powerstore.git to clone the git repository Ensure that you\u0026rsquo;ve created namespace where you want to install the driver. You can run kubectl create namespace csi-powerstore to create a new one Check helm/csi-powerstore/driver-image.yaml and confirm the driver image points to new image. Edit the helm/secret.yaml, point to correct namespace and replace the values for the username and password parameters. These values can be obtained using base64 encoding as described in the following example: echo -n \u0026#34;myusername\u0026#34; | base64 echo -n \u0026#34;mypassword\u0026#34; | base64 where myusername \u0026amp; mypassword are credentials that would be used for accessing PowerStore API. NOTE: If you want to use iSCSI CHAP you need fill chapsecret and chapuser fields in similar manner\n Create the secret by running kubectl create -f helm/secret.yaml Copy the default values.yaml file cd dell-csi-helm-installer \u0026amp;\u0026amp; cp ../helm/csi-powerstore/values.yaml ./my-powerstore-settings.yaml Edit the newly created values file and provide values for the following parameters vi my-powerstore-settings.yaml:     Parameter Description Required Default     powerStoreApi Defines the full URL path to the PowerStore API Yes \u0026quot; \u0026quot;   volumeNamePrefix Defines the string added to each volume that the CSI driver creates No \u0026ldquo;csi\u0026rdquo;   nodeNamePrefix Defines the string added to each node that the CSI driver registers No \u0026ldquo;csi-node\u0026rdquo;   nodeIDPath Defines a path to file with a unique identifier identifying the node in the Kubernetes cluster No \u0026ldquo;/etc/machine-id\u0026rdquo;   connection.scsiProtocol Defines which transport protocol to use (FC, ISCSI, None, or auto). - By default, the driver scans available SCSI adapters and tries to register them with the storage array under the SCSI hostname using nodeNamePrefix and the ID read from the file pointed to by nodeIDPath. If an adapter is already registered with the storage under a different hostname, the adapter is not used by the driver. - A hostname the driver uses for registration of adapters is in the form \u0026lt;nodeNamePrefix\u0026gt;-\u0026lt;nodeID\u0026gt;-\u0026lt;nodeIP\u0026gt;. By default, these are csi-node and the machine ID read from the file /etc/machine-id. - To customize the hostname, for example if you want to make them more user friendly, adjust nodeIDPath and nodeNamePrefix accordingly. - For example, you can set nodeNamePrefix to k8s and nodeIDPath to /etc/hostname to produce names such as k8s-worker1-192.168.1.2. Yes \u0026ldquo;auto\u0026rdquo;   connection.nfs.enable Enables or disables NFS support No FALSE   connection.nfs.nasServerName Points to the NAS server that would be used - If you have nfs.enabled set to true, it will try to use nfs.nasServerName. This will fail if you do not provide nfs.nasServerName. No \u0026ldquo;nas-server\u0026rdquo;   connection.nfs.version Defines version of NFS protocol No \u0026ldquo;v3\u0026rdquo;   connection.enableCHAP Defines whether the driver should use CHAP for iSCSI connections or not No FALSE   controller.nodeSelector Defines what nodes would be selected for pods of controller deployment Yes \u0026quot; \u0026quot;   controller.tolerations Defines tolerations that would be applied to controller deployment Yes \u0026quot; \u0026quot;   controller.replicas Defines number of replicas of controller deployment Yes 2   node.nodeSelector Defines what nodes would be selected for pods of node daemonset Yes \u0026quot; \u0026quot;   node.tolerations Defines tolerations that would be applied to node daemonset Yes \u0026quot; \u0026quot;    Install the driver using csi-install.sh bash script by running ./csi-install.sh --namespace csi-powerstore --values ./my-powerstore-settings.yaml  After that the driver should be installed, you can check condition of driver pods by running kubectl get all -n csi-powerstore    NOTE:\n For detailed instructions on how to run the install scripts, refer to the readme document in the dell-csi-helm-installer folder. (Optional) Enable additional Mount Options - A user is able to specify additional mount options as needed for the driver.  Mount options are specified in storageclass yaml under mountOptions. WARNING: Before utilizing mount options, you must first be fully aware of the potential impact and understand your environment\u0026rsquo;s requirements for the specified option.    Storage Classes As part of the driver installation, a set of storage classes is created along with the driver pods. This is done to demonstrate how storage classes need to be created to consume storage from Dell EMC storage arrays.\nThe StorageClass object in Kubernetes is immutable and can\u0026rsquo;t be modified once created. It creates challenges when we need to change or update a parameter, for example when a version of the driver introduces new configurable parameters for the storage classes. To avoid issues during upgrades, future releases of the drivers will have the installation separated from the creation of Storage Classes. In preparation for that, starting in Q4 of 2020, an annotation \u0026quot;helm.sh/resource-policy\u0026quot;: keep is applied to the storage classes created by the dell-csi-helm-installer.\nBecause of this annotation, these storage classes are not going to be deleted even after the driver has been uninstalled. This annotation has been applied to give you an opportunity to keep using these storage classes even with a future release of the driver. In case you wish to not use these storage classes, you will need to delete them by using the kubectl delete storageclass command.\nNOTE: If you uninstall the driver and reinstall it, you can still face errors if any update in the values.yaml file leads to an update of the storage class(es):\n Error: cannot patch \u0026quot;\u0026lt;sc-name\u0026gt;\u0026quot; with kind StorageClass: StorageClass.storage.k8s.io \u0026quot;\u0026lt;sc-name\u0026gt;\u0026quot; is invalid: parameters: Forbidden: updates to parameters are forbidden In case you want to make such updates, make sure to delete the existing storage classes using the kubectl delete storageclass command.\nDeleting a storage class has no impact on a running Pod with mounted PVCs. You won\u0026rsquo;t be able to provision new PVCs until at least one storage class is newly created.\n","excerpt":"The CSI Driver for Dell EMC PowerStore can be deployed by using the provided Helm v3 charts and …","ref":"/ansible-docs/v2/installation/helm/powerstore/","title":"PowerStore"},{"body":"Installing PowerStore CSI Driver via Operator The CSI Driver for Dell EMC PowerStore can be installed via the Dell CSI Operator.\nTo deploy the Operator, follow the instructions available here.\nThere are sample manifests provided which can be edited to do an easy installation of the driver. Please note that the deployment of the driver using the operator doesn’t use any Helm charts and the installation \u0026amp; configuration parameters will be slightly different from the ones specified via the Helm installer.\nKubernetes Operators make it easy to deploy and manage entire lifecycle of complex Kubernetes applications. Operators use Custom Resource Definitions (CRD) which represents the application and use custom controllers to manage them.\nInstall Driver  Create namespace: Run kubectl create namespace \u0026lt;driver-namespace\u0026gt; using the desired name to create the namespace. Create PowerStore credentials: Create a file called powerstore-creds.yaml with the following content apiVersion:v1kind:Secretmetadata:name:powerstore-creds# Replace driver-namespace with the namespace where driver is being deployednamespace:\u0026lt;driver-namespace\u0026gt; type: Opaquedata:# set username to the base64 encoded usernameusername:\u0026lt;base64username\u0026gt; # set password to the base64 encoded passwordpassword:\u0026lt;base64password\u0026gt;Replace the values for the username and password parameters. These values can be optioned using base64 encoding as described in the following example:\necho -n \u0026quot;myusername\u0026quot; | base64 echo -n \u0026quot;mypassword\u0026quot; | base64 Run kubectl create -f powerstore-creds.yaml command to create the secret.\n Create a Custom Resource (CR) for PowerStore using the sample files provided here. Users should configure the parameters in CR. The following table lists the primary configurable parameters of the PowerStore driver and their default values:    Parameter Description Required Default     replicas Controls the amount of controller pods you deploy. If controller pods is greater than number of available nodes, excess pods will become stuck in pending. Defaults is 2 which allows for Controller high availability. Yes 2   Common parameters for node and controller      X_CSI_POWERSTORE_ENDPOINT Must provide a PowerStore HTTPS API url Yes https://127.0.0.1/api/rest   X_CSI_TRANSPORT_PROTOCOL Choose what transport protocol to use (ISCSI, FC, auto or None) Yes auto   X_CSI_POWERSTORE_NODE_NAME_PREFIX Prefix to add to each node registered by the CSI driver Yes \u0026ldquo;csi-node\u0026rdquo;   X_CSI_FC_PORTS_FILTER_FILE_PATH To set path to the file which provide list of WWPN which should be used by the driver for FC connection on this node No \u0026ldquo;/etc/fc-ports-filter\u0026rdquo;   StorageClass parameters      allowedTopologies:key This is to enable topology to allow pods/and volumes to always be scheduled on nodes that have access to the storage. You need to replace the \u0026ldquo;127.0.0.1-nfs\u0026rdquo; portion in the key with PowerStore endpoint IP with its value and append -nfs, -fc or -iscsi at the end of it No \u0026ldquo;127.0.0.1-nfs\u0026rdquo;     Execute the following command to create PowerStore custom resource:kubectl create -f \u0026lt;input_sample_file.yaml\u0026gt;. The above command will deploy the CSI-PowerStore driver.  ","excerpt":"Installing PowerStore CSI Driver via Operator The CSI Driver for Dell EMC PowerStore can be …","ref":"/ansible-docs/v2/installation/operator/powerstore/","title":"PowerStore"},{"body":"In the repository, a simple test manifest exists that creates three different PersistentVolumeClaims using default ext4, xfs and nfs storage classes, and automatically mounts them to the pod. Note that nfs storage class is optional and will not be created if you haven\u0026rsquo;t turned it on in myvalues.yaml.\nSteps\n To run this test, run the kubectl command from the root directory of the repository: kubectl create -f ./tests/simple/   You can find all the created resources in testpowerstore namespace.\n Check if the pod is created and Ready and Running by running:\nkubectl get all -n testpowerstore If it\u0026rsquo;s in CrashLoopback state then the driver installation wasn\u0026rsquo;t successful. Check the logs of the node and the controller.\n  Go into the created container and verify that everything is mounted correctly.\n  After verifying, you can uninstall the testing PVCs and StatefulSet.\nkubectl delete -f ./tests/simple/   ","excerpt":"In the repository, a simple test manifest exists that creates three different PersistentVolumeClaims …","ref":"/ansible-docs/v2/installation/test/powerstore/","title":"Test PowerStore CSI Driver"},{"body":"Release Notes - CSI PowerStore v1.2.0 New Features/Changes  Added support for OpenShift 4.5/4.6 with RHEL and CoreOS worker nodes Added support for Red Hat Enterprise Linux (RHEL) 7.9 Added support for Ubuntu 20.04 Added support for Docker EE 3.1 Added support for Controller high availability (multiple-controllers) Added support for Topology Added support for ephemeral volumes Added support for mount options Changed driver base image to UBI 8.x  Fixed Issues There are no fixed issues in this release.\nKnown Issues    Issue Workaround     Slow volume attached/detach If your Kubernetes 1.17 or 1.18 cluster has a lot of VolumeAttachment objects, the attach/detach operations will be very slow. This is a known issue and affects all CSI plugins. It is tracked here: CSI VolumeAttachment slows pod startup time. To get around this problem you can upgrade to latest Kubernetes/OpenShift patches, which contains a partial fix: 1.17.8+, 1.18.5+    ","excerpt":"Release Notes - CSI PowerStore v1.2.0 New Features/Changes  Added support for OpenShift 4.5/4.6 with …","ref":"/ansible-docs/v2/release/powerstore/","title":"PowerStore"},{"body":"   Symptoms Prevention, Resolution or Workaround     When you run the command kubectl describe pods powerstore-controller-\u0026lt;suffix\u0026gt; –n csi-powerstore, the system indicates that the driver image could not be loaded. - If on Kubernetes, edit the daemon.json file found in the registry location and add { \u0026quot;insecure-registries\u0026quot; :[ \u0026quot;hostname.cloudapp.net:5000\u0026quot; ] } - If on OpenShift, run the command oc edit image.config.openshift.io/cluster and add registries to yaml file that is displayed when you run the command.   The kubectl logs -n csi-powerstore powerstore-node-\u0026lt;suffix\u0026gt; driver logs shows that the driver can\u0026rsquo;t connect to PowerStore API. Check if you\u0026rsquo;ve created secret with correct credentials    ","excerpt":"Symptoms Prevention, Resolution or Workaround     When you run the command kubectl describe pods …","ref":"/ansible-docs/v2/troubleshooting/powerstore/","title":"PowerStore"},{"body":"Getting started with Kubernetes on Dell EMC Storage   Dell EMC CSI Operator deployment in OpenShift   ","excerpt":"Getting started with Kubernetes on Dell EMC Storage   Dell EMC CSI Operator deployment in OpenShift …","ref":"/ansible-docs/v1/grasp/video/","title":"Quick video lessons"},{"body":"Getting started with Kubernetes on Dell EMC Storage   Dell EMC CSI Operator deployment in OpenShift   ","excerpt":"Getting started with Kubernetes on Dell EMC Storage   Dell EMC CSI Operator deployment in OpenShift …","ref":"/ansible-docs/v2/grasp/video/","title":"Quick video lessons"},{"body":"","excerpt":"","ref":"/ansible-docs/search/","title":"Search Results"},{"body":"","excerpt":"","ref":"/ansible-docs/v1/installation/test/","title":"Testing Drivers"},{"body":"","excerpt":"","ref":"/ansible-docs/v2/installation/test/","title":"Testing Drivers"},{"body":"Creating volumes and consuming them Create a file simple.yaml using sample yaml files located at tests/sample.yaml\nThe following command creates a statefulset that consumes three volumes of default storage classes:\nkubectl create -f tests/sample.yaml After executing this command 3 PVC and statefulset are created in the test-unity namespace. You can check created PVCs by running kubectl get pvc -n test-unity and check statefulset\u0026rsquo;s pods by running kubectl get pods -n test-unitycommand. Pod should be Ready and Running.\n If Pod is in CrashLoopback or PVCs is in Pending state then driver installation is not successful, check logs of node and controller.\n Deleting volumes To delete volumes, pod and statefulset run the command\nkubectl delete -f tests/sample.yaml Consuming existing volumes with static provisioning You can use existent volumes from Unity array as Persistent Volumes in your Kubernetes, to do that you must perform the following steps:\n Open your volume in Unity Management UI (unisphere), and take a note of volume-id. The volume-id looks like csiunity-xxxxx and CLI ID looks like sv_xxxx. Create PersistentVolume and use this volume-id as a volumeHandle in the manifest. Modify other parameters according to your needs.  apiVersion:v1kind:PersistentVolumemetadata:name:static-1annotations:pv.kubernetes.io/provisioned-by:csi-unity.dellemc.comspec:accessModes:- ReadWriteOncecapacity:storage:5Gicsi:driver:csi-unity.dellemc.comvolumeHandle:existingvol-\u0026lt;protocol\u0026gt;-\u0026lt;array_id\u0026gt;-\u0026lt;volume-id\u0026gt; persistentVolumeReclaimPolicy: RetainclaimRef:namespace:defaultname:static-pvc1storageClassName:unityvolumeMode:FilesystemCreate PersistentVolumeClaim to use this PersistentVolume.  kind:PersistentVolumeClaimapiVersion:v1metadata:name:static-pvc1spec:accessModes:- ReadWriteManyresources:requests:storage:5GivolumeName:static-1storageClassName:unityThen use this PVC as a volume in a pod.  apiVersion:v1kind:Podmetadata:name:static-prov-podspec:containers:- name:testimage:docker.io/centos:latestcommand:[\u0026#34;/bin/sleep\u0026#34;,\u0026#34;3600\u0026#34;]volumeMounts:- mountPath:\u0026#34;/data0\u0026#34;name:pvolvolumes:- name:pvolpersistentVolumeClaim:claimName:static-pvc1After the pod becomes Ready and Running, you can start to use this pod and volume.  Volume Snapshot Feature The Volume Snapshot feature was introduced in alpha (v1alpha1) in Kubernetes 1.13 and then moved to beta (v1beta1) in Kubernetes version 1.17 and generally available (v1) in Kubernetes version 1.20.\nThe CSI Unity driver version 1.5 supports v1beta1 snapshots on Kubernetes 1.18/1.19 and v1 snapshots on Kubernetes 1.20.\nIn order to use Volume Snapshots, ensure the following components have been deployed to your cluster:\n Kubernetes Volume Snaphshot CRDs Volume Snapshot Controller  Volume Snapshot Class During the installation of CSI Unity 1.5 driver, a Volume Snapshot Class is created. This is the only Volume Snapshot Class required and there is no need to create any other Volume Snapshot Class.\nFollowing is the manifest for a Volume Snapshot Class created during installation:\napiVersion:snapshot.storage.k8s.io/v1#For beta snapshots the apiVersion will be snapshot.storage.k8s.io/v1beta1kind:VolumeSnapshotClassmetadata:name:unity-snapclassdriver:csi-unity.dellemc.comdeletionPolicy:DeleteCreate Volume Snapshot The following is a sample manifest for creating a Volume Snapshot using the v1 snapshot APIs:\napiVersion:snapshot.storage.k8s.io/v1#For beta snapshots the apiVersion will be snapshot.storage.k8s.io/v1beta1kind:VolumeSnapshotmetadata:name:pvol0-snapnamespace:test-unityspec:volumeSnapshotClassName:unity-snapclasssource:persistentVolumeClaimName:pvolOnce the VolumeSnapshot is successfully created by the CSI Unity driver, a VolumeSnapshotContent object is automatically created. Once the status of the VolumeSnapshot object has the readyToUse field set to true , it is available for use.\nFollowing is the relevant section of VolumeSnapshot object status:\nstatus:boundVolumeSnapshotContentName:snapcontent-xxxxxxxxxxxxxcreationTime:\u0026#34;2020-07-16T08:42:12Z\u0026#34;readyToUse:trueCreating PVCs with Volume Snapshots as Source The following is a sample manifest for creating a PVC with a VolumeSnapshot as a source:\napiVersion:v1kind:PersistentVolumeClaimmetadata:name:restorepvcnamespace:test-unityspec:storageClassName:unity-iscsidataSource:name:pvol0-snapkind:VolumeSnapshotapiGroup:snapshot.storage.k8s.ioaccessModes:- ReadWriteOnceresources:requests:storage:8GiVolume Expansion The CSI Unity driver version 1.3 and later supports the expansion of Persistent Volumes (PVs). This expansion can be done either online (for example, when a PVC is attached to a node) or offline (for example, when a PVC is not attached to any node).\nTo use this feature, the storage class that is used to create the PVC must have the attribute allowVolumeExpansion set to true. The storage classes created during the installation (both using Helm or dell-csi-operator) have the allowVolumeExpansion set to true by default.\nIf you are creating more storage classes, ensure that this attribute is set to true to expand any PVs created using these new storage classes.\nThe following is a sample manifest for a storage class which allows for Volume Expansion:\napiVersion:storage.k8s.io/v1kind:StorageClassmetadata:name:unity-expand-scannotations:storageclass.beta.kubernetes.io/is-default-class:falseprovisioner:csi-unity.dellemc.comreclaimPolicy:DeleteallowVolumeExpansion:true# Set this attribute to true if you plan to expand any PVCs created using this storage classparameters:FsType:xfsTo resize a PVC, edit the existing PVC spec and set spec.resources.requests.storage to the intended size. For example, if you have a PVC unity-pvc-demo of size 3Gi, then you can resize it to 30Gi by updating the PVC.\napiVersion:v1kind:PersistentVolumeClaimmetadata:name:unity-pvc-demonamespace:testspec:accessModes:- ReadWriteOncevolumeMode:Filesystemresources:requests:storage:30Gi# Updated size from 3Gi to 30GistorageClassName:unity-expand-sc The Kubernetes Volume Expansion feature can only be used to increase the size of a volume. It cannot be used to shrink a volume.\n Raw block support The CSI Unity driver supports Raw Block Volumes from v1.4 onwards. Raw Block volumes are created using the volumeDevices list in the pod template spec with each entry accessing a volumeClaimTemplate specifying a volumeMode: Block. An example configuration is outlined here:\napiVersion:v1kind:PersistentVolumeClaimmetadata:name:rawblockpvcnamespace:defaultspec:accessModes:- ReadWriteOncevolumeMode:Blockresources:requests:storage:5GistorageClassName:unity-iscsiapiVersion:v1kind:Podmetadata:name:rawblockpodnamespace:defaultspec:containers:- name:task-pv-containerimage:nginxports:- containerPort:80name:\u0026#34;http-server\u0026#34;volumeDevices:- devicePath:/usr/share/nginx/html/devicename:nov-eleventh-1-pv-storagevolumes:- name:nov-eleventh-1-pv-storagepersistentVolumeClaim:claimName:rawblockpvcAccess modes allowed are ReadWriteOnce and ReadWriteMany. Raw Block volumes are presented as a block device to the pod by using a bind mount to a block device in the node\u0026rsquo;s file system. The driver does not format or check the format of any file system on the block device. Raw Block volumes do support online Volume Expansion, but it is up to the application to manage reconfiguring the file system (if any) to the new size. Access mode ReadOnlyMany is not supported with raw block since we cannot restrict volumes to be readonly from Unity.\nFor additional information, see the kubernetes website.\nVolume Cloning Feature The CSI Unity driver version 1.3 supports volume cloning. This allows specifying existing PVCs in the dataSource field to indicate a user would like to clone a Volume.\nSource and destination PVC must be in the same namespace and have the same Storage Class.\nTo clone a volume, you should first have an existing PVC, example: vol0:\nkind:PersistentVolumeClaimapiVersion:v1metadata:name:vol0namespace:test-unityspec:storageClassName:unity-nfsaccessModes:- ReadWriteOncevolumeMode:Filesystemresources:requests:storage:8GiThe following is a sample manifest for cloning pvol0:\napiVersion:v1kind:PersistentVolumeClaimmetadata:name:cloned-pvcnamespace:test-unityspec:storageClassName:unity-nfsdataSource:name:vol0kind:PersistentVolumeClaimaccessModes:- ReadWriteOnceresources:requests:storage:8GiEphemeral Inline Volume The CSI Unity driver version 1.4 supports ephemeral inline CSI volumes. This feature allows CSI volumes to be specified directly in the pod specification.\nAt runtime, nested inline volumes follow the ephemeral lifecycle of their associated pods where the driver handles all phases of volume operations as pods are created and destroyed.\nThe following is a sample manifest for creating ephemeral volume in pod manifest with CSI Unity driver.\nkind:PodapiVersion:v1metadata:name:test-unity-ephemeral-volumespec:containers:- name:test-containerimage:busyboxcommand:[\u0026#34;sleep\u0026#34;,\u0026#34;3600\u0026#34;]volumeMounts:- mountPath:\u0026#34;/data\u0026#34;name:volumevolumes:- name:volumecsi:driver:csi-unity.dellemc.comfsType:\u0026#34;ext4\u0026#34;volumeAttributes:size:\u0026#34;10Gi\u0026#34;This manifest creates a pod and attach newly created ephemeral inline CSI volume to it.\nTo create NFS volume you need to provide nasName: parameters that points to the name of your NAS Server in pod manifest like so\nvolumes:- name:volumecsi:driver:csi-unity.dellemc.comfsType:\u0026#34;nfs\u0026#34;volumeAttributes:size:\u0026#34;20Gi\u0026#34;nasName:\u0026#34;csi-nas-name\u0026#34;Controller HA The CSI Unity driver version 1.4 introduces controller HA feature. Instead of StatefulSet controller pods deployed as a Deployment.\nBy default number of replicas set to 2, you can set controllerCount parameter to 1 in myvalues.yaml if you want to disable controller HA for your installation. When installing via Operator you can change replicas parameter in spec.driver section in your Unity Custom Resource.\nWhen multiple replicas of controller pods are in cluster each sidecar (Attacher, Provisioner, Resizer and Snapshotter) tries to get a lease so only one instance of each sidecar is active in the cluster at a time.\nDriver pod placement You can configure where driver controller and worker pods should be placed. To do that you will need to use nodeSelector and tolerations mechanisms you can configure in your myvalues.yaml\nFor example you can specify tolerations to assign driver controller pods on controller nodes too:\n# \u0026#34;controller\u0026#34; allows to configure controller specific parameterscontroller:# \u0026#34;controller.nodeSelector\u0026#34; defines what nodes would be selected for pods of controller deploymentnodeSelector:# \u0026#34;controller.tolerations\u0026#34; defines tolerations that would be applied to controller deploymenttolerations:- key:\u0026#34;node-role.kubernetes.io/master\u0026#34;operator:\u0026#34;Exists\u0026#34;effect:\u0026#34;NoSchedule\u0026#34;If you want to assign controller pods ONLY on controller nodes you need to configure nodeSelector:\n# \u0026#34;controller\u0026#34; allows to configure controller specific parameterscontroller:# \u0026#34;controller.nodeSelector\u0026#34; defines what nodes would be selected for pods of controller deploymentnodeSelector:node-role.kubernetes.io/master:\u0026#34;\u0026#34;# \u0026#34;controller.tolerations\u0026#34; defines tolerations that would be applied to controller deploymenttolerations:- key:\u0026#34;node-role.kubernetes.io/master\u0026#34;operator:\u0026#34;Exists\u0026#34;effect:\u0026#34;NoSchedule\u0026#34;As said before you can configure where node driver pods would be assigned in the similar way in node section of myvalues.yaml\nTopology The CSI Unity driver version 1.4 supports Topology which forces volumes to be placed on worker nodes that have connectivity to the backend storage. This covers use cases where users have chosen to restrict the nodes on which the CSI driver is deployed.\nThis Topology support does not include customer defined topology, users cannot create their own labels for nodes, they should use whatever labels are returned by driver and applied automatically by Kubernetes on its nodes.\nTopology Usage To use the Topology feature, user can install driver by setting createStorageClassesWithTopology to true in the myvalues.yaml which will create default storage classes by adding topology keys (based on the arrays specified in myvalues.yaml) and with WaitForFirstConsumer binding mode.\nAnother option is the user can create custom storage classes on their own by specifying the valid topology keys and binding mode.\nThe following is one of example storage class manifest:\napiVersion:storage.k8s.io/v1kind:StorageClassmetadata:name:unity-topology-fcprovisioner:csi-unity.dellemc.comreclaimPolicy:DeleteallowVolumeExpansion:truevolumeBindingMode:WaitForFirstConsumerallowedTopologies:- matchLabelExpressions:- key:csi-unity.dellemc.com/\u0026lt;array_id\u0026gt;-fcvalues:- \u0026#34;true\u0026#34;This example will match all nodes where driver has a connection to Unity array with array ID mentioned via Fiber Channel. Similarly by replacing fc with iscsi in the key will check for iSCSI connectivity with the node.\nYou can check what labels your nodes contain by running kubectl get nodes --show-labels command.\n Note that volumeBindingMode: is set to WaitForFirstConsumer this is required for topology feature to work properly.\n For any additional information about topology, see the Kubernetes Topology documentation.\nSupport for Docker EE The CSI Driver for Dell EMC Unity supports Docker EE and deployment on clusters bootstrapped with UCP (Universal Control Plane).\n*UCP version 3.3.3 supports Kubernetes 1.18 and CSI driver can be installed on UCP 3.3 with Helm.\nThe installation process for the driver on such clusters remains the same as the installation process on upstream clusters.\nOn UCP based clusters, kubectl may not be installed by default, it is important that kubectl is installed prior to the installation of the driver.\nThe worker nodes in UCP backed clusters may run any of the OSs which we support with upstream clusters.\nSupport for SLES 15 SP2 The CSI Driver for Dell EMC Unity requires the following set of packages installed on all worker nodes that run on SLES 15 SP2.\n open-iscsi open-iscsi is required in order to make use of iSCSI protocol for provisioning nfs-utils nfs-utils is required in order to make use of NFS protocol for provisioning multipath-tools multipath-tools is required in order to make use of FC and iSCSI protocols for provisioning  After installing open-iscsi, ensure \u0026ldquo;iscsi\u0026rdquo; and \u0026ldquo;iscsid\u0026rdquo; services have been started and /etc/isci/initiatorname.iscsi is created and has the host initiator id. The pre-requisites are mandatory for provisioning with iSCSI protocol to work.\n","excerpt":"Creating volumes and consuming them Create a file simple.yaml using sample yaml files located at …","ref":"/ansible-docs/v1/features/unity/","title":"Unity"},{"body":"The CSI Driver for Dell EMC Unity can be deployed by using the provided Helm v3 charts and installation scripts on both Kubernetes and OpenShift platforms. For more detailed information on the installation scripts, review the script documentation.\nThe controller section of the Helm chart installs the following components in a Deployment:\n CSI Driver for Unity Kubernetes External Provisioner, which provisions the volumes Kubernetes External Attacher, which attaches the volumes to the containers Kubernetes External Snapshotter, which provides snapshot support Kubernetes External Resizer, which resizes the volume  The node section of the Helm chart installs the following component in a DaemonSet:\n CSI Driver for Unity Kubernetes Node Registrar, which handles the driver registration  Prerequisites Before you install CSI Driver for Unity, verify the requirements that are mentioned in this topic are installed and configured.\nRequirements  Install Kubernetes or OpenShift (see supported versions) Configure Docker service Install Helm v3 To use FC protocol, host must be zoned with Unity array To use iSCSI protocol, iSCSI initiator utils packages needs to be installed To use NFS protocol, NFS utility packages needs to be installed  Configure Docker service The mount propagation in Docker must be configured on all Kubernetes nodes before installing CSI Driver for Unity.\nProcedure   Edit the service section of /etc/systemd/system/multi-user.target.wants/docker.service file as follows:\n[Service] ... MountFlags=shared   Restart the Docker service with following commands:\nsystemctl daemon-reload systemctl restart docker   Install CSI Driver Install CSI Driver for Unity using this procedure.\nBefore you begin\n You must have the downloaded files, including the Helm chart from the source git repository with command git clone https://github.com/dell/csi-unity.git, ready for this procedure. In the top-level dell-csi-helm-installer directory, there should be two scripts, csi-install.sh and csi-uninstall.sh. Ensure \u0026ldquo;unity\u0026rdquo; namespace exists in kubernetes cluster. Use kubectl create namespace unity command to create the namespace, if the namespace is not present.  Procedure\n  Collect information from the Unity Systems like Unique ArrayId, IP address, username and password. Make a note of the value for these parameters as they must be entered in the secret.json and myvalues.yaml file.\n  Copy the helm/csi-unity/values.yaml into a file named myvalues.yaml in the same directory of csi-install.sh, to customize settings for installation.\n  Edit myvalues.yaml to set the following parameters for your installation:\nThe following table lists the primary configurable parameters of the Unity driver chart and their default values. More detailed information can be found in the values.yaml file in this repository.\n   Parameter Description Required Default     certSecretCount Represents number of certificate secrets, which user is going to create for ssl authentication. (unity-cert-0..unity-cert-n). Minimum value should be 1 false 1   syncNodeInfoInterval Time interval to add node info to array. Default 15 minutes. Minimum value should be 1 minute false 15   controllerCount Controller replication count to maintain high availability yes 2   volumeNamePrefix String to prepend to any volumes created by the driver false csivol   snapNamePrefix String to prepend to any snapshot created by the driver false csi-snap   csiDebug To set the debug log policy for CSI driver false \u0026ldquo;false\u0026rdquo;   imagePullPolicy The default pull policy is IfNotPresent which causes the Kubelet to skip pulling an image if it already exists. false IfNotPresent   createStorageClassesWithTopology Flag to enable or disable topology. true false   allowRWOMultiPodAccess Flag to enable multiple pods use the same pvc on the same node with RWO access mode. false false   Storage Array List Following parameters is a list of parameters to provide multiple storage arrays     storageArrayList[i].name Name of the storage class to be defined. A suffix of ArrayId and protocol will be added to the name. No suffix will be added to default array. false unity   storageArrayList[i].isDefaultArray To handle the existing volumes created in csi-unity v1.0, 1.1 and 1.1.0.1. The user needs to provide \u0026ldquo;isDefaultArray\u0026rdquo;: true in secret.json. This entry should be present only for one array and that array will be marked default for existing volumes. false \u0026ldquo;false\u0026rdquo;   Storage Class parameters Following parameters are not present in values.yaml     storageArrayList[i].storageClass.storagePool Unity Storage Pool CLI ID to use with in the Kubernetes storage class true -   storageArrayList[i].storageClass.thinProvisioned To set volume thinProvisioned false \u0026ldquo;true\u0026rdquo;   storageArrayList[i].storageClass.isDataReductionEnabled To set volume data reduction false \u0026ldquo;false\u0026rdquo;   storageArrayList[i].storageClass.volumeTieringPolicy To set volume tiering policy false 0   storageArrayList[i].storageClass.FsType Block volume related parameter. To set File system type. Possible values are ext3,ext4,xfs. Supported for FC/iSCSI protocol only. false ext4   storageArrayList[i].storageClass.hostIOLimitName Block volume related parameter. To set unity host IO limit. Supported for FC/iSCSI protocol only. false \u0026quot;\u0026rdquo;   storageArrayList[i].storageClass.nasServer NFS related parameter. NAS Server CLI ID for filesystem creation. true \u0026quot;\u0026rdquo;   storageArrayList[i].storageClass.hostIoSize NFS related parameter. To set filesystem host IO Size. false \u0026ldquo;8192\u0026rdquo;   storageArrayList[i].storageClass.reclaimPolicy What should happen when a volume is removed false Delete   Snapshot Class parameters Following parameters are not present in values.yaml     storageArrayList[i] .snapshotClass.retentionDuration TO set snapshot retention duration. Format:\u0026ldquo;1:23:52:50\u0026rdquo; (number of days:hours:minutes:sec) false \u0026quot;\u0026rdquo;    Note:\n  User should provide all boolean values with double quotes. This applicable only for myvalues.yaml. Example: \u0026ldquo;true\u0026rdquo;/\u0026ldquo;false\u0026rdquo;\n  controllerCount parameter value should be \u0026lt;= number of nodes in the kubernetes cluster else install script fails.\n  \u0026lsquo;createStorageClassesWithTopology\u0026rsquo; key is applicable only in the helm based installation but not with the operator based installation. In operator based installation, however user can create custom storage class with topology related key/values.\n  User can create separate storage class (with topology related keys) by referring to existing default storageclasses.\n  Host IO Limit must have a minimum bandwidth of 1 MBPS to discover the volumes on node successfully.\n  User must not change the value of allowRWOMultiPodAccess to true unless intended to use the feature and is aware of the consequences. Enabling multiple pods access the same pvc with RWO access mode on the same node might cause data to be overwritten and therefore leading to data loss in some cases.\n  Example myvalues.yaml\ncsiDebug:\u0026#34;true\u0026#34;volumeNamePrefix :csivolsnapNamePrefix:csi-snapimagePullPolicy:AlwayscertSecretCount:1syncNodeInfoInterval:5controllerCount:2createStorageClassesWithTopology:trueallowRWOMultiPodAccess:falsestorageClassProtocols:- protocol:\u0026#34;FC\u0026#34;- protocol:\u0026#34;iSCSI\u0026#34;- protocol:\u0026#34;NFS\u0026#34;storageArrayList:- name:\u0026#34;APM00******1\u0026#34;isDefaultArray:\u0026#34;true\u0026#34;storageClass:storagePool:pool_1FsType:ext4nasServer:\u0026#34;nas_1\u0026#34;thinProvisioned:\u0026#34;true\u0026#34;isDataReductionEnabled:truehostIOLimitName:\u0026#34;value_from_array\u0026#34;tieringPolicy:\u0026#34;2\u0026#34;snapshotClass:retentionDuration:\u0026#34;2:2:23:45\u0026#34;- name:\u0026#34;APM001******2\u0026#34;storageClass:storagePool:pool_1reclaimPolicy:DeletehostIoSize:\u0026#34;8192\u0026#34;nasServer:\u0026#34;nasserver_2\u0026#34;  Create an empty secret with file helm/emptysecret.yaml file by running the kubectl create -f helm/emptysecret.yaml command.\n  Prepare the secret.json for driver configuration. The following table lists driver configuration parameters for multiple storage arrays.\n   Parameter Description Required Default     username Username for accessing unity system true -   password Password for accessing unity system true -   restGateway REST API gateway HTTPS endpoint Unity system true -   arrayId ArrayID for unity system true -   insecure \u0026ldquo;unityInsecure\u0026rdquo; determines if the driver is going to validate unisphere certs while connecting to the Unisphere REST API interface. If it is set to false, then a secret unity-certs has to be created with a X.509 certificate of CA which signed the Unisphere certificate true true   isDefaultArray An array having isDefaultArray=true is for backward compatibility. This parameter should occur once in the list. false false    Example: secret.json\n{ \u0026#34;storageArrayList\u0026#34;: [ { \u0026#34;username\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;password\u0026#34;: \u0026#34;password\u0026#34;, \u0026#34;restGateway\u0026#34;: \u0026#34;https://10.1.1.1\u0026#34;, \u0026#34;arrayId\u0026#34;: \u0026#34;APM00******1\u0026#34;, \u0026#34;insecure\u0026#34;: true, \u0026#34;isDefaultArray\u0026#34;: true }, { \u0026#34;username\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;password\u0026#34;: \u0026#34;password\u0026#34;, \u0026#34;restGateway\u0026#34;: \u0026#34;https://10.1.1.2\u0026#34;, \u0026#34;arrayId\u0026#34;: \u0026#34;APM00******2\u0026#34;, \u0026#34;insecure\u0026#34;: true } ] } kubectl create secret generic unity-creds -n unity --from-file=config=secret.json\nUse the following command to replace or update the secret:\nkubectl create secret generic unity-creds -n unity --from-file=config=secret.json -o yaml --dry-run | kubectl replace -f -\nNote: The user needs to validate the JSON syntax and array related key/values while replacing the unity-creds secret. The driver will continue to use previous values in case of an error found in the JSON file.\nNote: \u0026ldquo;isDefaultArray\u0026rdquo; parameter in values.yaml and secret.json should match each other.\n  Setup for snapshots.\nThe Kubernetes Volume Snapshot feature is beta in Kubernetes v1.18 and v1.19 and move to GA in v1.20.\n The following section summarizes the changes in the GA  In order to use the Kubernetes Volume Snapshot feature, you must ensure the following components have been deployed on your Kubernetes cluster.\n  Install Snapshot CRDs:\nFor Kubernetes 1.18 and 1.19, Snapshot CRDs versioned 3.0.3 must be installed. CRDs\nFor Kubernetes 1.20 , Snapshot CRDs versioned 4.0.0 must be installed. CRDs\n  Install Snapshot Controller:\nFor Kubernetes 1.18 and 1.19, Snapshot Controller versioned 3.0.3 must be installed. Controller\nFor Kubernetes 1.20, Snapshot Controller versioned 4.0.0 must be installed.\nController\n    Run the ./csi-install.sh --namespace unity --values ./myvalues.yaml command to proceed with the installation.\nA successful installation must display messages that look similar to the following samples:\n------------------------------------------------------ \u0026gt; Installing CSI Driver: csi-unity on 1.20 ------------------------------------------------------ ------------------------------------------------------ \u0026gt; Checking to see if CSI Driver is already installed ------------------------------------------------------ ------------------------------------------------------ \u0026gt; Verifying Kubernetes and driver configuration ------------------------------------------------------ |- Kubernetes Version: 1.20 | |- Driver: csi-unity | |- Verifying Kubernetes versions | |--\u0026gt; Verifying minimum Kubernetes version Success | |--\u0026gt; Verifying maximum Kubernetes version Success | |- Verifying that required namespaces have been created Success | |- Verifying that required secrets have been created Success | |- Verifying that required secrets have been created Success | |- Verifying alpha snapshot resources | |--\u0026gt; Verifying that alpha snapshot CRDs are not installed Success | |- Verifying sshpass installation.. | |- Verifying iSCSI installation Enter the root password of 10.**.**.**: Enter the root password of 10.**.**.**: Success | |- Verifying snapshot support | |--\u0026gt; Verifying that snapshot CRDs are available Success | |--\u0026gt; Verifying that the snapshot controller is available Success | |- Verifying helm version Success ------------------------------------------------------ \u0026gt; Verification Complete - Success ------------------------------------------------------ | |- Installing Driver Success | |--\u0026gt; Waiting for Deployment unity-controller to be ready Success | |--\u0026gt; Waiting for DaemonSet unity-node to be ready Success ------------------------------------------------------ \u0026gt; Operation complete ------------------------------------------------------ Results At the end of the script unity-controller Deployment and DaemonSet unity-node will be ready, execute command kubectl get pods -n unity to get the status of the pods and you will see the following:\n One or more Unity Controller (based on controllerCount) with 5/5 containers ready, and status displayed as Running. Agent pods with 2/2 containers and the status displayed as Running.  Finally, the script creates storageclasses such as, \u0026ldquo;unity\u0026rdquo;. Additional storage classes can be created for different combinations of file system types and Unity storage pools.\nThe script also creates one or more volumesnapshotclasses based on number of arrays . \u0026ldquo;unity-snapclass\u0026rdquo; will be the volumesnapshotclass for default array. The output will be similar to following:\n[root@host ~]# kubectl get volumesnapshotclass NAME AGE unity-apm***********-snapclass 12m unity-snapclass 12m\n  Certificate validation for Unisphere REST API calls This topic provides details about setting up the certificate validation for the CSI Driver for Dell EMC Unity.\nBefore you begin\nAs part of the CSI driver installation, the CSI driver requires a secret with the name unity-certs-0 to unity-certs-n based on \u0026ldquo;.Values.certSecretCount\u0026rdquo; parameter present in the namespace unity.\nThis secret contains the X509 certificates of the CA which signed the Unisphere SSL certificate in PEM format.\nIf the install script does not find the secret, it creates one empty secret with the name unity-certs-0.\nThe CSI driver exposes an install parameter in secret.json, which is like storageArrayList[i].insecure, which determines if the driver performs client-side verification of the Unisphere certificates.\nThe storageArrayList[i].insecure parameter set to true by default, and the driver does not verify the Unisphere certificates.\nIf the storageArrayList[i].insecure set to false, then the secret unity-certs-n must contain the CA certificate for Unisphere.\nIf this secret is empty secret, then the validation of the certificate fails, and the driver fails to start.\nIf the storageArrayList[i].insecure parameter set to false and a previous installation attempt created the empty secret, then this secret must be deleted and re-created using the CA certs.\nIf the Unisphere certificate is self-signed or if you are using an embedded Unisphere, then perform the following steps.\n To fetch the certificate, run the following command. openssl s_client -showcerts -connect \u0026lt;Unisphere IP:Port\u0026gt; \u0026lt;/dev/null 2\u0026gt;/dev/null | openssl x509 -outform PEM \u0026gt; ca_cert_0.pem Example: openssl s_client -showcerts -connect 1.1.1.1:443 \u0026lt;/dev/null 2\u0026gt;/dev/null | openssl x509 -outform PEM \u0026gt; ca_cert_0.pem Run the following command to create the cert secret with index \u0026lsquo;0\u0026rsquo;: kubectl create secret generic unity-certs-0 --from-file=cert-0=ca_cert_0.pem -n unity Use the following command to replace the secret: kubectl create secret generic unity-certs-0 -n unity --from-file=cert-0=ca_cert_0.pem -o yaml --dry-run | kubectl replace -f - Repeat step 1 and 2 to create multiple cert secrets with incremental index (example: unity-certs-1, unity-certs-2, etc)  Note: \u0026ldquo;unity\u0026rdquo; is the namespace for helm based installation but namespace can be user defined in operator based installation.\nNote: User can add multiple certificates in the same secret. The certificate file should not exceed more than 1Mb due to kubernetes secret size limitation.\nNote: Whenever certSecretCount parameter changes in myvalues.yaml user needs to uninstall and install the driver.\nStorage Classes As part of the driver installation, a set of storage classes is created along with the driver pods. This is done to demonstrate how storage classes need to be created to consume storage from Dell EMC storage arrays.\nThe StorageClass object in Kubernetes is immutable and cannot be modified once created. It creates challenges when we need to change or update a parameter, for example when a version of the driver introduces new configurable parameters for the storage classes. To avoid issues during upgrades, future releases of the drivers will have the installation separated from the creation of Storage Classes. In preparation for that, starting from CSI Unity v1.5, an annotation \u0026ldquo;helm.sh/resource-policy\u0026rdquo;: keep is applied to the storage classes created by the dell-csi-helm-installer.\nBecause of this annotation, these storage classes are not going to be deleted even after the driver has been uninstalled. This annotation has been applied to give you an opportunity to keep using these storage classes even with a future release of the driver. In case you wish to not use these storage classes, you will need to delete them by using the kubectl delete storageclass command.\nNote: If you uninstall the driver and reinstall it, you can still face errors if any update in the values.yaml file leads to an update of the storage class(es):\nError: cannot patch \u0026quot;\u0026lt;sc-name\u0026gt;\u0026quot; with kind StorageClass: StorageClass.storage.k8s.io \u0026quot;\u0026lt;sc-name\u0026gt;\u0026quot; is invalid: parameters: Forbidden: updates to parameters are forbidden In case you want to make such updates, ensure to delete the existing storage classes using the kubectl delete storageclass command. Deleting a storage class has no impact on a running Pod with mounted PVCs. You will not be able to provision new PVCs until at least one storage class is newly created.\nDynamically update the unity-creds secrets Users can dynamically add delete array information from secret. Whenever an update happens the driver updates the \u0026ldquo;Host\u0026rdquo; information in an array. User can update secret using the following command:\n`kubectl create secret generic unity-creds -n unity --from-file=config=secret.json -o yaml --dry-run=client | kubectl replace -f - `  Note: Updating unity-certs-x secrets is a manual process, unlike unity-creds. Users have to re-install the driver in case of updating/adding the SSL certificates or changing the certSecretCount parameter.\n","excerpt":"The CSI Driver for Dell EMC Unity can be deployed by using the provided Helm v3 charts and …","ref":"/ansible-docs/v1/installation/helm/unity/","title":"Unity"},{"body":"CSI Unity Pre-requisites Create secret to store Unity credentials Create a namespace called unity (it can be any user-defined name; But commands in this section assumes that the namespace is unity) Prepare the secret.json for driver configuration. The following table lists driver configuration parameters for multiple storage arrays.\n   Parameter Description Required Default     username Username for accessing unity system true -   password Password for accessing unity system true -   restGateway REST API gateway HTTPS endpoint Unity system true -   arrayId ArrayID for unity system true -   insecure \u0026ldquo;unityInsecure\u0026rdquo; determines if the driver is going to validate unisphere certs while connecting to the Unisphere REST API interface If it is set to false, then a secret unity-certs has to be created with a X.509 certificate of CA which signed the Unisphere certificate true true   isDefaultArray An array having isDefaultArray=true is for backward compatibility. This parameter should occur once in the list. false false    Ex: secret.json\n{ \u0026quot;storageArrayList\u0026quot;: [ { \u0026quot;username\u0026quot;: \u0026quot;user\u0026quot;, \u0026quot;password\u0026quot;: \u0026quot;password\u0026quot;, \u0026quot;restGateway\u0026quot;: \u0026quot;https://10.1.1.1\u0026quot;, \u0026quot;arrayId\u0026quot;: \u0026quot;APM00******1\u0026quot;, \u0026quot;insecure\u0026quot;: true, \u0026quot;isDefaultArray\u0026quot;: true }, { \u0026quot;username\u0026quot;: \u0026quot;user\u0026quot;, \u0026quot;password\u0026quot;: \u0026quot;password\u0026quot;, \u0026quot;restGateway\u0026quot;: \u0026quot;https://10.1.1.2\u0026quot;, \u0026quot;arrayId\u0026quot;: \u0026quot;APM00******2\u0026quot;, \u0026quot;insecure\u0026quot;: true } ] } kubectl create secret generic unity-creds -n unity --from-file=config=secret.json\nUse the following command to replace or update the secret\nkubectl create secret generic unity-creds -n unity --from-file=config=secret.json -o yaml --dry-run | kubectl replace -f -\nNote: The user needs to validate the JSON syntax and array related key/values while replacing the unity-creds secret. The driver will continue to use previous values in case of an error found in the JSON file.\nCreate secret for client side TLS verification Please refer detailed documentation on how to create this secret here\nIf certificate validation is skipped, empty secret must be created. To create an empty secret. Ex: empty-secret.yaml\napiVersion:v1kind:Secretmetadata:name:unity-certs-0namespace:unitytype:Opaquedata:cert-0:\u0026#34;\u0026#34;Execute command: kubectl create -f empty-secret.yaml\nModify/Set the following optional environment variables Users should configure the parameters in CR. The following table lists the primary configurable parameters of the Unity driver and their default values:\n   Parameter Description Required Default     Common parameters for node and controller      CSI_ENDPOINT Specifies the HTTP endpoint for Unity. No /var/run/csi/csi.sock   X_CSI_DEBUG To enable debug mode No false   GOUNITY_DEBUG To enable debug mode for gounity library No false   X_CSI_UNITY_ALLOW_MULTI_POD_ACCESS Flag to enable multiple pods use the same pvc on the same node with RWO access mode No false   Controller parameters      X_CSI_MODE Driver starting mode No controller   X_CSI_UNITY_AUTOPROBE To enable auto probing for driver No true   Node parameters      X_CSI_MODE Driver starting mode No node   X_CSI_ISCSI_CHROOT Path to which the driver will chroot before running any iscsi commands. No /noderoot    StorageClass Parameters    Parameter Description Required Default     storagePool Unity Storage Pool CLI ID to use with in the Kubernetes storage class true -   thinProvisioned To set volume thinProvisioned false \u0026ldquo;true\u0026rdquo;   isDataReductionEnabled To set volume data reduction false \u0026ldquo;false\u0026rdquo;   volumeTieringPolicy To set volume tiering policy false 0   FsType Block volume related parameter. To set File system type. Possible values are ext3,ext4,xfs. Supported for FC/iSCSI protocol only. false ext4   hostIOLimitName Block volume related parameter. To set unity host IO limit. Supported for FC/iSCSI protocol only. false \u0026quot;\u0026rdquo;   nasServer NFS related parameter. NAS Server CLI ID for filesystem creation. true \u0026quot;\u0026rdquo;   hostIoSize NFS related parameter. To set filesystem host IO Size. false \u0026ldquo;8192\u0026rdquo;   reclaimPolicy What should happen when a volume is removed false Delete    SnapshotClass parameters Following parameters are not present in values.yaml in the Helm based installer\n   Parameter Description Required Default     snapshotRetentionDuration TO set snapshot retention duration. Format:\u0026ldquo;1:23:52:50\u0026rdquo; (number of days:hours:minutes:sec) false \u0026quot;\u0026rdquo;    Example CR for Unity Refer samples from here. Below is an example CR:\napiVersion:storage.dell.com/v1kind:CSIUnitymetadata:name:test-unitynamespace:test-unityspec:driver:configVersion:v4replicas:2common:image:\u0026#34;dellemc/csi-unity:v1.5.0\u0026#34;imagePullPolicy:IfNotPresentenvs:- name:X_CSI_UNITY_DEBUGvalue:\u0026#34;true\u0026#34;- name:X_CSI_UNITY_ALLOW_MULTI_POD_ACCESSvalue:\u0026#34;false\u0026#34;sideCars:- name:provisionerargs:[\u0026#34;--volume-name-prefix=csiunity\u0026#34;]storageClass:- name:virt2016****-fcdefault:truereclaimPolicy:\u0026#34;Delete\u0026#34;allowVolumeExpansion:trueparameters:storagePool:pool_1arrayId:\u0026#34;VIRT2016****\u0026#34;protocol:\u0026#34;FC\u0026#34;- name:virt2017****-iscsireclaimPolicy:\u0026#34;Delete\u0026#34;allowVolumeExpansion:trueparameters:storagePool:pool_1arrayId:\u0026#34;VIRT2017****\u0026#34;protocol:\u0026#34;iSCSI\u0026#34;- name:virt2017****-nfsreclaimPolicy:\u0026#34;Delete\u0026#34;allowVolumeExpansion:trueparameters:storagePool:pool_1arrayId:\u0026#34;VIRT2017****\u0026#34;protocol:\u0026#34;NFS\u0026#34;hostIoSize:\u0026#34;8192\u0026#34;nasServer:nas_1- name:virt2017****-iscsi-topologyreclaimPolicy:\u0026#34;Delete\u0026#34;allowVolumeExpansion:truevolumeBindingMode:WaitForFirstConsumerallowedTopologies:- matchLabelExpressions:- key:csi-unity.dellemc.com/virt2017****-iscsivalues:- \u0026#34;true\u0026#34;parameters:storagePool:pool_1arrayId:\u0026#34;VIRT2017****\u0026#34;protocol:\u0026#34;iSCSI\u0026#34;snapshotClass:- name:test-snapparameters:retentionDuration:\u0026#34;\u0026#34;","excerpt":"CSI Unity Pre-requisites Create secret to store Unity credentials Create a namespace called unity …","ref":"/ansible-docs/v1/installation/operator/unity/","title":"Unity"},{"body":"In the repository, a simple test manifest exists that creates three different PersistentVolumeClaims using default NFS and iSCSI and FC storage classes, and automatically mounts them to the pod.\nSteps\n To run this test, run the kubectl command from the root directory of the repository: kubectl create -f ./tests/sample.yaml   You can find all the created resources in test-unity namespace.\n Check if the pod is created and Ready and Running by running:\nkubectl get all -n test-unity If it is in CrashLoopback state then the driver installation was not successful. Check the logs of the node and the controller.\n  Go into the created container and verify that everything is mounted correctly.\n  After verifying, you can uninstall the testing PVCs and StatefulSet.\nkubectl delete -f ./tests/sample.yaml   ","excerpt":"In the repository, a simple test manifest exists that creates three different PersistentVolumeClaims …","ref":"/ansible-docs/v1/installation/test/unity/","title":"Test Unity CSI Driver"},{"body":"Release Notes - CSI Unity v1.5.0 New Features/Changes  Added support for Kubernetes v1.20 Added support for OpenShift 4.7 with RHEL and CoreOS worker nodes Changed driver base image to UBI 8.x Added support for Red Hat Enterprise Linux (RHEL) 8.3 Qualified with Docker - UCP 3.3.5 Added support for SLES 15SP2  Fixed Issues  Raw-Block volume with accessmode RWX can be mounted to multiple nodes. PVC creation fails on a cluster with only NFS protocol enabled by adding topology keys for NFS protocol.  Known Issues    Issue Workaround     Topology related node labels are not removed automatically. Currently, when the driver is uninstalled, topology related node labels are not getting removed automatically. There is an open issue in the Kubernetes to fix this. Until the fix is released we need to manually remove the node labels mentioned here https://github.com/dell/csi-unity#known-issues (Point 1)   Dynamic array detection will not work in Topology based environment Whenever a new array is added or removed, then the driver should be restarted with the below command only if the topology-based storage classes are used. Otherwise, the driver will automatically detect the newly added or removed arrays https://github.com/dell//csi-unity#known-issues (Point 2)   If source PVC is deleted when cloned PVC exists, then source PVC will be deleted in cluster but on array it will still be present and marked for deletion. All the cloned pvc should be deleted in order to delete the source pvc from array.   PVC creation fails on a fresh cluster with iSCSI and NFS protocols alone enabled with error failed to provision volume with StorageClass \u0026ldquo;unity-iscsi\u0026rdquo;: error generating accessibility requirements: no available topology found. This is because iSCSI initiator login takes longer than the node pod startup time. This can be overcome by bouncing the node pods in the cluster using the below command the driver pods with kubectl get pods -n unity --no-headers=true   On deleting pods sometimes the corresponding \u0026lsquo;volumeattachment\u0026rsquo; will not get removed. This issue is intermittent and happens with one specific protocol (FC, iSCSI or NFS) based storageclasses. This issue occurs across kubernetes versions 1.18 and 1.19 and both versions of OpenShift (4.5/4.6). On deleting the stale volumeattachment manually, Controller Unpublish gets invoked and then the corresponding PVCs can be deleted.   The flag allowRWOMultiPodAccess:false is not applicable for Raw Block volumes and the driver allows creation of multiple pods on the same node with RWO access mode. Workaround not necessary as this issue does not block any usecase.   Driver installation warning: \u0026ldquo;OpenShift version 4.7, is newer than the version that has been tested. Latest tested version is: 4.6\u0026rdquo; Ignore this warning and continue with the installation. v1.5.0 release of the driver supports OpenShift 4.6/4.7 .    ","excerpt":"Release Notes - CSI Unity v1.5.0 New Features/Changes  Added support for Kubernetes v1.20 Added …","ref":"/ansible-docs/v1/release/unity/","title":"Unity"},{"body":"    Symptoms Prevention, Resolution or Workaround     When you run the command kubectl describe pods unity-controller-\u0026lt;suffix\u0026gt; –n unity, the system indicates that the driver image could not be loaded. You may need to put an insecure-registries entry in /etc/docker/daemon.json or login to the docker registry   The kubectl logs -n unity unity-node-\u0026lt;suffix\u0026gt; driver logs shows that the driver can\u0026rsquo;t connect to Unity - Authentication failure. Check if you have created secret with correct credentials   Installation of the driver on Kubernetes v1.20 fails with the following error: Error: unable to build kubernetes objects from release manifest: unable to recognize \u0026quot;\u0026quot;: no matches for kind \u0026quot;VolumeSnapshotClass\u0026quot; in version \u0026quot;snapshot.storage.k8s.io/v1\u0026quot; Kubernetes v1.20 requires v1 version of snapshot CRDs. If on Kubernetes 1.20 (v1 snapshots) install CRDs from v4.0.0, see point 6 here    ","excerpt":"Symptoms Prevention, Resolution or Workaround     When you run the command kubectl describe pods …","ref":"/ansible-docs/v1/troubleshooting/unity/","title":"Unity"},{"body":"Creating volumes and consuming them Create a file simple.yaml using sample yaml files located at tests/sample.yaml\nThis command will create a statefulset that consumes three volumes of default storage classes\nkubectl create -f tests/sample.yaml After executing this command 3 PVC and statefulset will be created in the test-unity namespace. You can check created PVCs by running kubectl get pvc -n test-unity and check statefulset\u0026rsquo;s pods by running kubectl get pods -n test-unitycommand. Pod should be Ready and Running.\n If Pod is in CrashLoopback or PVCs is in Pending state then driver installation is not successful, check logs of node and controller.\n Deleting volumes To delete volumes, pod and statefulset run the command\nkubectl delete -f tests/sample.yaml Consuming existing volumes with static provisioning You can use existent volumes from Unity array as Persistent Volumes in your Kubernetes, to do that you must perform the following steps:\n Open your volume in Unity Management UI (unisphere), and take a note of volume-id. The volume-id looks like csiunity-xxxxx and CLI ID looks like sv_xxxx. Create PersistentVolume and use this volume-id as a volumeHandle in the manifest. Modify other parameters according to your needs.  apiVersion:v1kind:PersistentVolumemetadata:name:static-1annotations:pv.kubernetes.io/provisioned-by:csi-unity.dellemc.comspec:accessModes:- ReadWriteOncecapacity:storage:5Gicsi:driver:csi-unity.dellemc.comvolumeHandle:existingvol-\u0026lt;protocol\u0026gt;-\u0026lt;array_id\u0026gt;-\u0026lt;volume-id\u0026gt; persistentVolumeReclaimPolicy: RetainclaimRef:namespace:defaultname:static-pvc1storageClassName:unityvolumeMode:FilesystemCreate PersistentVolumeClaim to use this PersistentVolume.  kind:PersistentVolumeClaimapiVersion:v1metadata:name:static-pvc1spec:accessModes:- ReadWriteManyresources:requests:storage:5GivolumeName:static-1storageClassName:unityThen use this PVC as a volume in a pod.  apiVersion:v1kind:Podmetadata:name:static-prov-podspec:containers:- name:testimage:docker.io/centos:latestcommand:[\u0026#34;/bin/sleep\u0026#34;,\u0026#34;3600\u0026#34;]volumeMounts:- mountPath:\u0026#34;/data0\u0026#34;name:pvolvolumes:- name:pvolpersistentVolumeClaim:claimName:static-pvc1After the pod becomes Ready and Running, you can start to use this pod and volume.  Volume Snapshot Feature The CSI Unity driver version 1.3 and later supports managing beta snapshots.\nTo use Volume Snapshots, ensure the following components have been deployed to your cluster:\n Kubernetes Volume Snaphshot CRDs Volume Snapshot Controller  You can install them by copy pasting the following commands (Copy entire thing in one shot and paste it in terminal):\nkubectl apply -f https://raw.githubusercontent.com/kubernetes-csi/external-snapshotter/release-2.0/config/crd/snapshot.storage.k8s.io_volumesnapshotclasses.yaml \u0026amp;\u0026amp; kubectl apply -f https://raw.githubusercontent.com/kubernetes-csi/external-snapshotter/release-2.0/config/crd/snapshot.storage.k8s.io_volumesnapshotcontents.yaml \u0026amp;\u0026amp; kubectl apply -f https://raw.githubusercontent.com/kubernetes-csi/external-snapshotter/release-2.0/config/crd/snapshot.storage.k8s.io_volumesnapshots.yaml \u0026amp;\u0026amp; kubectl apply -f https://raw.githubusercontent.com/kubernetes-csi/external-snapshotter/v3.0.3/deploy/kubernetes/snapshot-controller/rbac-snapshot-controller.yaml \u0026amp;\u0026amp; kubectl apply -f https://raw.githubusercontent.com/kubernetes-csi/external-snapshotter/v3.0.3/deploy/kubernetes/snapshot-controller/setup-snapshot-controller.yaml  For general use, update the snapshot controller YAMLs with an appropriate namespace before installing. For example, on a Vanilla Kubernetes cluster, update the namespace from default to kube-system before issuing the kubectl create command.\n Volume Snapshot Class During the installation of the CSI Unity driver version 1.3 and later, a Volume Snapshot Class is created using the new v1beta1 snapshot APIs. This is the only Volume Snapshot Class required and there is no need to create any other Volume Snapshot Class.\nFollowing is the manifest for the Volume Snapshot Class created during installation:\napiVersion:snapshot.storage.k8s.io/v1beta1kind:VolumeSnapshotClassmetadata:name:unity-snapclassdriver:csi-unity.dellemc.comdeletionPolicy:DeleteCreate Volume Snapshot The following is a sample manifest for creating a Volume Snapshot using the v1beta1 snapshot APIs:\napiVersion:snapshot.storage.k8s.io/v1beta1kind:VolumeSnapshotmetadata:name:pvol0-snapnamespace:test-unityspec:volumeSnapshotClassName:unity-snapclasssource:persistentVolumeClaimName:pvolOnce the VolumeSnapshot is successfully created by the CSI Unity driver, a VolumeSnapshotContent object is automatically created. Once the status of the VolumeSnapshot object has the readyToUse field set to true , it is available for use.\nFollowing is the relevant section of VolumeSnapshot object status:\nstatus:boundVolumeSnapshotContentName:snapcontent-xxxxxxxxxxxxxcreationTime:\u0026#34;2020-07-16T08:42:12Z\u0026#34;readyToUse:trueCreating PVCs with Volume Snapshots as Source The following is a sample manifest for creating a PVC with a VolumeSnapshot as a source:\napiVersion:v1kind:PersistentVolumeClaimmetadata:name:restorepvcnamespace:test-unityspec:storageClassName:unity-iscsidataSource:name:pvol0-snapkind:VolumeSnapshotapiGroup:snapshot.storage.k8s.ioaccessModes:- ReadWriteOnceresources:requests:storage:8GiVolume Expansion The CSI Unity driver version 1.3 and later supports the expansion of Persistent Volumes (PVs). This expansion can be done either online (for example, when a PVC is attached to a node) or offline (for example, when a PVC is not attached to any node).\nTo use this feature, the storage class that is used to create the PVC must have the attribute allowVolumeExpansion set to true. The storage classes created during the installation (both using Helm or dell-csi-operator) have the allowVolumeExpansion set to true by default.\nIf you are creating more storage classes, ensure that this attribute is set to true to expand any PVs created using these new storage classes.\nThe following is a sample manifest for a storage class which allows for Volume Expansion:\napiVersion:storage.k8s.io/v1kind:StorageClassmetadata:name:unity-expand-scannotations:storageclass.beta.kubernetes.io/is-default-class:falseprovisioner:csi-unity.dellemc.comreclaimPolicy:DeleteallowVolumeExpansion:true# Set this attribute to true if you plan to expand any PVCs created using this storage classparameters:FsType:xfsTo resize a PVC, edit the existing PVC spec and set spec.resources.requests.storage to the intended size. For example, if you have a PVC unity-pvc-demo of size 3Gi, then you can resize it to 30Gi by updating the PVC.\napiVersion:v1kind:PersistentVolumeClaimmetadata:name:unity-pvc-demonamespace:testspec:accessModes:- ReadWriteOncevolumeMode:Filesystemresources:requests:storage:30Gi# Updated size from 3Gi to 30GistorageClassName:unity-expand-sc The Kubernetes Volume Expansion feature can only be used to increase the size of a volume. It cannot be used to shrink a volume.\n Raw block support The CSI Unity driver version 1.4 and later supports managing Raw Block volumes.\nRaw Block volumes are created using the volumeDevices list in the pod template spec with each entry accessing a volumeClaimTemplate specifying a volumeMode: Block. An example configuration is outlined here:\napiVersion:apps/v1kind:StatefulSetmetadata:name:unitytestnamespace:{{.Values.namespace}}spec:...spec:...containers:- name:test...volumeDevices:- devicePath:\u0026#34;/dev/data0\u0026#34;name:pvolvolumeClaimTemplates:- metadata:name:pvolspec:accessModes:- ReadWriteOncevolumeMode:BlockstorageClassName:unityresources:requests:storage:8GiAllowable access modes are ReadWriteOnce, ReadWriteMany, and for block devices that have been previously initialized, ReadOnlyMany.\nRaw Block volumes are presented as a block device to the pod by using a bind mount to a block device in the node\u0026rsquo;s file system. The driver does not format or check the format of any file system on the block device. Raw Block volumes do support online Volume Expansion, but it is up to the application to manage reconfiguring the file system (if any) to the new size.\nNote: Raw block volume creation supports only for FC and iSCSI protocols\nFor additional information, see the kubernetes website.\nVolume Cloning Feature The CSI Unity driver version 1.3 supports volume cloning. This allows specifying existing PVCs in the dataSource field to indicate a user would like to clone a Volume.\nSource and destination PVC must be in the same namespace and have the same Storage Class.\nTo clone a volume, you should first have an existing PVC, example: vol0:\nkind:PersistentVolumeClaimapiVersion:v1metadata:name:vol0namespace:test-unityspec:storageClassName:unity-nfsaccessModes:- ReadWriteOncevolumeMode:Filesystemresources:requests:storage:8GiThe following is a sample manifest for cloning pvol0:\napiVersion:v1kind:PersistentVolumeClaimmetadata:name:cloned-pvcnamespace:test-unityspec:storageClassName:unity-nfsdataSource:name:vol0kind:PersistentVolumeClaimaccessModes:- ReadWriteOnceresources:requests:storage:8GiEphemeral Inline Volume The CSI Unity driver version 1.4 supports ephemeral inline CSI volumes. This feature allows CSI volumes to be specified directly in the pod specification.\nAt runtime, nested inline volumes follow the ephemeral lifecycle of their associated pods where the driver handles all phases of volume operations as pods are created and destroyed.\nThe following is a sample manifest for creating ephemeral volume in pod manifest with CSI Unity driver.\nkind:PodapiVersion:v1metadata:name:test-unity-ephemeral-volumespec:containers:- name:test-containerimage:busyboxcommand:[\u0026#34;sleep\u0026#34;,\u0026#34;3600\u0026#34;]volumeMounts:- mountPath:\u0026#34;/data\u0026#34;name:volumevolumes:- name:volumecsi:driver:csi-unity.dellemc.comfsType:\u0026#34;ext4\u0026#34;volumeAttributes:size:\u0026#34;10Gi\u0026#34;This manifest will create a pod and attach newly created ephemeral inline CSI volume to it.\nTo create NFS volume you need to provide nasName: parameters that points to the name of your NAS Server in pod manifest like so\nvolumes:- name:volumecsi:driver:csi-unity.dellemc.comfsType:\u0026#34;nfs\u0026#34;volumeAttributes:size:\u0026#34;20Gi\u0026#34;nasName:\u0026#34;csi-nas-name\u0026#34;Controller HA The CSI Unity driver version 1.4 introduces controller HA feature. Instead of StatefulSet controller pods deployed as a Deployment.\nBy default number of replicas set to 2, you can set controllerCount parameter to 1 in myvalues.yaml if you want to disable controller HA for your installation. When installing via Operator you can change replicas parameter in spec.driver section in your Unity Custom Resource.\nWhen multiple replicas of controller pods are in cluster each sidecar (Attacher, Provisioner, Resizer and Snapshotter) tries to get a lease so only one instance of each sidecar would be active in the cluster at a time.\nDriver pod placement You can configure where driver controller and worker pods should be placed. To do that you will need to use nodeSelector and tolerations mechanisms you can configure in your myvalues.yaml\nFor example you can specify tolerations to assign driver controller pods on controller nodes too:\n# \u0026#34;controller\u0026#34; allows to configure controller specific parameterscontroller:# \u0026#34;controller.nodeSelector\u0026#34; defines what nodes would be selected for pods of controller deploymentnodeSelector:# \u0026#34;controller.tolerations\u0026#34; defines tolerations that would be applied to controller deploymenttolerations:- key:\u0026#34;node-role.kubernetes.io/master\u0026#34;operator:\u0026#34;Exists\u0026#34;effect:\u0026#34;NoSchedule\u0026#34;If you want to assign controller pods ONLY on controller nodes you need to configure nodeSelector:\n# \u0026#34;controller\u0026#34; allows to configure controller specific parameterscontroller:# \u0026#34;controller.nodeSelector\u0026#34; defines what nodes would be selected for pods of controller deploymentnodeSelector:node-role.kubernetes.io/master:\u0026#34;\u0026#34;# \u0026#34;controller.tolerations\u0026#34; defines tolerations that would be applied to controller deploymenttolerations:- key:\u0026#34;node-role.kubernetes.io/master\u0026#34;operator:\u0026#34;Exists\u0026#34;effect:\u0026#34;NoSchedule\u0026#34;As said before you can configure where node driver pods would be assigned in the similar way in node section of myvalues.yaml\nTopology The CSI Unity driver version 1.4 supports Topology which forces volumes to be placed on worker nodes that have connectivity to the backend storage. This covers use cases where users have chosen to restrict the nodes on which the CSI driver is deployed.\nThis Topology support does not include customer defined topology, users cannot create their own labels for nodes, they should use whatever labels are returned by driver and applied automatically by Kubernetes on its nodes.\nTopology Usage To use the Topology feature user can install driver by setting createStorageClassesWithTopology to true in the myvalues.yaml which will create default storage classes by adding topology keys (based on the arrays specified in myvalues.yaml) and with WaitForFirstConsumer binding mode.\nAnother option is the user can create custom storage classes on their own by specifying the valid topology keys and binding mode.\nThe following is one of example storage class manifest:\napiVersion:storage.k8s.io/v1kind:StorageClassmetadata:name:unity-topology-fcprovisioner:csi-unity.dellemc.comreclaimPolicy:DeleteallowVolumeExpansion:truevolumeBindingMode:WaitForFirstConsumerallowedTopologies:- matchLabelExpressions:- key:csi-unity.dellemc.com/\u0026lt;array_id\u0026gt;-fcvalues:- \u0026#34;true\u0026#34;This example will match all nodes where driver has a connection to Unity array with array ID mentioned via Fiber Channel. Similarly by replacing fc with iscsi in the key will check for iSCSI connectivity with the node.\nYou can check what labels your nodes contain by running kubectl get nodes --show-labels command.\n Note that volumeBindingMode: is set to WaitForFirstConsumer this is required for topology feature to work properly.\n For any additional information about topology, see the Kubernetes Topology documentation.\n","excerpt":"Creating volumes and consuming them Create a file simple.yaml using sample yaml files located at …","ref":"/ansible-docs/v2/features/unity/","title":"Unity"},{"body":"The CSI Driver for Dell EMC Unity can be deployed by using the provided Helm v3 charts and installation scripts on both Kubernetes and OpenShift platforms. For more detailed information on the installation scripts, review the script documentation.\nThe controller section of the Helm chart installs the following components in a Deployment:\n CSI Driver for Unity Kubernetes External Provisioner, which provisions the volumes Kubernetes External Attacher, which attaches the volumes to the containers Kubernetes External Snapshotter, which provides snapshot support Kubernetes External Resizer, which resizes the volume  The node section of the Helm chart installs the following component in a DaemonSet:\n CSI Driver for Unity Kubernetes Node Registrar, which handles the driver registration  Prerequisites Before you install CSI Driver for Unity, verify the requirements that are mentioned in this topic are installed and configured.\nRequirements  Install Kubernetes Configure Docker service Install Helm v3 To use FC protocol, host must be zoned with Unity array To use iSCSI and NFS protocol, iSCSI initiator and NFS utility packages need to be installed  Configure Docker service The mount propagation in Docker must be configured on all Kubernetes nodes before installing CSI Driver for Unity.\nProcedure   Edit the service section of /etc/systemd/system/multi-user.target.wants/docker.service file as follows:\n[Service] ... MountFlags=shared   Restart the Docker service with systemctl daemon-reload and\nsystemctl daemon-reload systemctl restart docker   Install CSI Driver Install CSI Driver for Unity using this procedure.\nBefore you begin\n You must have the downloaded files, including the Helm chart from the source git repository, ready for this procedure. In the top-level dell-csi-helm-installer directory, there should be two scripts, csi-install.sh and csi-uninstall.sh. These scripts handle some of the pre and post operations that cannot be performed in the helm chart, such as creating Custom Resource Definitions (CRDs), if needed. Make sure \u0026ldquo;unity\u0026rdquo; namespace exists in kubernetes cluster. Use kubectl create namespace unity command to create the namespace, if the namespace is not present.  Procedure\n  Collect information from the Unity Systems like Unique ArrayId, IP address, username and password. Make a note of the value for these parameters as they must be entered in the secret.json and myvalues.yaml file.\n  Copy the csi-unity/values.yaml into a file named myvalues.yaml in the same directory of csi-install.sh, to customize settings for installation.\n  Edit myvalues.yaml to set the following parameters for your installation:\nThe following table lists the primary configurable parameters of the Unity driver chart and their default values. More detailed information can be found in the values.yaml file in this repository.\n   Parameter Description Required Default     certSecretCount Represents number of certificate secrets, which user is going to create for ssl authentication. (unity-cert-0..unity-cert-n). Minimum value should be 1 false 1   syncNodeInfoInterval Time interval to add node info to array. Default 15 minutes. Minimum value should be 1 minute false 15   controllerCount Controller replication count to maintain high availability yes 2   volumeNamePrefix String to prepend to any volumes created by the driver false csivol   snapNamePrefix String to prepend to any snapshot created by the driver false csi-snap   csiDebug To set the debug log policy for CSI driver false \u0026ldquo;false\u0026rdquo;   imagePullPolicy The default pull policy is IfNotPresent which causes the Kubelet to skip pulling an image if it already exists. false IfNotPresent   createStorageClassesWithTopology Flag to enable or disable topology. true false   Storage Array List Following parameters is a list of parameters to provide multiple storage arrays     storageArrayList[i].name Name of the storage class to be defined. A suffix of ArrayId and protocol will be added to the name. No suffix will be added to default array. false unity   storageArrayList[i].isDefaultArray To handle the existing volumes created in csi-unity v1.0, 1.1 and 1.1.0.1. The user needs to provide \u0026ldquo;isDefaultArray\u0026rdquo;: true in secret.json. This entry should be present only for one array and that array will be marked default for existing volumes. false \u0026ldquo;false\u0026rdquo;   Storage Class parameters Following parameters are not present in values.yaml     storageArrayList[i].storageClass.storagePool Unity Storage Pool CLI ID to use with in the Kubernetes storage class true -   storageArrayList[i].storageClass.thinProvisioned To set volume thinProvisioned false \u0026ldquo;true\u0026rdquo;   storageArrayList[i].storageClass.isDataReductionEnabled To set volume data reduction false \u0026ldquo;false\u0026rdquo;   storageArrayList[i].storageClass.volumeTieringPolicy To set volume tiering policy false 0   storageArrayList[i].storageClass.FsType Block volume related parameter. To set File system type. Possible values are ext3,ext4,xfs. Supported for FC/iSCSI protocol only. false ext4   storageArrayList[i].storageClass.hostIOLimitName Block volume related parameter. To set unity host IO limit. Supported for FC/iSCSI protocol only. false \u0026quot;\u0026rdquo;   storageArrayList[i].storageClass.nasServer NFS related parameter. NAS Server CLI ID for filesystem creation. true \u0026quot;\u0026rdquo;   storageArrayList[i].storageClass.hostIoSize NFS related parameter. To set filesystem host IO Size. false \u0026ldquo;8192\u0026rdquo;   storageArrayList[i].storageClass.reclaimPolicy What should happen when a volume is removed false Delete   Snapshot Class parameters Following parameters are not present in values.yaml     storageArrayList[i] .snapshotClass.retentionDuration TO set snapshot retention duration. Format:\u0026ldquo;1:23:52:50\u0026rdquo; (number of days:hours:minutes:sec) false \u0026quot;\u0026rdquo;    Note: User should provide all boolean values with double quotes. This applicable only for myvalues.yaml. Example: \u0026ldquo;true\u0026rdquo;/\u0026ldquo;false\u0026rdquo;.\nExample myvalues.yaml\ncsiDebug: \u0026quot;true\u0026quot; volumeNamePrefix : csivol snapNamePrefix: csi-snap imagePullPolicy: Always certSecretCount: 1 syncNodeInfoInterval: 5 controllerCount: 2 createStorageClassesWithTopology: true storageClassProtocols: - protocol: \u0026quot;FC\u0026quot; - protocol: \u0026quot;iSCSI\u0026quot; - protocol: \u0026quot;NFS\u0026quot; storageArrayList: - name: \u0026quot;APM00******1\u0026quot; isDefaultArray: \u0026quot;true\u0026quot; storageClass: storagePool: pool_1 FsType: ext4 nasServer: \u0026quot;nas_1\u0026quot; thinProvisioned: \u0026quot;true\u0026quot; isDataReductionEnabled: true hostIOLimitName: \u0026quot;value_from_array\u0026quot; tieringPolicy: \u0026quot;2\u0026quot; snapshotClass: retentionDuration: \u0026quot;2:2:23:45\u0026quot; - name: \u0026quot;APM001******2\u0026quot; storageClass: storagePool: pool_1 reclaimPolicy: Delete hostIoSize: \u0026quot;8192\u0026quot; nasServer: \u0026quot;nasserver_2\u0026quot;   Create an empty secret by navigating to helm folder that contains emptysecret.yaml file and running the kubectl create -f emptysecret.yaml command.\n  Prepare the secret.json for driver configuration. The following table lists driver configuration parameters for multiple storage arrays.\n   Parameter Description Required Default     username Username for accessing unity system true -   password Password for accessing unity system true -   restGateway REST API gateway HTTPS endpoint Unity system true -   arrayId ArrayID for unity system true -   insecure \u0026ldquo;unityInsecure\u0026rdquo; determines if the driver is going to validate unisphere certs while connecting to the Unisphere REST API interface If it is set to false, then a secret unity-certs has to be created with a X.509 certificate of CA which signed the Unisphere certificate true true   isDefaultArray An array having isDefaultArray=true is for backward compatibility. This parameter should occur once in the list. false false    Example: secret.json\n{ \u0026quot;storageArrayList\u0026quot;: [ { \u0026quot;username\u0026quot;: \u0026quot;user\u0026quot;, \u0026quot;password\u0026quot;: \u0026quot;password\u0026quot;, \u0026quot;restGateway\u0026quot;: \u0026quot;https://10.1.1.1\u0026quot;, \u0026quot;arrayId\u0026quot;: \u0026quot;APM00******1\u0026quot;, \u0026quot;insecure\u0026quot;: true, \u0026quot;isDefaultArray\u0026quot;: true }, { \u0026quot;username\u0026quot;: \u0026quot;user\u0026quot;, \u0026quot;password\u0026quot;: \u0026quot;password\u0026quot;, \u0026quot;restGateway\u0026quot;: \u0026quot;https://10.1.1.2\u0026quot;, \u0026quot;arrayId\u0026quot;: \u0026quot;APM00******2\u0026quot;, \u0026quot;insecure\u0026quot;: true } ] } kubectl create secret generic unity-creds -n unity --from-file=config=secret.json\nUse the following command to replace or update the secret\nkubectl create secret generic unity-creds -n unity --from-file=config=secret.json -o yaml --dry-run | kubectl replace -f -\nNote: The user needs to validate the JSON syntax and array related key/values while replacing the unity-creds secret. The driver will continue to use previous values in case of an error found in the JSON file.\nNote: \u0026ldquo;isDefaultArray\u0026rdquo; parameter in values.yaml and secret.json should match each other.\n  Setup for snapshots\nThe Kubernetes Volume Snapshot feature is now beta in Kubernetes v1.17.\n  The following section summarizes the changes in the beta release.\nTo use the Kubernetes Volume Snapshot feature, ensure the following components have been deployed on your Kubernetes cluster.\n Install Snapshot Beta CRDs using the following command  kubectl apply -f https://raw.githubusercontent.com/kubernetes-csi/external-snapshotter/v3.0.3/client/config/crd/snapshot.storage.k8s.io_volumesnapshotclasses.yaml kubectl apply -f https://raw.githubusercontent.com/kubernetes-csi/external-snapshotter/v3.0.3config/crd/snapshot.storage.k8s.io_volumesnapshotcontents.yaml kubectl apply -f https://raw.githubusercontent.com/kubernetes-csi/external-snapshotter/v3.0.3/config/crd/snapshot.storage.k8s.io_volumesnapshots.yaml   Volume snapshot controller\n The manifests available on GitHub install v3.0.3 of the snapshotter image - quay.io/k8scsi/csi-snapshotter:v3.0.3 Dell recommends using v3.0.3 image of the snapshot-controller - quay.io/k8scsi/snapshot-controller:v3.0.3  After executing these commands, a snapshot-controller pod should be up and running.\n      Run the ./csi-install.sh --namespace unity --values ./myvalues.yaml command to proceed with the installation.\nA successful installation should emit messages that look similar to the following samples:\n------------------------------------------------------ \u0026gt; Installing CSI Driver: csi-unity on 1.19 ------------------------------------------------------ ------------------------------------------------------ \u0026gt; Checking to see if CSI Driver is already installed ------------------------------------------------------ ------------------------------------------------------ \u0026gt; Verifying Kubernetes and driver configuration ------------------------------------------------------ |- Kubernetes Version: 1.18 | |- Driver: csi-unity | |- Verifying Kubernetes versions | |--\u0026gt; Verifying minimum Kubernetes version Success | |--\u0026gt; Verifying maximum Kubernetes version Success | |- Verifying that required namespaces have been created Success | |- Verifying that required secrets have been created Success | |- Verifying that required secrets have been created Success | |- Verifying snapshot support | |--\u0026gt; Verifying that beta snapshot CRDs are available Success | |--\u0026gt; Verifying that beta snapshot controller is available Success | |- Verifying helm version Success ------------------------------------------------------ \u0026gt; Verification Complete ------------------------------------------------------ | |- Installing Driver Success | |--\u0026gt; Waiting for statefulset unity-controller to be ready Success | |--\u0026gt; Waiting for daemonset unity-node to be ready Success ------------------------------------------------------ \u0026gt; Operation complete ------------------------------------------------------ Results: At the end of the script statefulset unity-controller and daemonset unity-node is ready, execute command kubectl get pods -n unity to get the status of the pods and you will see the following:\n unity-controller-xxxx with 5/5 containers ready, and status displayed as Running.      Agent pods with 2/2 containers and the status displayed as Running.\nFinally, the script creates storageclasses such as, \u0026ldquo;unity\u0026rdquo;. Additional storage classes can be created for different combinations of file system types and Unity storage pools. The script also creates volumesnapshotclass \u0026ldquo;unity-snapclass\u0026rdquo;.\n  Certificate validation for Unisphere REST API calls This topic provides details about setting up the certificate validation for the CSI Driver for Dell EMC Unity.\nBefore you begin\nAs part of the CSI driver installation, the CSI driver requires a secret with the name unity-certs-0 to unity-certs-n based on \u0026ldquo;.Values.certSecretCount\u0026rdquo; parameter present in the namespace unity.\nThis secret contains the X509 certificates of the CA which signed the Unisphere SSL certificate in PEM format.\nIf the install script does not find the secret, it creates one empty secret with the name unity-certs-0.\nThe CSI driver exposes an install parameter in secret.json, which is like storageArrayList[i].insecure, which determines if the driver performs client-side verification of the Unisphere certificates.\nThe storageArrayList[i].insecure parameter set to true by default, and the driver does not verify the Unisphere certificates.\nIf the storageArrayList[i].insecure set to false, then the secret unity-certs-n must contain the CA certificate for Unisphere.\nIf this secret is empty secret, then the validation of the certificate fails, and the driver fails to start.\nIf the storageArrayList[i].insecure parameter set to false and a previous installation attempt created the empty secret, then this secret must be deleted and re-created using the CA certs.\nIf the Unisphere certificate is self-signed or if you are using an embedded Unisphere, then perform the following steps.\n To fetch the certificate, run the following command. openssl s_client -showcerts -connect \u0026lt;Unisphere IP:Port\u0026gt; \u0026lt;/dev/null 2\u0026gt;/dev/null | openssl x509 -outform PEM \u0026gt; ca_cert_0.pem Example: openssl s_client -showcerts -connect 1.1.1.1:443 \u0026lt;/dev/null 2\u0026gt;/dev/null | openssl x509 -outform PEM \u0026gt; ca_cert_0.pem Run the following command to create the cert secret with index \u0026lsquo;0\u0026rsquo; kubectl create secret generic unity-certs-0 --from-file=cert-0=ca_cert_0.pem -n unity Use the following command to replace the secret kubectl create secret generic unity-certs-0 -n unity --from-file=cert-0=ca_cert_0.pem -o yaml --dry-run | kubectl replace -f - Repeat step 1 and 2 to create multiple cert secrets with incremental index (example: unity-certs-1, unity-certs-2, etc)  Note: \u0026ldquo;unity\u0026rdquo; is the namespace for helm based installation but namespace can be user defined in operator based installation.\nNote: User can add multiple certificates in the same secret. The certificate file should not exceed more than 1Mb due to kubernetes secret size limitation.\nNote: Whenever certSecretCount parameter changes in myvalues.yaml user needs to uninstall and install the driver.\n","excerpt":"The CSI Driver for Dell EMC Unity can be deployed by using the provided Helm v3 charts and …","ref":"/ansible-docs/v2/installation/helm/unity/","title":"Unity"},{"body":"Installing Unity CSI Driver via Operator The CSI Driver for Dell EMC Unity can be installed via the Dell CSI Operator.\nTo deploy the Operator, follow the instructions available here.\nThere are sample manifests provided which can be edited to do an easy installation of the driver. Note that the deployment of the driver using the operator does not use any Helm charts and the installation and configuration parameters will be slightly different from the ones specified via the Helm installer.\nKubernetes Operators make it easy to deploy and manage entire lifecycle of complex Kubernetes applications. Operators use Custom Resource Definitions (CRD) which represents the application and use custom controllers to manage them.\nInstall Driver   Create namespace run kubectl create namespace test-unity to create the a namespace called test-unity. It can be any user-defined name.\n  Create unity-creds\nCreate secret mentioned in Install csi-driver section. The secret should be created in user-defined namespace (test-unity, in this case)\n  Create certificate secrets\nAs part of the CSI driver installation, the CSI driver requires a secret with the name unity-certs-0 to unity-certs-n in the user-defined namespace (test-unity, in this case) Create certificate procedure explained in the link\nNote: \u0026lsquo;certSecretCount\u0026rsquo; parameter is not required for operator. Based on secret name pattern (unity-certs-*) operator reads all the secrets. Secret name suffix should have 0 to N order to read the secrets. Secrets will not be considered, if any number missing in suffix.\nExample: If unity-certs-0, unity-certs-1, unity-certs-3 are present in the namespace, then only first two secrets are considered for SSL verification.\n  Create a CR (Custom Resource) for unity using the sample provided below\n  Create a new file csiunity.yaml by referring the following content. Replace the given sample values according to your environment. You can find may CRDs under deploy/crds folder when you install dell-csi-operator\napiVersion:storage.dell.com/v1kind:CSIUnitymetadata:name:test-unitynamespace:test-unityspec:driver:configVersion:v3replicas:2common:image:\u0026#34;dellemc/csi-unity:v1.4.0.000R\u0026#34;imagePullPolicy:IfNotPresentenvs:- name:X_CSI_UNITY_DEBUGvalue:\u0026#34;true\u0026#34;sideCars:- name:provisionerargs:[\u0026#34;--volume-name-prefix=csiunity\u0026#34;]storageClass:- name:virt2016****-fcdefault:truereclaimPolicy:\u0026#34;Delete\u0026#34;allowVolumeExpansion:trueparameters:storagePool:pool_1arrayId:\u0026#34;VIRT2016****\u0026#34;protocol:\u0026#34;FC\u0026#34;- name:virt2017****-iscsireclaimPolicy:\u0026#34;Delete\u0026#34;allowVolumeExpansion:trueparameters:storagePool:pool_1arrayId:\u0026#34;VIRT2017****\u0026#34;protocol:\u0026#34;iSCSI\u0026#34;- name:virt2017****-nfsreclaimPolicy:\u0026#34;Delete\u0026#34;allowVolumeExpansion:trueparameters:storagePool:pool_1arrayId:\u0026#34;VIRT2017****\u0026#34;protocol:\u0026#34;NFS\u0026#34;hostIoSize:\u0026#34;8192\u0026#34;nasServer:nas_1- name:virt2017****-iscsi-topologyreclaimPolicy:\u0026#34;Delete\u0026#34;allowVolumeExpansion:truevolumeBindingMode:WaitForFirstConsumerallowedTopologies:- matchLabelExpressions:- key:csi-unity.dellemc.com/virt2017****-iscsivalues:- \u0026#34;true\u0026#34;parameters:storagePool:pool_1arrayId:\u0026#34;VIRT2017****\u0026#34;protocol:\u0026#34;iSCSI\u0026#34;snapshotClass:- name:test-snapparameters:retentionDuration:\u0026#34;\u0026#34;  Execute the following command to create unity custom resource kubectl create -f csiunity.yaml. This command will deploy the csi-unity driver in the test-unity namespace.\n  Any deployment error can be found out by logging the operator pod which is in default namespace (example, kubectl logs dell-csi-operator-64c58559f6-cbgv7)\n  Users should configure the parameters in CR. The following table lists the primary configurable parameters of the Unity driver and their default values:\n   Parameter Description Required Default     Common parameters for node and controller      CSI_ENDPOINT Specifies the HTTP endpoint for Unity. No /var/run/csi/csi.sock   X_CSI_DEBUG To enable debug mode No false   GOUNITY_DEBUG To enable debug mode for gounity library No false   Controller parameters      X_CSI_MODE Driver starting mode No controller   X_CSI_UNITY_AUTOPROBE To enable auto probing for driver No true   Node parameters      X_CSI_MODE Driver starting mode No node   X_CSI_ISCSI_CHROOT Path to which the driver will chroot before running any iscsi commands. No /noderoot      ","excerpt":"Installing Unity CSI Driver via Operator The CSI Driver for Dell EMC Unity can be installed via the …","ref":"/ansible-docs/v2/installation/operator/unity/","title":"Unity"},{"body":"In the repository, a simple test manifest exists that creates three different PersistentVolumeClaims using default NFS and iSCSI and FC storage classes, and automatically mounts them to the pod.\nSteps\n To run this test, run the kubectl command from the root directory of the repository: kubectl create -f ./tests/sample.yaml   You can find all the created resources in test-unity namespace.\n Check if the pod is created and Ready and Running by running:\nkubectl get all -n test-unity If it\u0026rsquo;s in CrashLoopback state then the driver installation wasn\u0026rsquo;t successful. Check the logs of the node and the controller.\n  Go into the created container and verify that everything is mounted correctly.\n  After verifying, you can uninstall the testing PVCs and StatefulSet.\nkubectl delete -f ./tests/sample.yaml   ","excerpt":"In the repository, a simple test manifest exists that creates three different PersistentVolumeClaims …","ref":"/ansible-docs/v2/installation/test/unity/","title":"Test Unity CSI Driver"},{"body":"Release Notes - CSI Unity v1.4.0 New Features/Changes  Added support for OpenShift 4.5/4.6 with RHEL and CoreOS worker nodes Added support for Controller high availability (multiple-controllers) Added support for Ubuntu 20.04 Added support for Red Hat Enterprise Linux (RHEL) 7.9 Added support for Docker EE 3.1 Added support for Topology Added support for ephemeral volumes Added raw-block volume creation capability for iSCSI and FC based volumes. Added support for Mount options Changed driver base image to UBI 8.x  Fixed Issues  Source NFS PVC cannot be deleted if cloned NFS PVC exists.  Known Issues    Issue Workaround     Topology related node labels are not removed automatically. Currently, when the driver is uninstalled, topology related node labels are not getting removed automatically. There is an open issue in the Kubernetes to fix this. Until the fix is released we need to manually remove the node labels mentioned here https://github.com/dell/csi-unity#known-issues (Point 1)   Dynamic array detection will not work in Topology based environment Whenever a new array is added or removed, then the driver should be restarted with the below command only if the topology-based storage classes are used. Otherwise, the driver will automatically detect the newly added or removed arrays https://github.com/dell//csi-unity#known-issues (Point 2)   If source PVC is deleted when cloned PVC exists, then source PVC will be deleted in cluster but on array it will still be present and marked for deletion. All the cloned PVC should be deleted in order to delete the source PVC from array.   PVC creation fails on a fresh cluster with iSCSI and NFS protocols alone enabled with error failed to provision volume with StorageClass \u0026ldquo;unity-iscsi\u0026rdquo;: error generating accessibility requirements: no available topology found. This is because iSCSI initiator login takes longer than the node pod startup time. This can be overcome by bouncing the node pods in the cluster using the below command the driver pods with kubectl get pods -n unity --no-headers=true   PVC creation fails on a cluster with only NFS protocol enabled with error failed to provision volume with StorageClass \u0026ldquo;unity-nfs\u0026rdquo;: error generating accessibility requirements: no available topology found. For NFS volume and pod creation to succeed there must be minimum one worker node with iSCSI support and with a successful iSCSI login in to the array. Following commands can be used as a reference (which should be executed on worker node with iSCSI support) iscsiadm -m discovery -t st -p \u0026lt;iscsi-interface-ip\u0026gt; iscsiadm -m node -T \u0026lt;target-iqn\u0026gt; -l   On deleting pods sometimes the corresponding volumeattachment will not get removed. This issue is intermittent and happens with one specific protocol (FC, iSCSI or NFS) based StorageClasses. This issue occurs across Kubernetes versions 1.18 and 1.19 and both versions of OpenShift (4.5/4.6). On deleting the stale volumeattachment manually, Controller Unpublish gets invoked and then the corresponding PVCs can be deleted.    ","excerpt":"Release Notes - CSI Unity v1.4.0 New Features/Changes  Added support for OpenShift 4.5/4.6 with RHEL …","ref":"/ansible-docs/v2/release/unity/","title":"Unity"},{"body":"    Symptoms Prevention, Resolution or Workaround     When you run the command kubectl describe pods unity-controller-\u0026lt;suffix\u0026gt; –n unity, the system indicates that the driver image could not be loaded. You may need to put an insecure-registries entry in /etc/docker/daemon.json or login to the docker registry   The kubectl logs -n unity unity-node-\u0026lt;suffix\u0026gt; driver logs shows that the driver can\u0026rsquo;t connect to Unity - Authentication failure. Check if you\u0026rsquo;ve created secret with correct credentials    ","excerpt":"Symptoms Prevention, Resolution or Workaround     When you run the command kubectl describe pods …","ref":"/ansible-docs/v2/troubleshooting/unity/","title":"Unity"}]