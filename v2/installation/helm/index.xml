<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Dell Technologies â€“ CSI Driver installation using Helm</title>
    <link>/v2/installation/helm/</link>
    <description>Recent content in CSI Driver installation using Helm on Dell Technologies</description>
    <generator>Hugo -- gohugo.io</generator>
    
	  <atom:link href="/v2/installation/helm/index.xml" rel="self" type="application/rss+xml" />
    
    
      
        
      
    
    
    <item>
      <title>V2: PowerFlex</title>
      <link>/v2/installation/helm/powerflex/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/v2/installation/helm/powerflex/</guid>
      <description>
        
        
        &lt;p&gt;The CSI Driver for Dell EMC PowerFlex can be deployed by using the provided Helm v3 charts and installation scripts on both Kubernetes and OpenShift platforms. For more detailed information on the installation scripts, please review the script &lt;a href=&#34;https://github.com/dell/csi-unity/tree/master/dell-csi-helm-installer&#34;&gt;documentation&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The controller section of the Helm chart installs the following components in a &lt;em&gt;Deployment&lt;/em&gt; in the namespace &lt;code&gt;vxflexos&lt;/code&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;CSI Driver for Dell EMC PowerFlex&lt;/li&gt;
&lt;li&gt;Kubernetes External Provisioner, which provisions the volumes&lt;/li&gt;
&lt;li&gt;Kubernetes External Attacher, which attaches the volumes to the containers&lt;/li&gt;
&lt;li&gt;Kubernetes External Snapshotter, which provides snapshot support&lt;/li&gt;
&lt;li&gt;Kubernetes External Resizer, which resizes the volume&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The node section of the Helm chart installs the following component in a &lt;em&gt;DaemonSet&lt;/em&gt; in the namespace &lt;code&gt;vxflexos&lt;/code&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;CSI Driver for Dell EMC PowerFlex&lt;/li&gt;
&lt;li&gt;Kubernetes Node Registrar, which handles the driver registration&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;prerequisites&#34;&gt;Prerequisites&lt;/h2&gt;
&lt;p&gt;The following are requirements must be met before installing the CSI Driver for Dell EMC PowerFlex:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Install Kubernetes (1.17, 1.18, 1.19) or OpenShift (4.5 or 4.6)&lt;/li&gt;
&lt;li&gt;Install Helm 3&lt;/li&gt;
&lt;li&gt;Enable Zero Padding on PowerFlex&lt;/li&gt;
&lt;li&gt;Configure Mount propagation on container runtime (i.e. Docker)&lt;/li&gt;
&lt;li&gt;Install PowerFlex Storage Data Client&lt;/li&gt;
&lt;li&gt;Volume Snapshot requirements&lt;/li&gt;
&lt;li&gt;A user must exist on the array with a role &lt;em&gt;&amp;gt;= FrontEndConfigure&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;install-helm-30&#34;&gt;Install Helm 3.0&lt;/h3&gt;
&lt;p&gt;Install Helm 3.0 on the master node before you install the CSI Driver for Dell EMC PowerFlex.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Steps&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Run the &lt;code&gt;curl https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3 | bash&lt;/code&gt; command to install Helm 3.0.&lt;/p&gt;
&lt;h3 id=&#34;enable-zero-padding-on-powerflex&#34;&gt;Enable Zero Padding on PowerFlex&lt;/h3&gt;
&lt;p&gt;Verify that zero padding is enabled on the PowerFlex storage pools that will be used. Use PowerFlex GUI or the PowerFlex CLI to check this setting. See &lt;a href=&#34;https://cpsdocs.dellemc.com/bundle/PF_CONF_CUST/page/GUID-D32BDFF7-3014-4894-8E1E-2A31A86D343A.html&#34;&gt;Dell EMC PowerFlex documentation&lt;/a&gt; for more information to configure this setting.&lt;/p&gt;
&lt;h3 id=&#34;configure-mount-propagation-on-container-runtime&#34;&gt;Configure Mount Propagation on Container Runtime&lt;/h3&gt;
&lt;p&gt;It is required to configure mount propagation on your container runtime on all Kubernetes nodes before installing the CSI Driver for Dell EMC PowerFlex. The following is instruction on how to do this with Docker. If you use another container runtime, follow the recommended instructions from the vendor to configure mount propagation.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Steps&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Edit the service section of &lt;code&gt;/etc/systemd/system/multi-user.target.wants/docker.service&lt;/code&gt; file to add the following lines:
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;docker.service
&lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;[&lt;/span&gt;Service&lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;]&lt;/span&gt;...
&lt;span style=&#34;color:#000&#34;&gt;MountFlags&lt;/span&gt;&lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;=&lt;/span&gt;shared
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;Restart the docker service with &lt;code&gt;systemctl daemon-reload&lt;/code&gt; and &lt;code&gt;systemctl restart docker&lt;/code&gt; on all the nodes.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: Some distribution, like Ubuntu, already has &lt;em&gt;MountFlags&lt;/em&gt; set by default&lt;/p&gt;
&lt;h3 id=&#34;install-powerflex-storage-data-client&#34;&gt;Install PowerFlex Storage Data Client&lt;/h3&gt;
&lt;p&gt;The CSI Driver for PowerFlex requires you to have installed the PowerFlex Storage Data Client (SDC) on all worker nodes. If installing on Red Hat CoreOS (RHCOS) nodes on OpenShift you can install using the automated SDC deployment feature. If installing on non-RHCOS nodes, you must install SDC manually.&lt;/p&gt;
&lt;h4 id=&#34;automatic-sdc-deployment&#34;&gt;Automatic SDC Deployment&lt;/h4&gt;
&lt;p&gt;The automated deployment of the SDC runs by default when installing the driver. It installs an SDC container to faciliate the installation. While the install is automated there are a few configuration options for this feature. Those are referenced in the &lt;strong&gt;Install the Driver&lt;/strong&gt; section. More details on how the automatic SDC deployment works can be found in the Feature section of this site on the PowerFlex page.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Optional:&lt;/strong&gt; For a typical install, you will pull SDC kernel modules from the Dell EMC ftp site, which is setup by default. Some users might want to mirror this repository to a local location. The PowerFlex documentation has instructions on how to do this. If a mirror is used, you need to create an SDC repo secret for managing the credentials to the mirror. Details on how to create the secret are in the &lt;strong&gt;Install the Driver&lt;/strong&gt; section.&lt;/p&gt;
&lt;h4 id=&#34;manually-sdc-deployment&#34;&gt;Manually SDC Deployment&lt;/h4&gt;
&lt;p&gt;For detailed PowerFlex installation procedure, see the &lt;em&gt;Dell EMC PowerFlex Deployment Guide&lt;/em&gt;. Install the PowerFlex SDC as follows:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Steps&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Download the PowerFlex SDC from &lt;a href=&#34;https://www.dell.com/support&#34;&gt;Dell EMC Online support&lt;/a&gt;. The filename is EMC-ScaleIO-sdc-*.rpm, where * is the SDC name corresponding to the PowerFlex installation version.&lt;/li&gt;
&lt;li&gt;Export the shell variable &lt;em&gt;MDM_IP&lt;/em&gt; in a comma-separated list using &lt;code&gt;export MDM_IP=xx.xxx.xx.xx,xx.xxx.xx.xx&lt;/code&gt;, where xxx represents the actual IP address in your environment. This list contains the IP addresses of the MDMs.&lt;/li&gt;
&lt;li&gt;Install the SDC per the &lt;em&gt;Dell EMC PowerFlex Deployment Guide&lt;/em&gt;:
&lt;ul&gt;
&lt;li&gt;For Red Hat Enterprise Linux and Cent OS, run &lt;code&gt;rpm -iv ./EMC-ScaleIO-sdc-*.x86_64.rpm&lt;/code&gt;, where * is the SDC name corresponding to the PowerFlex installation version.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;volume-snapshot-requirements&#34;&gt;Volume Snapshot requirements&lt;/h3&gt;
&lt;h4 id=&#34;volume-snapshot-crds&#34;&gt;Volume Snapshot CRD&amp;rsquo;s&lt;/h4&gt;
&lt;p&gt;The Kubernetes Volume Snapshot CRDs can be obtained and installed from the external-snapshotter project on &lt;a href=&#34;https://github.com/kubernetes-csi/external-snapshotter/tree/v3.0.3/client/config/crd&#34;&gt;Github&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;You can also install the CRDs by supplying the option &lt;em&gt;--snapshot-crd&lt;/em&gt; while installing the driver using the &lt;code&gt;csi-install.sh&lt;/code&gt; script.&lt;/p&gt;
&lt;h4 id=&#34;volume-snapshot-controller&#34;&gt;Volume Snapshot Controller&lt;/h4&gt;
&lt;p&gt;Starting with beta Volume Snapshots in Kubernetes 1.17, the CSI external-snapshotter sidecar is split into two controllers:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;A common snapshot controller&lt;/li&gt;
&lt;li&gt;A CSI external-snapshotter sidecar&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The common snapshot controller must be installed only once in the cluster irrespective of the number of CSI drivers installed in the cluster. On OpenShift clusters 4.4 and later, the common snapshot-controller is pre-installed. In the clusters where it is not present, it can be installed using &lt;code&gt;kubectl&lt;/code&gt; and the manifests available on &lt;a href=&#34;https://github.com/kubernetes-csi/external-snapshotter/tree/v3.0.3/deploy/kubernetes/snapshot-controller&#34;&gt;GitHub&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;NOTE:&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The manifests available on GitHub install v3.0.3 of the snapshotter image - &lt;a href=&#34;https://quay.io/repository/k8scsi/csi-snapshotter?tag=v3.0.3&amp;amp;tab=tags&#34;&gt;quay.io/k8scsi/csi-snapshotter:v3.0.3&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Dell recommends using v3.0.3 image of the snapshot-controller - &lt;a href=&#34;https://quay.io/repository/k8scsi/snapshot-controller?tag=v3.0.3&amp;amp;tab=tags&#34;&gt;quay.io/k8scsi/snapshot-controller:v3.0.3&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;The CSI external-snapshotter sidecar is still installed along with the driver and does not involve any extra configuration.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;install-the-driver&#34;&gt;Install the Driver&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Steps&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Run &lt;code&gt;git clone https://github.com/dell/csi-powerflex.git&lt;/code&gt; to clone the git repository.&lt;/li&gt;
&lt;li&gt;Ensure that you have created namespace where you want to install the driver. You can run &lt;code&gt;kubectl create namespace vxflexos&lt;/code&gt; to create a new one.&lt;/li&gt;
&lt;li&gt;Check helm/csi-vxflexos/driver-image.yaml and confirm the driver image points to new image.&lt;/li&gt;
&lt;li&gt;Edit the &lt;code&gt;helm/secret.yaml&lt;/code&gt;, point to correct namespace and replace the values for the username and password parameters.
These values can be obtained using base64 encoding as described in the following example:
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;color:#204a87&#34;&gt;echo&lt;/span&gt; -n &lt;span style=&#34;color:#4e9a06&#34;&gt;&amp;#34;myusername&amp;#34;&lt;/span&gt; &lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;|&lt;/span&gt; base64
&lt;span style=&#34;color:#204a87&#34;&gt;echo&lt;/span&gt; -n &lt;span style=&#34;color:#4e9a06&#34;&gt;&amp;#34;mypassword&amp;#34;&lt;/span&gt; &lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;|&lt;/span&gt; base64
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;where &lt;em&gt;myusername&lt;/em&gt; &amp;amp; &lt;em&gt;mypassword&lt;/em&gt; are credentials for a user with PowerFlex priviledges.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;Create the secret by running &lt;code&gt;kubectl create -f secret.yaml&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;If not using automated SDC deployment, create a dummy SDC repo secret file:  &lt;code&gt;kubectl create -f sdc-repo-secret.yaml&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;If using automated SDC deployment:
&lt;ul&gt;
&lt;li&gt;Check the SDC container image is the correct version for your version of PowerFlex.&lt;/li&gt;
&lt;li&gt;Create a secret for the SDC repo credentials and provide the URL for the repo.
&lt;ul&gt;
&lt;li&gt;To create the secret, you must update the details in helm/sdc-repo-secret.yaml file and running &lt;code&gt;kubectl create -f sdc-repo-secret.yaml&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;To set the repo URL, you must set the &lt;code&gt;repoUrl&lt;/code&gt; parameter in the &lt;code&gt;myvalues.yaml&lt;/code&gt; file.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Collect information from the PowerFlex SDC by executing the &lt;code&gt;get_vxflexos_info.sh&lt;/code&gt; script located in the top-level helm directory.  This script shows the &lt;em&gt;VxFlex OS system ID&lt;/em&gt; and &lt;em&gt;MDM IP&lt;/em&gt; addresses. Make a note of the value for these parameters as they must be entered in the &lt;code&gt;myvalues.yaml&lt;/code&gt; file.
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;NOTE:&lt;/em&gt; Your SDC might have multiple VxFlex OS systems registered. Ensure that you choose the correct values.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Copy the default values.yaml file &lt;code&gt;cd helm &amp;amp;&amp;amp; cp csi-vxflexos/values.yaml myvalues.yaml&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Edit the newly created values file and provide values for the following parameters &lt;code&gt;vi myvalues.yaml&lt;/code&gt;:&lt;/li&gt;
&lt;/ol&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Parameter&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;th&gt;Required&lt;/th&gt;
&lt;th&gt;Default&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;systemName&lt;/td&gt;
&lt;td&gt;Set to the PowerFlex/VxFlex OS system name or system ID to be used with the driver.&lt;/td&gt;
&lt;td&gt;Yes&lt;/td&gt;
&lt;td&gt;&amp;ldquo;systemname&amp;rdquo;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;restGateway&lt;/td&gt;
&lt;td&gt;Set to the URL of your systemâ€™s REST API Gateway. You can obtain this value from the PowerFlex administrator.&lt;/td&gt;
&lt;td&gt;Yes&lt;/td&gt;
&lt;td&gt;&amp;ldquo;https://123.0.0.1&amp;rdquo;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;storagePool&lt;/td&gt;
&lt;td&gt;Set to a default (existing) storage pool name in your PowerFlex/VxFlex OS system.&lt;/td&gt;
&lt;td&gt;Yes&lt;/td&gt;
&lt;td&gt;&amp;ldquo;sp&amp;rdquo;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;volumeNamePrefix&lt;/td&gt;
&lt;td&gt;Set so that volumes created by the driver have a default prefix. If one PowerFlex/VxFlex OS system is   servicing several different Kubernetes installations or users, these prefixes help you distinguish them.&lt;/td&gt;
&lt;td&gt;No&lt;/td&gt;
&lt;td&gt;&amp;ldquo;k8s&amp;rdquo;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;controllerCount&lt;/td&gt;
&lt;td&gt;Set to deploy multiple controller instances. If controller count is greater than the number of available nodes, excess pods will be left in pending state. You can increase number of available nodes by configuring the &amp;ldquo;controller&amp;rdquo; section in your values.yaml. For more details on the new controller pod configurations, see the Features section for Powerflex specifics.&lt;/td&gt;
&lt;td&gt;Yes&lt;/td&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;enablelistvolumesnapshot&lt;/td&gt;
&lt;td&gt;Set to have snapshots included in the CSI operation ListVolumes. Disabled by default.&lt;/td&gt;
&lt;td&gt;No&lt;/td&gt;
&lt;td&gt;FALSE&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;StorageClass&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;Helm charts create a Kubernetes StorageClass while deploying CSI Driver for Dell EMC PowerFlex. This section   includes relevant variables.&lt;/td&gt;
&lt;td&gt;-&lt;/td&gt;
&lt;td&gt;-&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;name&lt;/td&gt;
&lt;td&gt;Defines the name of the Kubernetes storage class that the Helm charts will create. For example, the   &lt;em&gt;vxflexos&lt;/em&gt; base name will be used to generate names such as &lt;em&gt;vxflexos&lt;/em&gt; and   &lt;em&gt;vxflexos-xfs&lt;/em&gt;.&lt;/td&gt;
&lt;td&gt;No&lt;/td&gt;
&lt;td&gt;&amp;ldquo;vxflexos&amp;rdquo;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;isDefault&lt;/td&gt;
&lt;td&gt;Sets the newly created storage class as default for Kubernetes. Set this value to &lt;code&gt;true&lt;/code&gt; only if you expect   PowerFlex to be your principle storage provider, as it will be used in PersitentVolumeClaims where no storageclass is provided. After installation, you can add custom storage classes, if desired.&lt;/td&gt;
&lt;td&gt;No&lt;/td&gt;
&lt;td&gt;TRUE&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;reclaimPolicy&lt;/td&gt;
&lt;td&gt;Defines whether the volumes will be retained or deleted when the assigned pod is destroyed. The valid values for this variable are &lt;code&gt;Retain&lt;/code&gt; or &lt;code&gt;Delete&lt;/code&gt;.&lt;/td&gt;
&lt;td&gt;No&lt;/td&gt;
&lt;td&gt;&amp;ldquo;Delete&amp;rdquo;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;controller&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;This section allows configuration of controller specific parameters. To maximize the number of available nodes for controller pods, see this section. For more details on the new controller pod configurations, see the &lt;a href=&#34;../../../features/powerflex/&#34;&gt;Features section&lt;/a&gt; for Powerflex specifics.&lt;/td&gt;
&lt;td&gt;-&lt;/td&gt;
&lt;td&gt;-&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;nodeSelector&lt;/td&gt;
&lt;td&gt;Defines what nodes would be selected for pods of controller deployment. Leave as blank to use all nodes. Uncomment this section to deploy on master nodes exclusively.&lt;/td&gt;
&lt;td&gt;No&lt;/td&gt;
&lt;td&gt;&amp;quot; &amp;quot;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;tolerations&lt;/td&gt;
&lt;td&gt;Defines tolerations that would be applied to controller deployment. Leave as blank to install controller on worker nodes only. If deploying on master nodes is desired, uncomment out this section.&lt;/td&gt;
&lt;td&gt;No&lt;/td&gt;
&lt;td&gt;&amp;quot; &amp;quot;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;monitor&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;This section allows configuration of the SDC monitoring pod.&lt;/td&gt;
&lt;td&gt;-&lt;/td&gt;
&lt;td&gt;-&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;enabled&lt;/td&gt;
&lt;td&gt;Set to enable the usage of the monitoring pod.&lt;/td&gt;
&lt;td&gt;No&lt;/td&gt;
&lt;td&gt;FALSE&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;hostNetwork&lt;/td&gt;
&lt;td&gt;Set whether the monitor pod should run on the host network or not.&lt;/td&gt;
&lt;td&gt;No&lt;/td&gt;
&lt;td&gt;TRUE&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;hostPID&lt;/td&gt;
&lt;td&gt;Set whether the monitor pod should run in the host namespace or not.&lt;/td&gt;
&lt;td&gt;No&lt;/td&gt;
&lt;td&gt;TRUE&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;sdcKernelMirror&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;[RHCOS only] The PowerFlex SDC may need to pull a new module that is known to work with newer Linux kernels. The default location of this mirror os at ftp.emc.com. The PowerFlex documentation has instructions for methods to mirror this repository to a local location if necessary.&lt;/td&gt;
&lt;td&gt;-&lt;/td&gt;
&lt;td&gt;-&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;repoUrl&lt;/td&gt;
&lt;td&gt;Set the URL of the ftp mirror containing SDC kernel modules. Only ftp locations are allowed. A blank string signifies the default mirror, which is &amp;ldquo;&lt;a href=&#34;ftp://ftp.emc.com&#34;&gt;ftp://ftp.emc.com&lt;/a&gt;&amp;rdquo;.&lt;/td&gt;
&lt;td&gt;No&lt;/td&gt;
&lt;td&gt;&amp;quot; &amp;quot;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;11. Install the driver using &lt;code&gt;csi-install.sh&lt;/code&gt; bash script by running &lt;code&gt;cd ../dell-csi-helm-installer &amp;amp;&amp;amp; ./csi-install.sh --namespace vxflexos --values ../helm/myvalues.yaml&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;em&gt;NOTE:&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;For detailed instructions on how to run the install scripts, refer to the README.md  in the dell-csi-helm-installer folder.&lt;/li&gt;
&lt;li&gt;This script also runs the verify.sh script that is present in the same directory. You will be prompted to enter the credentials for each of the Kubernetes nodes. The &lt;code&gt;verify.sh&lt;/code&gt; script needs the credentials to check if SDC has been configured on all nodes. You can also skip the verification step by specifiying the &lt;code&gt;--skip-verify-node&lt;/code&gt; option.&lt;/li&gt;
&lt;li&gt;(Optional) Enable additional Mount Options - A user is able to specify additional mount options as needed for the driver.
&lt;ul&gt;
&lt;li&gt;Mount options are specified in storageclass yaml under &lt;em&gt;mountOptions&lt;/em&gt;.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;WARNING&lt;/em&gt;: Before utilizing mount options, you must first be fully aware of the potential impact and understand your environment&amp;rsquo;s requirements for the specified option.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;storage-classes&#34;&gt;Storage Classes&lt;/h2&gt;
&lt;p&gt;As part of the driver installation, a set of storage classes is created along with the driver pods. This is done to demonstrate how storage classes need to be created to consume storage from Dell EMC storage arrays.&lt;/p&gt;
&lt;p&gt;The &lt;code&gt;StorageClass&lt;/code&gt; object in Kubernetes is immutable and can&amp;rsquo;t be modified once created. It creates challenges when we need to change or update a parameter, for example when a version of the driver introduces new configurable parameters for the storage classes. To avoid issues during upgrades, future releases of the drivers will have the installation separated from the creation of Storage Classes.
In preparation for that, starting in Q4 of 2020, an annotation &lt;code&gt;&amp;quot;helm.sh/resource-policy&amp;quot;: keep&lt;/code&gt; is applied to the storage classes created by the &lt;code&gt;dell-csi-helm-installer&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Because of this annotation, these storage classes are not going to be deleted even after the driver has been uninstalled.
This annotation has been applied to give you an opportunity to keep using  these storage classes even with a future release of the driver. In case you wish to not use these storage classes, you will need to delete them by using the &lt;code&gt;kubectl delete storageclass&lt;/code&gt; command.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;NOTE&lt;/em&gt;: If you uninstall the driver and reinstall it, you can still face errors if any update in the &lt;code&gt;values.yaml&lt;/code&gt; file leads to an update of the storage class(es):&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;    Error: cannot patch &amp;quot;&amp;lt;sc-name&amp;gt;&amp;quot; with kind StorageClass: StorageClass.storage.k8s.io &amp;quot;&amp;lt;sc-name&amp;gt;&amp;quot; is invalid: parameters: Forbidden: updates to parameters are forbidden
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;In case you want to make such updates, make sure to delete the existing storage classes using the &lt;code&gt;kubectl delete storageclass&lt;/code&gt; command.&lt;br&gt;
Deleting a storage class has no impact on a running Pod with mounted PVCs. You won&amp;rsquo;t be able to provision new PVCs until at least one storage class is newly created.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>V2: PowerMax</title>
      <link>/v2/installation/helm/powermax/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/v2/installation/helm/powermax/</guid>
      <description>
        
        
        &lt;p&gt;The CSI Driver for Dell EMC PowerMax can be deployed by using the provided Helm v3 charts and installation scripts on both Kubernetes and OpenShift platforms. For more detailed information on the installation scripts, review the script &lt;a href=&#34;https://github.com/dell/csi-unity/tree/master/dell-csi-helm-installer&#34;&gt;documentation&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The controller section of the Helm chart installs the following components in a &lt;em&gt;Deployment&lt;/em&gt; in the &lt;code&gt;powermax&lt;/code&gt; namespace:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;CSI Driver for Dell EMC PowerMax&lt;/li&gt;
&lt;li&gt;Kubernetes External Provisioner, which provisions the volumes&lt;/li&gt;
&lt;li&gt;Kubernetes External Attacher, which attaches the volumes to the containers&lt;/li&gt;
&lt;li&gt;Kubernetes External Snapshotter, which provides snapshot support&lt;/li&gt;
&lt;li&gt;Kubernetes External Resizer, which resizes the volume&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The node section of the Helm chart installs the following component in a &lt;em&gt;DaemonSet&lt;/em&gt; in the namespace &lt;code&gt;powermax&lt;/code&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;CSI Driver for Dell EMC PowerMax&lt;/li&gt;
&lt;li&gt;Kubernetes Node Registrar, which handles the driver registration&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;prerequisites&#34;&gt;Prerequisites&lt;/h2&gt;
&lt;p&gt;The following requirements must be met before installing the CSI Driver for Dell EMC PowerMax:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Install Kubernetes (1.17, 1.18, 1.19) or OpenShift (4.5 or 4.6)&lt;/li&gt;
&lt;li&gt;Install Helm 3&lt;/li&gt;
&lt;li&gt;Fibre Channel requirements&lt;/li&gt;
&lt;li&gt;iSCSI requirements&lt;/li&gt;
&lt;li&gt;Certificate validation for Unisphere REST API calls&lt;/li&gt;
&lt;li&gt;Configure Mount propagation on container runtime (that is, Docker)&lt;/li&gt;
&lt;li&gt;Linux multipathing requirements&lt;/li&gt;
&lt;li&gt;Volume Snapshot requirements&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;install-helm-3&#34;&gt;Install Helm 3&lt;/h3&gt;
&lt;p&gt;Install Helm 3 on the master node before you install the CSI Driver for Dell EMC PowerMax.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Steps&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Run the &lt;code&gt;curl https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3 | bash&lt;/code&gt; command to install Helm 3.&lt;/p&gt;
&lt;h3 id=&#34;fibre-channel-requirements&#34;&gt;Fibre Channel Requirements&lt;/h3&gt;
&lt;p&gt;CSI Driver for Dell EMC PowerMax supports Fibre Channel communication. Ensure that the following requirements are met before you install the CSI Driver:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Zoning of the Host Bus Adapters (HBAs) to the Fibre Channel port director must be completed.&lt;/li&gt;
&lt;li&gt;Ensure that the HBA WWNs (initiators) appear on the list of initiators that are logged into the array.&lt;/li&gt;
&lt;li&gt;If number of volumes that will be published to nodes is high, then configure the maximum number of LUNs for your HBAs on each node. See the appropriate HBA document to configure the maximum number of LUNs.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;iscsi-requirements&#34;&gt;iSCSI Requirements&lt;/h3&gt;
&lt;p&gt;The CSI Driver for Dell EMC PowerMax supports iSCSI connectivity. These requirements are applicable for the nodes that use iSCSI initiator to connect to the PowerMax arrays.&lt;/p&gt;
&lt;p&gt;Set up the iSCSI initiators as follows:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;All Kubernetes nodes must have the &lt;em&gt;iscsi-initiator-utils&lt;/em&gt; package installed.&lt;/li&gt;
&lt;li&gt;Ensure that the iSCSI initiators are available on all the nodes where the driver node plugin will be installed.&lt;/li&gt;
&lt;li&gt;Kubernetes nodes should have access (network connectivity) to an iSCSI director on the Dell EMC PowerMax array that has IP interfaces. Manually create IP routes for each node that connects to the Dell EMC PowerMax if required.&lt;/li&gt;
&lt;li&gt;Ensure that the iSCSI initiators on the nodes are not a part of any existing Host (Initiator Group) on the Dell EMC PowerMax array.&lt;/li&gt;
&lt;li&gt;The CSI Driver needs the port group names containing the required iSCSI director ports. These port groups must be set up on each Dell EMC PowerMax array. All the port groups names supplied to the driver must exist on each Dell EMC PowerMax with the same name.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For information about configuring iSCSI, see &lt;em&gt;Dell EMC PowerMax documentation&lt;/em&gt; on Dell EMC Support.&lt;/p&gt;
&lt;h3 id=&#34;certificate-validation-for-unisphere-rest-api-calls&#34;&gt;Certificate validation for Unisphere REST API calls&lt;/h3&gt;
&lt;p&gt;As part of the CSI driver installation, the CSI driver requires a secret with the name &lt;em&gt;powermax-certs&lt;/em&gt; present in the namespace &lt;em&gt;powermax&lt;/em&gt;. This secret contains the X509 certificates of the CA which signed the Unisphere SSL certificate in PEM format. This secret is mounted as a volume in the driver container. In earlier releases, if the install script did not find the secret, it created an empty secret with the same name. From the 1.2.0 release, the secret volume has been made optional. The install script no longer attempts to create an empty secret.&lt;/p&gt;
&lt;p&gt;The CSI driver exposes an install parameter &lt;code&gt;skipCertificateValidation&lt;/code&gt; which determines if the driver performs client-side verification of the Unisphere certificates. The &lt;code&gt;skipCertificateValidation&lt;/code&gt; parameter is set to &lt;em&gt;true&lt;/em&gt; by default, and the driver does not verify the Unisphere certificates.&lt;/p&gt;
&lt;p&gt;If the &lt;code&gt;skipCertificateValidation&lt;/code&gt; parameter is set to &lt;em&gt;false&lt;/em&gt; and a previous installation attempt created an empty secret, then this secret must be deleted and re-created using the CA certs.&lt;/p&gt;
&lt;p&gt;If the Unisphere certificate is self-signed or if you are using an embedded Unisphere, then perform the following steps:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;To fetch the certificate, run &lt;code&gt;openssl s_client -showcerts -connect [Unisphere IP]:8443 &amp;lt;/dev/null&amp;gt; /dev/null | openssl x509 -outform PEM &amp;gt; ca_cert.pem&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;NOTE&lt;/em&gt;: The IP address varies for each user.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;To create the secret, run &lt;code&gt;kubectl create secret generic powermax-certs --from-file=ca_cert.pem -n powermax&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;ports-in-port-group&#34;&gt;Ports in port group&lt;/h3&gt;
&lt;p&gt;There are no restrictions around how many ports can be present in the iSCSI port groups provided to the driver.&lt;/p&gt;
&lt;p&gt;The same applies to Fibre Channel where there are no restrictions on the number of FA directors a host HBA can be zoned to. See the best practices for host connectivity to Dell EMC PowerMax to ensure that you have multiple paths to your data volumes.&lt;/p&gt;
&lt;h3 id=&#34;configure-mount-propagation-on-container-runtime&#34;&gt;Configure Mount Propagation on Container Runtime&lt;/h3&gt;
&lt;p&gt;You must configure mount propagation on your container runtime on all Kubernetes nodes before installing the CSI Driver for Dell EMC PowerMax.  The following steps explain how to do this with Docker.  If you use another container runtime please follow the recommended instructions from the vendor to configure mount propagation.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Steps&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Edit the service section of &lt;code&gt;/etc/systemd/system/multi-user.target.wants/docker.service&lt;/code&gt; file to add the following lines:
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;docker.service
&lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;[&lt;/span&gt;Service&lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;]&lt;/span&gt;...
&lt;span style=&#34;color:#000&#34;&gt;MountFlags&lt;/span&gt;&lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;=&lt;/span&gt;shared
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;Restart the docker service with &lt;code&gt;systemctl daemon-reload&lt;/code&gt; and &lt;code&gt;systemctl restart docker&lt;/code&gt; on all the nodes.&lt;/li&gt;
&lt;li&gt;Restart the docker service with systemctl daemon-reload and systemctl restart docker on all the nodes.&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;linux-multipathing-requirements&#34;&gt;Linux multipathing requirements&lt;/h3&gt;
&lt;p&gt;CSI Driver for Dell EMC PowerMax supports Linux multipathing. Configure Linux multipathing before installing the CSI Driver.&lt;/p&gt;
&lt;p&gt;Set up Linux multipathing as follows:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;All the nodes must have &lt;em&gt;Device Mapper Multipathing&lt;/em&gt; package installed.&lt;br&gt;
&lt;em&gt;NOTE:&lt;/em&gt; When this package is installed it creates a multipath configuration file which is located at &lt;code&gt;/etc/multipath.conf&lt;/code&gt;. Please ensure that this file always exists.&lt;/li&gt;
&lt;li&gt;Enable multipathing using &lt;code&gt;mpathconf --enable --with_multipathd y&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Enable &lt;code&gt;user_friendly_names&lt;/code&gt; and &lt;code&gt;find_multipaths&lt;/code&gt; in the &lt;code&gt;multipath.conf&lt;/code&gt; file.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;volume-snapshot-requirements&#34;&gt;Volume Snapshot requirements&lt;/h3&gt;
&lt;h4 id=&#34;volume-snapshot-crds&#34;&gt;Volume Snapshot CRDs&lt;/h4&gt;
&lt;p&gt;The Kubernetes Volume Snapshot CRDs can be obtained and installed from the external-snapshotter project on &lt;a href=&#34;https://github.com/kubernetes-csi/external-snapshotter/tree/release-2.1/config/crd&#34;&gt;Github&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Alternately, you can install the CRDs by supplying the option &lt;em&gt;&amp;ndash;snapshot-crd&lt;/em&gt; while installing the driver using the &lt;code&gt;csi-install.sh&lt;/code&gt; script.&lt;/p&gt;
&lt;h4 id=&#34;volume-snapshot-controller&#34;&gt;Volume Snapshot Controller&lt;/h4&gt;
&lt;p&gt;Starting with the beta Volume Snapshots, the CSI external-snapshotter sidecar is split into two controllers:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;A common snapshot controller&lt;/li&gt;
&lt;li&gt;A CSI external-snapshotter sidecar&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The common snapshot controller must be installed only once in the cluster irrespective of the number of CSI drivers installed in the cluster. On OpenShift clusters 4.4 and later, the common snapshot-controller is pre-installed. In the clusters where it is not present, it can be installed using &lt;code&gt;kubectl&lt;/code&gt; and the manifests available on &lt;a href=&#34;https://github.com/kubernetes-csi/external-snapshotter/tree/v3.0.3/deploy/kubernetes/snapshot-controller&#34;&gt;GitHub&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;NOTE:&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The manifests available on the GitHub repository for snapshot controller will install v3.0.3 of the snapshotter controller - (k8s.gcr.io/sig-storage/snapshot-controller:v3.0.3)&lt;/li&gt;
&lt;li&gt;Dell EMC recommends using the v3.0.3 image of the CSI external snapshotter - (k8s.gcr.io/sig-storage/csi-snapshotter:v3.0.3)&lt;/li&gt;
&lt;li&gt;The CSI external-snapshotter sidecar is still installed with the driver and does not involve any extra configuration.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;install-the-driver&#34;&gt;Install the Driver&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Steps&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Run &lt;code&gt;git clone https://github.com/dell/csi-powermax.git&lt;/code&gt; to clone the git repository.  This will include the Helm charts and dell-csi-helm-installer scripts.&lt;/li&gt;
&lt;li&gt;Ensure that you have created a namespace where you want to install the driver. You can run &lt;code&gt;kubectl create namespace powermax&lt;/code&gt; to create a new one&lt;/li&gt;
&lt;li&gt;Edit the `helm/secret.yaml, point to the correct namespace and replace the values for the username and password parameters.
These values can be obtained using base64 encoding as described in the following example:
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;color:#204a87&#34;&gt;echo&lt;/span&gt; -n &lt;span style=&#34;color:#4e9a06&#34;&gt;&amp;#34;myusername&amp;#34;&lt;/span&gt; &lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;|&lt;/span&gt; base64
&lt;span style=&#34;color:#204a87&#34;&gt;echo&lt;/span&gt; -n &lt;span style=&#34;color:#4e9a06&#34;&gt;&amp;#34;mypassword&amp;#34;&lt;/span&gt; &lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;|&lt;/span&gt; base64
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;where &lt;em&gt;myusername&lt;/em&gt; and &lt;em&gt;mypassword&lt;/em&gt; are credentials for a user with PowerMax priviledges.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;Create the secret by running &lt;code&gt;kubectl create -f helm/secret.yaml&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;If you are going to install the new CSI PowerMax ReverseProxy service, create a TLS secret with the name - &lt;em&gt;csireverseproxy-tls-secret&lt;/em&gt; which holds a SSL certificate and the corresponding private key in the namespace where you are installing the driver.&lt;/li&gt;
&lt;li&gt;Copy the default values.yaml file `cd helm &amp;amp;&amp;amp; cp csi-powermax/values.yaml my-powermax-settings.yaml&lt;/li&gt;
&lt;li&gt;Edit the newly created file and provide values for the following parameters &lt;code&gt;vi my-powermax-settings.yaml&lt;/code&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Parameter&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;th&gt;Required&lt;/th&gt;
&lt;th&gt;Default&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;unisphere&lt;/td&gt;
&lt;td&gt;Specifies the URL of the Unisphere for PowerMax server. If using the CSI PowerMax Reverse Proxy, leave this value unchanged at https://127.0.0.1:8443.&lt;/td&gt;
&lt;td&gt;Yes&lt;/td&gt;
&lt;td&gt;&amp;ldquo;https://127.0.0.1:8443&amp;rdquo;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;clusterPrefix&lt;/td&gt;
&lt;td&gt;Prefix that is used during the creation of various masking-related entities (Storage Groups, Masking Views, Hosts, and Volume Identifiers) on the array. The value that you specify here must be unique. Ensure that no other CSI PowerMax driver is managing the same arrays that are configured with the same prefix. The maximum length for this   prefix is three characters.&lt;/td&gt;
&lt;td&gt;Yes&lt;/td&gt;
&lt;td&gt;&amp;ldquo;ABC&amp;rdquo;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;controller&lt;/td&gt;
&lt;td&gt;Allows configuration of the controller-specific parameters.&lt;/td&gt;
&lt;td&gt;-&lt;/td&gt;
&lt;td&gt;-&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;node&lt;/td&gt;
&lt;td&gt;Allows configuration of the node-specific parameters.&lt;/td&gt;
&lt;td&gt;-&lt;/td&gt;
&lt;td&gt;-&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;tolerations&lt;/td&gt;
&lt;td&gt;Add tolerations as per requirement&lt;/td&gt;
&lt;td&gt;No&lt;/td&gt;
&lt;td&gt;-&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;nodeSelector&lt;/td&gt;
&lt;td&gt;Add node selectors as per requirement&lt;/td&gt;
&lt;td&gt;No&lt;/td&gt;
&lt;td&gt;-&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;defaultFsType&lt;/td&gt;
&lt;td&gt;Used to set the default FS type for external provisioner&lt;/td&gt;
&lt;td&gt;Yes&lt;/td&gt;
&lt;td&gt;ext4&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;portGroups&lt;/td&gt;
&lt;td&gt;List of comma-separated port group names. Any port group that is specified here must be present on all the arrays that the driver manages.&lt;/td&gt;
&lt;td&gt;For iSCSI Only&lt;/td&gt;
&lt;td&gt;&amp;ldquo;PortGroup1, PortGroup2, PortGroup3&amp;rdquo;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;arrayWhitelist&lt;/td&gt;
&lt;td&gt;List of comma-separated array IDs. If this parameter remains empty, the driver manages all the arrays that are managed by the Unisphere instance that is configured for the driver.  Specify the IDs of the arrays that you want to manage, using the driver.&lt;/td&gt;
&lt;td&gt;No&lt;/td&gt;
&lt;td&gt;Empty&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;symmetrixID&lt;/td&gt;
&lt;td&gt;Specify a Dell EMC PowerMax array that the driver manages. This value is used to create a default storage class.&lt;/td&gt;
&lt;td&gt;Yes&lt;/td&gt;
&lt;td&gt;&amp;ldquo;000000000000&amp;rdquo;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;storageResourcePool&lt;/td&gt;
&lt;td&gt;Must mention one of the SRPs on  the PowerMax array that the symmetrixID specifies. This value is used to create the default storage class.&lt;/td&gt;
&lt;td&gt;Yes&lt;/td&gt;
&lt;td&gt;&amp;ldquo;SRP_1&amp;rdquo;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;serviceLevel&lt;/td&gt;
&lt;td&gt;This parameter must mention one of the Service Levels on the PowerMax array. This value is used to create the default storage class.&lt;/td&gt;
&lt;td&gt;Yes&lt;/td&gt;
&lt;td&gt;&amp;ldquo;Bronze&amp;rdquo;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;skipCertificateValidation&lt;/td&gt;
&lt;td&gt;Skip client-side TLS verification of Unisphere certificates&lt;/td&gt;
&lt;td&gt;No&lt;/td&gt;
&lt;td&gt;&amp;ldquo;True&amp;rdquo;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;transportProtocol&lt;/td&gt;
&lt;td&gt;Set preferred transport protocol for the Kubernetes cluster which helps the driver choose between FC and iSCSI when a node has both FC and iSCSI connectivity to a PowerMax array.&lt;/td&gt;
&lt;td&gt;No&lt;/td&gt;
&lt;td&gt;Empty&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;nodeNameTemplate&lt;/td&gt;
&lt;td&gt;Used to specify a template which will be used by the driver to create Host/IG names on the PowerMax array. To use the default naming convention, then leave this value empty.&lt;/td&gt;
&lt;td&gt;No&lt;/td&gt;
&lt;td&gt;Empty&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;csireverseproxy&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;This section refers to configuration options for CSI PowerMax Reverse Proxy&lt;/td&gt;
&lt;td&gt;-&lt;/td&gt;
&lt;td&gt;-&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;enabled&lt;/td&gt;
&lt;td&gt;Boolean parameter which indicates if CSI PowerMax Reverse Proxy is going to be configured and installed.&lt;br&gt;&lt;strong&gt;NOTE:&lt;/strong&gt; If not enabled, then there is no requirement to configure any of the following values.&lt;/td&gt;
&lt;td&gt;No&lt;/td&gt;
&lt;td&gt;&amp;ldquo;False&amp;rdquo;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;port&lt;/td&gt;
&lt;td&gt;Specify the port number that is used by the NodePort service created by the CSI PowerMax Reverse Proxy installation&lt;/td&gt;
&lt;td&gt;No&lt;/td&gt;
&lt;td&gt;2222&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;primary&lt;/td&gt;
&lt;td&gt;Mandatory section for Reverse Proxy&lt;/td&gt;
&lt;td&gt;-&lt;/td&gt;
&lt;td&gt;-&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;unisphere&lt;/td&gt;
&lt;td&gt;This must specify the URL of the Unisphere for PowerMax server&lt;/td&gt;
&lt;td&gt;Yes, if using Reverse Proxy&lt;/td&gt;
&lt;td&gt;&amp;ldquo;https://0.0.0.0:8443&amp;rdquo;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;skipCertificateValidation&lt;/td&gt;
&lt;td&gt;This parameter should be set to false if you want to do client-side TLS verification of Unisphere for PowerMax SSL certificates. It is set to true by default.&lt;/td&gt;
&lt;td&gt;No&lt;/td&gt;
&lt;td&gt;&amp;ldquo;True&amp;rdquo;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;certSecret&lt;/td&gt;
&lt;td&gt;The name of the secret in the same namespace containing the CA certificates of the Unisphere server&lt;/td&gt;
&lt;td&gt;Yes, if skipCertificateValidation is set to false&lt;/td&gt;
&lt;td&gt;Empty&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;backup&lt;/td&gt;
&lt;td&gt;Optional section for Reverse Proxy. Specify Unisphere server address which the Reverse Proxy can fall back to if the primary Unisphere is unreachable or unresponsive.&lt;br&gt;&lt;strong&gt;NOTE:&lt;/strong&gt; If you do not want to specify a backup Unisphere server, then remove the backup section from the file&lt;/td&gt;
&lt;td&gt;-&lt;/td&gt;
&lt;td&gt;-&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;unisphere&lt;/td&gt;
&lt;td&gt;Specify the IP address of the Unisphere for PowerMax server which manages the arrays being used by the CSI driver&lt;/td&gt;
&lt;td&gt;No&lt;/td&gt;
&lt;td&gt;&amp;ldquo;https://0.0.0.0:8443&amp;rdquo;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;skipCertificateValidation&lt;/td&gt;
&lt;td&gt;This parameter should be set to false if you want to do client side TLS verification of Unisphere for PowerMax SSL certificates. It is set to true by default.&lt;/td&gt;
&lt;td&gt;No&lt;/td&gt;
&lt;td&gt;&amp;ldquo;True&amp;rdquo;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;certSecret&lt;/td&gt;
&lt;td&gt;The name of the secret in the same namespace containing the CA certificates of the Unisphere server&lt;/td&gt;
&lt;td&gt;No&lt;/td&gt;
&lt;td&gt;Empty&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;ol start=&#34;8&#34;&gt;
&lt;li&gt;Install the driver using &lt;code&gt;csi-install.sh&lt;/code&gt; bash script by running &lt;code&gt;cd ../dell-csi-helm-installer &amp;amp;&amp;amp; ./csi-install.sh --namespace powermax --values ../helm/my-powermax-settings.yaml&lt;/code&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;em&gt;Note:&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;For detailed instructions on how to run the install scripts, refer to the readme document in the dell-csi-helm-installer folder.&lt;/li&gt;
&lt;li&gt;This script also runs the verify.sh script in the same directory. You will be prompted to enter the credentials for each of the Kubernetes nodes. The &lt;code&gt;verify.sh&lt;/code&gt; script needs the credentials to check if the iSCSI initiators have been configured on all nodes. You can also skip the verification step by specifying the &lt;code&gt;--skip-verify-node&lt;/code&gt; option&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;storage-classes&#34;&gt;Storage Classes&lt;/h2&gt;
&lt;p&gt;As part of the driver installation, a set of storage classes is created along with the driver pods. This is done to demonstrate how storage classes need to be created to consume storage from Dell EMC storage arrays.&lt;/p&gt;
&lt;p&gt;The &lt;code&gt;StorageClass&lt;/code&gt; object in Kubernetes is immutable and can&amp;rsquo;t be modified once created. It creates challenges when we need to change or update a parameter, for example when a version of the driver introduces new configurable parameters for the storage classes. To avoid issues during upgrades, future releases of the drivers will have the installation separated from the creation of Storage Classes.
In preparation for that, starting in Q4 of 2020, an annotation &lt;code&gt;&amp;quot;helm.sh/resource-policy&amp;quot;: keep&lt;/code&gt; is applied to the storage classes created by the &lt;code&gt;dell-csi-helm-installer&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Because of this annotation, these storage classes are not going to be deleted even after the driver has been uninstalled.
This annotation has been applied to give you an opportunity to keep using  these storage classes even with a future release of the driver. In case you wish to not use these storage classes, you will need to delete them by using the &lt;code&gt;kubectl delete storageclass&lt;/code&gt; command.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;NOTE&lt;/em&gt;: If you uninstall the driver and reinstall it, you can still face errors if any update in the &lt;code&gt;values.yaml&lt;/code&gt; file leads to an update of the storage class(es):&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;    Error: cannot patch &amp;quot;&amp;lt;sc-name&amp;gt;&amp;quot; with kind StorageClass: StorageClass.storage.k8s.io &amp;quot;&amp;lt;sc-name&amp;gt;&amp;quot; is invalid: parameters: Forbidden: updates to parameters are forbidden
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;In case you want to make such updates, make sure to delete the existing storage classes using the &lt;code&gt;kubectl delete storageclass&lt;/code&gt; command.&lt;br&gt;
Deleting a storage class has no impact on a running Pod with mounted PVCs. You won&amp;rsquo;t be able to provision new PVCs until at least one storage class is newly created.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>V2: PowerScale</title>
      <link>/v2/installation/helm/isilon/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/v2/installation/helm/isilon/</guid>
      <description>
        
        
        &lt;p&gt;The CSI Driver for Dell EMC PowerScale can be deployed by using the provided Helm v3 charts and installation scripts on both Kubernetes and OpenShift platforms. For more detailed information on the installation scripts, review the script &lt;a href=&#34;https://github.com/dell/csi-unity/tree/master/dell-csi-helm-installer&#34;&gt;documentation&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The controller section of the Helm chart installs the following components in a &lt;em&gt;Deployment&lt;/em&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;CSI Driver for PowerScale&lt;/li&gt;
&lt;li&gt;Kubernetes External Provisioner, which provisions the volumes&lt;/li&gt;
&lt;li&gt;Kubernetes External Attacher, which attaches the volumes to the containers&lt;/li&gt;
&lt;li&gt;Kubernetes External Snapshotter, which provides snapshot support&lt;/li&gt;
&lt;li&gt;Kubernetes External Resizer, which resizes the volume&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The node section of the Helm chart installs the following component in a Daemon Set:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;CSI Driver for PowerScale&lt;/li&gt;
&lt;li&gt;Kubernetes Node Registrar, which handles the driver registration&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;prerequisites&#34;&gt;Prerequisites&lt;/h3&gt;
&lt;p&gt;Before you install CSI Driver for PowerScale, verify the requirements that are mentioned in this topic are installed and configured.&lt;/p&gt;
&lt;h4 id=&#34;requirements&#34;&gt;Requirements&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Install Kubernetes.&lt;/li&gt;
&lt;li&gt;Configure Docker service&lt;/li&gt;
&lt;li&gt;Install Helm v3&lt;/li&gt;
&lt;li&gt;Install volume snapshot components&lt;/li&gt;
&lt;li&gt;Deploy PowerScale driver using Helm&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; There is no feature gate that needs to be set explicitly for CSI drivers version 1.17 and later. All the required feature gates are either beta/GA.&lt;/p&gt;
&lt;h2 id=&#34;configure-docker-service&#34;&gt;Configure Docker service&lt;/h2&gt;
&lt;p&gt;The mount propagation in Docker must be configured on all Kubernetes nodes before installing CSI Driver for PowerScale.&lt;/p&gt;
&lt;h3 id=&#34;procedure&#34;&gt;Procedure&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;Edit the service section of &lt;em&gt;/etc/systemd/system/multi-user.target.wants/docker.service&lt;/em&gt; file as follows:
&lt;pre&gt;&lt;code&gt;[Service]
...
MountFlags=shared
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;li&gt;Restart the Docker service with systemctl daemon-reload and
&lt;pre&gt;&lt;code&gt;systemctl daemon-reload
systemctl restart docker
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;install-volume-snapshot-components&#34;&gt;Install volume snapshot components&lt;/h2&gt;
&lt;h3 id=&#34;install-snapshot-beta-crds&#34;&gt;Install Snapshot Beta CRDs&lt;/h3&gt;
&lt;p&gt;To install snapshot CRDs specify &lt;code&gt;--snapshot-crd&lt;/code&gt; flag to driver installation script &lt;code&gt;dell-csi-helm-installer/csi-install.sh&lt;/code&gt; during driver installation.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://kubernetes.io/blog/2019/12/09/kubernetes-1-17-feature-cis-volume-snapshot-beta/#how-do-i-deploy-support-for-volume-snapshots-on-my-kubernetes-cluster&#34;&gt;Install Common Snapshot Controller&lt;/a&gt;, if not already installed for the cluster.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The manifests available on GitHub install v3.0.3 of the snapshotter image - &lt;a href=&#34;https://quay.io/repository/k8scsi/csi-snapshotter?tag=v3.0.3&amp;amp;tab=tags&#34;&gt;quay.io/k8scsi/csi-snapshotter:v3.0.3&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Dell recommends using v3.0.3 image of the snapshot-controller - &lt;a href=&#34;https://quay.io/repository/k8scsi/snapshot-controller?tag=v3.0.3&amp;amp;tab=tags&#34;&gt;quay.io/k8scsi/snapshot-controller:v3.0.3&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;install-csi-driver-for-powerscale&#34;&gt;Install CSI Driver for PowerScale&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Before you begin&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;You must clone the source code from &lt;a href=&#34;https://github.com/dell/csi-isilon&#34;&gt;git repository&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;In the &lt;code&gt;dell-csi-helm-installer&lt;/code&gt; directory, there should be two shell scripts, &lt;em&gt;csi-install.sh&lt;/em&gt; and &lt;em&gt;csi-uninstall.sh&lt;/em&gt;. These scripts handle some of the pre and post operations that cannot be performed in the helm chart.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Steps&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Collect information from the PowerScale Systems like IP address, username and password. Make a note of the value for these parameters as they must be entered in the &lt;em&gt;secret.yaml&lt;/em&gt; and values file.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Copy the helm/csi-isilon/values.yaml into a new location with name say &lt;em&gt;my-isilon-settings.yaml&lt;/em&gt;, to customize settings for installation.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Edit &lt;em&gt;my-isilon-settings.yaml&lt;/em&gt; to set the following parameters for your installation:
The following table lists the primary configurable parameters of the PowerScale driver Helm chart and their default values. More detailed information can be
found in the  &lt;a href=&#34;https://github.com/dell/csi-powerscale/blob/master/helm/csi-isilon/values.yaml&#34;&gt;&lt;code&gt;values.yaml&lt;/code&gt;&lt;/a&gt; file in this repository.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Parameter&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;th&gt;Required&lt;/th&gt;
&lt;th&gt;Default&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;isiIP&lt;/td&gt;
&lt;td&gt;&amp;ldquo;isiIP&amp;rdquo; defines the HTTPs endpoint of the PowerScale OneFS API server&lt;/td&gt;
&lt;td&gt;true&lt;/td&gt;
&lt;td&gt;-&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;isiPort&lt;/td&gt;
&lt;td&gt;&amp;ldquo;isiPort&amp;rdquo; defines the HTTPs port number of the PowerScale OneFS API server&lt;/td&gt;
&lt;td&gt;false&lt;/td&gt;
&lt;td&gt;8080&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;isiInsecure&lt;/td&gt;
&lt;td&gt;&amp;ldquo;isiInsecure&amp;rdquo; specifies whether the PowerScale OneFS API server&amp;rsquo;s certificate chain and host name should be verified.&lt;/td&gt;
&lt;td&gt;false&lt;/td&gt;
&lt;td&gt;true&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;isiAccessZone&lt;/td&gt;
&lt;td&gt;The name of the access zone a volume can be created in&lt;/td&gt;
&lt;td&gt;false&lt;/td&gt;
&lt;td&gt;System&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;volumeNamePrefix&lt;/td&gt;
&lt;td&gt;&amp;ldquo;volumeNamePrefix&amp;rdquo; defines a string prepended to each volume created by the CSI driver.&lt;/td&gt;
&lt;td&gt;false&lt;/td&gt;
&lt;td&gt;k8s&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;controllerCount&lt;/td&gt;
&lt;td&gt;&amp;ldquo;controllerCount&amp;rdquo; defines the number of CSI PowerScale controller nodes to deploy to the Kubernetes release.&lt;/td&gt;
&lt;td&gt;true&lt;/td&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;enableDebug&lt;/td&gt;
&lt;td&gt;Indicates whether debug level logs should be logged&lt;/td&gt;
&lt;td&gt;false&lt;/td&gt;
&lt;td&gt;true&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;verbose&lt;/td&gt;
&lt;td&gt;Indicates what content of the OneFS REST API message should be logged in debug level logs&lt;/td&gt;
&lt;td&gt;false&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;enableQuota&lt;/td&gt;
&lt;td&gt;Indicates whether the provisioner should attempt to set (later unset) quota on a newly provisioned volume. This requires SmartQuotas to be  enabled.&lt;/td&gt;
&lt;td&gt;false&lt;/td&gt;
&lt;td&gt;true&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;noProbeOnStart&lt;/td&gt;
&lt;td&gt;Indicates whether the controller/node should probe during initialization&lt;/td&gt;
&lt;td&gt;false&lt;/td&gt;
&lt;td&gt;false&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;isiPath&lt;/td&gt;
&lt;td&gt;The default base path for the volumes to be created, this will be used if a storage class does not have the IsiPath parameter specified&lt;/td&gt;
&lt;td&gt;false&lt;/td&gt;
&lt;td&gt;/ifs/data/csi&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;autoProbe&lt;/td&gt;
&lt;td&gt;Enable auto probe.&lt;/td&gt;
&lt;td&gt;false&lt;/td&gt;
&lt;td&gt;true&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;nfsV3&lt;/td&gt;
&lt;td&gt;Specify whether to set the version to v3 when mounting an NFS export. If the value is &amp;ldquo;false&amp;rdquo;, then the default version supported will be used (that is, the mount command will not explicitly specify &amp;ldquo;-o vers=3&amp;rdquo; option). This flag has now been deprecated and will be removed in a future release. Use the StorageClass.mountOptions if you want to specify &amp;lsquo;vers=3&amp;rsquo; as a mount option.&lt;/td&gt;
&lt;td&gt;false&lt;/td&gt;
&lt;td&gt;false&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;enableCustomTopology&lt;/td&gt;
&lt;td&gt;Indicates PowerScale FQDN/IP which will be fetched from node label and the same will be used by controller and node pod to establish connection to Array. This requires enableCustomTopology to be enabled.&lt;/td&gt;
&lt;td&gt;false&lt;/td&gt;
&lt;td&gt;false&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;em&gt;&lt;strong&gt;Storage Class parameters&lt;/strong&gt;&lt;/em&gt;&lt;/td&gt;
&lt;td&gt;Following parameters are related to Storage Class&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;name&lt;/td&gt;
&lt;td&gt;&amp;ldquo;storageClass.name&amp;rdquo; defines the name of the storage class to be defined.&lt;/td&gt;
&lt;td&gt;false&lt;/td&gt;
&lt;td&gt;isilon&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;isDefault&lt;/td&gt;
&lt;td&gt;&amp;ldquo;storageClass.isDefault&amp;rdquo; defines whether the primary storage class should be the default.&lt;/td&gt;
&lt;td&gt;false&lt;/td&gt;
&lt;td&gt;true&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;reclaimPolicy&lt;/td&gt;
&lt;td&gt;&amp;ldquo;storageClass.reclaimPolicy&amp;rdquo; defines what will happen when a volume is removed from the Kubernetes API. Valid values are &amp;ldquo;Retain&amp;rdquo; and &amp;ldquo;Delete&amp;rdquo;.&lt;/td&gt;
&lt;td&gt;false&lt;/td&gt;
&lt;td&gt;Delete&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;accessZone&lt;/td&gt;
&lt;td&gt;The Access Zone where the Volume would be created&lt;/td&gt;
&lt;td&gt;false&lt;/td&gt;
&lt;td&gt;System&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;AzServiceIP&lt;/td&gt;
&lt;td&gt;Access Zone service IP if different from isiIP, specify here and refer in storageClass&lt;/td&gt;
&lt;td&gt;false&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;rootClientEnabled&lt;/td&gt;
&lt;td&gt;When a PVC is being created, it takes the storage class&amp;rsquo; value of &amp;ldquo;storageclass.rootClientEnabled&amp;rdquo;&lt;/td&gt;
&lt;td&gt;false&lt;/td&gt;
&lt;td&gt;false&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;em&gt;&lt;strong&gt;Controller parameters&lt;/strong&gt;&lt;/em&gt;&lt;/td&gt;
&lt;td&gt;Set nodeSelector and tolerations for controller&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;nodeSelector&lt;/td&gt;
&lt;td&gt;Define nodeSelector for the controllers, if required&lt;/td&gt;
&lt;td&gt;false&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;tolerations&lt;/td&gt;
&lt;td&gt;Define tolerations for the controllers, if required&lt;/td&gt;
&lt;td&gt;false&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; User should provide all boolean values with double quotes. This is applicable only for &lt;em&gt;my-isilon-settings.yaml&lt;/em&gt;. Example: &amp;ldquo;true&amp;rdquo;/&amp;ldquo;false&amp;rdquo;&lt;br&gt;
&lt;strong&gt;Note:&lt;/strong&gt; controllerCount parameter value should not exceed number of nodes in the kubernetes cluster. Otherwise some of the controller pods will be in &amp;ldquo;Pending&amp;rdquo; state till new nodes are available for scheduling. The installer will exit with a WARNING on the same.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Create namespace
Run &lt;code&gt;kubectl create namespace isilon&lt;/code&gt; to create the &lt;em&gt;isilon&lt;/em&gt; namespace. Specify the same namespace name while installing the driver.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; CSI PowerScale also supports installation of driver in custom namespace.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Create a secret file for the OneFS credentials by editing the secret.yaml present under helm directory. Replace the values for the username and password parameters.
Use the following command to convert username/password to base64 encoded string:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;echo -n &#39;admin&#39; | base64
echo -n &#39;password&#39; | base64 
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Run &lt;code&gt;kubectl create -f secret.yaml&lt;/code&gt; to create the secret.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; The username specified in secret.yaml must be from the authentication providers of PowerScale. The user must have enough privileges to perform the actions. The suggested privileges are as follows:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;ISI_PRIV_LOGIN_PAPI
ISI_PRIV_NFS
ISI_PRIV_QUOTA
ISI_PRIV_SNAPSHOT
ISI_PRIV_IFS_RESTORE
ISI_PRIV_NS_IFS_ACCESS
ISI_PRIV_LOGIN_SSH
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Install OneFS CA certificates by following the instructions from next section, if you want to validate OneFS API server&amp;rsquo;s certificates. If not, create an empty secret using the following command and empty secret should be created for the successful CSI Driver for Dell EMC Powerscale installation.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;kubectl create -f emptysecret.yaml
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Install the driver using &lt;code&gt;csi-install.sh&lt;/code&gt; bash script by running &lt;code&gt;cd ../dell-csi-helm-installer &amp;amp;&amp;amp; ./csi-install.sh --namespace isilon --values ../helm/myvalues.yaml&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;certificate-validation-for-onefs-rest-api-calls&#34;&gt;Certificate validation for OneFS REST API calls&lt;/h2&gt;
&lt;p&gt;The CSI driver exposes an install parameter &amp;lsquo;isiInsecure&amp;rsquo; which determines if the driver
performs client-side verification of the OneFS certificates. The &amp;lsquo;isiInsecure&amp;rsquo; parameter is set to true by default and the driver does not verify the OneFS certificates.&lt;/p&gt;
&lt;p&gt;If the &amp;lsquo;isiInsecure&amp;rsquo; is set to false, then the secret isilon-certs must contain the CA certificate for OneFS.
If this secret is an empty secret, then the validation of the certificate fails, and the driver fails to start.&lt;/p&gt;
&lt;p&gt;If the &amp;lsquo;isiInsecure&amp;rsquo; parameter is set to false and a previous installation attempt to create the empty secret, then this secret must be deleted and re-created using the CA certs. If the OneFS certificate is self-signed, then perform the following steps:&lt;/p&gt;
&lt;h3 id=&#34;procedure-1&#34;&gt;Procedure&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;To fetch the certificate, run &lt;code&gt;openssl s_client -showcerts -connect [OneFS IP] &amp;lt;/dev/null 2&amp;gt;/dev/null | openssl x509 -outform PEM &amp;gt; ca_cert.pem&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;To create the secret, run &lt;code&gt;kubectl create secret generic isilon-certs --from-file=ca_cert.pem -n isilon&lt;/code&gt;
Â &lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;storage-classes&#34;&gt;Storage Classes&lt;/h2&gt;
&lt;p&gt;As part of the driver installation, a set of storage classes is created along with the driver pods. This is done to demonstrate how storage classes need to be created to consume storage from Dell EMC storage arrays.&lt;/p&gt;
&lt;p&gt;The &lt;code&gt;StorageClass&lt;/code&gt; object in Kubernetes is immutable and can&amp;rsquo;t be modified once created. It creates challenges when we need to change or update a parameter, for example when a version of the driver introduces new configurable parameters for the storage classes. To avoid issues during upgrades, future releases of the drivers will have the installation separated from the creation of Storage Classes.
In preparation for that, starting in Q4 of 2020, an annotation &lt;code&gt;&amp;quot;helm.sh/resource-policy&amp;quot;: keep&lt;/code&gt; is applied to the storage classes created by the &lt;code&gt;dell-csi-helm-installer&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Because of this annotation, these storage classes are not going to be deleted even after the driver has been uninstalled.
This annotation has been applied to give you an opportunity to keep using  these storage classes even with a future release of the driver. In case you wish to not use these storage classes, you will need to delete them by using the &lt;code&gt;kubectl delete storageclass&lt;/code&gt; command.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;NOTE&lt;/em&gt;: If you uninstall the driver and reinstall it, you can still face errors if any update in the &lt;code&gt;values.yaml&lt;/code&gt; file leads to an update of the storage class(es):&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;    Error: cannot patch &amp;quot;&amp;lt;sc-name&amp;gt;&amp;quot; with kind StorageClass: StorageClass.storage.k8s.io &amp;quot;&amp;lt;sc-name&amp;gt;&amp;quot; is invalid: parameters: Forbidden: updates to parameters are forbidden
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;In case you want to make such updates, make sure to delete the existing storage classes using the &lt;code&gt;kubectl delete storageclass&lt;/code&gt; command.&lt;br&gt;
Deleting a storage class has no impact on a running Pod with mounted PVCs. You won&amp;rsquo;t be able to provision new PVCs until at least one storage class is newly created.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>V2: PowerStore</title>
      <link>/v2/installation/helm/powerstore/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/v2/installation/helm/powerstore/</guid>
      <description>
        
        
        &lt;p&gt;The CSI Driver for Dell EMC PowerStore can be deployed by using the provided Helm v3 charts and installation scripts on both Kubernetes and OpenShift platforms. For more detailed information on the installation scripts, please review the script &lt;a href=&#34;https://github.com/dell/csi-unity/tree/master/dell-csi-helm-installer&#34;&gt;documentation&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The controller section of the Helm chart installs the following components in a &lt;em&gt;Deployment&lt;/em&gt; in the namespace &lt;code&gt;csi-powerstore&lt;/code&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;CSI Driver for Dell EMC PowerStore&lt;/li&gt;
&lt;li&gt;Kubernetes External Provisioner, which provisions the volumes&lt;/li&gt;
&lt;li&gt;Kubernetes External Attacher, which attaches the volumes to the containers&lt;/li&gt;
&lt;li&gt;Kubernetes External Snapshotter, which provides snapshot support&lt;/li&gt;
&lt;li&gt;Kubernetes External Resizer, which resizes the volume&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The node section of the Helm chart installs the following component in a &lt;em&gt;DaemonSet&lt;/em&gt; in the namespace &lt;code&gt;csi-powerstore&lt;/code&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;CSI Driver for Dell EMC PowerStore&lt;/li&gt;
&lt;li&gt;Kubernetes Node Registrar, which handles the driver registration&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;prerequisites&#34;&gt;Prerequisites&lt;/h2&gt;
&lt;p&gt;The following are requirements to be met before installing the CSI Driver for Dell EMC PowerStore:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Install Kubernetes (1.17, 1.18, 1.19) or OpenShift (4.5 or 4.6)&lt;/li&gt;
&lt;li&gt;Install Helm 3&lt;/li&gt;
&lt;li&gt;If you plan to use either the Fibre Channel or iSCSI protocol, refer to either &lt;em&gt;Fibre Channel requirements&lt;/em&gt; or &lt;em&gt;Set up the iSCSI Initiator&lt;/em&gt; sections below. You can use NFS volumes without FC or iSCSI configuration.&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;You can use either the Fibre Channel or iSCSI protocol, but you do not need both.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;If you want to use preconfigured iSCSI/FC hosts be sure to check that they are not part of any host group&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;Linux native multipathing requirements&lt;/li&gt;
&lt;li&gt;Configure Mount propagation on container runtime (i.e. Docker)&lt;/li&gt;
&lt;li&gt;Volume Snapshot requirements&lt;/li&gt;
&lt;li&gt;The nonsecure registries are defined in Docker or other container runtime, for CSI drivers that are hosted in a nonsecure location.&lt;/li&gt;
&lt;li&gt;You can access your cluster with kubectl and helm.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;install-helm-30&#34;&gt;Install Helm 3.0&lt;/h3&gt;
&lt;p&gt;Install Helm 3.0 on the master node before you install the CSI Driver for Dell EMC PowerFlex.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Steps&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Run the &lt;code&gt;curl https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3 | bash&lt;/code&gt; command to install Helm 3.0.&lt;/p&gt;
&lt;h3 id=&#34;fibre-channel-requirements&#34;&gt;Fibre Channel requirements&lt;/h3&gt;
&lt;p&gt;Dell EMC PowerStore supports Fibre Channel communication. If you will use the Fibre Channel protocol, ensure that the
following requirement is met before you install the CSI Driver for Dell EMC PowerStore:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Zoning of the Host Bus Adapters (HBAs) to the Fibre Channel port director must be done.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;set-up-the-iscsi-initiator&#34;&gt;Set up the iSCSI Initiator&lt;/h3&gt;
&lt;p&gt;The CSI Driver for Dell EMC PowerStore v1.2 supports iSCSI connectivity.&lt;/p&gt;
&lt;p&gt;If you will use the iSCSI protocol, set up the iSCSI initiators as follows:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Make sure that the iSCSI initiators are available on both Controller and Worker nodes.&lt;/li&gt;
&lt;li&gt;Kubernetes nodes should have access (network connectivity) to an iSCSI director on the Dell EMC PowerStore array that
has IP interfaces. Manually create IP routes for each node that connects to the Dell EMC PowerStore.&lt;/li&gt;
&lt;li&gt;All Kubernetes nodes must have the &lt;em&gt;iscsi-initiator-utils&lt;/em&gt; package for CentOS/RHEL or &lt;em&gt;open-iscsi&lt;/em&gt; package for Ubuntu installed, and the &lt;em&gt;iscsid&lt;/em&gt; service must be enabled and running.
To do this, run the &lt;code&gt;systemctl enable --now iscsid&lt;/code&gt; command.&lt;/li&gt;
&lt;li&gt;Make sure that the unique initiator name is set in &lt;em&gt;/etc/iscsi/initiatorname.iscsi&lt;/em&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For information about configuring iSCSI, see &lt;em&gt;Dell EMC PowerStore documentation&lt;/em&gt; on Dell EMC Support.&lt;/p&gt;
&lt;h3 id=&#34;linux-multipathing-requirements&#34;&gt;Linux multipathing requirements&lt;/h3&gt;
&lt;p&gt;Dell EMC PowerStore supports Linux multipathing. Configure Linux multipathing before installing the CSI Driver for Dell EMC
PowerStore.&lt;/p&gt;
&lt;p&gt;Set up Linux multipathing as follows:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Ensure that all nodes have the &lt;em&gt;Device Mapper Multipathing&lt;/em&gt; package installed.&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;You can install it by running yum install device-mapper-multipath on CentOS or apt install multipath-tools on Ubuntu. This package should create a multipath configuration file located in &lt;code&gt;/etc/multipath.conf&lt;/code&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;Enable multipathing using the &lt;code&gt;mpathconf --enable --with_multipathd y&lt;/code&gt; command.&lt;/li&gt;
&lt;li&gt;Enable &lt;code&gt;user_friendly_names&lt;/code&gt; and &lt;code&gt;find_multipaths&lt;/code&gt; in the &lt;code&gt;multipath.conf&lt;/code&gt; file.&lt;/li&gt;
&lt;li&gt;Ensure that the multipath command for &lt;code&gt;multipath.conf&lt;/code&gt; is available on all Kubernetes nodes.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;configure-mount-propagation-on-container-runtime&#34;&gt;Configure Mount Propagation on Container Runtime&lt;/h3&gt;
&lt;p&gt;It is required to configure mount propagation on your container runtime on all Kubernetes nodes before installing the CSI Driver for Dell EMC PowerStore.  The following is instruction on how to do this with Docker.  If you use another container runtime please follow the recommended instructions from the vendor to configure mount propagation.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Steps&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Edit the service section of &lt;code&gt;/etc/systemd/system/multi-user.target.wants/docker.service&lt;/code&gt; file to add the following lines:
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;docker.service
&lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;[&lt;/span&gt;Service&lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;]&lt;/span&gt;...
&lt;span style=&#34;color:#000&#34;&gt;MountFlags&lt;/span&gt;&lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;=&lt;/span&gt;shared
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;Restart the docker service with &lt;code&gt;systemctl daemon-reload&lt;/code&gt; and &lt;code&gt;systemctl restart docker&lt;/code&gt; on all the nodes.&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;volume-snapshot-requirements&#34;&gt;Volume Snapshot requirements&lt;/h3&gt;
&lt;h4 id=&#34;volume-snapshot-crds&#34;&gt;Volume Snapshot CRD&amp;rsquo;s&lt;/h4&gt;
&lt;p&gt;The Kubernetes Volume Snapshot CRDs can be obtained and installed from the external-snapshotter project on &lt;a href=&#34;https://github.com/kubernetes-csi/external-snapshotter/tree/v3.0.3/client/config/crd&#34;&gt;Github&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Alternately, you can install the CRDs by supplying the option &lt;em&gt;--snapshot-crd&lt;/em&gt; while installing the driver using the &lt;code&gt;csi-install.sh&lt;/code&gt; script.&lt;/p&gt;
&lt;h4 id=&#34;volume-snapshot-controller&#34;&gt;Volume Snapshot Controller&lt;/h4&gt;
&lt;p&gt;Starting with beta Volume Snapshots in Kubernetes 1.17, the CSI external-snapshotter sidecar is split into two controllers:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;A common snapshot controller&lt;/li&gt;
&lt;li&gt;A CSI external-snapshotter sidecar&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The common snapshot controller must be installed only once in the cluster irrespective the number of CSI drivers installed in the cluster. On OpenShift clusters 4.4 onwards, the common snapshot-controller is pre-installed. In the clusters where it is not present, it can be installed using &lt;code&gt;kubectl&lt;/code&gt; and the manifests available on &lt;a href=&#34;https://github.com/kubernetes-csi/external-snapshotter/tree/v3.0.3/deploy/kubernetes/snapshot-controller&#34;&gt;GitHub&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;NOTE:&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The manifests available on GitHub install v3.0.3 of the snapshotter image - &lt;a href=&#34;https://quay.io/repository/k8scsi/csi-snapshotter?tag=v3.0.3&amp;amp;tab=tags&#34;&gt;quay.io/k8scsi/csi-snapshotter:v3.0.3&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Dell EMC recommends using v3.0.3 image of the snapshot-controller - &lt;a href=&#34;https://quay.io/repository/k8scsi/snapshot-controller?tag=v3.0.3&amp;amp;tab=tags&#34;&gt;quay.io/k8scsi/snapshot-controller:v3.0.3&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;The CSI external-snapshotter sidecar is still installed along with the driver and does not involve any extra configuration.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;install-the-driver&#34;&gt;Install the Driver&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Steps&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Run &lt;code&gt;git clone https://github.com/dell/csi-powerstore.git&lt;/code&gt; to clone the git repository&lt;/li&gt;
&lt;li&gt;Ensure that you&amp;rsquo;ve created namespace where you want to install the driver. You can run &lt;code&gt;kubectl create namespace csi-powerstore&lt;/code&gt; to create a new one&lt;/li&gt;
&lt;li&gt;Check helm/csi-powerstore/driver-image.yaml and confirm the driver image points to new image.&lt;/li&gt;
&lt;li&gt;Edit the &lt;code&gt;helm/secret.yaml&lt;/code&gt;, point to correct namespace and replace the values for the username and password parameters.
These values can be obtained using base64 encoding as described in the following example:
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;color:#204a87&#34;&gt;echo&lt;/span&gt; -n &lt;span style=&#34;color:#4e9a06&#34;&gt;&amp;#34;myusername&amp;#34;&lt;/span&gt; &lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;|&lt;/span&gt; base64
&lt;span style=&#34;color:#204a87&#34;&gt;echo&lt;/span&gt; -n &lt;span style=&#34;color:#4e9a06&#34;&gt;&amp;#34;mypassword&amp;#34;&lt;/span&gt; &lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;|&lt;/span&gt; base64
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;where &lt;em&gt;myusername&lt;/em&gt; &amp;amp; &lt;em&gt;mypassword&lt;/em&gt; are credentials that would be used for accessing PowerStore API.
&lt;em&gt;NOTE:&lt;/em&gt; If you want to use iSCSI CHAP you need fill &lt;code&gt;chapsecret&lt;/code&gt; and &lt;code&gt;chapuser&lt;/code&gt; fields in similar manner&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;Create the secret by running &lt;code&gt;kubectl create -f helm/secret.yaml&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Copy the default values.yaml file &lt;code&gt;cd dell-csi-helm-installer &amp;amp;&amp;amp; cp ../helm/csi-powerstore/values.yaml ./my-powerstore-settings.yaml&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Edit the newly created values file and provide values for the following parameters &lt;code&gt;vi my-powerstore-settings.yaml&lt;/code&gt;:&lt;/li&gt;
&lt;/ol&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Parameter&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;th&gt;Required&lt;/th&gt;
&lt;th&gt;Default&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;powerStoreApi&lt;/td&gt;
&lt;td&gt;Defines the full URL path to the PowerStore API&lt;/td&gt;
&lt;td&gt;Yes&lt;/td&gt;
&lt;td&gt;&amp;quot; &amp;quot;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;volumeNamePrefix&lt;/td&gt;
&lt;td&gt;Defines the string added to each volume that the CSI driver creates&lt;/td&gt;
&lt;td&gt;No&lt;/td&gt;
&lt;td&gt;&amp;ldquo;csi&amp;rdquo;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;nodeNamePrefix&lt;/td&gt;
&lt;td&gt;Defines the string added to each node that the CSI driver registers&lt;/td&gt;
&lt;td&gt;No&lt;/td&gt;
&lt;td&gt;&amp;ldquo;csi-node&amp;rdquo;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;nodeIDPath&lt;/td&gt;
&lt;td&gt;Defines a path to file with a unique identifier identifying the node in the Kubernetes cluster&lt;/td&gt;
&lt;td&gt;No&lt;/td&gt;
&lt;td&gt;&amp;ldquo;/etc/machine-id&amp;rdquo;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;connection.scsiProtocol&lt;/td&gt;
&lt;td&gt;Defines which transport protocol to use (FC, ISCSI, None, or auto). &lt;br /&gt;- By default, the driver scans available SCSI adapters and tries to register them with the storage array under the SCSI hostname using &lt;code&gt;nodeNamePrefix&lt;/code&gt; and the ID read from the file pointed to by &lt;code&gt;nodeIDPath&lt;/code&gt;. If an adapter is already registered with the storage under a different hostname, the adapter is not used by the driver. &lt;br /&gt;- A hostname the driver uses for registration of adapters is in the form &lt;code&gt;&amp;lt;nodeNamePrefix&amp;gt;-&amp;lt;nodeID&amp;gt;-&amp;lt;nodeIP&amp;gt;&lt;/code&gt;. By default, these are csi-node and the machine ID read from the file &lt;code&gt;/etc/machine-id&lt;/code&gt;. &lt;br /&gt;- To customize the hostname, for example if you want to make them more user friendly, adjust nodeIDPath and nodeNamePrefix accordingly. &lt;br /&gt;- For example, you can set &lt;code&gt;nodeNamePrefix&lt;/code&gt; to &lt;code&gt;k8s&lt;/code&gt; and &lt;code&gt;nodeIDPath&lt;/code&gt; to &lt;code&gt;/etc/hostname&lt;/code&gt; to produce names such as &lt;code&gt;k8s-worker1-192.168.1.2&lt;/code&gt;.&lt;/td&gt;
&lt;td&gt;Yes&lt;/td&gt;
&lt;td&gt;&amp;ldquo;auto&amp;rdquo;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;connection.nfs.enable&lt;/td&gt;
&lt;td&gt;Enables or disables NFS support&lt;/td&gt;
&lt;td&gt;No&lt;/td&gt;
&lt;td&gt;FALSE&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;connection.nfs.nasServerName&lt;/td&gt;
&lt;td&gt;Points to the NAS server that would be used - If you have nfs.enabled set to true, it will try to use &lt;em&gt;nfs.nasServerName&lt;/em&gt;. This will fail if you do not provide &lt;em&gt;nfs.nasServerName&lt;/em&gt;.&lt;/td&gt;
&lt;td&gt;No&lt;/td&gt;
&lt;td&gt;&amp;ldquo;nas-server&amp;rdquo;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;connection.nfs.version&lt;/td&gt;
&lt;td&gt;Defines version of NFS protocol&lt;/td&gt;
&lt;td&gt;No&lt;/td&gt;
&lt;td&gt;&amp;ldquo;v3&amp;rdquo;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;connection.enableCHAP&lt;/td&gt;
&lt;td&gt;Defines whether the driver should use CHAP for iSCSI connections or not&lt;/td&gt;
&lt;td&gt;No&lt;/td&gt;
&lt;td&gt;FALSE&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;controller.nodeSelector&lt;/td&gt;
&lt;td&gt;Defines what nodes would be selected for pods of controller deployment&lt;/td&gt;
&lt;td&gt;Yes&lt;/td&gt;
&lt;td&gt;&amp;quot; &amp;quot;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;controller.tolerations&lt;/td&gt;
&lt;td&gt;Defines tolerations that would be applied to controller deployment&lt;/td&gt;
&lt;td&gt;Yes&lt;/td&gt;
&lt;td&gt;&amp;quot; &amp;quot;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;controller.replicas&lt;/td&gt;
&lt;td&gt;Defines number of replicas of controller deployment&lt;/td&gt;
&lt;td&gt;Yes&lt;/td&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;node.nodeSelector&lt;/td&gt;
&lt;td&gt;Defines what nodes would be selected for pods of node daemonset&lt;/td&gt;
&lt;td&gt;Yes&lt;/td&gt;
&lt;td&gt;&amp;quot; &amp;quot;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;node.tolerations&lt;/td&gt;
&lt;td&gt;Defines tolerations that would be applied to node daemonset&lt;/td&gt;
&lt;td&gt;Yes&lt;/td&gt;
&lt;td&gt;&amp;quot; &amp;quot;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;ol start=&#34;8&#34;&gt;
&lt;li&gt;Install the driver using &lt;code&gt;csi-install.sh&lt;/code&gt; bash script by running &lt;code&gt;./csi-install.sh --namespace csi-powerstore --values ./my-powerstore-settings.yaml&lt;/code&gt;
&lt;ul&gt;
&lt;li&gt;After that the driver should be installed, you can check condition of driver pods by running &lt;code&gt;kubectl get all -n csi-powerstore&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;em&gt;NOTE:&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;For detailed instructions on how to run the install scripts, refer to the readme document in the dell-csi-helm-installer folder.&lt;/li&gt;
&lt;li&gt;(Optional) Enable additional Mount Options - A user is able to specify additional mount options as needed for the driver.
&lt;ul&gt;
&lt;li&gt;Mount options are specified in storageclass yaml under &lt;em&gt;mountOptions&lt;/em&gt;.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;WARNING&lt;/em&gt;: Before utilizing mount options, you must first be fully aware of the potential impact and understand your environment&amp;rsquo;s requirements for the specified option.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;storage-classes&#34;&gt;Storage Classes&lt;/h2&gt;
&lt;p&gt;As part of the driver installation, a set of storage classes is created along with the driver pods. This is done to demonstrate how storage classes need to be created to consume storage from Dell EMC storage arrays.&lt;/p&gt;
&lt;p&gt;The &lt;code&gt;StorageClass&lt;/code&gt; object in Kubernetes is immutable and can&amp;rsquo;t be modified once created. It creates challenges when we need to change or update a parameter, for example when a version of the driver introduces new configurable parameters for the storage classes. To avoid issues during upgrades, future releases of the drivers will have the installation separated from the creation of Storage Classes.
In preparation for that, starting in Q4 of 2020, an annotation &lt;code&gt;&amp;quot;helm.sh/resource-policy&amp;quot;: keep&lt;/code&gt; is applied to the storage classes created by the &lt;code&gt;dell-csi-helm-installer&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Because of this annotation, these storage classes are not going to be deleted even after the driver has been uninstalled.
This annotation has been applied to give you an opportunity to keep using  these storage classes even with a future release of the driver. In case you wish to not use these storage classes, you will need to delete them by using the &lt;code&gt;kubectl delete storageclass&lt;/code&gt; command.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;NOTE&lt;/em&gt;: If you uninstall the driver and reinstall it, you can still face errors if any update in the &lt;code&gt;values.yaml&lt;/code&gt; file leads to an update of the storage class(es):&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;    Error: cannot patch &amp;quot;&amp;lt;sc-name&amp;gt;&amp;quot; with kind StorageClass: StorageClass.storage.k8s.io &amp;quot;&amp;lt;sc-name&amp;gt;&amp;quot; is invalid: parameters: Forbidden: updates to parameters are forbidden
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;In case you want to make such updates, make sure to delete the existing storage classes using the &lt;code&gt;kubectl delete storageclass&lt;/code&gt; command.&lt;br&gt;
Deleting a storage class has no impact on a running Pod with mounted PVCs. You won&amp;rsquo;t be able to provision new PVCs until at least one storage class is newly created.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>V2: Unity</title>
      <link>/v2/installation/helm/unity/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/v2/installation/helm/unity/</guid>
      <description>
        
        
        &lt;p&gt;The CSI Driver for Dell EMC Unity can be deployed by using the provided Helm v3 charts and installation scripts on both Kubernetes and OpenShift platforms. For more detailed information on the installation scripts, review the script &lt;a href=&#34;https://github.com/dell/csi-unity/tree/master/dell-csi-helm-installer&#34;&gt;documentation&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The controller section of the Helm chart installs the following components in a &lt;em&gt;Deployment&lt;/em&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;CSI Driver for Unity&lt;/li&gt;
&lt;li&gt;Kubernetes External Provisioner, which provisions the volumes&lt;/li&gt;
&lt;li&gt;Kubernetes External Attacher, which attaches the volumes to the containers&lt;/li&gt;
&lt;li&gt;Kubernetes External Snapshotter, which provides snapshot support&lt;/li&gt;
&lt;li&gt;Kubernetes External Resizer, which resizes the volume&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The node section of the Helm chart installs the following component in a &lt;em&gt;DaemonSet&lt;/em&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;CSI Driver for Unity&lt;/li&gt;
&lt;li&gt;Kubernetes Node Registrar, which handles the driver registration&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;prerequisites&#34;&gt;Prerequisites&lt;/h2&gt;
&lt;p&gt;Before you install CSI Driver for Unity, verify the requirements that are mentioned in this topic are installed and configured.&lt;/p&gt;
&lt;h3 id=&#34;requirements&#34;&gt;Requirements&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Install Kubernetes&lt;/li&gt;
&lt;li&gt;Configure Docker service&lt;/li&gt;
&lt;li&gt;Install Helm v3&lt;/li&gt;
&lt;li&gt;To use FC protocol, host must be zoned with Unity array&lt;/li&gt;
&lt;li&gt;To use iSCSI and NFS protocol, iSCSI initiator and NFS utility packages need to be installed&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;configure-docker-service&#34;&gt;Configure Docker service&lt;/h2&gt;
&lt;p&gt;The mount propagation in Docker must be configured on all Kubernetes nodes before installing CSI Driver for Unity.&lt;/p&gt;
&lt;h3 id=&#34;procedure&#34;&gt;Procedure&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Edit the service section of &lt;em&gt;/etc/systemd/system/multi-user.target.wants/docker.service&lt;/em&gt; file as follows:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;[&lt;/span&gt;Service&lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;]&lt;/span&gt;
...
&lt;span style=&#34;color:#000&#34;&gt;MountFlags&lt;/span&gt;&lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;=&lt;/span&gt;shared
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Restart the Docker service with systemctl daemon-reload and&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;systemctl daemon-reload
systemctl restart docker
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;install-csi-driver&#34;&gt;Install CSI Driver&lt;/h2&gt;
&lt;p&gt;Install CSI Driver for Unity using this procedure.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Before you begin&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;You must have the downloaded files, including the Helm chart from the source &lt;a href=&#34;https://github.com/dell/csi-unity&#34;&gt;git repository&lt;/a&gt;, ready for this procedure.&lt;/li&gt;
&lt;li&gt;In the top-level dell-csi-helm-installer directory, there should be two scripts, &lt;em&gt;csi-install.sh&lt;/em&gt; and &lt;em&gt;csi-uninstall.sh&lt;/em&gt;. These scripts handle some of the pre and post operations that cannot be performed in the helm chart, such as creating Custom Resource Definitions (CRDs), if needed.&lt;/li&gt;
&lt;li&gt;Make sure &amp;ldquo;unity&amp;rdquo; namespace exists in kubernetes cluster. Use &lt;code&gt;kubectl create namespace unity&lt;/code&gt; command to create the namespace, if the namespace is not present.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Procedure&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Collect information from the Unity Systems like Unique ArrayId, IP address, username  and password. Make a note of the value for these parameters as they must be entered in the secret.json and myvalues.yaml file.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Copy the csi-unity/values.yaml into a file named myvalues.yaml in the same directory of csi-install.sh, to customize settings for installation.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Edit myvalues.yaml to set the following parameters for your installation:&lt;/p&gt;
&lt;p&gt;The following table lists the primary configurable parameters of the Unity driver chart and their default values. More detailed information can be found in the &lt;a href=&#34;helm/csi-unity/values.yaml&#34;&gt;&lt;code&gt;values.yaml&lt;/code&gt;&lt;/a&gt; file in this repository.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Parameter&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;th&gt;Required&lt;/th&gt;
&lt;th&gt;Default&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;certSecretCount&lt;/td&gt;
&lt;td&gt;Represents number of certificate secrets, which user is going to create for ssl authentication. (unity-cert-0..unity-cert-n). Minimum value should be 1&lt;/td&gt;
&lt;td&gt;false&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;syncNodeInfoInterval&lt;/td&gt;
&lt;td&gt;Time interval to add node info to array. Default 15 minutes. Minimum value should be 1 minute&lt;/td&gt;
&lt;td&gt;false&lt;/td&gt;
&lt;td&gt;15&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;controllerCount&lt;/td&gt;
&lt;td&gt;Controller replication count to maintain high availability&lt;/td&gt;
&lt;td&gt;yes&lt;/td&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;volumeNamePrefix&lt;/td&gt;
&lt;td&gt;String to prepend to any volumes created by the driver&lt;/td&gt;
&lt;td&gt;false&lt;/td&gt;
&lt;td&gt;csivol&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;snapNamePrefix&lt;/td&gt;
&lt;td&gt;String to prepend to any snapshot created by the driver&lt;/td&gt;
&lt;td&gt;false&lt;/td&gt;
&lt;td&gt;csi-snap&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;csiDebug&lt;/td&gt;
&lt;td&gt;To set the debug log policy for CSI driver&lt;/td&gt;
&lt;td&gt;false&lt;/td&gt;
&lt;td&gt;&amp;ldquo;false&amp;rdquo;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;imagePullPolicy&lt;/td&gt;
&lt;td&gt;The default pull policy is IfNotPresent which causes the Kubelet to skip pulling an image if it already exists.&lt;/td&gt;
&lt;td&gt;false&lt;/td&gt;
&lt;td&gt;IfNotPresent&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;createStorageClassesWithTopology&lt;/td&gt;
&lt;td&gt;Flag to enable or disable topology.&lt;/td&gt;
&lt;td&gt;true&lt;/td&gt;
&lt;td&gt;false&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;em&gt;&lt;strong&gt;Storage Array List&lt;/strong&gt;&lt;/em&gt;&lt;/td&gt;
&lt;td&gt;Following parameters is a list of parameters to provide multiple storage arrays&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;storageArrayList[i].name&lt;/td&gt;
&lt;td&gt;Name of the storage class to be defined. A suffix of ArrayId and protocol will be added to the name. No suffix will be added to default array.&lt;/td&gt;
&lt;td&gt;false&lt;/td&gt;
&lt;td&gt;unity&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;storageArrayList[i].isDefaultArray&lt;/td&gt;
&lt;td&gt;To handle the existing volumes created in csi-unity v1.0, 1.1 and 1.1.0.1. The user needs to provide &amp;ldquo;isDefaultArray&amp;rdquo;: true in secret.json. This entry should be present only for one array and that array will be marked default for existing volumes.&lt;/td&gt;
&lt;td&gt;false&lt;/td&gt;
&lt;td&gt;&amp;ldquo;false&amp;rdquo;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;em&gt;&lt;strong&gt;Storage Class parameters&lt;/strong&gt;&lt;/em&gt;&lt;/td&gt;
&lt;td&gt;Following parameters are not present in values.yaml&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;storageArrayList[i].storageClass.storagePool&lt;/td&gt;
&lt;td&gt;Unity Storage Pool CLI ID to use with in the Kubernetes storage class&lt;/td&gt;
&lt;td&gt;true&lt;/td&gt;
&lt;td&gt;-&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;storageArrayList[i].storageClass.thinProvisioned&lt;/td&gt;
&lt;td&gt;To set volume thinProvisioned&lt;/td&gt;
&lt;td&gt;false&lt;/td&gt;
&lt;td&gt;&amp;ldquo;true&amp;rdquo;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;storageArrayList[i].storageClass.isDataReductionEnabled&lt;/td&gt;
&lt;td&gt;To set volume data reduction&lt;/td&gt;
&lt;td&gt;false&lt;/td&gt;
&lt;td&gt;&amp;ldquo;false&amp;rdquo;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;storageArrayList[i].storageClass.volumeTieringPolicy&lt;/td&gt;
&lt;td&gt;To set volume tiering policy&lt;/td&gt;
&lt;td&gt;false&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;storageArrayList[i].storageClass.FsType&lt;/td&gt;
&lt;td&gt;Block volume related parameter. To set File system type. Possible values are ext3,ext4,xfs. Supported for FC/iSCSI protocol only.&lt;/td&gt;
&lt;td&gt;false&lt;/td&gt;
&lt;td&gt;ext4&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;storageArrayList[i].storageClass.hostIOLimitName&lt;/td&gt;
&lt;td&gt;Block volume related parameter.  To set unity host IO limit. Supported for FC/iSCSI protocol only.&lt;/td&gt;
&lt;td&gt;false&lt;/td&gt;
&lt;td&gt;&amp;quot;&amp;rdquo;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;storageArrayList[i].storageClass.nasServer&lt;/td&gt;
&lt;td&gt;NFS related parameter. NAS Server CLI ID for filesystem creation.&lt;/td&gt;
&lt;td&gt;true&lt;/td&gt;
&lt;td&gt;&amp;quot;&amp;rdquo;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;storageArrayList[i].storageClass.hostIoSize&lt;/td&gt;
&lt;td&gt;NFS related parameter. To set filesystem host IO Size.&lt;/td&gt;
&lt;td&gt;false&lt;/td&gt;
&lt;td&gt;&amp;ldquo;8192&amp;rdquo;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;storageArrayList[i].storageClass.reclaimPolicy&lt;/td&gt;
&lt;td&gt;What should happen when a volume is removed&lt;/td&gt;
&lt;td&gt;false&lt;/td&gt;
&lt;td&gt;Delete&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;em&gt;&lt;strong&gt;Snapshot Class parameters&lt;/strong&gt;&lt;/em&gt;&lt;/td&gt;
&lt;td&gt;Following parameters are not present in values.yaml&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;storageArrayList[i] .snapshotClass.retentionDuration&lt;/td&gt;
&lt;td&gt;TO set snapshot retention duration. Format:&amp;ldquo;1:23:52:50&amp;rdquo; (number of days:hours:minutes:sec)&lt;/td&gt;
&lt;td&gt;false&lt;/td&gt;
&lt;td&gt;&amp;quot;&amp;rdquo;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: User should provide all boolean values with double quotes. This applicable only for myvalues.yaml. Example: &amp;ldquo;true&amp;rdquo;/&amp;ldquo;false&amp;rdquo;.&lt;/p&gt;
&lt;p&gt;Example &lt;em&gt;myvalues.yaml&lt;/em&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;csiDebug: &amp;quot;true&amp;quot;
volumeNamePrefix : csivol
snapNamePrefix: csi-snap
imagePullPolicy: Always
certSecretCount: 1
syncNodeInfoInterval: 5
controllerCount: 2
createStorageClassesWithTopology: true
storageClassProtocols:
   - protocol: &amp;quot;FC&amp;quot;
   - protocol: &amp;quot;iSCSI&amp;quot;
   - protocol: &amp;quot;NFS&amp;quot;
storageArrayList:
   - name: &amp;quot;APM00******1&amp;quot;
     isDefaultArray: &amp;quot;true&amp;quot;
     storageClass:
       storagePool: pool_1
       FsType: ext4
       nasServer: &amp;quot;nas_1&amp;quot;
       thinProvisioned: &amp;quot;true&amp;quot;
       isDataReductionEnabled: true
       hostIOLimitName: &amp;quot;value_from_array&amp;quot;
       tieringPolicy: &amp;quot;2&amp;quot;
     snapshotClass:
       retentionDuration: &amp;quot;2:2:23:45&amp;quot;
   - name: &amp;quot;APM001******2&amp;quot;
     storageClass:
       storagePool: pool_1
       reclaimPolicy: Delete
       hostIoSize: &amp;quot;8192&amp;quot;
       nasServer: &amp;quot;nasserver_2&amp;quot;
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Create an empty secret by navigating to helm folder that contains emptysecret.yaml file and running the kubectl create -f emptysecret.yaml command.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Prepare the secret.json for driver configuration.
The following table lists driver configuration parameters for multiple storage arrays.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Parameter&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;th&gt;Required&lt;/th&gt;
&lt;th&gt;Default&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;username&lt;/td&gt;
&lt;td&gt;Username for accessing unity system&lt;/td&gt;
&lt;td&gt;true&lt;/td&gt;
&lt;td&gt;-&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;password&lt;/td&gt;
&lt;td&gt;Password for accessing unity system&lt;/td&gt;
&lt;td&gt;true&lt;/td&gt;
&lt;td&gt;-&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;restGateway&lt;/td&gt;
&lt;td&gt;REST API gateway HTTPS endpoint Unity system&lt;/td&gt;
&lt;td&gt;true&lt;/td&gt;
&lt;td&gt;-&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;arrayId&lt;/td&gt;
&lt;td&gt;ArrayID for unity system&lt;/td&gt;
&lt;td&gt;true&lt;/td&gt;
&lt;td&gt;-&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;insecure&lt;/td&gt;
&lt;td&gt;&amp;ldquo;unityInsecure&amp;rdquo; determines if the driver is going to validate unisphere certs while connecting to the Unisphere REST API interface If it is set to false, then a secret unity-certs has to be created with a X.509 certificate of CA which signed the Unisphere certificate&lt;/td&gt;
&lt;td&gt;true&lt;/td&gt;
&lt;td&gt;true&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;isDefaultArray&lt;/td&gt;
&lt;td&gt;An array having isDefaultArray=true is for backward compatibility. This parameter should occur once in the list.&lt;/td&gt;
&lt;td&gt;false&lt;/td&gt;
&lt;td&gt;false&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Example: secret.json&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-json5&#34; data-lang=&#34;json5&#34;&gt;   {
     &amp;quot;storageArrayList&amp;quot;: [
       {
         &amp;quot;username&amp;quot;: &amp;quot;user&amp;quot;,
         &amp;quot;password&amp;quot;: &amp;quot;password&amp;quot;,
         &amp;quot;restGateway&amp;quot;: &amp;quot;https://10.1.1.1&amp;quot;,
         &amp;quot;arrayId&amp;quot;: &amp;quot;APM00******1&amp;quot;,
         &amp;quot;insecure&amp;quot;: true,
         &amp;quot;isDefaultArray&amp;quot;: true
       },
       {
         &amp;quot;username&amp;quot;: &amp;quot;user&amp;quot;,
         &amp;quot;password&amp;quot;: &amp;quot;password&amp;quot;,
         &amp;quot;restGateway&amp;quot;: &amp;quot;https://10.1.1.2&amp;quot;,
         &amp;quot;arrayId&amp;quot;: &amp;quot;APM00******2&amp;quot;,
         &amp;quot;insecure&amp;quot;: true
       }
     ]
   }
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;code&gt;kubectl create secret generic unity-creds -n unity --from-file=config=secret.json&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Use the following command to replace or update the secret&lt;/p&gt;
&lt;p&gt;&lt;code&gt;kubectl create secret generic unity-creds -n unity --from-file=config=secret.json -o yaml --dry-run | kubectl replace -f -&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: The user needs to validate the JSON syntax and array related key/values while replacing the unity-creds secret.
The driver will continue to use previous values in case of an error found in the JSON file.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: &amp;ldquo;isDefaultArray&amp;rdquo; parameter in values.yaml and secret.json should match each other.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Setup for snapshots&lt;/p&gt;
&lt;p&gt;The Kubernetes Volume Snapshot feature is now beta in Kubernetes v1.17.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;The following section summarizes the changes in the &lt;strong&gt;&lt;a href=&#34;https://kubernetes.io/blog/2019/12/09/kubernetes-1-17-feature-cis-volume-snapshot-beta/&#34;&gt;beta&lt;/a&gt;&lt;/strong&gt; release.&lt;/p&gt;
&lt;p&gt;To use the Kubernetes Volume Snapshot feature, ensure the following components have been deployed on your Kubernetes cluster.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://kubernetes.io/blog/2019/12/09/kubernetes-1-17-feature-cis-volume-snapshot-beta/#how-do-i-deploy-support-for-volume-snapshots-on-my-kubernetes-cluster&#34;&gt;Install Snapshot Beta CRDs using the following command&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;kubectl apply -f https://raw.githubusercontent.com/kubernetes-csi/external-snapshotter/v3.0.3/client/config/crd/snapshot.storage.k8s.io_volumesnapshotclasses.yaml
kubectl apply -f https://raw.githubusercontent.com/kubernetes-csi/external-snapshotter/v3.0.3config/crd/snapshot.storage.k8s.io_volumesnapshotcontents.yaml
kubectl apply -f https://raw.githubusercontent.com/kubernetes-csi/external-snapshotter/v3.0.3/config/crd/snapshot.storage.k8s.io_volumesnapshots.yaml
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://kubernetes.io/blog/2019/12/09/kubernetes-1-17-feature-cis-volume-snapshot-beta/#how-do-i-deploy-support-for-volume-snapshots-on-my-kubernetes-cluster&#34;&gt;Volume snapshot controller&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The manifests available on GitHub install v3.0.3 of the snapshotter image - &lt;a href=&#34;https://quay.io/repository/k8scsi/csi-snapshotter?tag=v3.0.3&amp;amp;tab=tags&#34;&gt;quay.io/k8scsi/csi-snapshotter:v3.0.3&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Dell recommends using v3.0.3 image of the snapshot-controller - &lt;a href=&#34;https://quay.io/repository/k8scsi/snapshot-controller?tag=v3.0.3&amp;amp;tab=tags&#34;&gt;quay.io/k8scsi/snapshot-controller:v3.0.3&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;After executing these commands, a snapshot-controller pod should be up and running.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Run the &lt;code&gt;./csi-install.sh --namespace unity --values ./myvalues.yaml&lt;/code&gt; command to proceed with the installation.&lt;/p&gt;
&lt;p&gt;A successful installation should emit messages that look similar to the following samples:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;------------------------------------------------------
&amp;gt; Installing CSI Driver: csi-unity on 1.19
------------------------------------------------------
------------------------------------------------------
&amp;gt; Checking to see if CSI Driver is already installed
------------------------------------------------------
------------------------------------------------------
&amp;gt; Verifying Kubernetes and driver configuration
------------------------------------------------------
|- Kubernetes Version: 1.18
|
|- Driver: csi-unity
|
|- Verifying Kubernetes versions
  |
  |--&amp;gt; Verifying minimum Kubernetes version                         Success
  |
  |--&amp;gt; Verifying maximum Kubernetes version                         Success
|
|- Verifying that required namespaces have been created             Success
|
|- Verifying that required secrets have been created                Success
|
|- Verifying that required secrets have been created                Success
|
|- Verifying snapshot support
  |
  |--&amp;gt; Verifying that beta snapshot CRDs are available              Success
  |
  |--&amp;gt; Verifying that beta snapshot controller is available         Success
|
|- Verifying helm version                                           Success

------------------------------------------------------
&amp;gt; Verification Complete
------------------------------------------------------
|
|- Installing Driver                                                Success
  |
  |--&amp;gt; Waiting for statefulset unity-controller to be ready         Success
  |
  |--&amp;gt; Waiting for daemonset unity-node to be ready                 Success
------------------------------------------------------
&amp;gt; Operation complete
------------------------------------------------------
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Results:
At the end of the script statefulset unity-controller and daemonset unity-node is ready, execute command &lt;strong&gt;kubectl get pods -n unity&lt;/strong&gt; to get the status of the pods and you will see the following:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;unity-controller-xxxx with 5/5 containers ready, and status displayed as Running.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Agent pods with 2/2 containers and the status displayed as Running.&lt;/p&gt;
&lt;p&gt;Finally, the script creates storageclasses such as, &amp;ldquo;unity&amp;rdquo;. Additional storage classes can be created for different combinations of file system types and Unity storage pools. The script also creates volumesnapshotclass &amp;ldquo;unity-snapclass&amp;rdquo;.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;certificate-validation-for-unisphere-rest-api-calls&#34;&gt;Certificate validation for Unisphere REST API calls&lt;/h2&gt;
&lt;p&gt;This topic provides details about setting up the certificate validation for the CSI Driver for Dell EMC Unity.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Before you begin&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;As part of the CSI driver installation, the CSI driver requires a secret with the name unity-certs-0 to unity-certs-n based on &amp;ldquo;.Values.certSecretCount&amp;rdquo; parameter present in the namespace unity.&lt;/p&gt;
&lt;p&gt;This secret contains the X509 certificates of the CA which signed the Unisphere SSL certificate in PEM format.&lt;/p&gt;
&lt;p&gt;If the install script does not find the secret, it creates one empty secret with the name unity-certs-0.&lt;/p&gt;
&lt;p&gt;The CSI driver exposes an install parameter in secret.json, which is like storageArrayList[i].insecure, which determines if the driver performs client-side verification of the Unisphere certificates.&lt;/p&gt;
&lt;p&gt;The storageArrayList[i].insecure parameter set to true by default, and the driver does not verify the Unisphere certificates.&lt;/p&gt;
&lt;p&gt;If the storageArrayList[i].insecure set to false, then the secret unity-certs-n must contain the CA certificate for Unisphere.&lt;/p&gt;
&lt;p&gt;If this secret is empty secret, then the validation of the certificate fails, and the driver fails to start.&lt;/p&gt;
&lt;p&gt;If the storageArrayList[i].insecure parameter set to false and a previous installation attempt created the empty secret, then this secret must be deleted and re-created using the CA certs.&lt;/p&gt;
&lt;p&gt;If the Unisphere certificate is self-signed or if you are using an embedded Unisphere, then perform the following steps.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;To fetch the certificate, run the following command.
&lt;code&gt;openssl s_client -showcerts -connect &amp;lt;Unisphere IP:Port&amp;gt; &amp;lt;/dev/null 2&amp;gt;/dev/null | openssl x509 -outform PEM &amp;gt; ca_cert_0.pem&lt;/code&gt;
Example: openssl s_client -showcerts -connect 1.1.1.1:443 &amp;lt;/dev/null 2&amp;gt;/dev/null | openssl x509 -outform PEM &amp;gt; ca_cert_0.pem&lt;/li&gt;
&lt;li&gt;Run the following command to create the cert secret with index &amp;lsquo;0&amp;rsquo;
&lt;code&gt;kubectl create secret generic unity-certs-0 --from-file=cert-0=ca_cert_0.pem -n unity&lt;/code&gt;
Use the following command to replace the secret
&lt;code&gt;kubectl create secret generic unity-certs-0 -n unity --from-file=cert-0=ca_cert_0.pem -o yaml --dry-run | kubectl replace -f -&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Repeat step 1 and 2 to create multiple cert secrets with incremental index (example: unity-certs-1, unity-certs-2, etc)&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: &amp;ldquo;unity&amp;rdquo; is the namespace for helm based installation but namespace can be user defined in operator based installation.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: User can add multiple certificates in the same secret. The certificate file should not exceed more than 1Mb due to kubernetes secret size limitation.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: Whenever certSecretCount parameter changes in myvalues.yaml user needs to uninstall and install the driver.&lt;/p&gt;

      </description>
    </item>
    
  </channel>
</rss>
